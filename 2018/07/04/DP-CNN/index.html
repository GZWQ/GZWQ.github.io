<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning," />










<meta name="description" content="Q 比如S2 -&amp;gt; C3，是不是C3的每一个感受野要感受S2的6张map？  事实是：每一个卷积核要感受前一层的所有feature_maps   卷积神经网络(Convolutional Neural Network, CNN)是深度学习技术中极具代表的网络结构之一, CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。 图">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="DP-CNN">
<meta property="og:url" content="http://yoursite.com/2018/07/04/DP-CNN/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Q 比如S2 -&amp;gt; C3，是不是C3的每一个感受野要感受S2的6张map？  事实是：每一个卷积核要感受前一层的所有feature_maps   卷积神经网络(Convolutional Neural Network, CNN)是深度学习技术中极具代表的网络结构之一, CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。 图">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-08%20at%209.29.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-03-02%20at%205.14.02%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-03-02%20at%205.25.55%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-03-02%20at%205.28.39%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430134321810.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%2011.02.43%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430143339630.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430144413942.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430145326135.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430145733209.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430150003326.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430161936244.gif">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%2012.34.21%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%2012.19.02%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430145733209-1243208.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430160824104.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430170435760.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%2012.35.21%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430172832308.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430204102339.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170430173715000.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%203.29.15%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-07-15%20at%2011.17.54%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-07-15%20at%2011.20.42%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-07-15%20at%2011.32.18%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-07-15%20at%2011.21.19%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-07-15%20at%2011.21.39%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen-Shot-2015-11-06-at-12.05.40-PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202019-07-16%20at%209.20.27%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%203.38.52%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-10%20at%203.41.42%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20170501131714709.png">
<meta property="og:image" content="http://yoursite.com/2018/07/04/DP-CNN/20160528001522947.jpg">
<meta property="og:updated_time" content="2019-07-18T01:37:22.848Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DP-CNN">
<meta name="twitter:description" content="Q 比如S2 -&amp;gt; C3，是不是C3的每一个感受野要感受S2的6张map？  事实是：每一个卷积核要感受前一层的所有feature_maps   卷积神经网络(Convolutional Neural Network, CNN)是深度学习技术中极具代表的网络结构之一, CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。 图">
<meta name="twitter:image" content="http://yoursite.com/2018/07/04/DP-CNN/Screen%20Shot%202018-07-08%20at%209.29.19%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/04/DP-CNN/"/>





  <title>DP-CNN | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/04/DP-CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DP-CNN</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-04T10:47:32-05:00">
                2018-07-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Q"><a href="#Q" class="headerlink" title="Q"></a>Q</h1><ul>
<li><p><strong>比如S2 -&gt; C3，是不是C3的每一个感受野要感受S2的6张map？</strong></p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-08 at 9.29.19 PM.png" alt="Screen Shot 2018-07-08 at 9.29.19 PM"></p>
<p>事实是：每一个卷积核要感受前一层的所有feature_maps</p>
</li>
</ul>
<p><strong><a href="http://www.jeyzhang.com/cnn-learning-notes-1.html" target="_blank" rel="noopener">卷积神经网络(Convolutional Neural Network, CNN)</a></strong>是深度学习技术中极具代表的网络结构之一, CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。</p>
<p>图像处理中，往往会将图像看成是一个或多个的二维向量，比如MNIST手写体图片就可以看做是一个28 × 28的二维向量（黑白图片，只有一个颜色通道；如果是RGB表示的彩色图片则有三个颜色通道，可表示为三张二维向量）。传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而CNN则通过<strong>局部连接</strong>、<strong>权值共享</strong>等方法避免这一困难。</p>
<ul>
<li><p><strong>Why using the patches of images as input instead of the whole images?</strong></p>
<p>I saw this phenomenon when I read papers about crowd counting with CNN, like <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Sam_Switching_Convolutional_Neural_CVPR_2017_paper.pdf" target="_blank" rel="noopener">S-CNN</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780439" target="_blank" rel="noopener">MCNN</a>. And according to the paper S-CNN, the complete image is divided in 9 non-overlapping patches so that crowd characteristics like density, appearance etc. can be assumed to be consistent in a given patch for a crowd scene.</p>
</li>
<li><p><strong>What is fine-tune of CNN model?</strong> fine tuning</p>
</li>
<li><p>如何理解“卷积运算，可以使原信号特征增强，并且降低噪音”？？</p>
</li>
<li><p>为什么神经网络可以提取特征？？</p>
</li>
<li><p>模型的鲁棒性？？？？</p>
</li>
</ul>
<h1 id="A"><a href="#A" class="headerlink" title="A"></a>A</h1><ol>
<li><p>In machine learning terms, this flashlight is called a <strong>filter</strong>(or sometimes referred to as a <strong>neuron</strong> or a <strong>kernel</strong>) and the region that it is shining over is called the <strong>receptive field</strong>.</p>
</li>
<li><p>A very important note is that the depth of this filter has to be the same as the depth of the input (this makes sure that the math works out), so the dimensions of this filter is 5 x 5 x 3.</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-03-02 at 5.14.02 PM.png" alt="creen Shot 2019-03-02 at 5.14.02 P"></p>
</li>
<li><p>For an input image with size $N \times N \times C$, after going through a filter $5 \times 5$, we have an output feature map with size $(N-5+1) \times (N-5+1) \times 1$.</p>
<p>output_size = (Height-Filter)/stride+1.</p>
</li>
<li><p>weight number</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-03-02 at 5.25.55 PM.png" alt="creen Shot 2019-03-02 at 5.25.55 P"></p>
</li>
<li><p>$1 \times 1$ filter size</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-03-02 at 5.28.39 PM.png" alt="creen Shot 2019-03-02 at 5.28.39 P"></p>
</li>
<li><p>​</p>
</li>
</ol>
<a id="more"></a>
<h1 id="CNN的引入"><a href="#CNN的引入" class="headerlink" title="CNN的引入"></a>CNN的引入</h1><p><a href="https://www.leiphone.com/news/201807/RQ4sBWYqLkGV4ZAW.html" target="_blank" rel="noopener">直观理解深度学习卷积部分</a> </p>
<p><a href="https://blog.csdn.net/cxmscb/article/details/71023576" target="_blank" rel="noopener">ThumbsUp</a></p>
<p>在人工的全连接神经网络中，每相邻两层之间的每个神经元之间都是有边相连的。当输入层的特征维度变得很高时，这时全连接网络需要训练的参数就会增大很多，计算速度就会变得很慢，例如一张黑白的 28×28 的手写数字图片，输入层的神经元就有784个，如下图所示：</p>
<p><img src="/2018/07/04/DP-CNN/20170430134321810.png" alt="20170430134321810"></p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 11.02.43 AM.png" alt="Screen Shot 2018-07-10 at 11.02.43 AM"></p>
<h1 id="CNN层次结构"><a href="#CNN层次结构" class="headerlink" title="CNN层次结构"></a>CNN层次结构</h1><h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><p>CNN的输入层的输入格式保留了图片本身的结构。</p>
<p>对于黑白的 28×2828×28 的图片，CNN的输入是一个 28×2828×28 的的二维神经元，如下图所示：</p>
<p><img src="/2018/07/04/DP-CNN/20170430143339630.png" alt="20170430143339630"></p>
<p>而对于RGB格式的$28\times 28$图片，CNN的输入则是一个$3\times 28\times 28$的三维神经元（RGB中的每一个颜色通道都有一个 $28\times 28$的矩阵）</p>
<p><img src="/2018/07/04/DP-CNN/20170430144413942.png" alt="20170430144413942"></p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>两个重要的概念：</p>
<ul>
<li>local receptive fields（局部视野野）</li>
<li>shared weights（权值共享）</li>
</ul>
<h3 id="局部感受野"><a href="#局部感受野" class="headerlink" title="局部感受野"></a>局部感受野</h3><p>相比传统的每个神经元与前一层的所有神经元全连接，CNN中每个神经元只与前一层的部分神经元连接，俗称局部感受。如下图，假设输入的是一个 $28\times 28$的的二维神经元，我们定义$5\times 5$的 一个 local receptive fields（感受视野），即 隐藏层的神经元与输入层的$5\times 5$个神经元相连，这个5*5的区域就称之为Local Receptive Fields，每一条直线对应一个权重$w$。</p>
<p><img src="/2018/07/04/DP-CNN/20170430145326135.png" alt="20170430145326135"></p>
<p>每个神经元只与$5\times5$大小范围的神经元连接，如果步长是1，则两个神经元之间的感受区域有重叠，所以下一层共需要神经元个数是$(28\times 28) \div(5\times5)=(28-(5-1))\times(28-(5-1))=24\times24$</p>
<p><img src="/2018/07/04/DP-CNN/20170430145733209.png" alt="20170430145733209"></p>
<p>移动的步长为1：从左到右扫描，每次移动 1 格，扫描完之后，再向下移动一格，再次从左到右扫描。</p>
<p><img src="/2018/07/04/DP-CNN/20170430150003326.png" alt="20170430150003326"></p>
<p><img src="/2018/07/04/DP-CNN/20170430161936244.gif" alt="20170430161936244"></p>
<p><strong>过滤器-卷积核-Filters-权重$w$-(神经元局部感受的模板)</strong></p>
<p>一个感受野带有一个卷积核，将感受野对输入的扫描间隔称为步长(Stride)，当步长比较大时(stride&gt;1)，为了扫描到边缘的一些特征，感受视野可能会“出界”，这时需要对<strong>边界扩充(pad)</strong>，边界扩充可以设为 00 或 其他值。步长 和 边界扩充值的大小由用户来定义。</p>
<blockquote>
<p>比如前一层是$5\times5$, 感受野大小是$2\times2$, 步长2，可以发现最右的边界是无法扫描，所以可以对前一层进行padding，扩充成$6\times6$。</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 12.34.21 PM.png" alt="Screen Shot 2018-07-10 at 12.34.21 PM"></p>
</blockquote>
<p>卷积核的权重矩阵的值，便是卷积神经网络的参数，为了有一个偏移项 $b$，卷积核可附带一个偏移项 bb ，它们的初值可以随机来生成，可通过训练进行变化。</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 12.19.02 PM.png" alt="Screen Shot 2018-07-10 at 12.19.02 PM"></p>
<p>我们将通过 一个带有<strong>卷积核</strong>的<strong>感受视野</strong> 扫描生成的下一层神经元矩阵 称为 一个<strong>feature map (特征映射图)</strong>，如下图的右边便是一个 feature map：</p>
<p><img src="/2018/07/04/DP-CNN/20170430145733209-1243208.png" alt="20170430145733209-1243208"></p>
<p>因此在同一个 <strong>feature map</strong> 上的神经元使用的卷积核是相同的，因此这些神经元 shared weights，共享卷积核中的权值和附带的偏移。一个 feature map 对应 一个卷积核，若我们使用 3 个不同的卷积核，可以输出3个feature map：（感受视野：5×5，布长stride：1）</p>
<p><img src="/2018/07/04/DP-CNN/20170430160824104.png" alt="20170430160824104"></p>
<p>因此在CNN的卷积层，我们需要训练的参数大大地减少到了 (5×5+1)×3=78个。</p>
<p>假设输入的是 $28\times28$的RGB图片，即输入的是一个 $3×28×28$的的二维神经元，这时卷积核的大小不只用长和宽来表示，还有深度，感受视野也对应的有了深度。如下图所示：</p>
<p><img src="/2018/07/04/DP-CNN/20170430170435760.png" alt="20170430170435760"></p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 12.35.21 PM.png" alt="Screen Shot 2018-07-10 at 12.35.21 PM"></p>
<blockquote>
<p>RGB图像在CNN中如何进行convolution? - 知乎 <a href="https://www.zhihu.com/question/46607672" target="_blank" rel="noopener">https://www.zhihu.com/question/46607672</a></p>
<p><a href="https://blog.csdn.net/u014114990/article/details/51125776" target="_blank" rel="noopener">https://blog.csdn.net/u014114990/article/details/51125776</a></p>
</blockquote>
<h2 id="池化层-下采样"><a href="#池化层-下采样" class="headerlink" title="池化层(下采样)"></a>池化层(下采样)</h2><p>当输入经过卷积层时，若感受视野比较小，布长stride比较小，得到的feature map （特征图）还是比较大，可以通过池化层来对每一个 feature map 进行降维操作，输出的深度还是不变的，依然为 feature map 的个数。</p>
<p>池化层也有一个“池化视野（filter）”来对feature map矩阵进行扫描，对“池化视野”中的矩阵值进行计算，一般有两种计算方式：</p>
<ul>
<li>Max pooling：取“池化视野”矩阵中的最大值</li>
<li>Average pooling：取“池化视野”矩阵中的平均值</li>
</ul>
<p>扫描的过程中同样地会涉及的扫描布长stride，扫描方式同卷积层一样，先从左到右扫描，结束则向下移动布长大小，再从左到右。如下图示例所示：</p>
<p><img src="/2018/07/04/DP-CNN/20170430172832308.png" alt="20170430172832308"></p>
<p>最后可将 3 个 $24\times24$ 的 feature map 下采样得到 3 个 $24\times24$ 的特征矩阵：</p>
<p><img src="/2018/07/04/DP-CNN/20170430204102339.png" alt="20170430204102339"></p>
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>全连接层主要对特征进行重新拟合，减少特征信息的丢失。用户指定全连接层的神经元个数，然后每个神经元都与前一层的所有神经元连接，计算。</p>
<h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><p>输出层主要准备做好最后目标结果的输出，用户指定神经元个数，如果是2分类问题，就设置两个神经元，每一个神经元输出某种分类的概率；十分类就10个神经元，属于全连接。</p>
<h2 id="归一化层-Batch-Normalization"><a href="#归一化层-Batch-Normalization" class="headerlink" title="归一化层(Batch Normalization)"></a>归一化层(Batch Normalization)</h2><p>实现了在神经网络层的中间进行预处理的操作，即在上一层的输入归一化处理后再进入网络的下一层，这样可有效地防止“梯度弥散”，加速网络训练。在卷积神经网络中进行批量归一化时，一般对 未进行ReLu激活的 feature map进行批量归一化，输出后再作为激励层的输入，可达到调整激励函数偏导的作用。</p>
<p><img src="/2018/07/04/DP-CNN/20170430173715000.png" alt="20170430173715000"></p>
<h2 id="近邻归一化-Local-Response-Normalization"><a href="#近邻归一化-Local-Response-Normalization" class="headerlink" title="近邻归一化(Local Response Normalization)"></a>近邻归一化(Local Response Normalization)</h2><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 3.29.15 PM.png" alt="Screen Shot 2018-07-10 at 3.29.15 PM"></p>
<h2 id="融合层"><a href="#融合层" class="headerlink" title="融合层"></a>融合层</h2><p>融合层可以对切分层进行融合，也可以对不同大小的卷积核学习到的特征进行融合。例如在GoogleLeNet 中，使用多种分辨率的卷积核对目标特征进行学习，通过 <strong>padding</strong> 使得每一个 <strong>feature map</strong> 的长宽都一致，之后再将多个 feature map 在深度上拼接在一起： </p>
<p>融合的方法有几种，一种是特征矩阵之间的拼接级联，另一种是在特征矩阵进行运算 (+,−,x,max,conv)(+,−,x,max,conv)。</p>
<p><a href="https://www.cnblogs.com/denny402/p/5071126.html" target="_blank" rel="noopener">parameters setting</a></p>
<p><a href="https://blog.csdn.net/u011276025/article/details/76050377" target="_blank" rel="noopener">图像归一化</a> </p>
<h1 id="CNN-in-NLP"><a href="#CNN-in-NLP" class="headerlink" title="CNN in NLP"></a>CNN in NLP</h1><p>Since CNNs, unlike RNNs, can output only fixed sized vectors, the natural fit for them seem to be in the classification tasks such as <strong>Sentiment Analysis, Spam Detection or Topic Categorization.</strong></p>
<p>In computer vision tasks, the filters used in CNNs slide over patches of an image whereas in NLP tasks, the filters slide over the sentence matrices, a few words at a time.</p>
<h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Instead of image pixels, the input to most NLP tasks are sentences or documents represented as a matrix. Each row of the matrix corresponds to one token, typically a word, but it could be a character. That is, each row is vector that represents a word. Typically, these vectors are <em>word embeddings</em> (low-dimensional representations) like <a href="https://code.google.com/p/word2vec/" target="_blank" rel="noopener">word2vec</a> or <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a>, but they could also be one-hot vectors that index the word into a vocabulary. For a 10 word sentence using a 100-dimensional embedding we would have a 10×100 matrix as our input. That’s our “image”.</p>
<h2 id="Fliter"><a href="#Fliter" class="headerlink" title="Fliter"></a>Fliter</h2><p>In vision, our filters slide over local patches of an image, but in NLP we typically use filters that slide over full rows of the matrix (words). Thus, the “width” of our filters is usually the same as the width of the input matrix. The height, or <em>region size</em>, may vary, but sliding windows over 2-5 words at a time is typical.</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.17.54 AM.png" alt="creen Shot 2019-07-15 at 11.17.54 A"></p>
<h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>We also have zero-padding in nlp, where all elements that would fall outside of the matrix are taken to be zero. By doing this you can apply the filter to every element of your input matrix, and get a larger or equally sized output. Adding zero-padding is also called <strong><em>wide convolution**</em></strong>,<strong> and not using (zero-)padding would be a *</strong>narrow convolution<em>*</em>.</p>
<h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><p>The last concept we need to understand are <em>channels</em>. Channels are different “views” of your input data. For example, in image recognition you typically have RGB (red, green, blue) channels. You can apply convolutions across channels, either with different or equal weights. In NLP you could imagine having various channels as well: You could have a separate channels for different word embeddings (<a href="https://code.google.com/p/word2vec/" target="_blank" rel="noopener">word2vec</a> and <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> for example), or you could have a channel for the same sentence represented in different languages, or phrased in different ways.</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.20.42 AM.png" alt="creen Shot 2019-07-15 at 11.20.42 A"></p>
<h2 id="Stride-Size"><a href="#Stride-Size" class="headerlink" title="Stride Size"></a>Stride Size</h2><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.32.18 AM.png" alt="creen Shot 2019-07-15 at 11.32.18 A"></p>
<h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>Why pooling? There are a couple of reasons. One property of pooling is that it provides a fixed size output matrix, which typically is required for classification. For example, if you have 1,000 filters and you apply max pooling to each, you will get a 1000-dimensional output, regardless of the size of your filters, or the size of your input. This allows you to use variable size sentences, and variable size filters, but always get the same output dimensions to feed into a classifier.</p>
<p>Pooling also reduces the output dimensionality but (hopefully) keeps the most salient information. You can think of each filter as detecting a specific feature, such as detecting if the sentence contains a negation like “not amazing” for example. If this phrase occurs somewhere in the sentence, the result of applying the filter to that region will yield a large value, but a small value in other regions. By performing the max operation you  are keeping information about whether or not the feature appeared in the sentence, but you are losing information about where exactly it appeared. But isn’t this information about locality really useful? Yes, it  is and it’s a bit similar to what a bag of n-grams model is doing. You are losing global information about locality (where in a sentence something happens), but you are keeping local information captured by your filters, like “not amazing” being very different from “amazing not”.</p>
<p>In imagine recognition, pooling also provides basic invariance to translating (shifting) and rotation. When you are pooling over a region, the output will stay approximately the same even if you shift/rotate the image by a few pixels, because the max operations will pick out the same value regardless.</p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.21.19 AM.png" alt="creen Shot 2019-07-15 at 11.21.19 A"></p>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.21.39 AM.png" alt="creen Shot 2019-07-15 at 11.21.39 A"></p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><a href="http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/" target="_blank" rel="noopener">link</a> </p>
<p><img src="/2018/07/04/DP-CNN/Screen-Shot-2015-11-06-at-12.05.40-PM.png" alt="creen-Shot-2015-11-06-at-12.05.40-P"></p>
<p>llustration of a Convolutional Neural Network (CNN) architecture for sentence classification. Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states. Source: Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification.</p>
<p><strong>Input</strong></p>
<p>The example is “I like this movie very much!”, there are 6 words here and the exclamation mark is treated like a word – some researchers do this differently and disregard the exclamation mark – in total there are 7 words in the sentence. The authors chose 5 to be the dimension of the word vectors. We let <em>s</em> denote the length of sentence and <em>d</em> denote the dimension of the word vector, hence we now have a sentence matrix of the shape <em>s</em> x <em>d</em>, or 7 x 5.</p>
<p><strong>Filters</strong></p>
<p>One of the desirable properties of CNN is that it preserves 2D spatial orientation in computer vision. Texts, like pictures, have an orientation. Instead of 2-dimensional, texts have a one-dimensional structure where words sequence matter. We also recall that all words in the example are each replaced by a 5-dimensional word vector, hence we fix one dimension of the filter to match the word vectors (5) and vary the region size, <em>h</em>. Region size refers to the number of rows – representing word – of the sentence matrix that would be filtered.</p>
<p>In the figure, #filters are the illustrations of the filters, not what has been filtered out from the sentence matrix by the filter, the next paragraph would make this distinction clearer. Here, the authors chose to use 6 filters – 2 complementary filters to consider (2,3,4) words.</p>
<p><strong>Featuremaps</strong></p>
<p>For filters with size being 4, the output featuremap size should be (7-4)/1+1=4. Similarly, we have output featuremaps of 5 and 6 respectively for filter size 3 and 2.</p>
<p><strong>Maxpooling</strong></p>
<p>For each featuremap, we perform 1-max pooling and extract the largest number.</p>
<p><strong>Softmax</strong></p>
<p>After 1-max pooling, we are certain to have a fixed-length vector of 6 elements ( = number of filters = number of filters per region size (2) x number of region size considered (3)). This fixed length vector can then be fed into a softmax (fully-connected) layer to perform the classification. The error from the classification is then back-propagated back into the following parameters as part of learning:</p>
<ul>
<li>The <strong>w</strong> matrices that produced <strong>o</strong></li>
<li>The bias term that is added to <strong>o</strong> to produce <strong>c</strong></li>
<li>Word vectors (optional, use validation performance to decide)</li>
</ul>
<h2 id="CNN-Applications-in-NLP"><a href="#CNN-Applications-in-NLP" class="headerlink" title="CNN Applications in NLP"></a>CNN Applications in NLP</h2><p>The most natural fit for CNNs seem to be classifications tasks, such as Sentiment Analysis, Spam Detection or Topic Categorization. Convolutions and pooling operations lose information about the local order of words, so that sequence tagging as in PoS Tagging or Entity Extraction is a bit harder to fit into a pure CNN architecture (though not impossible, you can add positional features to the input).</p>
<h2 id="Pytorch-Implementation"><a href="#Pytorch-Implementation" class="headerlink" title="Pytorch Implementation"></a>Pytorch Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p><a href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/" target="_blank" rel="noopener">link</a> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
<div class="line">104</div>
<div class="line">105</div>
<div class="line">106</div>
<div class="line">107</div>
<div class="line">108</div>
<div class="line">109</div>
<div class="line">110</div>
<div class="line">111</div>
<div class="line">112</div>
<div class="line">113</div>
<div class="line">114</div>
<div class="line">115</div>
<div class="line">116</div>
<div class="line">117</div>
<div class="line">118</div>
<div class="line">119</div>
<div class="line">120</div>
<div class="line">121</div>
<div class="line">122</div>
<div class="line">123</div>
<div class="line">124</div>
<div class="line">125</div>
<div class="line">126</div>
<div class="line">127</div>
<div class="line">128</div>
<div class="line">129</div>
<div class="line">130</div>
<div class="line">131</div>
<div class="line">132</div>
<div class="line">133</div>
<div class="line">134</div>
<div class="line">135</div>
<div class="line">136</div>
<div class="line">137</div>
<div class="line">138</div>
<div class="line">139</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div>
<div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div>
<div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div>
<div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div>
<div class="line"><span class="keyword">import</span> torch</div>
<div class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</div>
<div class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</div>
<div class="line"><span class="keyword">import</span> random</div>
<div class="line"><span class="keyword">import</span> spacy</div>
<div class="line"></div>
<div class="line"><span class="comment"># https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb</span></div>
<div class="line"></div>
<div class="line">SEED = <span class="number">1234</span></div>
<div class="line"></div>
<div class="line">torch.manual_seed(SEED)</div>
<div class="line">torch.backends.cudnn.deterministic = <span class="keyword">True</span></div>
<div class="line"></div>
<div class="line">spacy_en = spacy.load(<span class="string">'en'</span>)</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(text)</span>:</span> <span class="comment"># create a tokenizer function</span></div>
<div class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</div>
<div class="line"></div>
<div class="line">TEXT = data.Field(tokenize = tokenizer)</div>
<div class="line">LABEL = data.LabelField(dtype = torch.float)</div>
<div class="line"></div>
<div class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</div>
<div class="line"></div>
<div class="line">train_data, valid_data = train_data.split(random_state = random.seed(SEED))</div>
<div class="line"></div>
<div class="line">MAX_VOCAB_SIZE = <span class="number">25</span>_000</div>
<div class="line"></div>
<div class="line">TEXT.build_vocab(train_data,</div>
<div class="line">                 max_size = MAX_VOCAB_SIZE,</div>
<div class="line">                 vectors = <span class="string">"glove.6B.100d"</span>,</div>
<div class="line">                 unk_init = torch.Tensor.normal_)</div>
<div class="line"></div>
<div class="line">LABEL.build_vocab(train_data)</div>
<div class="line">BATCH_SIZE = <span class="number">64</span></div>
<div class="line"></div>
<div class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</div>
<div class="line"></div>
<div class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</div>
<div class="line">    (train_data, valid_data, test_data),</div>
<div class="line">    batch_size = BATCH_SIZE,</div>
<div class="line">    device = device)</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="comment">#round predictions to the closest integer</span></div>
<div class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</div>
<div class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division</span></div>
<div class="line">    acc = correct.sum() / len(correct)</div>
<div class="line">    <span class="keyword">return</span> acc</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></div>
<div class="line">    epoch_loss = <span class="number">0</span></div>
<div class="line">    epoch_acc = <span class="number">0</span></div>
<div class="line">    model.train()</div>
<div class="line"></div>
<div class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</div>
<div class="line">        optimizer.zero_grad()</div>
<div class="line"></div>
<div class="line">        batch_ = batch.text <span class="comment">#torch.Size([1225, 64])</span></div>
<div class="line"></div>
<div class="line">        predictions = model(batch_).squeeze(<span class="number">1</span>)</div>
<div class="line"></div>
<div class="line">        loss = criterion(predictions, batch.label)</div>
<div class="line"></div>
<div class="line">        acc = binary_accuracy(predictions, batch.label)</div>
<div class="line"></div>
<div class="line">        loss.backward()</div>
<div class="line"></div>
<div class="line">        optimizer.step()</div>
<div class="line"></div>
<div class="line">        epoch_loss += loss.item()</div>
<div class="line">        epoch_acc += acc.item()</div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx)</span>:</span></div>
<div class="line">        super(TextCNN,self).__init__()</div>
<div class="line">        self.embedding = nn.Embedding(vocab_size,embedding_dim,pad_idx)</div>
<div class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>,out_channels=n_filters,kernel_size=(filter_sizes[<span class="number">0</span>],embedding_dim))</div>
<div class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">1</span>,out_channels=n_filters,kernel_size=(filter_sizes[<span class="number">1</span>],embedding_dim))</div>
<div class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">1</span>,out_channels=n_filters,kernel_size=(filter_sizes[<span class="number">2</span>],embedding_dim))</div>
<div class="line">        self.fc = nn.Linear(n_filters*len(filter_sizes),output_dim)</div>
<div class="line">        self.dropout = nn.Dropout(p=dropout)</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></div>
<div class="line">        <span class="comment">#text.shape = (sent_length,batch_size), sent_length is varing</span></div>
<div class="line">        text = torch.transpose(text, <span class="number">0</span>, <span class="number">1</span>) <span class="comment">#(batch_size,sent_length)=(64,1225)</span></div>
<div class="line">        text_embedding = self.embedding(text) <span class="comment"># shape = (batch_size,sent_length,embedding_size)=(64,1225,100)</span></div>
<div class="line">        </div>
<div class="line">        <span class="comment"># since we use conv2d, the input should be (batch,channel,height,width)</span></div>
<div class="line">        text_embedding = torch.unsqueeze(text_embedding,dim=<span class="number">1</span>)</div>
<div class="line">        </div>
<div class="line">        <span class="comment">#kernel_size is (?,embedding_dim), so output is (batch,filter_num,??,1)</span></div>
<div class="line">        conv1 = F.relu(self.conv1(text_embedding)) <span class="comment">#Size([64, 100, 1223, 1])</span></div>
<div class="line">        conv2 = F.relu(self.conv2(text_embedding)) <span class="comment">#Size([64, 100, 1222, 1])</span></div>
<div class="line">        conv3 = F.relu(self.conv3(text_embedding)) <span class="comment">#Size([64, 100, 1221, 1])</span></div>
<div class="line"></div>
<div class="line">        pooling1 = F.max_pool1d(conv1.squeeze(),conv1.size(<span class="number">2</span>))</div>
<div class="line">        pooling2 = F.max_pool1d(conv2.squeeze(),conv2.size(<span class="number">2</span>))</div>
<div class="line">        pooling3 = F.max_pool1d(conv3.squeeze(),conv3.size(<span class="number">2</span>))</div>
<div class="line"></div>
<div class="line">        cat = torch.cat((pooling1,pooling2,pooling3),dim=<span class="number">1</span>).squeeze() <span class="comment">#Size([64,300,1,1]-&gt;[64,300])</span></div>
<div class="line">        dropout = self.dropout(cat)</div>
<div class="line">        output = self.fc(dropout) <span class="comment">#fully-connected layers can only be (batch_size,dim)</span></div>
<div class="line">        <span class="keyword">return</span> output</div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    INPUT_DIM = len(TEXT.vocab) <span class="comment">#25002</span></div>
<div class="line">    EMBEDDING_DIM = <span class="number">100</span></div>
<div class="line">    N_FILTERS = <span class="number">100</span></div>
<div class="line">    FILTER_SIZES = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</div>
<div class="line">    OUTPUT_DIM = <span class="number">1</span></div>
<div class="line">    DROPOUT = <span class="number">0.5</span></div>
<div class="line">    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</div>
<div class="line">    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</div>
<div class="line">    print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</div>
<div class="line">    pretrained_embeddings = TEXT.vocab.vectors</div>
<div class="line">    model.embedding.weight.data.copy_(pretrained_embeddings)</div>
<div class="line">    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</div>
<div class="line">    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</div>
<div class="line">    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</div>
<div class="line">    optimizer = optim.Adam(model.parameters())</div>
<div class="line">    criterion = nn.BCEWithLogitsLoss()</div>
<div class="line">    model = model.to(device)</div>
<div class="line">    criterion = criterion.to(device)</div>
<div class="line">    N_EPOCHS = <span class="number">5</span></div>
<div class="line">    best_valid_loss = float(<span class="string">'inf'</span>)</div>
<div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</div>
<div class="line">        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</div>
<div class="line">        print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-16 at 9.20.27 PM.png" alt="creen Shot 2019-07-16 at 9.20.27 P"></p>

</div></div>
<h1 id="Tensorflow代码"><a href="#Tensorflow代码" class="headerlink" title="Tensorflow代码"></a>Tensorflow代码</h1><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<h2 id="卷积层tf-nn-conv2d"><a href="#卷积层tf-nn-conv2d" class="headerlink" title="卷积层tf.nn.conv2d()"></a>卷积层tf.nn.conv2d()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 3.38.52 PM.png" alt="Screen Shot 2018-07-10 at 3.38.52 PM"></p>
<h2 id="池化层tf-nn-max-pool"><a href="#池化层tf-nn-max-pool" class="headerlink" title="池化层tf.nn.max_pool()"></a>池化层tf.nn.max_pool()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">tf.nn.max_pool( value, ksize,strides,padding,data_format=’NHWC’,name=<span class="keyword">None</span>) </div>
<div class="line"><span class="comment">#或者 </span></div>
<div class="line">tf.nn.avg_pool(…)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 3.41.42 PM.png" alt="Screen Shot 2018-07-10 at 3.41.42 PM"></p>
<h2 id="归一化层tf-nn-batch-normalization"><a href="#归一化层tf-nn-batch-normalization" class="headerlink" title="归一化层tf.nn.batch_normalization()"></a>归一化层tf.nn.batch_normalization()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">batch_normalization( x,mean,variance,offset,scale, variance_epsilon,name=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<ul>
<li>mean 和 variance 通过 tf.nn.moments 来进行计算：<br>batch_mean, batch_var = tf.nn.moments(x, axes = [0, 1, 2], keep_dims=True)，注意axes的输入。对于以feature map 为维度的全局归一化，若feature map 的shape 为[batch, height, width, depth]，则将axes赋值为[0, 1, 2]</li>
<li>x 为输入的feature map 四维数据，offset、scale为一维Tensor数据，shape 等于 feature map 的深度depth。</li>
</ul>
<h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p>通过搭建卷积神经网络来实现sklearn库中的手写数字识别，搭建的卷积神经网络结构如下图所示：</p>
<p><img src="/2018/07/04/DP-CNN/20170501131714709.png" alt="20170501131714709"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div>
<div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div>
<div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"></div>
<div class="line">digits = load_digits()</div>
<div class="line">x_data = digits.data.astype(np.float32)</div>
<div class="line">y_data = digits.target.astype(np.float32)</div>
<div class="line">print(y_data) <span class="comment"># [0. 1. 2. ... 8. 9. 8.]</span></div>
<div class="line">y_data = y_data.reshape(<span class="number">-1</span>,<span class="number">1</span>)</div>
<div class="line">print(y_data)</div>
<div class="line"><span class="comment"># [[0.]</span></div>
<div class="line"><span class="comment">#  [1.]</span></div>
<div class="line"><span class="comment">#  [2.]</span></div>
<div class="line"><span class="comment">#  ...</span></div>
<div class="line"><span class="comment">#  [8.]</span></div>
<div class="line"><span class="comment">#  [9.]</span></div>
<div class="line"><span class="comment">#  [8.]]</span></div>
<div class="line">print(x_data.shape) <span class="comment">#(1797,64)</span></div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler <span class="comment"># 将属性缩放到一个指定的最大和最小值（通常是1-0）之间</span></div>
<div class="line">scaler = MinMaxScaler()</div>
<div class="line">x_data = scaler.fit_transform(x_data)</div>
<div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</div>
<div class="line">y = OneHotEncoder().fit_transform(y_data).todense()</div>
<div class="line">print(y)</div>
<div class="line"><span class="comment"># [[1. 0. 0. ... 0. 0. 0.]</span></div>
<div class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></div>
<div class="line"><span class="comment">#  [0. 0. 1. ... 0. 0. 0.]</span></div>
<div class="line"><span class="comment">#  ...</span></div>
<div class="line"><span class="comment">#  [0. 0. 0. ... 0. 1. 0.]</span></div>
<div class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 1.]</span></div>
<div class="line"><span class="comment">#  [0. 0. 0. ... 0. 1. 0.]]</span></div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment">#转换为图片的格式 （batch，height，width，channels）</span></div>
<div class="line">x = x_data.reshape(<span class="number">-1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>) <span class="comment">#-1表示适应大小，原来是64长度，变成8*8，则适应大小是1，即one batch的图片数目是1</span></div>
<div class="line">batch_size = <span class="number">8</span> <span class="comment"># 使用MBGD算法，设定batch_size为8</span></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generatebatch</span><span class="params">(x,y,n_examples,batch_size)</span>:</span></div>
<div class="line">    <span class="keyword">for</span> batch_i <span class="keyword">in</span> range(n_examples // batch_size):</div>
<div class="line">        start = batch_i * batch_size</div>
<div class="line">        end = start + batch_size</div>
<div class="line">        batch_xs = x[start:end]</div>
<div class="line">        batch_ys = y[start:end]</div>
<div class="line">        <span class="keyword">yield</span> batch_xs, batch_ys  <span class="comment"># 生成每一个batch</span></div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment">#输入层</span></div>
<div class="line">tf.reset_default_graph()</div>
<div class="line">tf_x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>]) <span class="comment">#灰度图片，CHANEL是1</span></div>
<div class="line">tf_y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</div>
<div class="line"><span class="comment">#卷积层+激活层</span></div>
<div class="line">conv_filter_w1 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">10</span>])) <span class="comment">#shape:10个filter，size是3*3，灰度图片1</span></div>
<div class="line">conv_filter_b1 = tf.Variable(tf.random_normal([<span class="number">10</span>])) <span class="comment">#10个filter</span></div>
<div class="line">relu_feature_maps1 = tf.nn.relu(</div>
<div class="line">    tf.nn.conv2d(tf.x,conv_filter_w1,</div>
<div class="line">                 strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],<span class="comment">#batch和in_chanel是1，1*1是移动步</span></div>
<div class="line">                 padding=<span class="string">'same'</span>)</div>
<div class="line">    +conv_filter_b1)</div>
<div class="line"><span class="comment">#池化层</span></div>
<div class="line">max_pool1 = tf.nn.max_pool(relu_feature_maps1,</div>
<div class="line">                           ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],</div>
<div class="line">                           strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</div>
<div class="line">                           padding=<span class="string">'same'</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment">#卷积层</span></div>
<div class="line">conv_filter_w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">10</span>,<span class="number">5</span>])) <span class="comment">#10是in_chanel,与输入的in_chanel相同，5是feature_maps数目</span></div>
<div class="line">conv_filter_b2 = tf.Variable(tf.random_normal([<span class="number">5</span>])) <span class="comment">#与神经元个数相同</span></div>
<div class="line">conv_out2 = tf.nn.conv2d(relu_feature_maps1,conv_filter_w2,strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)+conv_filter_b2</div>
<div class="line"></div>
<div class="line"><span class="comment">#BN归一化层+激励层</span></div>
<div class="line">batch_mean, batch_var = tf.nn.moments(conv_out2, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keep_dims=<span class="keyword">True</span>)</div>
<div class="line">shift = tf.Variable(tf.zeros([<span class="number">5</span>]))</div>
<div class="line">scale = tf.Variable(tf.ones([<span class="number">5</span>]))</div>
<div class="line">epsilon = <span class="number">1e-3</span></div>
<div class="line">BN_out = tf.nn.batch_normalization(conv_out2, batch_mean, batch_var, shift, scale, epsilon)</div>
<div class="line">relu_BN_maps2 = tf.nn.relu(BN_out)</div>
<div class="line"></div>
<div class="line"><span class="comment">#池化层</span></div>
<div class="line">max_pool2 = tf.nn.max_pool(BN_out,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment">#特征图展开，输入大小是8*8，两次pooling，变成2*2，而且有5个神经元，则展开大小是2*2*5</span></div>
<div class="line">max_pool2_flat = tf.reshape(max_pool2,[<span class="number">-1</span>,<span class="number">2</span>*<span class="number">2</span>*<span class="number">5</span>])</div>
<div class="line"><span class="comment">#全连接层</span></div>
<div class="line">fc_w1 = tf.Variable(tf.random_normal([<span class="number">2</span>*<span class="number">2</span>*<span class="number">5</span>,<span class="number">50</span>])) <span class="comment">#50神经元</span></div>
<div class="line">fc_b1 = tf.Variable(tf.random_normal([<span class="number">50</span>]))</div>
<div class="line">fc_out1 = tf.nn.relu(tf.matmul(max_pool2_flat,fc_w1)+fc_b1)</div>
<div class="line"><span class="comment">#输出层</span></div>
<div class="line">out_w1 = tf.Variable(tf.random_normal([<span class="number">50</span>,<span class="number">10</span>]))</div>
<div class="line">out_b1 = tf.Variable(tf.random_normal([<span class="number">10</span>]))</div>
<div class="line">pred = tf.nn.softmax(tf.matmul(fc_out1,out_w1)+out_b1)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
</pre></td><td class="code"><pre><div class="line">loss = -tf.reduce_mean(tf_y*tf.log(tf.clip_by_value(pred,<span class="number">1e-11</span>,<span class="number">1.0</span>)))</div>
<div class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</div>
<div class="line">y_pred = tf.arg_max(pred,<span class="number">1</span>)</div>
<div class="line">bool_pred = tf.equal(tf.arg_max(tf_y,<span class="number">1</span>),y_pred)</div>
<div class="line">accuracy = tf.reduce_mean(tf.cast(bool_pred,tf.float32)) <span class="comment"># 准确率</span></div>
<div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div>
<div class="line">    sess.run(tf.global_variables_initializer())</div>
<div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment"># 迭代1000个周期</span></div>
<div class="line">        <span class="keyword">for</span> batch_xs,batch_ys <span class="keyword">in</span> generatebatch(x,y,y.shape[<span class="number">0</span>],batch_size): <span class="comment"># 每个周期进行MBGD算法</span></div>
<div class="line">            sess.run(train_step,feed_dict=&#123;tf_x:batch_xs , tf_y:batch_ys&#125;)</div>
<div class="line">        <span class="keyword">if</span>(epoch%<span class="number">100</span>==<span class="number">0</span>):</div>
<div class="line">            res = sess.run(accuracy,feed_dict=&#123;tf_x:x,tf_y:y&#125;)</div>
<div class="line">            <span class="keyword">print</span> (epoch,res)</div>
<div class="line">    res_ypred = y_pred.eval(feed_dict=&#123;tf_x:x,tf_y:y&#125;).flatten() <span class="comment"># 只能预测一批样本，不能预测一个样本</span></div>
<div class="line">    <span class="keyword">print</span> (res_ypred)</div>
</pre></td></tr></table></figure>
<p>在第100次个batch size 迭代时，准确率就快速接近收敛了，这得归功于Batch Normalization 的作用！需要注意的是，这个模型还不能用来预测单个样本，因为在进行BN层计算时，单个样本的均值和方差都为0，会得到相反的预测效果，解决方法详见归一化层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span>  accuracy_score</div>
<div class="line">print(accuracy_score(Y_data,res_ypred.reshape(<span class="number">-1</span>,<span class="number">1</span>)))</div>
<div class="line"><span class="comment"># 0.998887033945</span></div>
</pre></td></tr></table></figure>

</div></div>
<h1 id="CNN结构扩展"><a href="#CNN结构扩展" class="headerlink" title="CNN结构扩展"></a>CNN结构扩展</h1><h2 id="GAP-Global-Average-Pooling"><a href="#GAP-Global-Average-Pooling" class="headerlink" title="GAP(Global Average Pooling)"></a>GAP(Global Average Pooling)</h2><p><a href="https://blog.csdn.net/yimingsilence/article/details/79227668" target="_blank" rel="noopener">[Ref1]</a> <a href="https://blog.csdn.net/Losteng/article/details/51520555" target="_blank" rel="noopener">[Ref2]</a></p>
<p>GAP是用来代替全连接层，由于全连接层过多的参数重要到会造成过拟合，所以GAP放弃了对前一层的全连接，使用pooling来降低参数。global average pooling 与 average pooling 的差别就在 “global” 这一个字眼上。global 与 local 在字面上都是用来形容 pooling 窗口区域的。 local 是取 feature map 的一个子区域求平均值，然后滑动这个子区域； global 显然就是对整个 feature map 求平均值了。因此，global average pooling 的最后输出结果仍然是 10 个 feature map，而不是一个，只不过每个 feature map 只剩下一个像素罢了。这个像素就是求得的平均值。</p>
<p>举个例子</p>
<p>假如，最后的一层的数据是10个6<em>6的特征图，global average pooling是将每一张特征图计算所有像素点的均值，输出一个数据值，这样10 个特征图就会输出10个数据点，将这些数据点组成一个1</em>10的向量的话，就成为一个特征向量，就可以送入到softmax的分类中计算了</p>
<p><img src="/2018/07/04/DP-CNN/20160528001522947.jpg" alt="20160528001522947"></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://towardsdatascience.com/how-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb" target="_blank" rel="noopener">Building a convolutional neural network for natural language processing</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/04/ML-Kernel-Function/" rel="next" title="ML-Kernel Function">
                <i class="fa fa-chevron-left"></i> ML-Kernel Function
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/04/Software-Installation/" rel="prev" title="Software Installation">
                Software Installation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">66</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Q"><span class="nav-number">1.</span> <span class="nav-text">Q</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A"><span class="nav-number">2.</span> <span class="nav-text">A</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN的引入"><span class="nav-number">3.</span> <span class="nav-text">CNN的引入</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN层次结构"><span class="nav-number">4.</span> <span class="nav-text">CNN层次结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#输入层"><span class="nav-number">4.1.</span> <span class="nav-text">输入层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层"><span class="nav-number">4.2.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#局部感受野"><span class="nav-number">4.2.1.</span> <span class="nav-text">局部感受野</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化层-下采样"><span class="nav-number">4.3.</span> <span class="nav-text">池化层(下采样)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#全连接层"><span class="nav-number">4.4.</span> <span class="nav-text">全连接层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#输出层"><span class="nav-number">4.5.</span> <span class="nav-text">输出层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化层-Batch-Normalization"><span class="nav-number">4.6.</span> <span class="nav-text">归一化层(Batch Normalization)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#近邻归一化-Local-Response-Normalization"><span class="nav-number">4.7.</span> <span class="nav-text">近邻归一化(Local Response Normalization)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#融合层"><span class="nav-number">4.8.</span> <span class="nav-text">融合层</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN-in-NLP"><span class="nav-number">5.</span> <span class="nav-text">CNN in NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Input"><span class="nav-number">5.1.</span> <span class="nav-text">Input</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fliter"><span class="nav-number">5.2.</span> <span class="nav-text">Fliter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Padding"><span class="nav-number">5.3.</span> <span class="nav-text">Padding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Channel"><span class="nav-number">5.4.</span> <span class="nav-text">Channel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stride-Size"><span class="nav-number">5.5.</span> <span class="nav-text">Stride Size</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pooling"><span class="nav-number">5.6.</span> <span class="nav-text">Pooling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture"><span class="nav-number">5.7.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-Applications-in-NLP"><span class="nav-number">5.8.</span> <span class="nav-text">CNN Applications in NLP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pytorch-Implementation"><span class="nav-number">5.9.</span> <span class="nav-text">Pytorch Implementation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow代码"><span class="nav-number">6.</span> <span class="nav-text">Tensorflow代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层tf-nn-conv2d"><span class="nav-number">6.1.</span> <span class="nav-text">卷积层tf.nn.conv2d()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化层tf-nn-max-pool"><span class="nav-number">6.2.</span> <span class="nav-text">池化层tf.nn.max_pool()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化层tf-nn-batch-normalization"><span class="nav-number">6.3.</span> <span class="nav-text">归一化层tf.nn.batch_normalization()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码示例"><span class="nav-number">6.4.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CNN结构扩展"><span class="nav-number">7.</span> <span class="nav-text">CNN结构扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GAP-Global-Average-Pooling"><span class="nav-number">7.1.</span> <span class="nav-text">GAP(Global Average Pooling)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">8.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
