<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Neural Network," />










<meta name="description" content="本节介绍最基本的神经网络。">
<meta name="keywords" content="Deep Learning,Neural Network">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Network">
<meta property="og:url" content="http://yoursite.com/2018/07/24/Neural-Network/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="本节介绍最基本的神经网络。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202018-08-24%20at%209.39.12%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/perceptron.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/tikz2.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202018-08-23%20at%208.45.15%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202018-08-23%20at%208.46.24%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/tikz11.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/2256672-bfbb364740f898d1.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/2256672-c1388dc8fdcce427.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/2256672-6f27ced45cf5c0d8.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/2256672-6f27ced45cf5c0d8-5514755.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-02-13%20at%2010.26.59%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202018-08-29%20at%2010.22.09%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202018-09-02%20at%204.03.40%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/v2-3c6aa9626ee8e4609b0d7c5712baf624_hd.jpg">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/v2-9618ff8556f19a00ede416312f3e57e6_r.jpg">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-03-06%20at%204.05.12%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-03-07%20at%204.24.27%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-03-08%20at%202.21.08%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-03-08%20at%202.24.14%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-03-08%20at%202.29.26%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202019-03-08%20at%202.34.05%20PM.png">
<meta property="og:updated_time" content="2019-04-29T16:32:20.789Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Network">
<meta name="twitter:description" content="本节介绍最基本的神经网络。">
<meta name="twitter:image" content="http://yoursite.com/2018/07/24/Neural-Network/Screen%20Shot%202018-08-24%20at%209.39.12%20AM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/24/Neural-Network/"/>





  <title>Neural Network | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/24/Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Neural Network</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-24T09:56:04-05:00">
                2018-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本节介绍最基本的神经网络。</p>
<a id="more"></a>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><p><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap1/c1s1.html" target="_blank" rel="noopener">ref1</a> <a href="https://www.leiphone.com/news/201706/QFydbeV7FXQtRIOl.html" target="_blank" rel="noopener">ref2</a> </p>
<h3 id="二分类线性模型"><a href="#二分类线性模型" class="headerlink" title="二分类线性模型"></a>二分类线性模型</h3><p>感知机作为一种二元线性分类模型，能（且一定能）将线性可分的数据集分开。什么叫线性可分？在二维平面上、线性可分意味着能用一条线将正负样本分开，在三维空间中、线性可分意味着能用一个平面将正负样本分开。</p>
<p>虽然简单，但它既可以发展成支持向量机（通过简单地修改损失函数），又可以发展为神经网络（通过简单地叠加），它的模型如下：</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2018-08-24 at 9.39.12 AM.png" alt="Screen Shot 2018-08-24 at 9.39.12 AM"></p>
<p>感知机接受几个输入$x_i$，对每一个权重赋予一个权重衡量该输入对输入的重要性，其输出为0或者1，由加权和$\sum_{j}{w_jx_j}$是否小于或者大于某一个阈值决定。和权重一样，阈值也是一个实数，同时它是神经元的一个参数。使用更严密的代数形式来表示：</p>
<script type="math/tex; mode=display">
\begin{eqnarray} \mbox{output} & = & \left\{ \begin{array}{ll} 0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\ 1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}</script><p>以上为感知机的工作方式。鉴于上述表示方法过于繁琐，我们通过使用两个新记法来简化它。</p>
<p>第一个是使用点乘代替$\sum_{j}w_jx_j$：$w\cdot x \equiv \sum_{j}w_jx_j$</p>
<p>第二个是将阈值移到不等式的另一侧，并使用偏置(bias)来代替阈值：$bias \equiv -threshold$ </p>
<script type="math/tex; mode=display">
\begin{eqnarray} \mbox{output} = \left\{ \begin{array}{ll} 0 & \mbox{if } w\cdot x + b \leq 0 \\ 1 & \mbox{if } w\cdot x + b > 0 \end{array} \right. \tag{2}\end{eqnarray}</script><p>如下图所示。</p>
<p><img src="/2018/07/24/Neural-Network/perceptron.png" alt="perceptron"></p>
<p>可以将偏置理解为感知机为了得到输出为1的容易度的度量，如果一个感知机的偏置非常大，那么这个感知机的输出很容易为1，相反如果偏置非常小，那么输出1就很困难。</p>
<h3 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><blockquote>
<p>为确定感知机模型参数$w​$和$b​$,需要定义一个损失函数并将损失函数最小化。样本点到超平面的距离:</p>
</blockquote>
<script type="math/tex; mode=display">
\frac{1}{||w||}{|w\times{x}+b|}</script><p>$||w||$是$w$的$L_2$范数，为保计算结果为正，对$w\times x+b$进行绝对值运算。</p>
<blockquote>
<p>对于误分类的数据$(x_i,y_i)$来说，$-y_i(w\times{x_i}+b)&gt;0$，假设超平面$S$的误分类点集合为$M$，那么损失函数为所有误分类点到平面距离:</p>
</blockquote>
 
$$
L(w,b)=-\frac{1}{||w||}\sum_{x_i\in{M}}{{y_i(w\times{x_i}+b)}}
$$
$\frac{1}{||w||}$是常量，不影响，$L(w,b)=-\sum_{x_i\in{M}}{{y_i(w\times{x_i}+b)}}$


<blockquote>
<p>采用梯度下降法，对于一个误分类样本点$(x_i,y_i)$，$\frac{\partial L}{\partial w}=-\eta{y_ix_i}$，$\frac{\partial{L}}{\partial{b}}=-\eta{y_i}$。</p>
<p>则参数更新公式为：</p>
<p>$w=w+\eta y_ix_i, b = b+\eta y_i$ </p>
</blockquote>
<h3 id="逻辑计算"><a href="#逻辑计算" class="headerlink" title="逻辑计算"></a>逻辑计算</h3><p>感知机可以用于计算初等逻辑函数：与、或、非。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">AND</th>
<th style="text-align:center">OR</th>
<th style="text-align:center">NOT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>感知机的输入是两个取值0、1的变量，</p>
<p>对于$AND$，它们对输出的贡献是相同的，则权重相同，我们取权重为-2和-2，但是发现4个里面才一个1，得到1比较困难，故有较高的阈值，设置阈值为3。</p>
<p><img src="/2018/07/24/Neural-Network/tikz2.png" alt="tikz2"></p>
<p>对于$OR$，权重相同，但是得到1比较容易，所以取较小的阈值，设置阈值为0.5。</p>
<p>对于$NOT$，发现只有一个输入，则只有一个输入对输出有影响，所以权重不同，$w_1=-1, w_2=0$，阈值为$-0.5$。</p>
<h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。</p>
<p>（1）M-P神经元模型：</p>
<p>神经元接收来自$n$个其他神经元传递过来的输入信号，这些输入信号通过待权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过”激活函数”处理以产生神经元的输出。</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2018-08-23 at 8.45.15 PM.png" alt="Screen Shot 2018-08-23 at 8.45.15 PM"></p>
<p>（2）激活函数</p>
<p>采用（a）阶跃函数作为激活函数，它将输入值映射为输出值“1”（对应于神经元兴奋）或“0”（神经元抑制），可是阶跃函数不连续、不光滑，故采用sigmoid函数作为激活函数。</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2018-08-23 at 8.46.24 PM.png" alt="Screen Shot 2018-08-23 at 8.46.24 PM"></p>
<p>故我们称上述神经元为sigmoid神经元。</p>
<blockquote>
<p>  感知机到sigmoid神经元的motivation：</p>
<p>We want a samll change in a weight (or bias) to cause a small change in output.</p>
<p>Compared to perceptron, both inputs and outputs of sigmoid neural can take on any values between 0 and 1 instead of just 0 or 1.</p>
</blockquote>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><p>多个sigmoid神经元构成复杂的神经网络。网络的最左边的一层被称为输入层，其中的神经元被称为输入神经元(input neurons)；最右边的一层是输出层(output layer)，包含的神经元被称为输出神经元(output neurons)。下图中我们的输出层只有一个神经元，网络的中间一层被称为隐层(hidden layer)，因为它既不是输入层，也不是输出层。</p>
<p><img src="/2018/07/24/Neural-Network/tikz11.png" alt="tikz11"></p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p><a href="https://ilewseu.github.io/2017/12/17/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/" target="_blank" rel="noopener">ref1</a> <a href="https://zhuanlan.zhihu.com/p/34378516" target="_blank" rel="noopener">ref2</a> <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">gotta</a> </p>
<p>神经网络的前向传播实质就是一个输入向量$\vec x$到输出向量$\vec y$的函数。下面我们使用一个例子来说明这个过程</p>
<p><img src="/2018/07/24/Neural-Network/2256672-bfbb364740f898d1.png" alt="2256672-bfbb364740f898d1"></p>
<p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$w_{41}, w_{42},w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？</p>
<script type="math/tex; mode=display">
\begin{align}
a_4&=sigmoid(\vec{w}^T\centerdot\vec{x})\\
&=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})
\end{align}</script><p>其中$w_{4b}$是节点4的偏置项，图中没有画出来。而$w_{41}, w_{42},w_{43}$分别为节点1、2、3到节点4连接的权重，在给权重编号时，我们把目标节点的编号$j$放在前面，把源节点的编号$i$放在后面。</p>
<p>同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$：</p>
<script type="math/tex; mode=display">
\begin{align}
y_1&=sigmoid(\vec{w}^T\centerdot\vec{a})\\
&=sigmoid(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})
\end{align}</script><p>同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$时，神经网络的输出向量$\vec{y}=\begin{bmatrix}y_1\\y_2\end{bmatrix}$。这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong>。 </p>
<h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便，我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：</p>
<script type="math/tex; mode=display">
a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\
a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\
a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\
a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\</script><p>接着，定义网络的输入向量和隐藏层每个节点的权重向量$\vec w_j$，</p>
<script type="math/tex; mode=display">
\begin{align}
\vec{x}&=\begin{bmatrix}x_1\\x_2\\x_3\\1\end{bmatrix}\\
\vec{w}_4&=[w_{41},w_{42},w_{43},w_{4b}]\\
\vec{w}_5&=[w_{51},w_{52},w_{53},w_{5b}]\\
\vec{w}_6&=[w_{61},w_{62},w_{63},w_{6b}]\\
\vec{w}_7&=[w_{71},w_{72},w_{73},w_{7b}]\\
f&=sigmoid\\

\end{align}</script><p>代入得：</p>
<script type="math/tex; mode=display">
\begin{align}
a_4&=f(\vec{w_4}\centerdot\vec{x})\\
a_5&=f(\vec{w_5}\centerdot\vec{x})\\
a_6&=f(\vec{w_6}\centerdot\vec{x})\\
a_7&=f(\vec{w_7}\centerdot\vec{x})
\end{align}</script><p>如果把$a_4,a_5,a_6,a_7$表示成一个矩阵，</p>
<script type="math/tex; mode=display">
\vec{a}=
\begin{bmatrix}
a_4 \\
a_5 \\
a_6 \\
a_7 \\
\end{bmatrix},\qquad W=
\begin{bmatrix}
\vec{w}_4 \\
\vec{w}_5 \\
\vec{w}_6 \\
\vec{w}_7 \\
\end{bmatrix}=
\begin{bmatrix}
w_{41},w_{42},w_{43},w_{4b} \\
w_{51},w_{52},w_{53},w_{5b} \\
w_{61},w_{62},w_{63},w_{6b} \\
w_{71},w_{72},w_{73},w_{7b} \\
\end{bmatrix}
,\qquad f(
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
.\\
.\\
.\\
\end{bmatrix})=
\begin{bmatrix}
f(x_1)\\
f(x_2)\\
f(x_3)\\
.\\
.\\
.\\
\end{bmatrix}</script><p>代入得：</p>
<script type="math/tex; mode=display">
\vec{a}=f(W\centerdot\vec{x})\qquad</script><p>上式中，$f$是激活函数；$W$是某一层的权重矩阵；$\vec x$是某层的输入向量；$\vec a$是某层的输出向量。上式说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p>
<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$W_1, W_2,W_3,W_4$，每个隐藏层的输出分别是$\vec a_1, \vec a_2, \vec a_3$，神经网络的输入为$\vec x$，神经网络的输出为$\vec y$，如下图所示：</p>
<p><img src="/2018/07/24/Neural-Network/2256672-c1388dc8fdcce427.png" alt="2256672-c1388dc8fdcce427"></p>
<p>则每一层的输出向量的计算可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
&\vec{a}_1=f(W_1\centerdot\vec{x})\\
&\vec{a}_2=f(W_2\centerdot\vec{a}_1)\\
&\vec{a}_3=f(W_3\centerdot\vec{a}_2)\\
&\vec{y}=f(W_4\centerdot\vec{a}_3)\\
\end{align}</script><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p>
<p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<p>我们假设每个训练样本为$(\vec{x},\vec{t})$，其中向量$\vec x$是训练样本的特征，而$\vec t$是样本的目标值。</p>
<p><img src="/2018/07/24/Neural-Network/2256672-6f27ced45cf5c0d8.png" alt="2256672-6f27ced45cf5c0d8"></p>
<p>首先，我们根据前向传播算法，用样本的特征$\vec x$，计算出神经网络中每个隐藏层节点的输出$a_i$，以及输出层每个节点的输出$y_i$。</p>
<p>然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$：</p>
<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>对于输出层的节点$i$ </p>
<script type="math/tex; mode=display">
\delta_i=y_i(1-y_i)(y_i-t_i)\qquad</script><p>其中$\delta_i$是节点$i$的误差项，$y_i$是节点$i$的输出值，$t_i$是样本对应于节点$i$的目标值。比如：节点8的误差是$\delta_8=y_1(1-y_1)(t_1-y_1)$。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p>按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用<strong>随机梯度下降</strong>优化算法去求目标函数最小值时的参数值。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：</p>
<script type="math/tex; mode=display">
E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2</script><p>其中，$E_d$表示是样本$d$的误差，展开得到：</p>
<script type="math/tex; mode=display">
E_d = E_{y_1}+E_{y_2}=\frac{1}{2}(t{_1}-{y_1})^2+\frac{1}{2}(t{_2}-{y_2})^2</script><p>然后我们用<strong>随机梯度下降</strong>算法对目标函数进行优化：</p>
<script type="math/tex; mode=display">
w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}</script><p>随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？</p>
<p><img src="/2018/07/24/Neural-Network/2256672-6f27ced45cf5c0d8-5514755.png" alt="2256672-6f27ced45cf5c0d8-5514755"></p>
<p>对于隐藏层与输出层间的权值，我们需要计算$\frac{\partial E_d}{\partial w_{ji} }$，由链式法则：</p>
<script type="math/tex; mode=display">
\frac{\partial E_d}{\partial w_{ji} }=\frac{\partial E_d}{y_j}\frac{y_i}{net_j}\frac{net_j}{w_{ji}}</script><p>其中，$net_j$是是节点$j$的<strong>加权输入</strong>，即$ net_j=\vec{w_j}\centerdot\vec{x_j}=\sum_{i}{w_{ji}}x_{ji} $ </p>
<p>$y_i$是激活函数处理之后的输出值。</p>
<p>考虑第一项：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial{E_d}}{\partial{y_j}}&=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2\\
&=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2\\
&=y_j-t_j
\end{align}</script><p>考虑第二项：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial{y_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\
&=y_j(1-y_j)\\
\end{align}</script><blockquote>
<p><a href="https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/" target="_blank" rel="noopener">source: Derivation: Derivatives for Common Neural Network Activation Functions</a>  </p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-02-13 at 10.26.59 AM.png" alt="creen Shot 2019-02-13 at 10.26.59 A"></p>
</blockquote>
<p>将第一项和第二项带入，得到：</p>
<script type="math/tex; mode=display">
\frac{\partial{E_d}}{\partial{net_j}}=(y_j-t_j)y_j(1-y_j)</script><p>如果令$\sigma_j=\frac{\partial{E_d}}{\partial{net_j}}$来表示该节点的误差，则</p>
<script type="math/tex; mode=display">
\sigma_j =(y_j-t_j)y_j(1-y_j)</script><p>将上述推导带入随机梯度下降公式，得到：</p>
<script type="math/tex; mode=display">
\begin{align}
w_{ji}&\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}\\
&=w_{ji}+\eta(t_j-y_j)y_j(1-y_j)x_{ji}\\
&=w_{ji}+\eta\delta_jx_{ji}
\end{align}</script><p>这样我们就得到了隐藏层权重的更新公式。</p>

</div></div> 
<h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><script type="math/tex; mode=display">
\delta_i=a_i(1-a_i)\sum_{k\in{outputs}}w_{ki}\delta_k\qquad</script><p>其中，$a_i$是节点的输出值，$w_{ki}$是节点$i$到它的下一层节点$k$的连接的权重，$\sigma_k$是节点$i$的下一层节点$k$的误差项。例如，对于隐藏层节点4来说，计算方法为：$\delta_4=a_4(1-a_4)(w_{84}\delta_8+w_{94}\delta_9)$ </p>
<p>最后更新每个连接上的权值：</p>
<script type="math/tex; mode=display">
w_{ji}\gets w_{ji}+\eta\delta_jx_{ji}\qquad</script><p>其中，$w_ji$是节点$i$到节点$j$的权重，$\eta$是学习速率，$\sigma_j$是节点$j$的误差项，$x_{ji}$是节点$i$传递给节点$j$的输入。例如，权重的更新方法如下：$w_{84}\gets w_{84}+\eta\delta_8 a_4$；类似的，权重$w_{41}$的更新方法如下：$w_{41}\gets w_{41}+\eta\delta_4 x_1$；</p>
<p>偏置项的输入值永远为1。例如，节点4的偏置项$w_{4b}$应该按照下面的方法计算： $w_{4b}\gets w_{4b}+\eta\delta_4$ </p>
<p>显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p>同样地，我们需要计算$\frac{\partial E_d } {\partial w_{ji} }$,由链式法则：</p>
<script type="math/tex; mode=display">
\frac{\partial{E_d}}{\partial{w_{ji}}}={\frac{\partial{E_d}}{\partial{a_j}}}{\frac{\partial{a_j}}{\partial{net_j}}}{\frac{\partial{net_j}}{\partial{w_{ji}}}}</script><p>考虑第一项：</p>
<p>首先，我们需要定义节点$j$的所有直接下游节点的集合$Downstream(j)$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$a_j$只能通过影响$Downstream(j)$再影响$E_d$。设$net_j$是节点$j$的下游节点的输入，则</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
{\frac{\partial{E_d}}{\partial{a_j}}}
&=\sum_{k\in Downstream(j)}{\frac{\partial{E_d}}{\partial{net_k}}}{\frac{\partial{net_k}}{\partial{a_j}}}\\
&=\sum_{k\in Downstream(j)}\sigma_k{\frac{\partial{net_k}}{\partial{a_j}}}\\
&=\sum_{k\in Downstream(j)}\sigma_k w_{kj}\\
\end{aligned}
\end{equation}</script><p>考虑第二项：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial{a_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\
&=a_j(1-a_j)\\
\end{align}</script><p>综合第一项和第二项：</p>
<script type="math/tex; mode=display">
{\frac{\partial{E_d}}{\partial{net_j}}}=a_j(1-a_j)\sum_{k\in Downstream(j)}\sigma_k w_{kj}</script><p>令$\delta_j=\frac{\partial E_d}{ \partial net_j }$，则</p>
<script type="math/tex; mode=display">
\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}</script>
</div></div> 
<h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><p>假设我们三层神经网络，即输入层、隐藏层和输出层，使用$L_2$损失函数。</p>
<ul>
<li><p>输入是<code>x</code>，令<code>a1=x</code>。</p>
</li>
<li><p>前向传播</p>
<ul>
<li><p>输入层-隐藏层的前向传播：<code>z2=a1*w1</code></p>
</li>
<li><p>输入层-隐藏层的激活函数：<code>a2=sigmoid(z2)</code></p>
</li>
<li><p>隐藏层-输出层的前向传播：<code>z3=a2*w2</code></p>
</li>
<li><p>隐藏层-输出层的激活函数：<code>h=sigmoid(z3)</code></p>
</li>
</ul>
</li>
<li><p>后向传播</p>
<ul>
<li>输出层-隐藏层的损失：<code>h_err=(h-y)*sigmoid_prime(h)</code></li>
<li>输出层-隐藏层的梯度：<code>h_delta=h_err*a2</code></li>
<li>隐藏层-输入层的损失：<code>z2_err=(h_err*w2)*sigmoid_prime(a2)</code></li>
<li>隐藏层-输入层的梯度：<code>z2_delta=z2_err*a1</code></li>
<li>更新梯度：<code>w1 -= z2_delta</code>,<code>w2 -= h_delta</code></li>
</ul>
</li>
</ul>
<h2 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h2><h3 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a><a href="https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python" target="_blank" rel="noopener">Toy Example</a></h3><p>在该例子中，我们将建模一个具有二输入、一输出和一隐藏层的神经网络，该网络用于预测考试成绩基于两个输入：学习时间和睡觉时间。以下为训练样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">学习时间</th>
<th style="text-align:center">睡觉时间</th>
<th style="text-align:center">考试成绩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">9</td>
<td style="text-align:center">92</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">5</td>
<td style="text-align:center">86</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">6</td>
<td style="text-align:center">89</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">8</td>
<td style="text-align:center">？</td>
</tr>
</tbody>
</table>
</div>
<h4 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h4><p>考虑到时间是小时制，而考试成绩的百分制，所以我们想要将数据标准化，故对每一个变量我们都除以它的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line">x = np.array(([<span class="number">2</span>,<span class="number">9</span>],[<span class="number">1</span>,<span class="number">5</span>],[<span class="number">3</span>,<span class="number">6</span>]),dtype=float)</div>
<div class="line">y = np.array(([<span class="number">92</span>],[<span class="number">86</span>],[<span class="number">89</span>]),dtype=float)</div>
<div class="line">x = x/np.max(x,axis=<span class="number">0</span>)</div>
<div class="line">y = y/<span class="number">100</span></div>
</pre></td></tr></table></figure>
<p>然后，我们定义一个类<code>class</code>和初始化函数<code>init</code>，在<code>init</code>函数中我们定义神经网络参数，如输入层，隐藏层和输出层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neural_Network</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div>
<div class="line">        self.input_size = <span class="number">2</span></div>
<div class="line">        self.output_size = <span class="number">1</span></div>
<div class="line">        self.hidden_size = <span class="number">3</span></div>
<div class="line">        self.w1 = np.random.randn(self.hidden_size,self.input_size)</div>
<div class="line">        self.w2 = np.random.randn(self.output_size,self.hidden_size)</div>
</pre></td></tr></table></figure>
<p>变量定义好之后，我们便可以写前向传播函数了，在前向传播中，我们要完成的工作有输入与权重的点乘，再应用激活函数；隐层与权重的点乘，再应用一个激活函数得到输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></div>
<div class="line">    self.z2 = x.dot(self.w1.T) <span class="comment"># x.shape=(n,2) , w1.shape=(3,2),z1.shape=(n,3)</span></div>
<div class="line">    self.a2 = self.sigmoid(self.z2)</div>
<div class="line">    self.z3 = self.a2.dot(self.w2.T)</div>
<div class="line">    o = self.sigmoid(self.z3)</div>
<div class="line">    <span class="keyword">return</span> o</div>
</pre></td></tr></table></figure>
<p>所以，我们还需要定义一个<code>sigmoid</code>激活函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</div>
</pre></td></tr></table></figure>
<h4 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h4><p>第一步我们需要定义一个损失函数，我们使用<code>MSE</code>作为损失函数</p>
<script type="math/tex; mode=display">
Loss=\frac{1}{2m}\sum_{i=1}^{m}(o_i-y_i)</script><p>其中，$o_i$是对$i_{th}$样本的预测，$y_i$是该样本的真实输出。</p>
<p>一旦定义好了损失函数，我们的目标就是使损失函数的值不断逼近0，即我们需要优化损失函数，我们需要激活函数的导数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(self,z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> z*(<span class="number">1</span>-z)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,x,y,o)</span>:</span></div>
<div class="line">    self.o_err = o-y <span class="comment">#shape=(n,output_size)</span></div>
<div class="line">    self.o_delta = self.o_err*self.sigmoid_prime(o) <span class="comment">#shape=(n,output_size)</span></div>
<div class="line"></div>
<div class="line">    self.z2_err = self.o_delta.dot(self.w2) <span class="comment">#(n,hidden_size)</span></div>
<div class="line">    self.z2_delta = self.z2_err*self.sigmoid_prime(self.a2) <span class="comment">#(n,hidden_size)</span></div>
<div class="line"></div>
<div class="line">    self.w1 -= self.z2_delta.T.dot(x)</div>
<div class="line">    self.w2 -= self.o_delta.T.dot(self.a2)</div>
</pre></td></tr></table></figure>
<p>最后，我们便可以定义一个训练函数，它完成前向传播和后向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,x,y)</span>:</span></div>
<div class="line">    o = self.forward(x)</div>
<div class="line">    self.backward(x,y,o)</div>
</pre></td></tr></table></figure>
<p>最后我们定义<code>main</code>函数，训练模型1000次</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    nn = Neural_Network()</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div>
<div class="line">        o = nn.forward(x)</div>
<div class="line">        <span class="comment"># print("Predicted Output:",o)</span></div>
<div class="line">        print(<span class="string">"LOSS:"</span>,np.mean(np.square(y-o)))</div>
<div class="line">        nn.train(x,y)</div>
</pre></td></tr></table></figure>
<h3 id="MNIST手写体数字分类"><a href="#MNIST手写体数字分类" class="headerlink" title="MNIST手写体数字分类"></a><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/" target="_blank" rel="noopener">MNIST手写体数字分类</a></h3><p>本例我们介绍如何使用神经网络进行多分类任务，数据集$ex3data1.mat$包含了5000个训练样本，每个样本都是$20\times 20$像素大小的灰度图片，这$20\times 20$大小的 图片被展开成一个400维的向量，也就是说，我们的训练数据集是一个$5000\times 400$的矩阵。同样地，图片的标签被记录在大小为5000维向量$y$里，范围从1-10。</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2018-08-29 at 10.22.09 PM.png" alt="Screen Shot 2018-08-29 at 10.22.09 PM">我们就构建一个最简单的神经网络：输入层-隐藏层-输出层。</p>
<p><strong>输入层神经元个数</strong>：$input_size=20\times 20 =400$</p>
<p><strong>隐藏层神经元个数</strong>：$hidden_size=25$</p>
<p><strong>输出层神经元个数</strong>：$output_size=10$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line">data = scipy.io.loadmat(<span class="string">'dataset/ex3data1.mat'</span>)</div>
<div class="line">x = data[<span class="string">'X'</span>]</div>
<div class="line">y = data[<span class="string">'y'</span>]</div>
<div class="line">encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</div>
<div class="line">y = encoder.fit_transform(y)</div>
<div class="line"></div>
<div class="line">input_size = <span class="number">400</span></div>
<div class="line">hidden_size = <span class="number">25</span></div>
<div class="line">output_size = <span class="number">10</span></div>
<div class="line">learning_rate = <span class="number">1</span></div>
</pre></td></tr></table></figure>
<p>那么<strong>输入层和隐藏层</strong>之间的参数个数：$\theta_1=hidden_size \times (input_size+1)=25\times 401=10025$</p>
<p>输出层和隐藏层之间的参数个数：</p>
<p>$\theta_2=output_size \times (hidden_size+1)=10\times 26=260$</p>
<blockquote>
<p>上式中的+1是偏置神经元。</p>
</blockquote>
<p>所以这个神经网络的参数总数为：</p>
<p>$\theta=10025+260=10285$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">theta = np.random.randn(hidden_size*(input_size+<span class="number">1</span>)+output_size*(hidden_size+<span class="number">1</span>))</div>
<div class="line">theta1 = theta[<span class="number">0</span>:hidden_size*(input_size+<span class="number">1</span>)]</div>
<div class="line">theta1 = theta1.reshape(hidden_size,input_size+<span class="number">1</span>)</div>
<div class="line">theta2 = theta[hidden_size*(input_size+<span class="number">1</span>):]</div>
<div class="line">theta2 = theta2.reshape(output_size,hidden_size+<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<p>不要将权重参数初始化为0</p>
</blockquote>
<h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">displayData</span><span class="params">(self,example_width=None, figsize=<span class="params">(<span class="number">10</span>, <span class="number">10</span>)</span>)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Displays 2D data stored in X in a nice grid.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="comment"># Compute rows, cols</span></div>
<div class="line">    rand_indices = np.random.choice(len(self.y), <span class="number">100</span>, replace=<span class="keyword">False</span>)</div>
<div class="line">    X = self.x[rand_indices, :]</div>
<div class="line">    <span class="keyword">if</span> X.ndim == <span class="number">2</span>:</div>
<div class="line">        m, n = X.shape</div>
<div class="line">    <span class="keyword">elif</span> X.ndim == <span class="number">1</span>:</div>
<div class="line">        n = X.size</div>
<div class="line">        m = <span class="number">1</span></div>
<div class="line">        X = X[<span class="keyword">None</span>]  <span class="comment"># Promote to a 2 dimensional array</span></div>
<div class="line">    <span class="keyword">else</span>:</div>
<div class="line">        <span class="keyword">raise</span> IndexError(<span class="string">'Input X should be 1 or 2 dimensional.'</span>)</div>
<div class="line"></div>
<div class="line">    example_width = example_width <span class="keyword">or</span> int(np.round(np.sqrt(n)))</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Compute number of items to display</span></div>
<div class="line">    display_rows = int(np.floor(np.sqrt(m)))</div>
<div class="line">    display_cols = int(np.ceil(m / display_rows))</div>
<div class="line"></div>
<div class="line">    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)</div>
<div class="line">    fig.subplots_adjust(wspace=<span class="number">0.025</span>, hspace=<span class="number">0.025</span>)</div>
<div class="line"></div>
<div class="line">    ax_array = [ax_array] <span class="keyword">if</span> m == <span class="number">1</span> <span class="keyword">else</span> ax_array.ravel()</div>
<div class="line"></div>
<div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(ax_array):</div>
<div class="line">        <span class="comment"># Display Image</span></div>
<div class="line">        h = ax.imshow(X[i].reshape(example_width, example_width, order=<span class="string">'F'</span>),</div>
<div class="line">                      cmap=<span class="string">'Greys'</span>, extent=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div>
<div class="line">        ax.axis(<span class="string">'off'</span>)</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>

</div></div>
<h4 id="前向传播-2"><a href="#前向传播-2" class="headerlink" title="前向传播"></a>前向传播</h4><p>输入是$x$，其维度是（$batch_size, 400$），我们需要对其插入一列全1偏置向量，得到新输入$a_1$变成（$batch_size, 401$）维：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">row,col = np.shape</div>
<div class="line">a1 = np.insert(x,np.ones(row),axis=<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<p>那么先全连接乘法，再对其使用激活函数：</p>
<p>$z_2=a1*theta1^T$</p>
<p>$a_2=sigmoid(z_2)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">z2 = a1.dot(theta1.T)</div>
<div class="line">a2 = sigmoid(z2)</div>
</pre></td></tr></table></figure>
<p>得到隐层输出$a_2$之后，我们计算输出层：</p>
<p>$a_2$的大小是（$batch_size,25$），我们需要插入偏置向量，变成$(batch_size,26)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">a2 = np.insert(a2,np.ones(row),axis=<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<p>$z_3=a2*theta2^T$</p>
<p>$ h=sigmoid(z_3)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">z3 = a2.dot(theta2.T)</div>
<div class="line">h = sigmoid(z3)</div>
</pre></td></tr></table></figure>
<p>这样我们就完成了前向传播的计算。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagate</span><span class="params">(x,theta1,theta2)</span>:</span></div>
<div class="line">    <span class="comment">#x_shape = (5000,400)</span></div>
<div class="line">    <span class="comment"># theta1_shape = [hidden_size, input_size+1]</span></div>
<div class="line">    <span class="comment"># theta1_shape = [num_labels,hidden_size+1]</span></div>
<div class="line">    row,col = np.shape(x)</div>
<div class="line">    a1 = np.insert(x,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>)</div>
<div class="line">    z2 = a1.dot(theta1.T)</div>
<div class="line">    a2 = sigmoid(z2)</div>
<div class="line">    a2 = np.insert(a2,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>)</div>
<div class="line">    z3 = a2.dot(theta2.T)</div>
<div class="line">    h = sigmoid(z3)</div>
<div class="line">    <span class="keyword">return</span> a1,z2,a2,z3,h</div>
</pre></td></tr></table></figure>

</div></div>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>我们利用神经网络进行分类任务，所以我们使用交叉熵来衡量损失：<a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" target="_blank" rel="noopener">ref</a> </p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2018-09-02 at 4.03.40 PM.png" alt="Screen Shot 2018-09-02 at 4.03.40 PM"></p>
<p>上式是增加了正则化项的损失函数，其中$m$是样本数，即$batch_{size}$；$K$是输出神经元数，即$output_{size}$；$\lambda$是学习速率；需要注意的是，正则化项没有偏置参数。</p>
<blockquote>
<p><a href="https://blog.csdn.net/sinat_29819401/article/details/60885679" target="_blank" rel="noopener">why regularization</a> </p>
<p>除了偏置神经元的权重之外，我们对网络的所有神经元的权重平方和作为正则化项。在求损失函数对权重的梯度时，在原来的梯度计算基础上，加上$\frac{\lambda}{m}\theta$。</p>
<p>回忆起$L_2$的梯度更新公式：</p>
<script type="math/tex; mode=display">
\frac{\partial Cost}{\partial \theta}=\frac{\partial Cost}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial \theta}=(h-y)\sigma'(h)x</script><p>其中$L_2$的$\frac{\partial Cost}{\partial h}=(h-y)$，但是对于交叉熵而言$\frac{\partial Cost}{\partial h}=\frac{h-y}{h(1-h)}=\frac{h-y}{\sigma’(h)}$</p>
<p>$\frac{\partial Cost}{\partial h}$代入上式：</p>
<script type="math/tex; mode=display">
\frac{\partial Cost}{\partial \theta}=\frac{\partial Cost}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial \theta}=\frac{h-y}{\sigma'(h)}\sigma'(h)x=(h-y)x</script><p>可以看出，偏导数是由$(h-y)$控制，模型的输出$h$与标签$y$之间的差异越大，偏导数就会越大，学习就会越快。</p>
</blockquote>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(thetas,x,y)</span>:</span></div>
<div class="line">    row, col = np.shape(x)</div>
<div class="line">    theta1 = thetas[:hidden_size*(input_size+<span class="number">1</span>)].reshape(hidden_size,input_size+<span class="number">1</span>)</div>
<div class="line">    theta2 = thetas[hidden_size*(input_size+<span class="number">1</span>   ):].reshape(num_labels,hidden_size+<span class="number">1</span>)</div>
<div class="line">    a1, z2, a2, z3, h = forward_propagate(x,theta1,theta2)</div>
<div class="line">    <span class="comment"># err = (h-y)**2</span></div>
<div class="line">    <span class="comment"># err = 0.5*sum(sum(err)/len(x))</span></div>
<div class="line">    l1 = -y*(np.log(h))</div>
<div class="line">    l2 = (y<span class="number">-1</span>)*np.log(<span class="number">1</span>-h)</div>
<div class="line">    err = l1+l2</div>
<div class="line">    reg_err = learning_rate/(<span class="number">2</span>*row)*(np.sum(theta1[:,<span class="number">1</span>:]**<span class="number">2</span>)+np.sum(theta2[:,<span class="number">1</span>:]**<span class="number">2</span>))</div>
<div class="line">    <span class="keyword">return</span> np.mean(err)+reg_err,a1,z2,a2,z3,h</div>
</pre></td></tr></table></figure>
<blockquote>
<p>初始时，err=0.6383642819248599；reg_err=0.016015625000000002</p>
</blockquote>

</div></div>
<h4 id="反向传播-2"><a href="#反向传播-2" class="headerlink" title="反向传播"></a>反向传播</h4><p>反向传播不断更新参数使样本的损失函数越来越小，不过我们需要一个工具计算激活函数的梯度：</p>
<script type="math/tex; mode=display">
sigmoid(z)=\frac{1}{1+e^{-z}}</script><p>其梯度为：</p>
<script type="math/tex; mode=display">
sigmoid'(z)=z(1-z)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span><span class="params">(z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> z*(<span class="number">1</span>-z)</div>
</pre></td></tr></table></figure>
<p>接下来我们需要逐层计算损失：</p>
<p>输出层损失：</p>
<p>$err_3=(a_3-y)*sigmoid_{gradient}(a_3)$，$shape=(batch_size,output_size)$</p>
<p>隐藏层与输出层之间的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">z2 = np.insert(z2,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>) <span class="comment">#shape = (batch_size,hidden_size+1)</span></div>
</pre></td></tr></table></figure>
<p>$\delta_3=[err_3]^T.*z_2$，$shape=(output_size,hidden_size+1)$</p>
<p>隐藏层损失：</p>
<p>$err_2=sigmoid_{gradient}(a_2)<em>err_3.  </em>theta_2$,$shape=(batch_size,hidden_size+1)$</p>
<p>输入层与隐藏层之间的梯度：</p>
<p>$\delta_2=[err_2]^T.*a_1$，$shape=(hidden_size,input_size+1)$</p>
<h3 id="KERAS版本实现"><a href="#KERAS版本实现" class="headerlink" title="KERAS版本实现"></a>KERAS版本实现</h3><p><code>100epoch</code>训练之后的正确率为<code>94.56%</code>。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</div>
<div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div>
<div class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</div>
<div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ann</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div>
<div class="line">        self.input_size = <span class="number">400</span></div>
<div class="line">        self.hidden_size = <span class="number">25</span></div>
<div class="line">        self.output_size = <span class="number">10</span></div>
<div class="line"></div>
<div class="line">        opt = Adam(lr=<span class="number">0.0002</span>,beta_1=<span class="number">0.5</span>)</div>
<div class="line"></div>
<div class="line">        self.ann = self.make_model()</div>
<div class="line">        self.ann.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div>
<div class="line">                         optimizer=opt,</div>
<div class="line">                         metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(self)</span>:</span></div>
<div class="line">        inputs = Input((self.input_size,))</div>
<div class="line">        h = Dense(self.hidden_size,activation=<span class="string">'sigmoid'</span>)(inputs)</div>
<div class="line">        outputs = Dense(self.output_size,activation=<span class="string">'sigmoid'</span>)(h)</div>
<div class="line">        model = Model(inputs,outputs)</div>
<div class="line">        model.summary()</div>
<div class="line">        <span class="keyword">return</span> model</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></div>
<div class="line">        data = loadmat(<span class="string">'dataset/ex3data1.mat'</span>)</div>
<div class="line">        X = data[<span class="string">'X'</span>]</div>
<div class="line">        y = data[<span class="string">'y'</span>]</div>
<div class="line">        encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</div>
<div class="line">        y = encoder.fit_transform(y)</div>
<div class="line"></div>
<div class="line">        results = self.ann.fit(X,y,epochs=<span class="number">100</span>)</div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    ann = Ann()</div>
<div class="line">    ann.train()</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h3><p>实现了神经网络的前向传播和后向传播之后，为保证我们的代码能正确运行，在进行神经网络训练数据之前，我们需要检查梯度是否正确。最简单的方法是将神经网络的梯度与我们手工计算得梯度进行比较。</p>
<script type="math/tex; mode=display">
\frac{\partial Cost}{\partial \theta}=\frac{Cost(\theta +\epsilon)-Cost(\theta -\epsilon)}{2\epsilon}</script><p>也就是说，反向传播计算出来的梯度应该近似上述式子计算出来的梯度，否则我们的代码有误。</p>
<p>一般的，我们设置$\epsilon=10^{-7}$，而两个梯度之间的差异应该小于$10^{-7}$。</p>
<ol>
<li><a href="https://www.leiphone.com/news/201711/MWEDFvRMdOyN7Evm.html" target="_blank" rel="noopener">神经网络中容易被忽视的基础知识</a> </li>
</ol>
<h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p><a href="https://towardsdatascience.com/deep-learning-concepts-part-2-9aed45e5e7ed" target="_blank" rel="noopener">to do</a> </p>
<p>y为ground_truth, a为预测。</p>
<h2 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h2><p>For each sample $i$, </p>
<script type="math/tex; mode=display">
L_i = \sum_{j\ne y_i}\left\{

\begin{align} 

0& & \text{if   }  s_{y_i}\ge s_j+1 \\

s_j-s_{y_i}+1& &\text{if otherwise}

\end{align}
=\sum_{j\ne y_i}max(0,s_j-s_{y_i}+1)
\right.</script><p>其中，$y_i$是groundtruth label，$s_{y_i}$是分类器给真实分类的分数，$s_j$是分类器给其他类的分数，该损失函数鼓励真实分类的分数大于其他分类的分数+1。</p>
<p><img src="/2018/07/24/Neural-Network/v2-3c6aa9626ee8e4609b0d7c5712baf624_hd.jpg" alt="2-3c6aa9626ee8e4609b0d7c5712baf624_h"></p>
<p><img src="/2018/07/24/Neural-Network/v2-9618ff8556f19a00ede416312f3e57e6_r.jpg" alt="2-9618ff8556f19a00ede416312f3e57e6_"></p>
<p>The two given figures are plots of function $f(x)=max(0,1-x)$ .We can see from the second figure that when x is between 0 and 1, the loss is in range $[0,1]$. </p>
<p>Say we have an image classification and we use it to classify three images, like the following:</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-03-06 at 4.05.12 PM.png" alt="creen Shot 2019-03-06 at 4.05.12 P"></p>
<p>The output of classification is a 3d vector, each element of the vector is the probability of image classification. For example, for the cat image, the vector is $[3.2, 5.1, -1.7]$, meaning this picture is cat with probability 3.2, is car with prob 5.1 and frog with prob -1.7.</p>
<p> Then, the $s_{y_i}=3.2$, $s_1=5.1$ and $s_2=-1.7$，so the loss is:</p>
<script type="math/tex; mode=display">
L_{cat}=max(0,5.1+1-3.2)+max(0,-1.7+1-3.2)=max(0,2.9)+max(0,-3.9)=2.9</script><p>Similarily, $L_{car}=0$, $L_{frog}=12.9$.</p>
<p>So the overall loss is:</p>
<script type="math/tex; mode=display">
L=\frac{1}{N}\sum_{i=1}^{N}L_i\\
L=\frac{(2.9+0+12.9)}{3}=5.27</script><blockquote>
<ol>
<li><p>What happens to loss if car scores change a bit?</p>
<p>The answer is the loss will not change. The SVM loss only cares about getting the correct score to be greater than one more than the incorrect scores. But in this case, the car score is already quite a bit large than the others. So if the scores for this class changes, this margin of one will still be retained and the loss will not change.</p>
</li>
<li><p>What is the min/max possible loss?</p>
<p>The min loss is 0 because across all the classes, if our correct score was much larger then we will incur zero loss across all the classes. </p>
<p>The max loss is infinite. According to the hinge loss figure, if correct score goes very very negative, then we could incur potentially infinite loss.</p>
</li>
<li><p>At initialization W is small so all $s\approx 0$. What is the loss?</p>
<p>The answer is the number of class minus one.</p>
</li>
<li><p>What if the sum was over all classes?(including $j=y_j$).</p>
<p>The loss increases by one.</p>
</li>
<li><p>What if we used mean instead of sum?</p>
<p>The answer is that it doesn’t change. We just rescale the whole loss function by a constant value and we don’t care about the true value of the loss.</p>
</li>
<li><p>What if we used $L_i=\sum_{j\ne y_i}max(0,s_j-s_{y_i}+1)^2$?</p>
<p>The answer is it is different. We are kind of changing the trade-offs between good and badness in kind of nonlinear way, and this would end up actually computing a different loss function.</p>
</li>
<li><p>Suppose we found a W such that $L=0$. Is this W unique?</p>
<p>The answer is no. $2W$ is also has $L=0$.</p>
</li>
</ol>
</blockquote>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>The whole idea of regularization is you try to penalize the complexity of the model, instead of explicitly trying to fit the training data.</p>
<script type="math/tex; mode=display">
L=\frac{1}{N}\sum_{i=1}^{N}\sum_{j\ne y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)+\lambda R(W)</script><p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-03-07 at 4.24.27 PM.png" alt="creen Shot 2019-03-07 at 4.24.27 P"></p>
<blockquote>
<p>$L1$ Regularization prefer the sparse weight vector, which \</p>
</blockquote>
<h3 id="Derivative-of-Hinge-Loss-ref1"><a href="#Derivative-of-Hinge-Loss-ref1" class="headerlink" title="Derivative of Hinge Loss ref1"></a>Derivative of Hinge Loss <a href="https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html" target="_blank" rel="noopener">ref1</a></h3><p>Say for a single datapoint $(x_i,y_i)$, we have the following hinge loss:</p>
<script type="math/tex; mode=display">
L_i=\sum_{j\ne y_i}^{c} max(0, s_j+1-s_i)</script><p>where $c$ is the class number and $s_j=w_j^T x_i$ is the score for the $j{th}$ class. What we do here is to iterate scores for all classes and compare them with the score of truth class.</p>
<p>To spread out, </p>
<script type="math/tex; mode=display">
\begin{align*}
L_i = &\max(0,1+w_1x_i-w_{y_i}x_i) + \\
 &\max(0, 1+w_2x_i-w_{y_i}x_i) + \\
& \quad \quad \quad \quad \quad \quad \vdots \\
&\max(0, 1+w_cx_i-w_{y_i}x_i)
\end{align*}</script><p>If $(w_jx_i+1-w_{y_i}x_i)&gt;0$, $\frac{dL_i}{dw_j}=x_i$.</p>
<p>But for $w_{y_i}$,</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{dL_i}{dw_{y_i}} &= - \sum_{j\neq y_i} \mathbb{1}(x_iw_j - x_iw_{y_i} + \Delta > 0) x_i \tag{3}
\end{align*}</script><p>So the non-vectorized implementation is:</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div>
<div class="line">  <span class="string">"""</span></div>
<div class="line"><span class="string">  Structured SVM loss function, naive implementation (with loops).</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></div>
<div class="line"><span class="string">  of N examples.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Inputs:</span></div>
<div class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></div>
<div class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></div>
<div class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></div>
<div class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></div>
<div class="line"><span class="string">  - reg: (float) regularization strength</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Returns a tuple of:</span></div>
<div class="line"><span class="string">  - loss as single float</span></div>
<div class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></div>
<div class="line"><span class="string">  """</span></div>
<div class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></div>
<div class="line"></div>
<div class="line">  <span class="comment"># compute the loss and the gradient</span></div>
<div class="line">  num_classes = W.shape[<span class="number">1</span>]</div>
<div class="line">  num_train = X.shape[<span class="number">0</span>]</div>
<div class="line">  loss = <span class="number">0.0</span></div>
<div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</div>
<div class="line">    scores = X[i].dot(W)</div>
<div class="line">    correct_class_score = scores[y[i]]</div>
<div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</div>
<div class="line">      <span class="keyword">if</span> j == y[i]:</div>
<div class="line">        <span class="keyword">continue</span></div>
<div class="line">      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></div>
<div class="line">      <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</div>
<div class="line">        loss += margin</div>
<div class="line">        dW[:,j] += X[i]</div>
<div class="line">        dW[:,y[i]] -= X[i]</div>
<div class="line"></div>
<div class="line">  <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></div>
<div class="line">  <span class="comment"># to be an average instead so we divide by num_train.</span></div>
<div class="line">    loss /= num_train</div>
<div class="line">    dW /= num_train</div>
<div class="line">  <span class="comment"># Add regularization to the loss.</span></div>
<div class="line">    loss += reg * np.sum(W * W)</div>
<div class="line">    dW += <span class="number">2</span>*reg*W</div>
<div class="line">  <span class="keyword">return</span> loss, dW</div>
</pre></td></tr></table></figure>

</div></div>
<p>The vectorized implementation is:</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div>
<div class="line">  <span class="string">"""</span></div>
<div class="line"><span class="string">  Structured SVM loss function, vectorized implementation.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Inputs and outputs are the same as svm_loss_naive.</span></div>
<div class="line"><span class="string">  """</span></div>
<div class="line">  loss = <span class="number">0.0</span></div>
<div class="line">  dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></div>
<div class="line">  score = X.dot(W)</div>
<div class="line">  num_train = np.shape(X)[<span class="number">0</span>]</div>
<div class="line">  num_class = np.shape(W)[<span class="number">1</span>]</div>
<div class="line">  correct_score = score[np.arange(num_train),y].reshape(num_train,<span class="number">-1</span>)</div>
<div class="line">  loss_ = score - correct_score+<span class="number">1</span></div>
<div class="line">  loss_[np.arange(num_train),y] = <span class="number">0</span></div>
<div class="line">  loss = np.sum(loss_[loss_&gt;<span class="number">0</span>])/num_train</div>
<div class="line">  loss += reg * np.sum(W * W)</div>
<div class="line">  mask = np.zeros_like(score)</div>
<div class="line">  mask[score&gt;<span class="number">0</span>] = <span class="number">1</span></div>
<div class="line">  mask[:,y] -= np.sum(mask,axis=<span class="number">1</span>)</div>
<div class="line">  dW += X.T.dot(mask)</div>
<div class="line">  dW /= num_train</div>
<div class="line">  dW += <span class="number">2</span>*reg*W</div>
<div class="line">  <span class="keyword">return</span> loss, dW</div>
</pre></td></tr></table></figure>

</div></div>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>对于多分类任务，模型的输出是一个多维向量，向量的每一位表示该输出属于某一类的概率。softmax则将输出建模成一个分布：</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-03-08 at 2.21.08 PM.png" alt="creen Shot 2019-03-08 at 2.21.08 P"></p>
<p>其中，$x_i$是第$i$个输入，$k$表示某一个类，则上式计算该输入属于某一个类的概率。</p>
<p>那么我们的损失函数就变成了：</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-03-08 at 2.24.14 PM.png" alt="creen Shot 2019-03-08 at 2.24.14 P"></p>
<p>对于每一个样本，我们最大化它属于正确类的概率。因为$ 0\le P(Y=y_i|X=x_i)\le 1$,所以$logP&lt;0$；我们追求的是$P()$越大，$L_i$越小，但是对于$logP$, 恰恰相反，所以对损失函数取负。</p>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-03-08 at 2.29.26 PM.png" alt="creen Shot 2019-03-08 at 2.29.26 P"></p>
<p>对于上面的例子，分类器的输出是$(3.2,5.1,-1.7)$,经过softmax函数计算，我们得到$(0.13,0.87,0.0)$，那么该样本的损失为$L_i=-log(0.13)$.</p>
<blockquote>
<ol>
<li><p>What is the min/max possible loss $L_i$?</p>
<p>The min loss is 0 when we classify the input correctly with a extremely high score. The max loss is $\infin$ when we classify the input to the correct category with a extremely low score. And according to the figure, it is infinite.</p>
</li>
<li><p>Usually at initialization $W$ is small so all $s\approx 0$. What is the loss?</p>
<p>The answer is $-log(\frac{1}{C})$, where $C$ is the class number.</p>
</li>
</ol>
</blockquote>
<p><img src="/2018/07/24/Neural-Network/Screen Shot 2019-03-08 at 2.34.05 PM.png" alt="creen Shot 2019-03-08 at 2.34.05 P"></p>
<h2 id="Softmax函数-ref"><a href="#Softmax函数-ref" class="headerlink" title=" Softmax函数 ref"></a><a href="https://deepnotes.io/softmax-crossentropy" target="_blank" rel="noopener"> Softmax函数</a> <a href="http://bigstuffgoingon.com/blog/posts/softmax-loss-gradient/#The-gradient-of-the-softmax-classifier-loss-function" target="_blank" rel="noopener">ref</a></h2><p>Softmax function takes an N-dimensional vector of real number and transforms it into a vector of real number in range $(0,1)$ which add up to 1. For each output element $a_j$ in the last layer:</p>
<script type="math/tex; mode=display">
f(a_j)=\frac{e^{a_j}}{\sum_{i=1}^{k}e^{a_i}}</script><p>In python, we have the code for softmax function as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div>
<div class="line">    exps = np.exp(x)</div>
<div class="line">    <span class="keyword">return</span> exps/np.sum(exps)</div>
</pre></td></tr></table></figure>
<p>We have to note that the numerical range of float number in numpy is limited. For <code>float64</code> the upper bound is $10^{308}$. For exponential, it is not difficult to overshoot that limit, in which case python returns <code>nan</code>.</p>
<p>To make our softmax function numerically stable, we simply normalize the values in the vector, by multiplying the numerator and denominator with a constant $C$.</p>
<script type="math/tex; mode=display">
\begin{align}
p_i &= \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}} \\
&= \frac{Ce^{a_i}}{C\sum_{k=1}^N e^{a_k}} \\
&= \frac{e^{a_i + \log(C)}}{\sum_{k=1}^N e^{a_k + \log(C)}} \\
\end{align}</script><p>We can choose an arbitrary value for $log(C)$ term, but generally $log(C)=−max(a)$ is chosen, as it shifts all of elements in the vector to negative to zero, and negatives with large exponents saturate to zero rather than the infinity, avoiding overflowing and resulting in <code>nan</code>.</p>
<p>The code for our stable softmax is as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div>
<div class="line">    exps = np.exp(x-np.max(x))</div>
<div class="line">    <span class="keyword">return</span> exps/np.sum(exps)</div>
</pre></td></tr></table></figure>
<h3 id="Derivative-of-softmax"><a href="#Derivative-of-softmax" class="headerlink" title="Derivative of softmax"></a>Derivative of softmax</h3><p>Due to the desirable property of softmax function outputting a probability distribution, we use it as the final layer in neural networks. For this we need to calculate the derivative or gradient and pass it back to the previous layer during backpropagation.</p>
<p>For each node $a_j$ in the output layer, we want to calculate:</p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} = \frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}</script><p>If $i=j$:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}&= \frac{e^{a_i} \sum_{k=1}^N e^{a_k} - e^{a_j}e^{a_i}}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\
&= \frac{e^{a_i} \left( \sum_{k=1}^N e^{a_k} - e^{a_j}\right )}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\
&= \frac{ e^{a_j} }{\sum_{k=1}^N e^{a_k} } \times \frac{\left( \sum_{k=1}^N e^{a_k} - e^{a_j}\right ) }{\sum_{k=1}^N e^{a_k} } \\
&= p_i(1-p_j)
\end{align}</script><p>For $i\ne j$:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}&= \frac{0 - e^{a_j}e^{a_i}}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\
&= \frac{- e^{a_j} }{\sum_{k=1}^N e^{a_k} } \times \frac{e^{a_i} }{\sum_{k=1}^N e^{a_k} } \\
&= - p_j.p_i
\end{align}</script><p>So the derivative of the softmax function is given as,</p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} = 
\begin{cases}p_i(1-p_j) &  if & i=j \\
-p_j.p_i & if & i \neq j
\end{cases}</script><p>Or using Kronecker delta $\delta{ij} = \begin{cases} 1 &amp; if &amp; i=j \\ 0 &amp; if &amp; i\neq j \end{cases}$ </p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} =  p_i(\delta_{ij}-p_j)</script><h3 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h3><p>Corss entropy loss indicates the distance between what the model belives the output distribution should be, and what the original distribution really is. It is defined as:</p>
<script type="math/tex; mode=display">
H(y,p) = - \sum_i y_i log(p_i)</script><p>It is widely used as an alternative of squared error. It is used when node activations can be understand as representing the probability that each hypothesis might be true, i.e. when the output is a probability is a probability distribution. Thus it is used as a loss function in neural networks which have softmax activations in the output layer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    X is the output from fully connected layer (num_examples x num_classes)</span></div>
<div class="line"><span class="string">    y is labels (num_examples x 1)</span></div>
<div class="line"><span class="string">    	Note that y is not one-hot encoded vector. </span></div>
<div class="line"><span class="string">    	It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    l = y.shape[<span class="number">0</span>]</div>
<div class="line">    prob = softmax(x)</div>
<div class="line">    prob_of_groundtruth = [row[y[i]] <span class="keyword">for</span> i,row <span class="keyword">in</span> enumerate(prob)]</div>
<div class="line">    log_likelihood = -np.log(prob_of_groundtruth)</div>
<div class="line">    loss = np.sum(log_likelihood)</div>
<div class="line">    <span class="keyword">return</span> loss</div>
</pre></td></tr></table></figure>
<h3 id="Derivative-of-Cross-Entropy-Loss-with-Softmax"><a href="#Derivative-of-Cross-Entropy-Loss-with-Softmax" class="headerlink" title="Derivative of Cross Entropy Loss with Softmax"></a>Derivative of Cross Entropy Loss with Softmax</h3><p>Cross Entropy Loss with Softmax function are used as the output layer extensively. Now we use the derivative of softmax that we derived earlier to derive the derivative of the cross entropy loss function.</p>
<p>For each sample $(x_i,y_i)$, there are $c$ classes</p>
<script type="math/tex; mode=display">
\begin{align}
L_i &= - \sum_i^c y_i log(p_i) \\
\end{align}</script><p>Then for each output node $o_i$, i.e., score of the class $i$ :</p>
<script type="math/tex; mode=display">
\begin{align}

\frac{\partial L_i}{\partial o_i} &= - \sum_k y_k \frac{\partial log(p_k)}{\partial o_i } \\
&= - \sum_k y_k \frac{\partial log(p_k)}{\partial p_k} \times \frac{\partial p_k}{ \partial o_i} \\
&= - \sum y_k \frac{1}{p_k} \times \frac{\partial p_k}{\partial o_i} \\

\end{align}</script><p>From derivative of softmax we derived earlier that $\frac{\partial p_i}{\partial a_j} = \begin{cases}p_i(1-p_j)&amp;  if &amp; i=j \\ -p_j.p_i &amp; if &amp; i \neq j\end{cases}$,</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L}{\partial o_i}  &= -y_i(1-p_i) - \sum_{k\neq i} y_k \frac{1}{p_k}(-p_k.p_i) \\
&= -y_i(1-p_i) + \sum_{k \neq 1} y_k.p_i \\
&= - y_i + y_ip_i + \sum_{k \neq 1} y_k.p_i \\
&= p_i\left( y_i +  \sum_{k \neq 1} y_k\right) - y_i \\
&= p_i\left( y_i +  \sum_{k \neq 1} y_k\right)  - y_i
\end{align}</script><p>$y$ is a one hot encoded vector for the label, so $\sum_{k}y_k =1$, and $y_i + \sum_{k\ne i}y_k =1$. So we have,</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial o_i} = p_i - y_i</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">delta_cross_entropy</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    X is the output from fully connected layer (num_examples x num_classes)</span></div>
<div class="line"><span class="string">    y is labels (num_examples x 1)</span></div>
<div class="line"><span class="string">    	Note that y is not one-hot encoded vector. </span></div>
<div class="line"><span class="string">    	It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    m = y.shape[<span class="number">0</span>]</div>
<div class="line">    prob = softmax(x)</div>
<div class="line">    prob_of_groundtruth = [row[y[i]] <span class="keyword">for</span> i,row <span class="keyword">in</span> enumerate(prob)]</div>
<div class="line">    prob_of_groundtruth -= <span class="number">1</span></div>
<div class="line">    <span class="keyword">return</span> prob_of_groundtruth/m</div>
</pre></td></tr></table></figure>
<p>In summary, the overall loss of cross entropy loss with sigmoid function is as following:</p>
<p>for each sample $(x_i,y_i)$, </p>
<script type="math/tex; mode=display">
Loss=-\frac{1}{n}\sum_{i}^{n}y_ilog(p_i) + \frac{1}{2}\sum_{m}\sum_{n}w_{m,n}^{2}\\
p_i=\frac{e^{o_i}}{\sum_{k}^{c}e^{o_k}}</script><p>where $o_i$ is the non-logit output, $p_i$ is the probability of being class $i$, $y_i$ is the groundtruth class. The second part of $Loss$ is the regularization.</p>
<p>The plaint implementation is as following:</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></div>
<div class="line">  <span class="string">"""</span></div>
<div class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></div>
<div class="line"><span class="string">  of N examples.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Inputs:</span></div>
<div class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></div>
<div class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></div>
<div class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></div>
<div class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></div>
<div class="line"><span class="string">  - reg: (float) regularization strength</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Returns a tuple of:</span></div>
<div class="line"><span class="string">  - loss as single float</span></div>
<div class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></div>
<div class="line"><span class="string">  """</span></div>
<div class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></div>
<div class="line">  loss = <span class="number">0.0</span></div>
<div class="line">  dW = np.zeros_like(W)</div>
<div class="line">  train_num = X.shape[<span class="number">0</span>]</div>
<div class="line">  class_num = W.shape[<span class="number">1</span>]</div>
<div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(train_num):</div>
<div class="line">    sample = X[i] <span class="comment"># (1,D)</span></div>
<div class="line">    score = X[i].dot(W) <span class="comment"># (1,D)*(D,C) = (1,C)</span></div>
<div class="line">    normalized_score = score-max(score)</div>
<div class="line">    exp_score = np.exp(normalized_score)</div>
<div class="line">    prob = exp_score/sum(exp_score)</div>
<div class="line">    loss += -np.log(prob[y[i]])</div>
<div class="line">    </div>
<div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(class_num):</div>
<div class="line">        <span class="keyword">if</span> k==y[i]: <span class="comment">#correct class</span></div>
<div class="line">            dW[:,y[i]] += (prob[y[i]]<span class="number">-1</span>)*sample</div>
<div class="line">        <span class="keyword">else</span>:</div>
<div class="line">            dW[:,k] += prob[k]*sample</div>
<div class="line">  loss = loss/train_num + <span class="number">0.5</span>*reg*np.sum(W*W) <span class="comment"># must use np.sum() instead of sum()</span></div>
<div class="line">  dw = dW/train_num + reg*W</div>
<div class="line"></div>
<div class="line">  <span class="keyword">return</span> loss, dW</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></div>
<div class="line">  <span class="string">"""</span></div>
<div class="line"><span class="string">  Softmax loss function, vectorized version.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></div>
<div class="line"><span class="string">  """</span></div>
<div class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></div>
<div class="line">  loss = <span class="number">0.0</span></div>
<div class="line">  dW = np.zeros_like(W)</div>
<div class="line">  train_num = X.shape[<span class="number">0</span>]</div>
<div class="line">  class_num = W.shape[<span class="number">1</span>]</div>
<div class="line">  score = X.dot(W) <span class="comment">#(N,C)</span></div>
<div class="line">  score -= np.max(score,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>) <span class="comment"># normalized</span></div>
<div class="line">  prob = np.exp(score)/np.sum(np.exp(score),axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>) <span class="comment">#(N,C)</span></div>
<div class="line">  loss = -np.sum(np.log(prob[np.arange(train_num),y])) <span class="comment">#for ith row of prob, pick y[i] value of that row</span></div>
<div class="line">  loss = loss/train_num + <span class="number">0.5</span>*reg*np.sum(W*W)</div>
<div class="line">    </div>
<div class="line">  y_ = np.zeros_like(prob)</div>
<div class="line">  y_[np.arange(train_num),y] = <span class="number">1</span></div>
<div class="line">  dW = X.T.dot(prob-y_)</div>
<div class="line">  dw = dW/train_num + reg*W</div>
<div class="line">  </div>
<div class="line">  <span class="keyword">return</span> loss, dW</div>
</pre></td></tr></table></figure>

</div></div>
<h2 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h2><script type="math/tex; mode=display">
mse = \sum_{i=1}^{n}(y-a)^2</script><h2 id="交叉熵误差-cross-entropy"><a href="#交叉熵误差-cross-entropy" class="headerlink" title="交叉熵误差(cross entropy)"></a>交叉熵误差(cross entropy)</h2><script type="math/tex; mode=display">
C =\frac{1}{n}\sum_{i=1}^{n}-[y\cdot log(a)+(1-y)\cdot log(1-a)]</script><h1 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h1><p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">zhihu</a> <a href="https://blog.csdn.net/whitesilence/article/details/75667002" target="_blank" rel="noopener">过程</a> </p>
<h1 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h1><p>pooling的本质是降维，</p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">ref1</a> <a href="https://www.cnblogs.com/makefile/p/dropout.html" target="_blank" rel="noopener">ref2</a> <a href="https://www.cnblogs.com/santian/p/5457412.html" target="_blank" rel="noopener">ref3</a> <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">ref4</a> <a href="https://yq.aliyun.com/articles/68901" target="_blank" rel="noopener">ref4-ch</a> <a href="http://lib.csdn.net/article/deeplearning/51257" target="_blank" rel="noopener">regularization vs dropout</a> </p>
<p>dropout的引入是为了防止网络的过拟合，本质是正则化。</p>
<p>神经元以概率$p$被保留，当一个神经元被drop out，该神经元的输出就被置0</p>
<p>被drop out的神经元对训练阶段(包括前向传播和后向传播)没有任何贡献，</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Neural-Network/" rel="tag"># Neural Network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/23/Keras/" rel="next" title="Keras">
                <i class="fa fa-chevron-left"></i> Keras
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/03/Training-Techiniques/" rel="prev" title="Training Techiniques">
                Training Techiniques <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">81</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">62</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络"><span class="nav-number">1.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#感知机模型"><span class="nav-number">1.1.</span> <span class="nav-text">感知机模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#二分类线性模型"><span class="nav-number">1.1.1.</span> <span class="nav-text">二分类线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机学习策略"><span class="nav-number">1.1.2.</span> <span class="nav-text">感知机学习策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑计算"><span class="nav-number">1.1.3.</span> <span class="nav-text">逻辑计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经元模型"><span class="nav-number">1.2.</span> <span class="nav-text">神经元模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络结构"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">1.3.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的矩阵表示"><span class="nav-number">1.3.2.</span> <span class="nav-text">神经网络的矩阵表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">1.3.3.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输出层"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">输出层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐藏层"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">隐藏层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法步骤"><span class="nav-number">1.4.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Python-实现"><span class="nav-number">1.5.</span> <span class="nav-text">Python 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Toy-Example"><span class="nav-number">1.5.1.</span> <span class="nav-text">Toy Example</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向传播-1"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播-1"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST手写体数字分类"><span class="nav-number">1.5.2.</span> <span class="nav-text">MNIST手写体数字分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据可视化"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">数据可视化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#前向传播-2"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播-2"><span class="nav-number">1.5.2.4.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KERAS版本实现"><span class="nav-number">1.5.3.</span> <span class="nav-text">KERAS版本实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度检查"><span class="nav-number">1.5.4.</span> <span class="nav-text">梯度检查</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#代价函数"><span class="nav-number">2.</span> <span class="nav-text">代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Hinge-Loss"><span class="nav-number">2.1.</span> <span class="nav-text">Hinge Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">2.1.1.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivative-of-Hinge-Loss-ref1"><span class="nav-number">2.1.2.</span> <span class="nav-text">Derivative of Hinge Loss ref1</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax"><span class="nav-number">2.2.</span> <span class="nav-text">Softmax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax函数-ref"><span class="nav-number">2.3.</span> <span class="nav-text"> Softmax函数 ref</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivative-of-softmax"><span class="nav-number">2.3.1.</span> <span class="nav-text">Derivative of softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Entropy-Loss"><span class="nav-number">2.3.2.</span> <span class="nav-text">Cross Entropy Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivative-of-Cross-Entropy-Loss-with-Softmax"><span class="nav-number">2.3.3.</span> <span class="nav-text">Derivative of Cross Entropy Loss with Softmax</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#均方误差"><span class="nav-number">2.4.</span> <span class="nav-text">均方误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉熵误差-cross-entropy"><span class="nav-number">2.5.</span> <span class="nav-text">交叉熵误差(cross entropy)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BatchNormalization"><span class="nav-number">3.</span> <span class="nav-text">BatchNormalization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pooling"><span class="nav-number">4.</span> <span class="nav-text">Pooling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout"><span class="nav-number">5.</span> <span class="nav-text">Dropout</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
