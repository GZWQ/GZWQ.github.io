<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,Regression,Logistic Regression," />










<meta name="description" content="用于分类任务的逻辑斯蒂回归。">
<meta name="keywords" content="Machine Learning,Regression,Logistic Regression">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic Regression">
<meta property="og:url" content="http://yoursite.com/2018/08/18/Logistic-Regression/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="用于分类任务的逻辑斯蒂回归。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-06-05%20at%2011.53.07%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-06-05%20at%2011.53.42%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-06-05%20at%2011.53.57%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-03-08%20at%202.21.08%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-03-08%20at%202.24.14%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-03-08%20at%202.29.26%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-03-08%20at%202.34.05%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-14%20at%205.48.16%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-14%20at%205.55.35%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/image-20180818165938933.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202018-08-18%20at%205.02.58%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202018-08-19%20at%208.23.02%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202018-08-19%20at%208.23.25%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/image-20180822205326946.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/image-20180822210526246.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-08%20at%2011.32.08%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808113730536.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808114529415.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808123047797.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/image-20180813110624641.png">
<meta property="og:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Linear-Regression/image-20180813111620624.png">
<meta property="og:updated_time" content="2019-06-05T18:04:10.390Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic Regression">
<meta name="twitter:description" content="用于分类任务的逻辑斯蒂回归。">
<meta name="twitter:image" content="http://yoursite.com/2018/08/18/Logistic-Regression/Screen%20Shot%202019-06-05%20at%2011.53.07%20AM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/18/Logistic-Regression/"/>





  <title>Logistic Regression | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/18/Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Logistic Regression</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-18T16:44:27-05:00">
                2018-08-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>用于分类任务的逻辑斯蒂回归。</p>
<a id="more"></a>
<h1 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>线性回归试图用一个线性函数的预测值取逼近真实值，那么是否能用一个线性函数的预测值去逼近真实值的衍生值呢？假设我们使用预测值去逼近一个对数函数$g(y)=In(y)$，那么$g(y)=f(y)$，</p>
<script type="math/tex; mode=display">
In(y)=w^Tx+b</script><p>这样我们就得到了对数线性回归，它实际上是试图让$e^{(w^Tx+b)}$去逼近$y$，本质上仍然是线性回归，但已是在求输入空间到输出空间的<strong>非线性</strong>函数的映射。如下图所示，指数空间的真实值通过对数函数映射到一个线性空间：</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-06-05 at 11.53.07 AM.png" alt="creen Shot 2019-06-05 at 11.53.07 A"></p>
<p>线性模型可以用于回归学习，但是如果要做的是分类任务该如何？启发自对数线性回归，我们可以找到一个类似对数函数的一个单调可微函数将分类任务的真实标记$y$映射到线性回归模型的预测值。</p>
<p>考虑二分类问题，其输出标记是$y\in\{0,1\}$，而线性回归模型的预测值$z=w^Tx+b$是实值，故使用一个阶跃函数，进行实值与离散值的映射：</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-06-05 at 11.53.42 AM.png" alt="creen Shot 2019-06-05 at 11.53.42 A"></p>
<p>若预测值$z$大于零就判为正例，小于零就判为负例，预测值为临界值零则任意判断，</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-06-05 at 11.53.57 AM.png" alt="creen Shot 2019-06-05 at 11.53.57 A"></p>
<p>因为阶跃函数是不连续的，所以我们使用了一个连续可微的函数替代函数：</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(w^Tx+b)}}</script><p>上式可变化为：</p>
<script type="math/tex; mode=display">
In\frac{y}{1-y}=w^Tx+b</script><p>如果将$y$看作样本$x$作为正例的概率，则$1-y$表示反例的概率，两者的比值反映了$x$作为正例的相对可能性，称为几率，对几率取对数称为对数几率。可以看出上式用线性回归模型的预测值去逼近真实标记的对数几率，因为对应模型称为逻辑斯蒂回归。需要注意的是虽然名字是”回归”，但是它是分类方法。</p>
<h2 id="算法求解"><a href="#算法求解" class="headerlink" title="算法求解"></a>算法求解</h2><p>如果将预测值$y$看作类后验概率估计$p(y=1|x)$，则上式：</p>
<script type="math/tex; mode=display">
In(\frac{p(y=1|x)}{p(y=0|x)})=w^Tx+b</script><p>又因为$p(y=1|x)+p(y=0|x)=1$，所以联合求解得到：</p>
<script type="math/tex; mode=display">
p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\
p(y=1|x)=\frac{1}{1+e^{w^Tx+b}}</script><p>可以通过极大似然法求解：</p>
<script type="math/tex; mode=display">
max \ P=\prod_{i=1}^{m}[p(y=1|x_i)^{y_i}p(y=0|x_i)^{(1-y_i)}]</script><p>即令每个样本属于其真实标记的概率越大越好，两边取对数：</p>
<script type="math/tex; mode=display">
max\ In(P)=\sum_{i=1}^{m}y_iIn[p(y=1|x_i)]+(1-y_i)In[p(y=0|x_i)]</script><p>如果真实标记$y_i=1$，则$y_iIn[p(y=1|x_i)]+(1-y_i)In[p(y=0|x_i)]=p(y=1|x_i)$；</p>
<p>如果真实标记$y_i=0$，则$y_iIn[p(y=1|x_i)]+(1-y_i)In[p(y=0|x_i)]=p(y=0|x_i)$；</p>
<p>即：</p>
<script type="math/tex; mode=display">
max \ In(Loss)=\sum_{i=1}^{m}y_i(w^Tx+b)+In(\frac{1}{e^{w^Tx+b}+1})</script><p>max上式等价于：</p>
<script type="math/tex; mode=display">
min \ In(Loss)=\sum_{i=1}^{m}-y_i(w^Tx+b)+In({1+e^{w^Tx+b}})</script><p>可以使用梯度下降法求解最佳的$w$和$b$：</p>
<script type="math/tex; mode=display">
w_j=w_j-\alpha\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_{j}^{(i)}</script><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p>样本集$D\{(x_i,y_i)\}^{n}_{i=1}$, 利用最大似然法, 即令每个样本属于其真实标签的概率越大越好，故似然函数为</p>
<script type="math/tex; mode=display">
\prod_{i=1}^{n}p^{y_i}(1-p)^{(1-y_i)}</script><p>对数似然函数:</p>
<script type="math/tex; mode=display">
L(p)=\sum_{i=1}^{n}y_iInP+\sum_{i=1}^{n}(1-y_i)In(1-p)</script><p>对$P$求$w$导数:</p>
<script type="math/tex; mode=display">
P=\frac{1}{1+e^{-(wx+b)}}\\
\frac{\partial{P}}{\partial{w}}=\frac{-1}{(1+e^{-(wx+b)})^2}*e^{-(wx+b)}*-x=P(1-P)</script><p>$L(P)$对$w$求导数:</p>
<script type="math/tex; mode=display">
\frac{\partial{L(P)}}{\partial{w}}=\sum_{i=1}^{n}y_i\frac{1}{P}\frac{\partial{P}}{\partial{w}}+\sum_{i=1}^{n}(1-y_i)\frac{-1}{1-p}\frac{\partial{P}}{\partial{w}}=XY(1-P)+(-1+Y)XP=(Y-P)X</script>
</div></div>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><script type="math/tex; mode=display">
E(w,b)=-\frac{1}{n}[\sum_{i=1}^{n}y^{(i)}logh_w(x^{(i)})+(1-y^{(i)})log(1-h_w(x^{(i)}))]+\lambda\sum_{i=1}^{n}w_{i}^{2}</script><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>$w_0=w_0-\alpha\frac{1}{n}\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_0^{(i)}$</p>
<p>$w_j=w_j-\alpha[\frac{1}{n}\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}-\frac{\lambda}{n}w_j]$</p>
<h2 id="Softmax-Loss-Multinomial-Logistic-Regression"><a href="#Softmax-Loss-Multinomial-Logistic-Regression" class="headerlink" title="Softmax Loss - Multinomial Logistic Regression"></a>Softmax Loss - Multinomial Logistic Regression</h2><p>对于多分类任务，模型的输出是一个多维向量，向量的每一位表示该输出属于某一类的概率。softmax则将输出建模成一个分布：</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.21.08 PM.png" alt="creen Shot 2019-03-08 at 2.21.08 P"></p>
<p>其中，$x_i$是第$i$个输入，$k$表示某一个类，则上式计算该输入属于某一个类的概率。</p>
<p>那么我们的损失函数就变成了：</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.24.14 PM.png" alt="creen Shot 2019-03-08 at 2.24.14 P"></p>
<p>对于每一个样本，我们最大化它属于正确类的概率。因为$ 0\le P(Y=y_i|X=x_i)\le 1$,所以$logP&lt;0$；我们追求的是$P()$越大，$L_i$越小，但是对于$logP$, 恰恰相反，所以对损失函数取负。</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.29.26 PM.png" alt="creen Shot 2019-03-08 at 2.29.26 P"></p>
<p>对于上面的例子，分类器的输出是$(3.2,5.1,-1.7)$,经过softmax函数计算，我们得到$(0.13,0.87,0.0)$，那么该样本的损失为$L_i=-log(0.13)$.</p>
<blockquote>
<ol>
<li><p>What is the min/max possible loss $L_i$?</p>
<p>The min loss is 0 when we classify the input correctly with a extremely high score. The max loss is $\infin$ when we classify the input to the correct category with a extremely low score. And according to the figure, it is infinite.</p>
</li>
<li><p>Usually at initialization $W$ is small so all $s\approx 0$. What is the loss?</p>
<p>The answer is $-log(\frac{1}{C})$, where $C$ is the class number.</p>
</li>
</ol>
</blockquote>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.34.05 PM.png" alt="creen Shot 2019-03-08 at 2.34.05 P"></p>
<h3 id="Softmax-Forward"><a href="#Softmax-Forward" class="headerlink" title="Softmax Forward"></a>Softmax Forward</h3><p><a href="https://deepnotes.io/softmax-crossentropy" target="_blank" rel="noopener">ref1</a>  <a href="http://bigstuffgoingon.com/blog/posts/softmax-loss-gradient/#The-gradient-of-the-softmax-classifier-loss-function" target="_blank" rel="noopener">ref2</a></p>
<p>Softmax function takes an N-dimensional vector of real number and transforms it into a vector of real number in range $(0,1)$ which add up to 1. For each output element $a_j$ in the last layer:</p>
<script type="math/tex; mode=display">
f(a_j)=\frac{e^{a_j}}{\sum_{i=1}^{k}e^{a_i}}</script><p>In python, we have the code for softmax function as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div>
<div class="line">    exps = np.exp(x)</div>
<div class="line">    <span class="keyword">return</span> exps/np.sum(exps)</div>
</pre></td></tr></table></figure>
<p>We have to note that the numerical range of float number in numpy is limited. For <code>float64</code> the upper bound is $10^{308}$. For exponential, it is not difficult to overshoot that limit, in which case python returns <code>nan</code>.</p>
<p>To make our softmax function numerically stable, we simply normalize the values in the vector, by multiplying the numerator and denominator with a constant $C$.</p>
<script type="math/tex; mode=display">
\begin{align}
p_i &= \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}} \\
&= \frac{Ce^{a_i}}{C\sum_{k=1}^N e^{a_k}} \\
&= \frac{e^{a_i + \log(C)}}{\sum_{k=1}^N e^{a_k + \log(C)}} \\
\end{align}</script><p>We can choose an arbitrary value for $log(C)$ term, but generally $log(C)=−max(a)$ is chosen, as it shifts all of elements in the vector to negative to zero, and negatives with large exponents saturate to zero rather than the infinity, avoiding overflowing and resulting in <code>nan</code>.</p>
<p>The code for our stable softmax is as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div>
<div class="line">    exps = np.exp(x-np.max(x))</div>
<div class="line">    <span class="keyword">return</span> exps/np.sum(exps)</div>
</pre></td></tr></table></figure>
<h3 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a>Derivative</h3><p>Due to the desirable property of softmax function outputting a probability distribution, we use it as the final layer in neural networks. For this we need to calculate the derivative or gradient and pass it back to the previous layer during backpropagation.</p>
<p>For each node $a_j$ in the output layer, we want to calculate:</p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} = \frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}</script><p>If $i=j$:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}&= \frac{e^{a_i} \sum_{k=1}^N e^{a_k} - e^{a_j}e^{a_i}}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\
&= \frac{e^{a_i} \left( \sum_{k=1}^N e^{a_k} - e^{a_j}\right )}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\
&= \frac{ e^{a_j} }{\sum_{k=1}^N e^{a_k} } \times \frac{\left( \sum_{k=1}^N e^{a_k} - e^{a_j}\right ) }{\sum_{k=1}^N e^{a_k} } \\
&= p_i(1-p_j)
\end{align}</script><p>For $i\ne j$:</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}&= \frac{0 - e^{a_j}e^{a_i}}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\
&= \frac{- e^{a_j} }{\sum_{k=1}^N e^{a_k} } \times \frac{e^{a_i} }{\sum_{k=1}^N e^{a_k} } \\
&= - p_j.p_i
\end{align}</script><p>So the derivative of the softmax function is given as,</p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} = 
\begin{cases}p_i(1-p_j) &  if & i=j \\
-p_j.p_i & if & i \neq j
\end{cases}</script><p>Or using Kronecker delta $\delta{ij} = \begin{cases} 1 &amp; if &amp; i=j \\ 0 &amp; if &amp; i\neq j \end{cases}$ </p>
<script type="math/tex; mode=display">
\frac{\partial p_i}{\partial a_j} =  p_i(\delta_{ij}-p_j)</script><h3 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h3><p>Corss entropy loss indicates the distance between what the model belives the output distribution should be, and what the original distribution really is. It is defined as:</p>
<script type="math/tex; mode=display">
H(y,p) = - \sum_i y_i log(p_i)</script><p>It is widely used as an alternative of squared error. It is used when node activations can be understand as representing the probability that each hypothesis might be true, i.e. when the output is a probability is a probability distribution. Thus it is used as a loss function in neural networks which have softmax activations in the output layer:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    X is the output from fully connected layer (num_examples x num_classes)</span></div>
<div class="line"><span class="string">    y is labels (num_examples x 1)</span></div>
<div class="line"><span class="string">    	Note that y is not one-hot encoded vector. </span></div>
<div class="line"><span class="string">    	It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    l = y.shape[<span class="number">0</span>]</div>
<div class="line">    prob = softmax(x)</div>
<div class="line">    prob_of_groundtruth = [row[y[i]] <span class="keyword">for</span> i,row <span class="keyword">in</span> enumerate(prob)]</div>
<div class="line">    log_likelihood = -np.log(prob_of_groundtruth)</div>
<div class="line">    loss = np.sum(log_likelihood)</div>
<div class="line">    <span class="keyword">return</span> loss</div>
</pre></td></tr></table></figure>
<h3 id="Derivative-of-Cross-Entropy-Loss-with-Softmax"><a href="#Derivative-of-Cross-Entropy-Loss-with-Softmax" class="headerlink" title="Derivative of Cross Entropy Loss with Softmax"></a>Derivative of Cross Entropy Loss with Softmax</h3><p>Cross Entropy Loss with Softmax function are used as the output layer extensively. Now we use the derivative of softmax that we derived earlier to derive the derivative of the cross entropy loss function.</p>
<p>For each sample $(x_i,y_i)$, there are $c$ classes</p>
<script type="math/tex; mode=display">
\begin{align}
L_i &= - \sum_i^c y_i log(p_i) \\
\end{align}</script><p>Then for each output node $o_i$, i.e., score of the class $i$ :</p>
<script type="math/tex; mode=display">
\begin{align}

\frac{\partial L_i}{\partial o_i} &= - \sum_k y_k \frac{\partial log(p_k)}{\partial o_i } \\
&= - \sum_k y_k \frac{\partial log(p_k)}{\partial p_k} \times \frac{\partial p_k}{ \partial o_i} \\
&= - \sum y_k \frac{1}{p_k} \times \frac{\partial p_k}{\partial o_i} \\

\end{align}</script><p>From derivative of softmax we derived earlier that $\frac{\partial p_i}{\partial a_j} = \begin{cases}p_i(1-p_j)&amp;  if &amp; i=j \\ -p_j.p_i &amp; if &amp; i \neq j\end{cases}$,</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L}{\partial o_i}  &= -y_i(1-p_i) - \sum_{k\neq i} y_k \frac{1}{p_k}(-p_k.p_i) \\
&= -y_i(1-p_i) + \sum_{k \neq 1} y_k.p_i \\
&= - y_i + y_ip_i + \sum_{k \neq 1} y_k.p_i \\
&= p_i\left( y_i +  \sum_{k \neq 1} y_k\right) - y_i \\
&= p_i\left( y_i +  \sum_{k \neq 1} y_k\right)  - y_i
\end{align}</script><p>$y$ is a one hot encoded vector for the label, so $\sum_{k}y_k =1$, and $y_i + \sum_{k\ne i}y_k =1$. So we have,</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial o_i} = p_i - y_i</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">delta_cross_entropy</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    X is the output from fully connected layer (num_examples x num_classes)</span></div>
<div class="line"><span class="string">    y is labels (num_examples x 1)</span></div>
<div class="line"><span class="string">    	Note that y is not one-hot encoded vector. </span></div>
<div class="line"><span class="string">    	It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    m = y.shape[<span class="number">0</span>]</div>
<div class="line">    prob = softmax(x)</div>
<div class="line">    prob_of_groundtruth = [row[y[i]] <span class="keyword">for</span> i,row <span class="keyword">in</span> enumerate(prob)]</div>
<div class="line">    prob_of_groundtruth -= <span class="number">1</span></div>
<div class="line">    <span class="keyword">return</span> prob_of_groundtruth/m</div>
</pre></td></tr></table></figure>
<p>In summary, the overall loss of cross entropy loss with sigmoid function is as following:</p>
<p>for each sample $(x_i,y_i)$, </p>
<script type="math/tex; mode=display">
Loss=-\frac{1}{n}\sum_{i}^{n}y_ilog(p_i) + \frac{1}{2}\sum_{m}\sum_{n}w_{m,n}^{2}\\
p_i=\frac{e^{o_i}}{\sum_{k}^{c}e^{o_k}}</script><p>where $o_i$ is the non-logit output, $p_i$ is the probability of being class $i$, $y_i$ is the groundtruth class. The second part of $Loss$ is the regularization.</p>
<h3 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h3><p>The plaint implementation is as following:</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Softmax</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div>
<div class="line">        self.W = <span class="keyword">None</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(self,W, X, y, reg)</span>:</span></div>
<div class="line">        <span class="string">'''</span></div>
<div class="line"><span class="string">        :param W:A numpy array of shape (D, C) containing weights.</span></div>
<div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing a minibatch of data.</span></div>
<div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c means</span></div>
<div class="line"><span class="string">        that X[i] has label c, where 0 &lt;= c &lt; C.</span></div>
<div class="line"><span class="string">        :param reg:(float) regularization strength</span></div>
<div class="line"><span class="string">        :return:a tuple of:</span></div>
<div class="line"><span class="string">                 - loss as single float</span></div>
<div class="line"><span class="string">                 - gradient with respect to weights W; an array of same shape as W</span></div>
<div class="line"><span class="string">        '''</span></div>
<div class="line">        loss = <span class="number">0.0</span></div>
<div class="line">        dW = np.zeros_like(W)</div>
<div class="line">        D, C = W.shape</div>
<div class="line">        N, D = X.shape</div>
<div class="line">        scores = X.dot(W)</div>
<div class="line">        norm_scores = scores - np.max(scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div>
<div class="line">        prob_scores = np.exp(norm_scores)</div>
<div class="line">        prob_scores /= np.sum(prob_scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div>
<div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div>
<div class="line">            loss -= np.log(prob_scores[i][y[i]])</div>
<div class="line">        loss = loss / N + reg * <span class="number">0.5</span> * np.sum(W ** <span class="number">2</span>)</div>
<div class="line"></div>
<div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div>
<div class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</div>
<div class="line">                <span class="keyword">if</span> y[i] == c:</div>
<div class="line">                    dW[:, c] += X[i] * (prob_scores[i][c] - <span class="number">1</span>)</div>
<div class="line">                <span class="keyword">else</span>:</div>
<div class="line">                    dW[:, c] += X[i] * prob_scores[i][c]</div>
<div class="line">        dW = dW / N + reg * W</div>
<div class="line">        <span class="keyword">return</span> loss, dW</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(self,W, X, y, reg)</span>:</span></div>
<div class="line"></div>
<div class="line">        loss = <span class="number">0.0</span></div>
<div class="line">        dW = np.zeros(W.shape)</div>
<div class="line"></div>
<div class="line">        D, C = W.shape</div>
<div class="line">        N, D = X.shape</div>
<div class="line">        scores = X.dot(W)</div>
<div class="line">        norm_scores = scores - np.max(scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div>
<div class="line">        prob_scores = np.exp(norm_scores)</div>
<div class="line">        prob_scores /= np.sum(prob_scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div>
<div class="line"></div>
<div class="line">        <span class="comment"># LOSS Calculation</span></div>
<div class="line">        loss = <span class="number">-1</span> * np.sum(np.log(prob_scores[np.arange(N), y]))</div>
<div class="line">        loss = loss / N + reg * <span class="number">0.5</span> * np.sum(W ** <span class="number">2</span>)</div>
<div class="line"></div>
<div class="line">        <span class="comment"># Gradient Calculation</span></div>
<div class="line">        prob_scores[np.arange(N), y] -= <span class="number">1</span></div>
<div class="line">        dW = X.T.dot(prob_scores)</div>
<div class="line">        dW = dW / N + reg * W</div>
<div class="line"></div>
<div class="line">        <span class="keyword">return</span> loss, dW</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></div>
<div class="line">        <span class="keyword">return</span> self.softmax_loss_vectorized(self.W, X_batch, y_batch, reg)</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></div>
<div class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></div>
<div class="line">        <span class="string">'''</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing training data; there are N</span></div>
<div class="line"><span class="string">          training samples each of dimension D.</span></div>
<div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c</span></div>
<div class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></div>
<div class="line"><span class="string">        :param learning_rate:(float) learning rate for optimization.</span></div>
<div class="line"><span class="string">        :param reg:(float) regularization strength.</span></div>
<div class="line"><span class="string">        :param num_iters:(integer) number of steps to take when optimizing</span></div>
<div class="line"><span class="string">        :param batch_size:(integer) number of training examples to use at each step.</span></div>
<div class="line"><span class="string">        :param verbose:(boolean) If true, print progress during optimization.</span></div>
<div class="line"><span class="string">        :return:A list containing the value of the loss function at each training iteration.</span></div>
<div class="line"><span class="string">        '''</span></div>
<div class="line">        num_train, dim = X.shape</div>
<div class="line">        num_classes = np.max(y) + <span class="number">1</span>  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></div>
<div class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="keyword">None</span>:</div>
<div class="line">            <span class="comment"># lazily initialize W</span></div>
<div class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</div>
<div class="line"></div>
<div class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></div>
<div class="line">        loss_history = []</div>
<div class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</div>
<div class="line">            mask = np.random.choice(num_train, batch_size)</div>
<div class="line">            X_batch = X[mask]</div>
<div class="line">            y_batch = y[mask]</div>
<div class="line"></div>
<div class="line">            <span class="comment"># evaluate loss and gradient</span></div>
<div class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</div>
<div class="line">            loss_history.append(loss)</div>
<div class="line">            self.W -= learning_rate * grad</div>
<div class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</div>
<div class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</div>
<div class="line">        <span class="keyword">return</span> loss_history</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div>
<div class="line">        scores = X.dot(self.W)</div>
<div class="line">        y_pred = np.argmax(scores,axis=<span class="number">1</span>)</div>
<div class="line">        <span class="keyword">return</span> y_pred</div>
</pre></td></tr></table></figure>

</div></div>
<h2 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h2><p>目前我们接触的问题是二分类问题，即每一个样本的label只有两种可能性(0|1)。但是，有时候会碰到多分类问题，即每一个样本label有两种以上的选择，比如对水果图片进行分类的任务就是多分类任务，因为一张图片可能是苹果，香蕉，橘子或者草莓等。</p>
<p>多分类的学习思路是将多分类任务拆分成若干个二分类任务求解，经典的拆分方法有三种：“一对一”(One vs One，简称OvO)，“一对余”(One vs Rest，简称OvR)和“多对多”(Many vs Many，简称MvM)。</p>
<p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-14%20at%205.48.16%20PM.png" alt="Screen Shot 2018-08-14 at 5.48.16 PM"></p>
<h3 id="一对一（OvO）"><a href="#一对一（OvO）" class="headerlink" title="一对一（OvO）"></a>一对一（OvO）</h3><p><strong>思想</strong>：给定具有$N$个类别的数据集，分为$N$个类别的堆，两两一组，一正一反，训练出$\frac{N(N-1)}{2}$个分类器。测试的时候，将测试样本送给所有的分类器，故产生$\frac{N(N-1)}{2}$个结果，投票选出最终的类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div>
<div class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</div>
<div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div>
<div class="line"></div>
<div class="line">iris = datasets.load_iris()</div>
<div class="line">x,y = iris.data, iris.target</div>
<div class="line">model = OneVsOneClassifier(LinearSVC(random_state=<span class="number">0</span>))</div>
<div class="line">model.fit(x,y)</div>
<div class="line">re = model.predict(x)</div>
<div class="line">print(re)</div>
</pre></td></tr></table></figure>
<h3 id="一对余-OvR"><a href="#一对余-OvR" class="headerlink" title="一对余(OvR)"></a>一对余(OvR)</h3><p><strong>思想</strong>：每次将一个类的样例作为正类，剩下的其他类的样例作为反例来训练$N$个分类器。测试时，若仅有一个分类器预测为正类，则对应的类别作为最终的分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记为分类结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div>
<div class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</div>
<div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div>
<div class="line">iris = datasets.load_iris()</div>
<div class="line">X, y = iris.data, iris.target</div>
<div class="line">OneVsRestClassifier(LinearSVC(random_state=<span class="number">0</span>)).fit(X, y).predict(X)</div>
</pre></td></tr></table></figure>
<p><strong>比较</strong>：<img src="/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-14%20at%205.55.35%20PM.png" alt="Screen Shot 2018-08-14 at 5.55.35 PM"></p>
<h3 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h3><p><a href="https://zhuanlan.zhihu.com/p/32940093" target="_blank" rel="noopener">ref1</a> </p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>机器学习中常常会遇到数据的<strong>类别不平衡（class imbalance）</strong>，也叫数据偏斜（class skew）。以常见的二分类问题为例，我们希望预测病人是否得了某种罕见疾病。但在历史数据中，阳性的比例可能很低（如百分之0.1）。在这种情况下，学习出好的分类器是很难的，而且在这种情况下得到结论往往也是很具迷惑性的。</p>
<p>以上面提到的场景来说，如果我们的分类器<strong>总是</strong>预测一个人未患病，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结果是没有意义的。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ul>
<li>试用其他评价指标</li>
<li>对数据集进行重采样</li>
<li>​</li>
</ul>
<h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>需要一个标准对数据进行约束：</p>
<ol>
<li><p>x增加全1行</p>
</li>
<li><p>shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">x.shape = (m,n) <span class="comment"># m:样本数；n：特征数</span></div>
<div class="line">y.shape = (m,) <span class="comment"># 一维行向量</span></div>
<div class="line">thetas = (n,<span class="number">1</span>) <span class="comment"># 一维列向量</span></div>
</pre></td></tr></table></figure>
</li>
</ol>
<h2 id="预测是否录取学生-ref"><a href="#预测是否录取学生-ref" class="headerlink" title="预测是否录取学生 ref"></a>预测是否录取学生 <a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/" target="_blank" rel="noopener">ref</a></h2><p>数据集包含学生的两次考试成绩以及其是否被录取的信息。</p>
<h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>数据载入的方式千千万万种方式，但是一旦载入之后，需要做的操作包括：</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line">data = np.loadtxt(<span class="string">'./dataset/ex2data1.txt'</span>,delimiter=<span class="string">','</span>)</div>
<div class="line">x = np.array(data[:,:<span class="number">-1</span>])</div>
<div class="line">y = np.array(data[:,<span class="number">-1</span>])</div>
<div class="line"></div>
<div class="line">row,_ = np.shape(x)</div>
<div class="line">ones = np.ones((row,<span class="number">1</span>))</div>
<div class="line">x_new = np.concatenate((ones,x),axis=<span class="number">1</span>)</div>
<div class="line">_,col = np.shape(x_new)</div>
<div class="line">thetas = np.zeros([col,<span class="number">1</span>])</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    plt.scatter(x[y==<span class="number">0</span>,<span class="number">0</span>],x[y==<span class="number">0</span>,<span class="number">1</span>],marker=<span class="string">'o'</span>,label=<span class="string">'Not admitted'</span>,c = <span class="string">'y'</span>)</div>
<div class="line">    plt.scatter(x[y == <span class="number">1</span>, <span class="number">0</span>], x[y == <span class="number">1</span>, <span class="number">1</span>], marker=<span class="string">'+'</span>, label=<span class="string">'Admitted'</span>,c=<span class="string">'black'</span>)</div>
<div class="line">    plt.xlabel(<span class="string">'Exam1 score'</span>)</div>
<div class="line">    plt.ylabel(<span class="string">'Exam2 score'</span>)</div>
<div class="line">    plt.legend()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/08/18/Logistic-Regression/image-20180818165938933.png" alt="image-20180818165938933"></p>

</div></div>
<h3 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a>Sigmoid激活函数</h3><p>我们的sigmoid函数为：</p>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2018-08-18 at 5.02.58 PM.png" alt="Screen Shot 2018-08-18 at 5.02.58 PM"></p>
<p>在实现激活函数之后，我们使用几个值取测试是否写对了：如果激活函数的输入是很大的值，那么我们的输出应该接近1；如果是很小的数，应该接近0；如果输入0，则输出0.5。而且，输入不应区分向量，矩阵的。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div>
<div class="line">    gz = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</div>
<div class="line">    <span class="keyword">return</span> gz</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="代价函数和梯度下降法"><a href="#代价函数和梯度下降法" class="headerlink" title="代价函数和梯度下降法"></a>代价函数和梯度下降法</h3><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2018-08-19 at 8.23.02 PM.png" alt="Screen Shot 2018-08-19 at 8.23.02 PM"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_func</span><span class="params">(thetas,x,y)</span>:</span></div>
<div class="line">    z = x.dot(thetas)</div>
<div class="line">    y_pred = sigmoid(z)</div>
<div class="line">    loss = y.T.dot(np.log(y_pred)) + (<span class="number">1</span>-y).T.dot(np.log(<span class="number">1</span>-y_pred))</div>
<div class="line">    <span class="keyword">return</span> -loss/row</div>
</pre></td></tr></table></figure>
<blockquote>
<p>初次调用时，loss=0.693</p>
</blockquote>
<p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2018-08-19 at 8.23.25 PM.png" alt="Screen Shot 2018-08-19 at 8.23.25 PM"></p>
<p>这次我们的梯度下降法返回梯度量，不直接返回参数更新</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(thetas,x,y)</span>:</span></div>
<div class="line">    z = x.dot(thetas)</div>
<div class="line">    y_pred = sigmoid(z)</div>
<div class="line">    gd = (y_pred-y).T.dot(x) / row</div>
<div class="line">    <span class="keyword">return</span> gd.T</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="损失函数优化"><a href="#损失函数优化" class="headerlink" title="损失函数优化"></a>损失函数优化</h3><p>这次，我们使用<code>scipy.optimize</code>寻找目标函数的最优值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">scipy.optimize.fmin_tnc(func, x0, fprime=<span class="keyword">None</span>, args=())</div>
</pre></td></tr></table></figure>
<ul>
<li><p>func：目标函数，它必须满足以下条件之一：</p>
<p>— 返回函数值<code>f</code>和梯度值<code>g</code></p>
<p>— 返回函数值<code>f</code>，但是必须提供梯度函数作为<code>fprime</code>的参数</p>
<p>—返回函数值<code>f</code>，设置 <code>approx_grad=True</code></p>
</li>
<li><p>x0：待优化参数的初始值</p>
</li>
<li><p>fprime：梯度函数</p>
</li>
<li><p>args：<code>func</code>函数的参数</p>
</li>
<li><p>该函数的返回值：x—solution；nfeval—函数优化次数；</p>
</li>
</ul>
<p>最优化之后的损失函数值是0.203左右。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">scipy_gd</span><span class="params">(thetas,x,y)</span>:</span></div>
<div class="line">    result = opt.fmin_tnc(</div>
<div class="line">        func=cost_func,x0=thetas,fprime=gradient_descent,args=(x,y))</div>
<div class="line">    <span class="keyword">return</span> result[<span class="number">0</span>]</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="预测及评价"><a href="#预测及评价" class="headerlink" title="预测及评价"></a>预测及评价</h3><p>我们定义一个预测函数，输入未知值，预测其分类</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(thetas,x)</span>:</span></div>
<div class="line">    z = x.dot(thetas)</div>
<div class="line">    prob = sigmoid(z)</div>
<div class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> prob]</div>
</pre></td></tr></table></figure>

</div></div>
<p>使用准确率来评估预测结果</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    thetas = scipy_gd(thetas,x_new,y)</div>
<div class="line">    predictions = predict(thetas,x_new)</div>
<div class="line">    correct = [<span class="number">1</span> <span class="keyword">if</span> ((a == <span class="number">1</span> <span class="keyword">and</span> b == <span class="number">1</span>) <span class="keyword">or</span> (a == <span class="number">0</span> <span class="keyword">and</span> b == <span class="number">0</span>)) <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y)]</div>
<div class="line">    accuracy = (sum(map(int, correct)) % len(correct))</div>
<div class="line">    plot_boundary(x,y,thetas)</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="画出分类边界"><a href="#画出分类边界" class="headerlink" title="画出分类边界"></a>画出分类边界</h3><p>需要注意的是，此时的x并不需要额外的全1行。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> pl</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_boundary</span><span class="params">(x,y,thetas)</span>:</span></div>
<div class="line">    plot_data(x,y)</div>
<div class="line">    h = <span class="number">0.1</span></div>
<div class="line">    x0_min, x0_max = x[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, x[:, <span class="number">0</span>].max() + <span class="number">0.1</span></div>
<div class="line">    x1_min, x1_max = x[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, x[:, <span class="number">1</span>].max() + <span class="number">0.1</span></div>
<div class="line">    x0, x1 = np.meshgrid(np.arange(x0_min, x0_max, h),</div>
<div class="line">                         np.arange(x1_min, x1_max, h))</div>
<div class="line">    z = np.c_[x0.ravel(), x1.ravel()].dot(thetas[<span class="number">1</span>:]) + thetas[<span class="number">0</span>]</div>
<div class="line">    z = sigmoid(z)</div>
<div class="line">    z = z.reshape(x0.shape)</div>
<div class="line">    plt.contour(x0, x1, z, cmap=pl.cm.Paired)</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/08/18/Logistic-Regression/image-20180822205326946.png" alt="image-20180822205326946"></p>

</div></div>
<h2 id="正则化的回归"><a href="#正则化的回归" class="headerlink" title="正则化的回归"></a>正则化的回归</h2><p><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/" target="_blank" rel="noopener">ref</a> </p>
<p>本题我们使用正则化的逻辑斯蒂回归来分类。给定的数据集是关于微芯片的两次测试数据，label是合格与不合格。</p>
<ol>
<li><p>数据可视化</p>
<p>直接利用上面的画图函数，可以看到我们需要使用一个非线性曲线去分类，类似椭圆。显然这不能是线性分类，对于非线性分类问题，逻辑回归会使用多项式扩展特征。这带来的弊端就是引入巨大特征。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p><img src="/2018/08/18/Logistic-Regression/image-20180822210526246.png" alt="image-20180822210526246"></p>

</div></div>
</li>
<li><p>​</p>
</li>
</ol>
<h2 id="逻辑斯蒂回归预测西瓜"><a href="#逻辑斯蒂回归预测西瓜" class="headerlink" title="逻辑斯蒂回归预测西瓜"></a>逻辑斯蒂回归预测西瓜</h2><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-08%20at%2011.32.08%20AM.png" alt="Screen Shot 2018-08-08 at 11.32.08 AM"></p>
<ol>
<li><p>数据载入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div>
<div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div>
<div class="line">dataset = np.loadtxt(<span class="string">'watermelon_3a.csv'</span>, delimiter=<span class="string">","</span>)</div>
<div class="line">X = dataset[:, <span class="number">1</span>:<span class="number">3</span>]</div>
<div class="line">y = dataset[:, <span class="number">3</span>]</div>
<div class="line">m, n = np.shape(X)</div>
</pre></td></tr></table></figure>
</li>
<li><p>数据可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># draw scatter diagram to show the raw data</span></div>
<div class="line">f1 = plt.figure(<span class="number">1</span>)</div>
<div class="line">plt.title(<span class="string">'watermelon_3a'</span>)</div>
<div class="line">plt.xlabel(<span class="string">'density'</span>)</div>
<div class="line">plt.ylabel(<span class="string">'ratio_sugar'</span>)</div>
<div class="line">plt.scatter(X[y == <span class="number">0</span>, <span class="number">0</span>], X[y == <span class="number">0</span>, <span class="number">1</span>], marker=<span class="string">'o'</span>, color=<span class="string">'k'</span>, s=<span class="number">100</span>, label=<span class="string">'bad'</span>)</div>
<div class="line">plt.scatter(X[y == <span class="number">1</span>, <span class="number">0</span>], X[y == <span class="number">1</span>, <span class="number">1</span>], marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>, s=<span class="number">100</span>, label=<span class="string">'good'</span>)</div>
<div class="line">plt.legend(loc=<span class="string">'upper right'</span>)</div>
<div class="line">plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808113730536.png" alt="image-20180808113730536"></p>
</li>
<li><p>利用sklearn求解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div>
<div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</div>
<div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div>
<div class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> pl</div>
<div class="line">X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">0</span>)</div>
<div class="line">log_model = LogisticRegression()  </div>
<div class="line">log_model.fit(X_train, y_train) </div>
<div class="line">weights , b = log_model.coef_, log_model.intercept_</div>
<div class="line"><span class="comment"># weights = [[-0.0865987   0.39410864]] </span></div>
<div class="line"><span class="comment"># b = [-0.02152586]</span></div>
</pre></td></tr></table></figure>
</li>
<li><p>结果可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment">###计算模型各种评价标准</span></div>
<div class="line">print(metrics.confusion_matrix(y_test, y_pred))</div>
<div class="line">print(metrics.classification_report(y_test, y_pred))</div>
<div class="line">y_pred = log_model.predict(X_test)</div>
<div class="line">precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line">h = <span class="number">0.001</span></div>
<div class="line">x0_min, x0_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></div>
<div class="line">x1_min, x1_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></div>
<div class="line">x0, x1 = np.meshgrid(np.arange(x0_min, x0_max, h),</div>
<div class="line">                     np.arange(x1_min, x1_max, h))</div>
<div class="line">z = log_model.predict(np.c_[x0.ravel(), x1.ravel()])</div>
<div class="line">z = z.reshape(x0.shape)</div>
<div class="line">plt.contourf(x0, x1, z, cmap=pl.cm.Paired)</div>
<div class="line">plt.contour(x0, x1, z, cmap=pl.cm.Paired)</div>
</pre></td></tr></table></figure>
<blockquote>
<ul>
<li><a href="https://www.cnblogs.com/lemonbit/p/7593898.html" target="_blank" rel="noopener">meshgrid</a> 通俗说，输入是两个一维向量x和y，对于y中的每一个值$y_i$，都为之复制一个x，相当于从$y_i$出发画一条平行x轴的直线，有多少个y点，就有多少条直线；同理，对于x中的灭一个$x_i$，为之复制一个y，等价于从该$x_i$出发画一条平行于$y$轴的直线。这样很多条直线相交就会产生$len(y) \ \cdot \ len(x)$形状的矩阵。</li>
<li><code>np.c_[a,b]</code>将两数组在column方向合并,等价于<code>np.concatenate((a,b),axis=1)</code></li>
<li><code>np.ravel()</code>将数组展开，多维降为一维，类似<code>np.flatten()</code>，只不过<code>np.flatten()</code>返回拷贝，而<code>ravel()</code>返回视图，会对原始数组产生影响。</li>
</ul>
</blockquote>
<p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808114529415.png" alt="image-20180808114529415"></p>
<p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808123047797.png" alt="image-20180808123047797"></p>
</li>
</ol>
<h2 id="交叉验证法-vs-留一法"><a href="#交叉验证法-vs-留一法" class="headerlink" title="交叉验证法 vs 留一法"></a>交叉验证法 vs 留一法</h2><p>基于UCI数据集，比较10折交叉验证和留一法所估计出的对率回归的准确率。</p>
<p>我们选择<a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener">Iris</a>数据和<a href="http://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center" target="_blank" rel="noopener">blood transfusion</a>数据</p>
<p><strong>Iris数据</strong>共有4个特征个一个类别label。</p>
<p>第一个是sepal length，第二个是sepal width，第三个是petal length，第四个特征是petal width。类别有三类：— Iris Setosa  — Iris Versicolour  — Iris Virginica</p>
<p><strong>blood transfusion</strong>数据总共有4个特征和一个label。</p>
<p>R(Recency):距离上次献血时间(month)<br>F(Frequency):总献血次数<br>M(Monetary):总献血量<br>T(Time):距离第一次献血(month)<br>Y:2007的三月是否献血;1-献血，0-没有献血</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div>
<div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div>
<div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(data,y)</span>:</span></div>
<div class="line">    sns.set(style=<span class="string">'white'</span>,color_codes=<span class="keyword">True</span>)</div>
<div class="line">    sns.pairplot(data,hue=y)</div>
<div class="line">    plt.show()</div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line"></div>
<div class="line">    iris = sns.load_dataset(<span class="string">'iris'</span>)</div>
<div class="line">    plot_data(iris,<span class="string">'species'</span>)</div>
<div class="line"></div>
<div class="line">    data = pd.read_csv(<span class="string">'blood_dataset.txt'</span>,sep=<span class="string">','</span>,header=<span class="keyword">None</span>)</div>
<div class="line">    data.columns = [<span class="string">'R'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>,<span class="string">'T'</span>,<span class="string">'Y'</span>]</div>
<div class="line">    plot_data(data,<span class="string">'Y'</span>)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180813110624641.png" alt="image-20180813110624641"></p>
<p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180813111620624.png" alt="image-20180813111620624"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div>
<div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict, LeaveOneOut</div>
<div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div>
<div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div>
<div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    log_regression = LogisticRegression()</div>
<div class="line">    pre = cross_val_predict(log_regression,x,y,cv=<span class="number">5</span>)</div>
<div class="line">    print(metrics.accuracy_score(y,pre))</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loo</span><span class="params">(x,y)</span>:</span></div>
<div class="line">    loo = LeaveOneOut()</div>
<div class="line">    accuracy = <span class="number">0</span></div>
<div class="line">    log_regression = LogisticRegression()</div>
<div class="line">    <span class="keyword">for</span> train,test <span class="keyword">in</span> loo.split(x):</div>
<div class="line">        log_regression.fit(x[train],y[train])</div>
<div class="line">        y_p = log_regression.predict(x[test])</div>
<div class="line">        <span class="keyword">if</span> y_p == y[test] : accuracy += <span class="number">1</span></div>
<div class="line">    print(accuracy/np.shape(x)[<span class="number">0</span>]) <span class="comment"># 0.9533333333333334</span></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_blood</span><span class="params">()</span>:</span></div>
<div class="line">    data = np.loadtxt(<span class="string">'blood_dataset.txt'</span>,delimiter=<span class="string">','</span>) <span class="comment">#shape = (748,5)</span></div>
<div class="line">    x = data[:,:<span class="number">-1</span>] <span class="comment"># shape = (748, 4)</span></div>
<div class="line">    y = data[:,<span class="number">-1</span>] <span class="comment"># shape = (748,)</span></div>
<div class="line">    <span class="keyword">return</span> x,y</div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line"></div>
<div class="line">    iris = load_iris()</div>
<div class="line">    x = iris.data  <span class="comment"># x.shape = (150,4)</span></div>
<div class="line">    y = iris.target  <span class="comment"># x.shape = (150,)</span></div>
<div class="line">    cv(x,y) <span class="comment"># 0.96</span></div>
<div class="line">    loo(x,y) <span class="comment"># 0.9533333333333334</span></div>
<div class="line"></div>
<div class="line">    x,y=load_blood()</div>
<div class="line">    cv(x,y) <span class="comment"># 0.7807486631016043</span></div>
<div class="line">    loo(x,y) <span class="comment"># 0.7700534759358288</span></div>
</pre></td></tr></table></figure>
<blockquote>
<p>也可以看到，两种交叉验证的结果相近，但是由于此数据集的类分性不如iris明显，所得结果也要差一些。同时由程序运行可以看出，<strong>LOOCV的运行时间相对较长</strong>，这一点随着数据量的增大而愈发明显。</p>
<p>所以，一般情况下选择K-折交叉验证即可满足精度要求，同时运算量相对小。</p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Regression/" rel="tag"># Regression</a>
          
            <a href="/tags/Logistic-Regression/" rel="tag"># Logistic Regression</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/09/Python-Sth/" rel="next" title="Python Sth">
                <i class="fa fa-chevron-left"></i> Python Sth
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/20/Variant-GAN/" rel="prev" title="Variant GAN">
                Variant GAN <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">90</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">68</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#逻辑斯蒂回归"><span class="nav-number">1.</span> <span class="nav-text">逻辑斯蒂回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#模型"><span class="nav-number">1.1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法求解"><span class="nav-number">1.2.</span> <span class="nav-text">算法求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化"><span class="nav-number">1.2.1.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">梯度下降法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-Loss-Multinomial-Logistic-Regression"><span class="nav-number">1.3.</span> <span class="nav-text">Softmax Loss - Multinomial Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-Forward"><span class="nav-number">1.3.1.</span> <span class="nav-text">Softmax Forward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivative"><span class="nav-number">1.3.2.</span> <span class="nav-text">Derivative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Entropy-Loss"><span class="nav-number">1.3.3.</span> <span class="nav-text">Cross Entropy Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Derivative-of-Cross-Entropy-Loss-with-Softmax"><span class="nav-number">1.3.4.</span> <span class="nav-text">Derivative of Cross Entropy Loss with Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numpy-Implementation"><span class="nav-number">1.3.5.</span> <span class="nav-text">Numpy Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多分类学习"><span class="nav-number">1.4.</span> <span class="nav-text">多分类学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一对一（OvO）"><span class="nav-number">1.4.1.</span> <span class="nav-text">一对一（OvO）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一对余-OvR"><span class="nav-number">1.4.2.</span> <span class="nav-text">一对余(OvR)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#类别不平衡"><span class="nav-number">1.4.3.</span> <span class="nav-text">类别不平衡</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解决方法"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">解决方法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#例子"><span class="nav-number">2.</span> <span class="nav-text">例子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#预测是否录取学生-ref"><span class="nav-number">2.1.</span> <span class="nav-text">预测是否录取学生 ref</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据加载"><span class="nav-number">2.1.1.</span> <span class="nav-text">数据加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据可视化"><span class="nav-number">2.1.2.</span> <span class="nav-text">数据可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid激活函数"><span class="nav-number">2.1.3.</span> <span class="nav-text">Sigmoid激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数和梯度下降法"><span class="nav-number">2.1.4.</span> <span class="nav-text">代价函数和梯度下降法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数优化"><span class="nav-number">2.1.5.</span> <span class="nav-text">损失函数优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预测及评价"><span class="nav-number">2.1.6.</span> <span class="nav-text">预测及评价</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#画出分类边界"><span class="nav-number">2.1.7.</span> <span class="nav-text">画出分类边界</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化的回归"><span class="nav-number">2.2.</span> <span class="nav-text">正则化的回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑斯蒂回归预测西瓜"><span class="nav-number">2.3.</span> <span class="nav-text">逻辑斯蒂回归预测西瓜</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉验证法-vs-留一法"><span class="nav-number">2.4.</span> <span class="nav-text">交叉验证法 vs 留一法</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
