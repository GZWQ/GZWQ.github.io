<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning," />










<meta name="description" content="Data Preprocessing Soft labelsThis is extremely important when training the discriminator. Having hard labels (1 or 0) nearly killed all learning early on, leading the discriminator to approach 0 loss">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Training Techiniques">
<meta property="og:url" content="http://yoursite.com/2018/08/03/Training-Techiniques/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Data Preprocessing Soft labelsThis is extremely important when training the discriminator. Having hard labels (1 or 0) nearly killed all learning early on, leading the discriminator to approach 0 loss">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%203.37.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/gif.gif">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1_Lv9TNpAXffRnO0p0WMGJwQ.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1_QIzXjH8uefVbcaycsjfdmw.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1_zxD6Nr6TyAb8JEG6oXAjkg.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/692825-20180328173642905-311674055.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/20171028233231084.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-13%20at%204.07.13%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/20171028235024692.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/20171028235337038.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.54.22%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.55.56%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.58.24%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.59.26%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202018-11-16%20at%202.33.45%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/residualnet_34.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/2228224-1a6202911b46d1dc.png">
<meta property="og:updated_time" content="2019-02-20T17:34:34.708Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Techiniques">
<meta name="twitter:description" content="Data Preprocessing Soft labelsThis is extremely important when training the discriminator. Having hard labels (1 or 0) nearly killed all learning early on, leading the discriminator to approach 0 loss">
<meta name="twitter:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%203.37.19%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/03/Training-Techiniques/"/>





  <title>Training Techiniques | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/03/Training-Techiniques/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Training Techiniques</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-03T11:47:08-05:00">
                2018-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-17 at 3.37.19 PM.png" alt="creen Shot 2019-02-17 at 3.37.19 P"></p>
<h1 id="Soft-labels"><a href="#Soft-labels" class="headerlink" title="Soft labels"></a>Soft labels</h1><p>This is extremely important when training the discriminator. Having hard labels (1 or 0) nearly killed all learning early on, leading the discriminator to approach 0 loss very rapidly. I ended up using a random number between 0 and 0.1 to represent 0 labels (real images) and a random number between 0.9 and 1.0 to represent 1 labels (generated images). This is not required when training the generator. <a href="https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9" target="_blank" rel="noopener">resource</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">valid = np.random.uniform(<span class="number">0.9</span>,<span class="number">1.0</span>,size=(batch_size,<span class="number">1</span>))</div>
<div class="line">fake = np.random.uniform(<span class="number">0</span>,<span class="number">0.1</span>,size=(batch_size,<span class="number">1</span>))</div>
</pre></td></tr></table></figure>
<h1 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h1><h2 id="Problem-with-initializing-all-weights-to-0"><a href="#Problem-with-initializing-all-weights-to-0" class="headerlink" title="Problem with initializing all weights to 0"></a>Problem with initializing all weights to 0</h2><p>In this case, the equations of the learning algorithm would fail to make any changes to the network weights, and the model will be stuck. It is important to note that the bias weight in each neuron is set to zero by default, not a small random value. <a href="https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/" target="_blank" rel="noopener">ref</a> </p>
<p>During forward propagation each unit in hidden layer gets signal:</p>
<p><img src="/2018/08/03/Training-Techiniques/gif.gif" alt="i"></p>
<p>That is, each hidden unit gets sum of inputs multiplied by the corresponding weight.</p>
<p>Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, <strong>each hidden unit will get exactly the same signal</strong>. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs <code>sigmoid(sum(inputs))</code>). If all weights are zeros, which is even worse, every hidden unit will get zero signal. <strong>No matter what was the input - if all weights are the same, all units in hidden layer will be the same too</strong>.</p>
<p>This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). Note, that this issue affects all architectures that use each-to-each connections. <a href="https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers" target="_blank" rel="noopener">ref</a></p>
<h2 id="Problems-with-initializing-weights-randomly-ref"><a href="#Problems-with-initializing-weights-randomly-ref" class="headerlink" title="Problems with initializing weights randomly ref"></a>Problems with initializing weights randomly <a href="https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" target="_blank" rel="noopener">ref</a></h2><h3 id="Vanishing-gradients"><a href="#Vanishing-gradients" class="headerlink" title="Vanishing gradients"></a>Vanishing gradients</h3><p>If weights are initialized with low values it gets mapped to 0, then the activation value would be small, say, almost 0. When backpropogating gradients, samll gradients times small weights, it will result in smaller gradients, meaning the earlier layers, the samller gradients, resulting vanishing gradients.</p>
<h3 id="Exploding-gradients"><a href="#Exploding-gradients" class="headerlink" title="Exploding gradients"></a>Exploding gradients</h3><p>Consider you have non-negative and large weights and small activations A (as can be the case for sigmoid(z)). When these weights are multiplied along the layers, they cause a large change in the cost. Thus, the gradients are also going to be large. This means that the changes in W, by <code>W — ⍺ * dW,</code> will be in huge steps, the downward moment will increase.</p>
<blockquote>
<p>This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn!</p>
</blockquote>
<p>Another impact of exploding gradients is that huge values of the gradients may cause number overflow resulting in incorrect computations or introductions of NaN’s. This might also lead to the loss taking the value NaN.</p>
<h2 id="New-Initialization-techniques-ref"><a href="#New-Initialization-techniques-ref" class="headerlink" title="New Initialization techniques ref"></a>New Initialization techniques <a href="https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78" target="_blank" rel="noopener">ref</a></h2><h3 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h3><p>It is used when we use tanh as our activation function.</p>
<p><img src="/2018/08/03/Training-Techiniques/1_Lv9TNpAXffRnO0p0WMGJwQ.png" alt="_Lv9TNpAXffRnO0p0WMGJw"></p>
<p>Some also use the following as initialization:</p>
<p><img src="/2018/08/03/Training-Techiniques/1_QIzXjH8uefVbcaycsjfdmw.png" alt="_QIzXjH8uefVbcaycsjfdm"></p>
<h3 id="He-initialization"><a href="#He-initialization" class="headerlink" title="He initialization"></a>He initialization</h3><p>When we use relu as activation, we just simply multiply random initialization with:</p>
<p><img src="/2018/08/03/Training-Techiniques/1_zxD6Nr6TyAb8JEG6oXAjkg.png" alt="_zxD6Nr6TyAb8JEG6oXAjk"></p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p><a href="https://blog.csdn.net/Jaster_wisdom/article/details/78380839" target="_blank" rel="noopener">ref1</a> <a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">zhihu</a> <a href="http://www.sohu.com/a/200918239_206784" target="_blank" rel="noopener">intuition</a> <a href="https://www.cnblogs.com/rgvb178/p/6055213.html" target="_blank" rel="noopener">details</a> <a href="https://towardsdatascience.com/deep-learning-concepts-part-1-ea0b14b234c8" target="_blank" rel="noopener">to do</a> </p>
<p>根据是否饱和，激活函数可以分类为“饱和激活函数”和“非饱和激活函数”。</p>
<p><strong>sigmoid和tanh</strong>是“饱和激活函数”，而ReLU及其变体则是“非饱和激活函数”，但是他们都属于非线性激活函数。使用“非饱和激活函数”的优势在于两点：     </p>
<p>1.首先，“非饱和激活函数”能解决所谓的“梯度消失”问题。 vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一   </p>
<p> 2.其次，它能加快收敛速度。    </p>
<p><img src="/2018/08/03/Training-Techiniques/692825-20180328173642905-311674055.png" alt="692825-20180328173642905-311674055"></p>
<h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>将实数压缩到$(0,1)$，用来二分类。</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{1+e^{-x}}</script><p><img src="/2018/08/03/Training-Techiniques/20171028233231084.png" alt="20171028233231084"></p>
<p>sigmoid的优缺点<a href="https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/" target="_blank" rel="noopener">source</a> </p>
<ul>
<li><p><strong>Vanishing gradients</strong>: Notice, the sigmoid function is flat near 0 and 1. In other words, the gradient of the sigmoid is near 0 and 1. During backpropagation through the network with sigmoid activation, the gradients in neurons whose output is near 0 or 1 are nearly 0. These neurons are called saturated neurons. Thus, the weights in these neurons do not update. Not only that, the weights of neurons connected to such neurons are also slowly updated. This problem is also known as vanishing gradient. So, imagine if there was a large network comprising of sigmoid neurons in which many of them are in a saturated regime, then the network will not be able to backpropagate.</p>
<blockquote>
<p><a href="http://www.sohu.com/a/148114422_500659" target="_blank" rel="noopener">ref1</a> 在GAN中， 给 D 最后的输出加个 Sigmoid 激活函数，让它取值在 0 到 1 之间？事实上这个方案在理论上是没有问题的，然而这会造成训练的困难。因为 Sigmoid 函数具有饱和区，一旦 D 进入了饱和区，就很难传回梯度来更新 G 了。</p>
</blockquote>
</li>
<li><p><strong>Not zero centered</strong>: Sigmoid outputs are not zero-centered. The output is always between 0 and 1, that means that the output after applying sigmoid is always positive hence, i.e. $x_i &gt; 0$.</p>
<p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-13%20at%204.07.13%20PM.png" alt="creen Shot 2019-02-13 at 4.07.13 P"></p>
<p>which means the gradients on the weights $w$ during backpropagation become either all positive or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. Say we </p>
<blockquote>
<ul>
<li>Tanh, Rectified Linear Unit (ReLU), Leaky ReLU and Parametric ReLU are all zero-centered activation functions.</li>
<li>This is also why you want zero-mean data</li>
</ul>
</blockquote>
</li>
<li><p><strong>Computationally expensive</strong>: The exp() function is computationally expensive compared with the other non-linear activation functions.</p>
</li>
</ul>
<h2 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h2><p>将实数压缩到$[-1,1]$</p>
<script type="math/tex; mode=display">
f(x) = tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
 f'(x)=1-f^2(x)</script><p><img src="/2018/08/03/Training-Techiniques/20171028235024692.png" alt="20171028235024692"></p>
<blockquote>
<p>ZERO-centered but still kills gradients when saturated</p>
</blockquote>
<h2 id="ReLu函数"><a href="#ReLu函数" class="headerlink" title="ReLu函数"></a>ReLu函数</h2><script type="math/tex; mode=display">
f(x)=max(0,x)</script><p><img src="/2018/08/03/Training-Techiniques/20171028235337038.png" alt="20171028235337038"></p>
<blockquote>
<p><strong>Advantages:</strong> Does not saturate (in +region); Very computationally efficient; Converges much faster than sigmoid/tanh in practice (e.g. 6x); </p>
<p><strong>Disadvantages:</strong> Not zero-centered output; It kills the gradient when $x \le 0$  </p>
<p><a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks" target="_blank" rel="noopener">dead unit problem</a> <a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" target="_blank" rel="noopener">dying unit</a> </p>
</blockquote>
<h2 id="LeakyReLu"><a href="#LeakyReLu" class="headerlink" title="LeakyReLu"></a>LeakyReLu</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.54.22%20PM.png" alt="creen Shot 2019-02-17 at 2.54.22 P"></p>
<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.55.56%20PM.png" alt="creen Shot 2019-02-17 at 2.55.56 P"></p>
<h1 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h1><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.58.24%20PM.png" alt="creen Shot 2019-02-17 at 2.58.24 P"></p>
<h2 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h2><p>用于多分类问题，输出每类可能出现的概率大小，和为1</p>
<script type="math/tex; mode=display">
f(x)_j=\frac{e^{x_j}}{\sum_{i=1}^{k}e^{x_i}}</script><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-17 at 2.59.26 PM.png" alt="creen Shot 2019-02-17 at 2.59.26 P"></p>
<h1 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_tensorboard</span><span class="params">(self,generator_step, summary_writer,losses)</span>:</span></div>
<div class="line"></div>
<div class="line">        summary = tf.Summary()</div>
<div class="line"></div>
<div class="line">        value = summary.value.add()</div>
<div class="line">        value.simple_value = losses[<span class="number">1</span>]</div>
<div class="line">        value.tag = <span class="string">'Critic Real Loss'</span></div>
<div class="line"></div>
<div class="line">        value = summary.value.add()</div>
<div class="line">        value.simple_value = losses[<span class="number">2</span>]</div>
<div class="line">        value.tag = <span class="string">'Critic Fake Loss'</span></div>
<div class="line"></div>
<div class="line">        value = summary.value.add()</div>
<div class="line">        value.simple_value = losses[<span class="number">3</span>]</div>
<div class="line">        value.tag = <span class="string">'Generator Loss'</span></div>
<div class="line"></div>
<div class="line">        value = summary.value.add()</div>
<div class="line">        value.simple_value = losses[<span class="number">1</span>] - losses[<span class="number">2</span>]</div>
<div class="line">        value.tag = <span class="string">'Critic Loss (D_real - D_fake)'</span></div>
<div class="line"></div>
<div class="line">        value = summary.value.add()</div>
<div class="line">        value.simple_value = losses[<span class="number">1</span>] + losses[<span class="number">2</span>]</div>
<div class="line">        value.tag = <span class="string">'Critic Loss (D_fake + D_real)'</span></div>
<div class="line"></div>
<div class="line">        summary_writer.add_summary(summary, generator_step)</div>
<div class="line">        summary_writer.flush()</div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,epochs,batch_size=<span class="number">10</span>,sample_interval=<span class="number">100</span>)</span>:</span></div>
<div class="line">        summary_writer = tf.summary.FileWriter(<span class="string">'./logs/trainBoth'</span>)</div>
<div class="line">        generator_step = <span class="number">1</span></div>
<div class="line"></div>
<div class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, epochs + <span class="number">1</span>)):</div>
<div class="line">            <span class="keyword">for</span> e, (figure_imgs, pose_imgs) <span class="keyword">in</span> tqdm(enumerate(self.data_loader.load_batch(batch_size=batch_size))):</div>
<div class="line">                    <span class="comment"># Train the critic</span></div>
<div class="line"></div>
<div class="line">                    figure_loss_real = self.figure_critic.train_on_batch(figure_imgs, valid)</div>
<div class="line">                    figure_loss_fake = self.figure_critic.train_on_batch(gen_figure_imgs, fake)</div>
<div class="line">                    d_figure_loss = <span class="number">0.5</span> * np.add(figure_loss_real, figure_loss_fake)</div>
<div class="line"></div>
<div class="line">                print(self.figure_critic.metrics_names,d_figure_loss)</div>
<div class="line">                losses = np.empty(shape=<span class="number">1</span>)</div>
<div class="line">                losses = np.append(losses, figure_loss_real)</div>
<div class="line">                losses = np.append(losses, figure_loss_fake)</div>
<div class="line"></div>
<div class="line">                <span class="comment"># ---------------------</span></div>
<div class="line">                <span class="comment">#  Train Generator</span></div>
<div class="line">                <span class="comment"># ---------------------</span></div>
<div class="line"></div>
<div class="line">                figure_loss = self.figure_EN.train_on_batch([figure_noise,pose_imgs],valid)</div>
<div class="line">                print(self.figure_EN.metrics_names,figure_loss)</div>
<div class="line">                losses = np.append(losses, figure_loss)</div>
<div class="line">                self.write_to_tensorboard(generator_step, summary_writer, losses)</div>
<div class="line">                generator_step += <span class="number">1</span></div>
</pre></td></tr></table></figure>
<h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p><a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">paper1</a> <a href="https://arxiv.org/pdf/1603.05027v2.pdf" target="_blank" rel="noopener">paper2</a> </p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>simplely stack layers exhibit higher training error when the depth increases.</p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2018-11-16 at 2.33.45 PM.png" alt="creen Shot 2018-11-16 at 2.33.45 P"></p>
<p><img src="/2018/08/03/Training-Techiniques/residualnet_34.png" alt="esidualnet_3"></p>
<p>Figure from <a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/residual_net.html" target="_blank" rel="noopener">ref</a> </p>
<p><img src="/2018/08/03/Training-Techiniques/2228224-1a6202911b46d1dc.png" alt="228224-1a6202911b46d1d"></p>
<p><a href="https://www.jianshu.com/p/e502e4b43e6d" target="_blank" rel="noopener">pic</a> </p>
<h1 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h1><p><strong>Optimizer(优化器)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">keras.optimizers.Adam(lr=<span class="number">0.001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>, amsgrad=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<ul>
<li>decay, learning rate decay over each update</li>
<li>epsilon, fuzz factor, almost zero but greater than zero, to avoid denominator being zero </li>
<li>according to my search, lr is usually among [0.001, 0.0001, 0.0002], beta_1 is chosen as 0.5</li>
</ul>
</blockquote>
<p><strong>LeakyReLU(激活函数)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">keras.layers.LeakyReLU(alpha=<span class="number">0.3</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<ul>
<li>It allows a small gradient when the unit is not active: <code>f(x) = alpha * x for x &lt; 0</code>, <code>f(x) = x for x &gt;= 0</code></li>
<li>alpha is usually chosen 0.2</li>
</ul>
</blockquote>
<p><strong>BatchNormalization(批正则化)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">keras.layers.BatchNormalization(axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="keyword">True</span>, scale=<span class="keyword">True</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<ul>
<li><strong>momentum</strong>: </li>
<li><strong>epsilon</strong>: Small float added to variance to avoid dividing by zero.</li>
</ul>
</blockquote>
<p><strong>Dropout</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">keras.layers.Dropout(rate, noise_shape=<span class="keyword">None</span>, seed=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<ul>
<li><strong>rate</strong>: float between 0 and 1. Fraction of the input units to drop. It seems that it has many options, like 0.1, 0.2, 0.5.</li>
</ul>
</blockquote>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<h3 id="Original"><a href="#Original" class="headerlink" title="Original"></a>Original</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">opt  = RMSprop(lr=<span class="number">0.0003</span>, decay=<span class="number">1e-6</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line">Encoder-Decoder</div>
<div class="line">BatchNormalization(momentum=<span class="number">0.8</span>)</div>
<div class="line">LeakyReLU(<span class="number">0.2</span>)</div>
<div class="line"></div>
<div class="line">Discriminator</div>
<div class="line">BatchNormalization(momentum=<span class="number">0.9</span>)</div>
<div class="line">LeakyReLU(<span class="number">0.2</span>)</div>
</pre></td></tr></table></figure>
<h3 id="DCGAN-CGAN"><a href="#DCGAN-CGAN" class="headerlink" title="DCGAN || CGAN"></a><a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="noopener">DCGAN</a> || <a href="https://github.com/zhangqianhui/Conditional-Gans" target="_blank" rel="noopener">CGAN</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">optim = tf.train.AdamOptimizer(lr=<span class="number">0.0002</span>, beta1=<span class="number">0.5</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<p>beta1 is Momentum term of adam [0.5]</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">lrelu(0.2) </div>
<div class="line">batch_norm(epsilon=1e-5, momentum = 0.9)</div>
</pre></td></tr></table></figure>
<h3 id="INFO-GAN"><a href="#INFO-GAN" class="headerlink" title="INFO-GAN"></a><a href="https://github.com/openai/InfoGAN" target="_blank" rel="noopener">INFO-GAN</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">d_opt = tf.train.AdamOptimizer(lr = <span class="number">0.0002</span>, beta1=<span class="number">0.5</span>)</div>
<div class="line">g_opt = tf.train.AdamOptimizer(lr = <span class="number">0.001</span>, beta1=<span class="number">0.5</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">batch_norm(epsilon=<span class="number">1e-5</span>,momentum=<span class="number">0.1</span>)</div>
<div class="line">Lrelu(<span class="number">0.1</span>)</div>
</pre></td></tr></table></figure>
<h3 id="AC-GAN"><a href="#AC-GAN" class="headerlink" title="AC-GAN"></a><a href="https://arxiv.org/pdf/1610.09585.pdf" target="_blank" rel="noopener">AC-GAN</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">optim = Adam(lr=[<span class="number">0.0001</span>,<span class="number">0.0002</span>,<span class="number">0.00003</span>],beta1=<span class="number">0.5</span>,beta2=<span class="number">0.999</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">LRELU(<span class="number">0.2</span>)</div>
<div class="line">dropout(<span class="number">0.5</span>)</div>
</pre></td></tr></table></figure>
<h3 id="Adversarial-Autoencoder"><a href="#Adversarial-Autoencoder" class="headerlink" title="Adversarial Autoencoder"></a>Adversarial Autoencoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">optim = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>,beta1=<span class="number">0.9</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">dropout(<span class="number">0.2</span>)</div>
</pre></td></tr></table></figure>
<h3 id="BI-GAN"><a href="#BI-GAN" class="headerlink" title="BI-GAN"></a>BI-GAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">optim = Adam(lr=<span class="number">0.0002</span>,beta1=<span class="number">0.5</span>,beta2=<span class="number">0.999</span>)</div>
<div class="line">lr decays exponentially to <span class="number">0.000002</span> starting halfway through training.</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">lrelu(<span class="number">0.2</span>)</div>
<div class="line">(params free) batch_normalization()</div>
</pre></td></tr></table></figure>
<h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">Adam(lr=0.0001,beta1=0,beta2=0.9) -&gt; WGAN with Gradient penalty</div>
<div class="line">RMSProp(lr=0.00005) -&gt; WGAN with weight clipping</div>
</pre></td></tr></table></figure>
<h3 id="LSGAN"><a href="#LSGAN" class="headerlink" title="LSGAN"></a>LSGAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">RMSProp(lr=<span class="number">0.0001</span>)[chosen by search over lr=<span class="number">.001</span>,<span class="number">.0002</span>,<span class="number">.0001</span>]</div>
</pre></td></tr></table></figure>
<h3 id="SR-GAN"><a href="#SR-GAN" class="headerlink" title="SR-GAN"></a>SR-GAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">Adam(lr=<span class="number">0.0001</span>,beta1=<span class="number">0.9</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">lrelu(0.2)</div>
<div class="line">batch_normalization()</div>
</pre></td></tr></table></figure>
<h3 id="PixelDA"><a href="#PixelDA" class="headerlink" title="PixelDA"></a>PixelDA</h3><p><a href="https://arxiv.org/pdf/1612.05424.pdf" target="_blank" rel="noopener">loss weights</a> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">Adam(lr=[<span class="number">0.0002</span>,<span class="number">0.001</span>],beta1=<span class="number">0.5</span>)</div>
<div class="line">Learning rate decayed by <span class="number">0.95</span> every <span class="number">20</span>,<span class="number">000</span> steps</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">lrelu(<span class="number">0.2</span>)</div>
<div class="line">dropout(<span class="number">0.1</span>) <span class="comment"># with keep probability of 90%</span></div>
</pre></td></tr></table></figure>

</div></div>
<h1 id="DC-GAN"><a href="#DC-GAN" class="headerlink" title="DC-GAN"></a>DC-GAN</h1><p><a href="https://julianzaidi.wordpress.com/2017/04/24/deep-convolution-gan-dcgan-architecture-and-training/" target="_blank" rel="noopener">Deep Convolutional GAN (DCGAN) : Architecture and choice of the good set of hyper-parameters</a></p>
<h1 id="VGG-FEATURE-LOSS"><a href="#VGG-FEATURE-LOSS" class="headerlink" title="VGG-FEATURE-LOSS"></a>VGG-FEATURE-LOSS</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG19</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vgg</span><span class="params">()</span>:</span></div>
<div class="line">    vgg = VGG19(weights=<span class="string">"imagenet"</span>)</div>
<div class="line">    vgg.outputs = [vgg.layers[<span class="number">9</span>].output]</div>
<div class="line">    img = layers.Input(shape=self.img_shape)</div>
<div class="line">    img_features = vgg(img)</div>
<div class="line">    <span class="keyword">return</span> Model(img, img_features)</div>
<div class="line"><span class="comment">##########################################</span></div>
<div class="line">vgg = build_vgg()</div>
<div class="line">vgg.trainable = <span class="keyword">False</span></div>
<div class="line">vgg.compile(loss=<span class="string">'mse'</span>,optimizer=optimizer,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line"><span class="comment">##########################################</span></div>
<div class="line">fake_pose_vgg_feature = vgg(pose_recons)</div>
<div class="line">pose_ende = Model(pose_img,fake_pose_vgg_feature)</div>
<div class="line"> D D</div>
<div class="line"><span class="comment">##########################################</span></div>
<div class="line">pose_real_vgg_feature = vgg.predict(pose_imgs)</div>
<div class="line">pose_loss = pose_ende.train_on_batch(pose_imgs,pose_real_vgg_feature)</div>
</pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/24/Neural-Network/" rel="next" title="Neural Network">
                <i class="fa fa-chevron-left"></i> Neural Network
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/05/Linux命令/" rel="prev" title="Linux命令">
                Linux命令 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">67</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">51</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Preprocessing"><span class="nav-number">1.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-labels"><span class="nav-number">2.</span> <span class="nav-text">Soft labels</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">3.</span> <span class="nav-text">Weight Initialization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-with-initializing-all-weights-to-0"><span class="nav-number">3.1.</span> <span class="nav-text">Problem with initializing all weights to 0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Problems-with-initializing-weights-randomly-ref"><span class="nav-number">3.2.</span> <span class="nav-text">Problems with initializing weights randomly ref</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-gradients"><span class="nav-number">3.2.1.</span> <span class="nav-text">Vanishing gradients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exploding-gradients"><span class="nav-number">3.2.2.</span> <span class="nav-text">Exploding gradients</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#New-Initialization-techniques-ref"><span class="nav-number">3.3.</span> <span class="nav-text">New Initialization techniques ref</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Xavier-initialization"><span class="nav-number">3.3.1.</span> <span class="nav-text">Xavier initialization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#He-initialization"><span class="nav-number">3.3.2.</span> <span class="nav-text">He initialization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激活函数"><span class="nav-number">4.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid函数"><span class="nav-number">4.1.</span> <span class="nav-text">Sigmoid函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tanh函数"><span class="nav-number">4.2.</span> <span class="nav-text">Tanh函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLu函数"><span class="nav-number">4.3.</span> <span class="nav-text">ReLu函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeakyReLu"><span class="nav-number">4.4.</span> <span class="nav-text">LeakyReLu</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELU"><span class="nav-number">4.5.</span> <span class="nav-text">ELU</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Maxout"><span class="nav-number">5.</span> <span class="nav-text">Maxout</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax函数"><span class="nav-number">5.1.</span> <span class="nav-text">Softmax函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorboard"><span class="nav-number">6.</span> <span class="nav-text">Tensorboard</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Residual-Network"><span class="nav-number">7.</span> <span class="nav-text">Residual Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation"><span class="nav-number">7.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Models"><span class="nav-number">7.2.</span> <span class="nav-text">Models</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GAN"><span class="nav-number">8.</span> <span class="nav-text">GAN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Original"><span class="nav-number">8.0.1.</span> <span class="nav-text">Original</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DCGAN-CGAN"><span class="nav-number">8.0.2.</span> <span class="nav-text">DCGAN || CGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#INFO-GAN"><span class="nav-number">8.0.3.</span> <span class="nav-text">INFO-GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AC-GAN"><span class="nav-number">8.0.4.</span> <span class="nav-text">AC-GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Autoencoder"><span class="nav-number">8.0.5.</span> <span class="nav-text">Adversarial Autoencoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BI-GAN"><span class="nav-number">8.0.6.</span> <span class="nav-text">BI-GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WGAN"><span class="nav-number">8.0.7.</span> <span class="nav-text">WGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSGAN"><span class="nav-number">8.0.8.</span> <span class="nav-text">LSGAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SR-GAN"><span class="nav-number">8.0.9.</span> <span class="nav-text">SR-GAN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PixelDA"><span class="nav-number">8.0.10.</span> <span class="nav-text">PixelDA</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DC-GAN"><span class="nav-number">9.</span> <span class="nav-text">DC-GAN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VGG-FEATURE-LOSS"><span class="nav-number">10.</span> <span class="nav-text">VGG-FEATURE-LOSS</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
