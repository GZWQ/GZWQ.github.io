<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning," />










<meta name="description" content="Techniques for training a Deep Learning model.">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Training Techiniques">
<meta property="og:url" content="http://yoursite.com/2018/08/03/Training-Techiniques/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Techniques for training a Deep Learning model.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%203.37.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-23%20at%203.01.12%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-21%20at%209.52.33%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-21%20at%209.53.03%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-21%20at%2010.01.01%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-21%20at%2010.01.40%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/gif.gif">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1_Lv9TNpAXffRnO0p0WMGJwQ.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1_QIzXjH8uefVbcaycsjfdmw.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1_zxD6Nr6TyAb8JEG6oXAjkg.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-21%20at%2010.12.09%20AM-0765570.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180405225246905-37854887.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180405225314624-527885612.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407143109455-1460017374.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407142351924-124461667.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180405213859690-1933561230.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180405213955224-1791925244.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407142802238-1209499294.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407142923190-79595046.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407142956288-903484055.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407143405654-1995556833.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407143658338-63450857.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/1192699-20180407143807788-1841864822.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-20%20at%204.09.16%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/BNcircuit.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step9.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step8.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step7.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step6.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step5.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step4-4426132.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step3.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step2.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/step0.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/692825-20180328173642905-311674055.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/20171028233231084.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-13%20at%204.07.13%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/20171028235024692.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/20171028235337038.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.54.22%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.55.56%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.58.24%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.59.26%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-23%20at%203.14.40%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-23%20at%203.18.02%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/nesterov_update_vector.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-06-18%20at%205.12.22%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202018-11-16%20at%202.33.45%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/residualnet_34.png">
<meta property="og:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/2228224-1a6202911b46d1dc.png">
<meta property="og:updated_time" content="2019-06-24T16:02:12.498Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Techiniques">
<meta name="twitter:description" content="Techniques for training a Deep Learning model.">
<meta name="twitter:image" content="http://yoursite.com/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%203.37.19%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/08/03/Training-Techiniques/"/>





  <title>Training Techiniques | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/03/Training-Techiniques/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Training Techiniques</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-03T11:47:08-05:00">
                2018-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Techniques for training a Deep Learning model.</p>
<a id="more"></a>
<h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-17 at 3.37.19 PM.png" alt="creen Shot 2019-02-17 at 3.37.19 P"></p>
<p>Say we have a binary classification problem where we want to draw a line to separate these red points from these blue points, like the following picture. On the left, thses data points are not normalized and not centered and far away from the origion, although we can still use a line to seperate them, if that line wiggles just a little bit, then our classification is going to get totally destroyed. That kind of means that in the example on the left, the loss function is now extremely sensitive to small perturbations in that linear classifier in our weight matrix. We can still represent the same functions, but that might make learning quite difficult.</p>
<p>On the right situation, if you take the data cloud and move it into the origin and you make it unit variance, then now again, we can still classfiy that data quite well, but now as we wiggle that line a little bit, our loss function is less sensitive to small perturbations in the parameter values. That maybe makes optimization a little bit easier.</p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-23 at 3.01.12 PM.png" alt="creen Shot 2019-02-23 at 3.01.12 P"></p>
<h1 id="Soft-labels"><a href="#Soft-labels" class="headerlink" title="Soft labels"></a>Soft labels</h1><p>This is extremely important when training the discriminator. Having hard labels (1 or 0) nearly killed all learning early on, leading the discriminator to approach 0 loss very rapidly. I ended up using a random number between 0 and 0.1 to represent 0 labels (real images) and a random number between 0.9 and 1.0 to represent 1 labels (generated images). This is not required when training the generator. <a href="https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9" target="_blank" rel="noopener">resource</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">valid = np.random.uniform(<span class="number">0.9</span>,<span class="number">1.0</span>,size=(batch_size,<span class="number">1</span>))</div>
<div class="line">fake = np.random.uniform(<span class="number">0</span>,<span class="number">0.1</span>,size=(batch_size,<span class="number">1</span>))</div>
</pre></td></tr></table></figure>
<h1 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h1><p>Loss not going down: learning rate is too low</p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 9.52.33 AM.png" alt="creen Shot 2019-02-21 at 9.52.33 A"></p>
<p>Loss exploding: learning rate too high</p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 9.53.03 AM.png" alt="creen Shot 2019-02-21 at 9.53.03 A"></p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 10.01.01 AM.png" alt="creen Shot 2019-02-21 at 10.01.01 A"></p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 10.01.40 AM.png" alt="creen Shot 2019-02-21 at 10.01.40 A"></p>
<h1 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h1><h2 id="Problem-with-initializing-all-weights-to-0"><a href="#Problem-with-initializing-all-weights-to-0" class="headerlink" title="Problem with initializing all weights to 0"></a>Problem with initializing all weights to 0</h2><p>In this case, the equations of the learning algorithm would fail to make any changes to the network weights, and the model will be stuck. It is important to note that the bias weight in each neuron is set to zero by default, not a small random value. <a href="https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/" target="_blank" rel="noopener">ref</a> </p>
<p>During forward propagation each unit in hidden layer gets signal:</p>
<p><img src="/2018/08/03/Training-Techiniques/gif.gif" alt="i"></p>
<p>That is, each hidden unit gets sum of inputs multiplied by the corresponding weight.</p>
<p>Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, <strong>each hidden unit will get exactly the same signal</strong>. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs <code>sigmoid(sum(inputs))</code>). If all weights are zeros, which is even worse, every hidden unit will get zero signal. <strong>No matter what was the input - if all weights are the same, all units in hidden layer will be the same too</strong>.</p>
<p>This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). Note, that this issue affects all architectures that use each-to-each connections. <a href="https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers" target="_blank" rel="noopener">ref</a></p>
<h2 id="Problems-with-initializing-weights-randomly-ref"><a href="#Problems-with-initializing-weights-randomly-ref" class="headerlink" title="Problems with initializing weights randomly ref"></a>Problems with initializing weights randomly <a href="https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" target="_blank" rel="noopener">ref</a></h2><h3 id="Vanishing-gradients"><a href="#Vanishing-gradients" class="headerlink" title="Vanishing gradients"></a>Vanishing gradients</h3><p>If weights are initialized with low values it gets mapped to 0, then the activation value would be small, say, almost 0. When backpropogating gradients, samll gradients times small weights, it will result in smaller gradients, meaning the earlier layers, the samller gradients, resulting vanishing gradients.</p>
<h3 id="Exploding-gradients"><a href="#Exploding-gradients" class="headerlink" title="Exploding gradients"></a>Exploding gradients</h3><p>Consider you have non-negative and large weights and small activations A (as can be the case for sigmoid(z)). When these weights are multiplied along the layers, they cause a large change in the cost. Thus, the gradients are also going to be large. This means that the changes in W, by <code>W — ⍺ * dW,</code> will be in huge steps, the downward moment will increase.</p>
<blockquote>
<p>This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn!</p>
</blockquote>
<p>Another impact of exploding gradients is that huge values of the gradients may cause number overflow resulting in incorrect computations or introductions of NaN’s. This might also lead to the loss taking the value NaN.</p>
<h2 id="New-Initialization-techniques-ref"><a href="#New-Initialization-techniques-ref" class="headerlink" title="New Initialization techniques ref"></a>New Initialization techniques <a href="https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78" target="_blank" rel="noopener">ref</a></h2><h3 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h3><p>It is used when we use tanh as our activation function.</p>
<p><img src="/2018/08/03/Training-Techiniques/1_Lv9TNpAXffRnO0p0WMGJwQ.png" alt="_Lv9TNpAXffRnO0p0WMGJw"></p>
<p>Some also use the following as initialization:</p>
<p><img src="/2018/08/03/Training-Techiniques/1_QIzXjH8uefVbcaycsjfdmw.png" alt="_QIzXjH8uefVbcaycsjfdm"></p>
<h3 id="He-initialization"><a href="#He-initialization" class="headerlink" title="He initialization"></a>He initialization</h3><p>When we use relu as activation, we just simply multiply random initialization with:</p>
<p><img src="/2018/08/03/Training-Techiniques/1_zxD6Nr6TyAb8JEG6oXAjkg.png" alt="_zxD6Nr6TyAb8JEG6oXAjk"></p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 10.12.09 AM-0765570.png" alt="creen Shot 2019-02-21 at 10.12.09 AM-076557"></p>
<h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p>
<p>The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.</p>
<p>It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension. [cs231n]</p>
<p>Usually, in order to train a neural network, we do some preprocessing to the input data. For example, we could normalize all data (whitening) so that it resembles a normal distribution (that means, zero mean and a unitary variance). Why do we do this preprocessing? Well, there are many reasons for that, some of them being: preventing the early saturation of non-linear activation functions like the sigmoid function, assuring that all input data is in the same range of values, etc.</p>
<p>But the problem appears in the intermediate layers because the distribution of the activations is constantly changing during training. This slows down the training process because each layer must learn to adapt themselves to a new distribution in every training step. This problem is known as <strong>internal covariate shift</strong>.</p>
<p>So… what happens if we force the input of every layer to have approximately the same distribution in every training step? <a href="https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad" target="_blank" rel="noopener">ref</a> </p>
<p>BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong> <a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">ref</a> </p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>Batch normalization is a method we can use to normalize the inputs of each layer, in order to fight the internal covariate shift problem.</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180405225246905-37854887.png" alt="192699-20180405225246905-3785488"></p>
<p>假设某个隐层神经元原先的激活输入x取值符合正态分布，正态分布均值是-2，方差是0.5，对应上图中最左端的浅蓝色曲线，通过BN后转换为均值为0，方差是1的正态分布（对应上图中的深蓝色图形），意味着什么，意味着输入x的取值正态分布整体右移2（均值的变化），图形曲线更平缓了（方差增大的变化）。这个图的意思是，BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差为1的正态分布通过平移均值压缩或者扩大曲线尖锐程度，调整为均值为0方差为1的正态分布。</p>
<p>　　那么把激活输入x调整到这个正态分布有什么用？首先我们看下均值为0，方差为1的标准正态分布代表什么含义：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180405225314624-527885612.png" alt="192699-20180405225314624-52788561"></p>
<p>这意味着在一个标准差范围内，也就是说64%的概率x其值落在[-1,1]的范围内，在两个标准差范围内，也就是说95%的概率x其值落在了[-2,2]的范围内。那么这又意味着什么？我们知道，激活值x=WU+B,U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid，那么看下sigmoid(x)其图形：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143109455-1460017374.png" alt="192699-20180407143109455-146001737"></p>
<p>及sigmoid(x)的导数为：G’=f(x)*(1-f(x))，因为f(x)=sigmoid(x)在0到1之间，所以G’在0到0.25之间，其对应的图如下：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142351924-124461667.png" alt="192699-20180407142351924-12446166"></p>
<p>假设没有经过BN调整前x的原先正态分布均值是-6，方差是1，那么意味着95%的值落在了[-8,-4]之间，那么对应的Sigmoid（x）函数的值明显接近于0，这是典型的梯度饱和区，在这个区域里梯度变化很慢，为什么是梯度饱和区？请看下sigmoid(x)如果取值接近0或者接近于1的时候对应导数函数取值，接近于0，意味着梯度变化很小甚至消失。而假设经过BN后，均值是0，方差是1，那么意味着95%的x值落在了[-2,2]区间内，很明显这一段是sigmoid(x)函数接近于线性变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也即是梯度变化较大，对应导数函数图中明显大于0的区域，就是梯度非饱和区。</p>
<p>　　从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？就是说<strong>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p>
<p>　　但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的<strong>表达能力</strong>下降了，这也意味着深度的意义就没有了。<strong>所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)</strong>，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。</p>
<p><strong>训练阶段如何做BatchNorm</strong></p>
<p>上面是对BN的抽象分析和解释，具体在Mini-Batch SGD下做BN怎么做？其实论文里面这块写得很清楚也容易理解。为了保证这篇文章完整性，这里简单说明下。</p>
<p>　　假设对于一个深层神经网络来说，其中两层结构如下：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180405213859690-1933561230.png" alt="192699-20180405213859690-193356123"></p>
<p>要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180405213955224-1791925244.png" alt="192699-20180405213955224-179192524"></p>
<p>对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142802238-1209499294.png" alt="192699-20180407142802238-120949929"></p>
<p>要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p>
<p>　　上文说过经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。</strong>但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142923190-79595046.png" alt="192699-20180407142923190-7959504"></p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142956288-903484055.png" alt="192699-20180407142956288-90348405"></p>
<p>Look at the last line of the algorithm. After normalizing the input <code>x</code> the result is squashed through a linear function with parameters <code>gamma</code> and <code>beta</code>. These are learnable parameters of the BatchNorm Layer and make it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.” If <code>gamma = sqrt(var(x))</code> and <code>beta = mean(x)</code>, the original activation is restored. This is, what makes BatchNorm really powerful. We initialize the BatchNorm Parameters to transform the input to zero mean/unit variance distributions but during training they can learn that any other distribution might be better. </p>
<p>Btw: it’s called “Batch” Normalization because we perform this transformation and calculate the statistics only for a subpart (a batch) of the entire training set. <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">source</a> </p>
<p><strong>BatchNorm的推理(Inference)过程</strong></p>
<p>BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p>
<p>　　既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p>
<p>　　决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143405654-1995556833.png" alt="192699-20180407143405654-199555683"></p>
<p>有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143658338-63450857.png" alt="192699-20180407143658338-6345085"></p>
<p>这个公式其实和训练时</p>
<p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143807788-1841864822.png" alt="192699-20180407143807788-184186482"></p>
<p>是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？我猜作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：</p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-20 at 4.09.16 PM.png" alt="creen Shot 2019-02-20 at 4.09.16 P"></p>
<p>都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p>
<p><strong>BatchNorm的好处</strong></p>
<p>BatchNorm为什么NB呢，关键还是效果好。<strong>①**</strong>不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。**总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h3><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
<div class="line">104</div>
<div class="line">105</div>
<div class="line">106</div>
<div class="line">107</div>
<div class="line">108</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Forward pass for batch normalization.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></div>
<div class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></div>
<div class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></div>
<div class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></div>
<div class="line"><span class="string">    data at test-time.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></div>
<div class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></div>
<div class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></div>
<div class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></div>
<div class="line"><span class="string">    large number of training images rather than using a running average. For</span></div>
<div class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></div>
<div class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></div>
<div class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Input:</span></div>
<div class="line"><span class="string">    - x: Data of shape (N, D)</span></div>
<div class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></div>
<div class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></div>
<div class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></div>
<div class="line"><span class="string">      - mode: 'train' or 'test'; required</span></div>
<div class="line"><span class="string">      - eps: Constant for numeric stability</span></div>
<div class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></div>
<div class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></div>
<div class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - out: of shape (N, D)</span></div>
<div class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    mode = bn_param[<span class="string">'mode'</span>]</div>
<div class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</div>
<div class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</div>
<div class="line"></div>
<div class="line">    N, D = x.shape</div>
<div class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</div>
<div class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</div>
<div class="line"></div>
<div class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></div>
<div class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></div>
<div class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></div>
<div class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></div>
<div class="line">        <span class="comment">#                                                                     #</span></div>
<div class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></div>
<div class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></div>
<div class="line">        <span class="comment"># variable.                                                           #</span></div>
<div class="line">        <span class="comment">#                                                                     #</span></div>
<div class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></div>
<div class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></div>
<div class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></div>
<div class="line">        <span class="comment"># variables.                                                          #</span></div>
<div class="line">        <span class="comment">#                                                                     #</span></div>
<div class="line">        <span class="comment"># Note that though you should be keeping track of the running         #</span></div>
<div class="line">        <span class="comment"># variance, you should normalize the data based on the standard       #</span></div>
<div class="line">        <span class="comment"># deviation (square root of variance) instead!                        # </span></div>
<div class="line">        <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)   #</span></div>
<div class="line">        <span class="comment"># might prove to be helpful.                                          #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">        sample_mean = np.mean(x,axis=<span class="number">0</span>)</div>
<div class="line">        sample_var = np.var(x,axis=<span class="number">0</span>)</div>
<div class="line">        x_hat = (x-sample_mean)/np.sqrt(sample_var+eps)</div>
<div class="line">        out = gamma*x_hat + beta</div>
<div class="line">        </div>
<div class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</div>
<div class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</div>
<div class="line">        </div>
<div class="line">        cache = (x,sample_mean,sample_var,x_hat,out,gamma,beta,eps)</div>
<div class="line"></div>
<div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></div>
<div class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></div>
<div class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></div>
<div class="line">        <span class="comment"># Store the result in the out variable.                               #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">        out = gamma*(x-running_mean)/np.sqrt(running_var+eps) + beta</div>
<div class="line"></div>
<div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">    <span class="keyword">else</span>:</div>
<div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Store the updated running means back into bn_param</span></div>
<div class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</div>
<div class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> out, cache</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="Navie-Backward"><a href="#Navie-Backward" class="headerlink" title="Navie Backward"></a><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">Navie Backward</a></h3><p><img src="/2018/08/03/Training-Techiniques/BNcircuit.png" alt="Ncircui"></p>
<h4 id="Step-9"><a href="#Step-9" class="headerlink" title="Step 9"></a>Step 9</h4><p><img src="/2018/08/03/Training-Techiniques/step9.png" alt="tep"></p>
<h4 id="Step-8"><a href="#Step-8" class="headerlink" title="Step 8"></a>Step 8</h4><p><img src="/2018/08/03/Training-Techiniques/step8.png" alt="tep"></p>
<h4 id="Step-7"><a href="#Step-7" class="headerlink" title="Step 7"></a>Step 7</h4><p><img src="/2018/08/03/Training-Techiniques/step7.png" alt="tep"></p>
<h4 id="Step-6"><a href="#Step-6" class="headerlink" title="Step 6"></a>Step 6</h4><p><img src="/2018/08/03/Training-Techiniques/step6.png" alt="tep"></p>
<h4 id="Step-5"><a href="#Step-5" class="headerlink" title="Step 5"></a>Step 5</h4><p><img src="/2018/08/03/Training-Techiniques/step5.png" alt="tep"></p>
<h4 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h4><p><img src="/2018/08/03/Training-Techiniques/step4-4426132.png" alt="tep4-442613"></p>
<h4 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h4><p><img src="/2018/08/03/Training-Techiniques/step3.png" alt="tep"></p>
<h4 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h4><p><img src="/2018/08/03/Training-Techiniques/step2.png" alt="tep"></p>
<h4 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h4><p><img src="/2018/08/03/Training-Techiniques/step1.png" alt="tep"></p>
<h4 id="Step-0"><a href="#Step-0" class="headerlink" title="Step 0"></a>Step 0</h4><p><img src="/2018/08/03/Training-Techiniques/step0.png" alt="tep"></p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Backward pass for batch normalization.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></div>
<div class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></div>
<div class="line"><span class="string">    intermediate nodes.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></div>
<div class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></div>
<div class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></div>
<div class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    </div>
<div class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></div>
<div class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></div>
<div class="line">    <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)       #</span></div>
<div class="line">    <span class="comment"># might prove to be helpful.                                              #</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    N,D = dout.shape</div>
<div class="line">    (x,sample_mean,sample_var,x_hat,out,gamma,beta,eps) = cache</div>
<div class="line">    </div>
<div class="line">    dbeta = np.sum(dout,axis=<span class="number">0</span>)</div>
<div class="line">    dgamma = np.sum(x_hat*dout,axis=<span class="number">0</span>)</div>
<div class="line">    dx_hat = dout*gamma</div>
<div class="line">    </div>
<div class="line">    ivar = <span class="number">1</span>/np.sqrt(sample_var+eps)</div>
<div class="line">    xmu = x-sample_mean</div>
<div class="line">    </div>
<div class="line">    dx_xmu1 = dx_hat*ivar </div>
<div class="line">    divar = np.sum(dx_hat*xmu,axis=<span class="number">0</span>)</div>
<div class="line">    </div>
<div class="line">    dsqrt_var = <span class="number">-1</span>*divar/(sample_var+eps)</div>
<div class="line">    </div>
<div class="line">    dvar = <span class="number">0.5</span>*dsqrt_var/np.sqrt(sample_var+eps)</div>
<div class="line">    </div>
<div class="line">    dsq = np.ones((N,D))*dvar/N</div>
<div class="line">    </div>
<div class="line">    dx_xmu2 = <span class="number">2</span>*xmu*dsq</div>
<div class="line">    </div>
<div class="line">    dx1 = dx_xmu1 + dx_xmu2</div>
<div class="line">    dmu = <span class="number">-1</span>*np.sum(dx_xmu1 + dx_xmu2,axis=<span class="number">0</span>)</div>
<div class="line">    </div>
<div class="line">    dx2 = np.ones((N,D))*dmu/N</div>
<div class="line">    </div>
<div class="line">    dx = dx1+dx2</div>
<div class="line">    </div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="Alternative-Backward"><a href="#Alternative-Backward" class="headerlink" title="Alternative Backward"></a><a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="noopener">Alternative Backward</a></h3><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p><a href="https://blog.csdn.net/Jaster_wisdom/article/details/78380839" target="_blank" rel="noopener">ref1</a> <a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">zhihu</a> <a href="http://www.sohu.com/a/200918239_206784" target="_blank" rel="noopener">intuition</a> <a href="https://www.cnblogs.com/rgvb178/p/6055213.html" target="_blank" rel="noopener">details</a> <a href="https://towardsdatascience.com/deep-learning-concepts-part-1-ea0b14b234c8" target="_blank" rel="noopener">to do</a> </p>
<p>根据是否饱和，激活函数可以分类为“饱和激活函数”和“非饱和激活函数”。</p>
<p><strong>sigmoid和tanh</strong>是“饱和激活函数”，而ReLU及其变体则是“非饱和激活函数”，但是他们都属于非线性激活函数。使用“非饱和激活函数”的优势在于两点：     </p>
<p>1.首先，“非饱和激活函数”能解决所谓的“梯度消失”问题。 vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一   </p>
<p> 2.其次，它能加快收敛速度。    </p>
<p><img src="/2018/08/03/Training-Techiniques/692825-20180328173642905-311674055.png" alt="692825-20180328173642905-311674055"></p>
<h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>将实数压缩到$(0,1)$，用来二分类。</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{1+e^{-x}}</script><p><img src="/2018/08/03/Training-Techiniques/20171028233231084.png" alt="20171028233231084"></p>
<p>sigmoid的优缺点<a href="https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/" target="_blank" rel="noopener">source</a> </p>
<ul>
<li><p><strong>Vanishing gradients</strong>: Notice, the sigmoid function is flat near 0 and 1. In other words, the gradient of the sigmoid is near 0 and 1. During backpropagation through the network with sigmoid activation, the gradients in neurons whose output is near 0 or 1 are nearly 0. These neurons are called saturated neurons. Thus, the weights in these neurons do not update. Not only that, the weights of neurons connected to such neurons are also slowly updated. This problem is also known as vanishing gradient. So, imagine if there was a large network comprising of sigmoid neurons in which many of them are in a saturated regime, then the network will not be able to backpropagate.</p>
<blockquote>
<p><a href="http://www.sohu.com/a/148114422_500659" target="_blank" rel="noopener">ref1</a> 在GAN中， 给 D 最后的输出加个 Sigmoid 激活函数，让它取值在 0 到 1 之间？事实上这个方案在理论上是没有问题的，然而这会造成训练的困难。因为 Sigmoid 函数具有饱和区，一旦 D 进入了饱和区，就很难传回梯度来更新 G 了。</p>
</blockquote>
</li>
<li><p><strong>Not zero centered</strong>: Sigmoid outputs are not zero-centered. The output is always between 0 and 1, that means that the output after applying sigmoid is always positive hence, i.e. $x_i &gt; 0$.</p>
<p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-13%20at%204.07.13%20PM.png" alt="creen Shot 2019-02-13 at 4.07.13 P"></p>
<p>which means the gradients on the weights $w$ during backpropagation become either all positive or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. Say we </p>
<blockquote>
<ul>
<li>Tanh, Rectified Linear Unit (ReLU), Leaky ReLU and Parametric ReLU are all zero-centered activation functions.</li>
<li>This is also why you want zero-mean data</li>
</ul>
</blockquote>
</li>
<li><p><strong>Computationally expensive</strong>: The exp() function is computationally expensive compared with the other non-linear activation functions.</p>
</li>
</ul>
<h2 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h2><p>将实数压缩到$[-1,1]$</p>
<script type="math/tex; mode=display">
f(x) = tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
 f'(x)=1-f^2(x)</script><p><img src="/2018/08/03/Training-Techiniques/20171028235024692.png" alt="20171028235024692"></p>
<blockquote>
<p>ZERO-centered but still kills gradients when saturated</p>
</blockquote>
<h2 id="ReLu函数"><a href="#ReLu函数" class="headerlink" title="ReLu函数"></a>ReLu函数</h2><script type="math/tex; mode=display">
f(x)=max(0,x)</script><p><img src="/2018/08/03/Training-Techiniques/20171028235337038.png" alt="20171028235337038"></p>
<blockquote>
<p><strong>Advantages:</strong> Does not saturate (in +region); Very computationally efficient; Converges much faster than sigmoid/tanh in practice (e.g. 6x); </p>
<p><strong>Disadvantages:</strong> Not zero-centered output; It kills the gradient when $x \le 0$  </p>
<p><a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks" target="_blank" rel="noopener">dead unit problem</a> <a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" target="_blank" rel="noopener">dying unit</a> </p>
</blockquote>
<h2 id="LeakyReLu"><a href="#LeakyReLu" class="headerlink" title="LeakyReLu"></a>LeakyReLu</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.54.22%20PM.png" alt="creen Shot 2019-02-17 at 2.54.22 P"></p>
<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.55.56%20PM.png" alt="creen Shot 2019-02-17 at 2.55.56 P"></p>
<h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.58.24%20PM.png" alt="creen Shot 2019-02-17 at 2.58.24 P"></p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-17 at 2.59.26 PM.png" alt="creen Shot 2019-02-17 at 2.59.26 P"></p>
<h1 id="Optimizer-ref"><a href="#Optimizer-ref" class="headerlink" title="Optimizer ref"></a>Optimizer <a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">ref</a></h1><p><a href="https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/" target="_blank" rel="noopener">ref</a> </p>
<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>The simplest form of update is to change the parameters along the negative gradient direction (<strong>since the gradient indicates the direction of increase</strong>, but we usually wish to minimize a loss function). Assuming a vector of parameters <code>x</code> and the gradient <code>dx</code>, the simplest update has the form:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># Vanilla update</span></div>
<div class="line">x += - learning_rate * dx</div>
</pre></td></tr></table></figure>
<p>where <code>learning_rate</code> is a hyperparameter - a fixed constant. When evaluated on the full dataset, and when the learning rate is low enough, this is guaranteed to make non-negative progress on the loss function.</p>
<p><a href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7" target="_blank" rel="noopener">problem in sgd</a> </p>
<p><a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" target="_blank" rel="noopener">Stochastic Gradient Descent with momentum</a> </p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-23 at 3.14.40 PM.png" alt="creen Shot 2019-02-23 at 3.14.40 P"></p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-23 at 3.18.02 PM.png" alt="creen Shot 2019-02-23 at 3.18.02 P"></p>
<p>Our gradients come from minibatches so they can be noisy!</p>
<h2 id="SGD-Momentum-ref"><a href="#SGD-Momentum-ref" class="headerlink" title="SGD+Momentum ref"></a>SGD+Momentum <a href="https://deepnotes.io/sgd-momentum-adaptive" target="_blank" rel="noopener">ref</a></h2><p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations.</p>
<script type="math/tex; mode=display">
v_t = \gamma v_{t-1}+\eta \Delta_{\theta}J(\theta)\\
\theta=\theta-v_t</script><p>The momentum term $\gamma$ tends to be 0.9/0.99.</p>
<p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. $\gamma &lt;1$). The same thing happens to our parameter updates: <strong>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.</strong> As a result, we gain faster convergence and reduced oscillation.</p>
<h2 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h2><p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p>
<p>Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term $ \gamma v_{t-1}$ to move the parameters $\theta$. <strong>Computing $\theta- \gamma v_{t-1}$ thus gives us an approximation of the next position of the parameters</strong> (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters $\theta$ but w.r.t. the approximate future position of our parameters:</p>
<script type="math/tex; mode=display">
\begin{aligned} v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\ \theta &=\theta-v_{t} \end{aligned}</script><p>Again, we set the momentum term $\gamma$ to a value of around 0.9. While Momentum first computes the current gradient (small blue vector in the following image) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks.</p>
<p><img src="/2018/08/03/Training-Techiniques/nesterov_update_vector.png" alt="esterov_update_vecto"></p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates<br>(i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data.</p>
<p>Previously, we performed an update for all parameters $\theta$ at once as every parameters $\theta_i$ used the same learning rate $\eta$. As Adagrad uses a different learning rate for every parameter $\theta_i$ at every time step $t$, we first show Adagrad’s per-parameter update, which we then vectorize. For brevity, we use $g_t$ to denote the gradient at time step $t$. $g_{t,i}$ is then the partial derivative of the objective function w.r.t. to the parameter $\theta_i$ at time step $t$:</p>
<script type="math/tex; mode=display">
g_{t, i}=\nabla_{\theta} J\left(\theta_{t, i}\right)</script><p>The SGD update for every parameter $\theta_i$ at each time step $t$ then becomes:</p>
<script type="math/tex; mode=display">
\theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i}</script><p>In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_i$ based on the past gradients that have been computed for $\theta_i$:</p>
<script type="math/tex; mode=display">
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}</script><p>$G_{t} \in \mathbb{R}^{d \times d}$ here is a diagonal matrix where each diagonal element $[i,i]$ is the sum of the squares of the gradients w.r.t. $\theta_i$ up to time step $t$, while $\epsilon$ is a smoothing term that avoids division by zero (usually on the order of $1e^{-8}$).</p>
<p>As $G_t$ contains the sum of the squares of the past gradients w.r.t. to all parameters $\theta$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product $\oplus$ between $G_t$ and $g_t$: </p>
<script type="math/tex; mode=display">
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t}</script><p>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. </p>
<p>Instead of inefficiently storing $w$ previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $E[g^2]_t$ at time step $t$ then depends (as a fraction $\gamma$ similarly to the Momentum term) only on the previous average and the current gradient:</p>
<script type="math/tex; mode=display">
E\left[g^{2}\right]_{t}=\gamma E\left[g^{2}\right]_{t-1}+(1-\gamma) g_{t}^{2} \\
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}</script><p>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests $\gamma$ to be set to 0.9, while a good default value for the learning rate is 0.001.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(w, dw, config=None)</span>:</span>  </div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Uses the RMSProp update rule, which uses a moving average of squared</span></div>
<div class="line"><span class="string">    gradient values to set adaptive per-parameter learning rates.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    config format:</span></div>
<div class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></div>
<div class="line"><span class="string">    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared</span></div>
<div class="line"><span class="string">      gradient cache.</span></div>
<div class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></div>
<div class="line"><span class="string">    - cache: Moving average of second moments of gradients.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>: config = &#123;&#125;</div>
<div class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-2</span>)</div>
<div class="line">    config.setdefault(<span class="string">'decay_rate'</span>, <span class="number">0.99</span>)</div>
<div class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</div>
<div class="line">    config.setdefault(<span class="string">'cache'</span>, np.zeros_like(w))</div>
<div class="line"></div>
<div class="line">    next_w = <span class="keyword">None</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the RMSprop update formula, storing the next value of w #</span></div>
<div class="line">    <span class="comment"># in the next_w variable. Don't forget to update cache value stored in    #</span></div>
<div class="line">    <span class="comment"># config['cache'].                                                        #</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">    next_cache = config[<span class="string">'decay_rate'</span>] * config[<span class="string">'cache'</span>] + (<span class="number">1</span> - config[<span class="string">'decay_rate'</span>]) * dw ** <span class="number">2</span></div>
<div class="line">    next_w = w - config[<span class="string">'learning_rate'</span>] * dw / np.sqrt(next_cache + config[<span class="string">'epsilon'</span>])</div>
<div class="line">    config[<span class="string">'cache'</span>] = next_cache</div>
<div class="line"></div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> next_w, config</div>
</pre></td></tr></table></figure>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>In addition to storing an exponentially decaying average of past squared gradients like RMSprop, Adam also keeps an exponentially decaying average of past gradients. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows:</p>
<script type="math/tex; mode=display">
\begin{aligned} m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\ v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \end{aligned}</script><p>$m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. $\beta_1$ and $\beta_2$ are close to 1).</p>
<p>They counteract these biases by computing bias-corrected first and second moment estimates:</p>
<script type="math/tex; mode=display">
\begin{aligned} \hat{m}_{t} &=\frac{m_{t}}{1-\beta_{1}^{t}} \\ \hat{v}_{t} &=\frac{v_{t}}{1-\beta_{2}^{t}} \end{aligned}</script><p><strong>Note that $t$ is the iteration number.</strong></p>
<p>They then use these to update the parameters, which yields the Adam update rule:</p>
<script type="math/tex; mode=display">
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}</script><p>The authors propose default values of 0.9 for $\beta_1$, 0.999 for $\beta_2$ and $10^{-8}$ for $\epsilon$. </p>
<p>It is worth noting that the next new <code>m</code> should be <code>mt</code> instead of $\hat m_t$.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(w, dw, config=None)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Uses the Adam update rule, which incorporates moving averages of both the</span></div>
<div class="line"><span class="string">    gradient and its square and a bias correction term.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    config format:</span></div>
<div class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></div>
<div class="line"><span class="string">    - beta1: Decay rate for moving average of first moment of gradient.</span></div>
<div class="line"><span class="string">    - beta2: Decay rate for moving average of second moment of gradient.</span></div>
<div class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></div>
<div class="line"><span class="string">    - m: Moving average of gradient.</span></div>
<div class="line"><span class="string">    - v: Moving average of squared gradient.</span></div>
<div class="line"><span class="string">    - t: Iteration number.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>: config = &#123;&#125;</div>
<div class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-3</span>)</div>
<div class="line">    config.setdefault(<span class="string">'beta1'</span>, <span class="number">0.9</span>)</div>
<div class="line">    config.setdefault(<span class="string">'beta2'</span>, <span class="number">0.999</span>)</div>
<div class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</div>
<div class="line">    config.setdefault(<span class="string">'m'</span>, np.zeros_like(w))</div>
<div class="line">    config.setdefault(<span class="string">'v'</span>, np.zeros_like(w))</div>
<div class="line">    config.setdefault(<span class="string">'t'</span>, <span class="number">0</span>)</div>
<div class="line"></div>
<div class="line">    next_w = <span class="keyword">None</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the Adam update formula, storing the next value of w in #</span></div>
<div class="line">    <span class="comment"># the next_w variable. Don't forget to update the m, v, and t variables   #</span></div>
<div class="line">    <span class="comment"># stored in config.                                                       #</span></div>
<div class="line">    <span class="comment">#                                                                         #</span></div>
<div class="line">    <span class="comment"># <span class="doctag">NOTE:</span> In order to match the reference output, please modify t _before_  #</span></div>
<div class="line">    <span class="comment"># using it in any calculations.                                           #</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">    mt = config[<span class="string">'beta1'</span>] * config[<span class="string">'m'</span>] + (<span class="number">1</span> - config[<span class="string">'beta1'</span>]) * dw</div>
<div class="line">    vt = config[<span class="string">'beta2'</span>] * config[<span class="string">'v'</span>] + (<span class="number">1</span> - config[<span class="string">'beta2'</span>]) * dw ** <span class="number">2</span></div>
<div class="line"></div>
<div class="line">    t = config[<span class="string">'t'</span>] + <span class="number">1</span></div>
<div class="line"></div>
<div class="line">    mu_mt = mt / (<span class="number">1</span> - config[<span class="string">'beta1'</span>] ** t)</div>
<div class="line">    mu_vt = vt / (<span class="number">1</span> - config[<span class="string">'beta2'</span>] ** t)</div>
<div class="line"></div>
<div class="line">    next_w = w - config[<span class="string">'learning_rate'</span>] * mu_mt / (np.sqrt(mu_vt) + config[<span class="string">'epsilon'</span>])</div>
<div class="line">    config[<span class="string">'m'</span>] = mt</div>
<div class="line">    config[<span class="string">'v'</span>] = vt</div>
<div class="line"></div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></div>
<div class="line">    <span class="comment">###########################################################################</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> next_w, config</div>
</pre></td></tr></table></figure>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><a href="https://towardsdatascience.com/coding-neural-network-dropout-3095632d25ce" target="_blank" rel="noopener">ref</a> <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">ref2</a> </p>
<p><strong>Dropout</strong> is a regularization technique. On each iteration, we randomly shut down some neurons (units) on each layer and don’t use those neurons in both forward propagation and back-propagation. <strong>Since the units that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures (units).</strong> Moreover, dropout help improving generalization error by:</p>
<ul>
<li><strong>Since we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).</strong></li>
<li>Can be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we’re dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won’t help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.</li>
</ul>
<p>We can use different probabilities on each layer; however, the output layer would always have <code>keep_prob = 1</code> and the input layer has high <code>keep_prob</code>such as 0.9 or 1. If a hidden layer has <code>keep_prob = 0.8</code>, this means that; on each iteration, each unit has 80% probablitity of being included and 20% probability of being dropped out.</p>
<p>It’s easy to remember things when the network has a lot of parameters (overfit), but it’s hard to remember things when effectively the network only has so many parameters to work with. Hence, the network must learn to generalize more to get the same performance as remembering things. So, that’s why Dropout will increase the test time performance: it improves generalization and reduce the risk of overfitting. <a href="https://wiseodd.github.io/techblog/2016/06/25/dropout/" target="_blank" rel="noopener">ref</a> </p>
<p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-06-18 at 5.12.22 PM.png" alt="creen Shot 2019-06-18 at 5.12.22 P"></p>
<p>Therefore, at test time we multiply by dropout probability; Or, at training time, we divide by dropout probability.</p>
<h2 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Performs the forward pass for (inverted) dropout.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - x: Input data, of any shape</span></div>
<div class="line"><span class="string">    - dropout_param: A dictionary with the following keys:</span></div>
<div class="line"><span class="string">      - p: Dropout parameter. We keep each neuron output with probability p.</span></div>
<div class="line"><span class="string">      - mode: 'test' or 'train'. If the mode is train, then perform dropout;</span></div>
<div class="line"><span class="string">        if the mode is test, then just return the input.</span></div>
<div class="line"><span class="string">      - seed: Seed for the random number generator. Passing seed makes this</span></div>
<div class="line"><span class="string">        function deterministic, which is needed for gradient checking but not</span></div>
<div class="line"><span class="string">        in real networks.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Outputs:</span></div>
<div class="line"><span class="string">    - out: Array of the same shape as x.</span></div>
<div class="line"><span class="string">    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout</span></div>
<div class="line"><span class="string">      mask that was used to multiply the input; in test mode, mask is None.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.</span></div>
<div class="line"><span class="string">    See http://cs231n.github.io/neural-networks-2/#reg for more details.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    NOTE 2: Keep in mind that p is the probability of **keep** a neuron</span></div>
<div class="line"><span class="string">    output; this might be contrary to some sources, where it is referred to</span></div>
<div class="line"><span class="string">    as the probability of dropping a neuron output.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    p, mode = dropout_param[<span class="string">'p'</span>], dropout_param[<span class="string">'mode'</span>]</div>
<div class="line">    <span class="keyword">if</span> <span class="string">'seed'</span> <span class="keyword">in</span> dropout_param:</div>
<div class="line">        np.random.seed(dropout_param[<span class="string">'seed'</span>])</div>
<div class="line"></div>
<div class="line">    mask = <span class="keyword">None</span></div>
<div class="line">    out = <span class="keyword">None</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase forward pass for inverted dropout.   #</span></div>
<div class="line">        <span class="comment"># Store the dropout mask in the mask variable.                        #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">        mask = np.random.rand(*x.shape)&gt;p <span class="comment">#unpack your shape tuple using * </span></div>
<div class="line">        out = x*mask/p</div>
<div class="line"></div>
<div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test phase forward pass for inverted dropout.   #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">        out = x</div>
<div class="line"></div>
<div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment">#                            END OF YOUR CODE                         #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line"></div>
<div class="line">    cache = (dropout_param, mask)</div>
<div class="line">    out = out.astype(x.dtype, copy=<span class="keyword">False</span>)</div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> out, cache</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Perform the backward pass for (inverted) dropout.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></div>
<div class="line"><span class="string">    - cache: (dropout_param, mask) from dropout_forward.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    dropout_param, mask = cache</div>
<div class="line">    mode = dropout_param[<span class="string">'mode'</span>]</div>
<div class="line"></div>
<div class="line">    dx = <span class="keyword">None</span></div>
<div class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase backward pass for inverted dropout   #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">        dx = mask*dout/dropout_param[<span class="string">'p'</span>]</div>
<div class="line"></div>
<div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></div>
<div class="line">        <span class="comment">#######################################################################</span></div>
<div class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div>
<div class="line">        dx = dout</div>
<div class="line">    <span class="keyword">return</span> dx</div>
</pre></td></tr></table></figure>
<h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p><a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">paper1</a> <a href="https://arxiv.org/pdf/1603.05027v2.pdf" target="_blank" rel="noopener">paper2</a> </p>
<h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>simplely stack layers exhibit higher training error when the depth increases.</p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2018-11-16 at 2.33.45 PM.png" alt="creen Shot 2018-11-16 at 2.33.45 P"></p>
<p><img src="/2018/08/03/Training-Techiniques/residualnet_34.png" alt="esidualnet_3"></p>
<p>Figure from <a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/residual_net.html" target="_blank" rel="noopener">ref</a> </p>
<p><img src="/2018/08/03/Training-Techiniques/2228224-1a6202911b46d1dc.png" alt="228224-1a6202911b46d1d"></p>
<p><a href="https://www.jianshu.com/p/e502e4b43e6d" target="_blank" rel="noopener">pic</a> </p>
<h1 id="PatchGan"><a href="#PatchGan" class="headerlink" title="PatchGan"></a>PatchGan</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate output shape of D (PatchGAN)</span></div>
<div class="line">patch = int(self.img_rows / <span class="number">2</span>**<span class="number">4</span>)</div>
<div class="line">self.disc_patch = (patch, patch, <span class="number">1</span>)</div>
<div class="line"></div>
<div class="line"><span class="comment"># Build and compile the discriminator</span></div>
<div class="line">self.discriminator = self.build_discriminator()</div>
<div class="line">self.discriminator.compile(loss=<span class="string">'mse'</span>,optimizer=optimizer,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_discriminator</span><span class="params">(self)</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">d_layer</span><span class="params">(layer_input, filters, f_size=<span class="number">4</span>, bn=True)</span>:</span></div>
<div class="line">        d = Conv2D(filters, kernel_size=f_size, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)(layer_input)</div>
<div class="line">        d = LeakyReLU(alpha=<span class="number">0.2</span>)(d)</div>
<div class="line">        <span class="keyword">if</span> bn:</div>
<div class="line">            d = BatchNormalization(momentum=<span class="number">0.8</span>)(d)</div>
<div class="line">        <span class="keyword">return</span> d</div>
<div class="line">    img_A = Input(shape=self.img_shape)</div>
<div class="line">    img_B = Input(shape=self.img_shape)</div>
<div class="line">    combined_imgs = Concatenate(axis=<span class="number">-1</span>)([img_A, img_B])</div>
<div class="line">    d1 = d_layer(combined_imgs, self.df, bn=<span class="keyword">False</span>)</div>
<div class="line">    d2 = d_layer(d1, self.df*<span class="number">2</span>)</div>
<div class="line">    d3 = d_layer(d2, self.df*<span class="number">4</span>)</div>
<div class="line">    d4 = d_layer(d3, self.df*<span class="number">8</span>)</div>
<div class="line">    validity = Conv2D(<span class="number">1</span>, kernel_size=<span class="number">4</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(d4)</div>
<div class="line">    <span class="keyword">return</span> Model([img_A, img_B], validity)</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></div>
<div class="line">    valid = np.ones((batch_size,) + self.disc_patch)</div>
<div class="line">    fake = np.zeros((batch_size,) + self.disc_patch)</div>
<div class="line">    </div>
<div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div>
<div class="line">        fake_A = self.generator.predict(imgs_B)</div>
<div class="line">        d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)</div>
<div class="line">        d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)</div>
<div class="line">        d_loss = <span class="number">0.5</span> * np.add(d_loss_real, d_loss_fake)</div>
<div class="line">        </div>
<div class="line">        g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])</div>
</pre></td></tr></table></figure>
<blockquote>
<ul>
<li><p>The output of discriminator is (None, patch, patch, 1), which can be seen using model.summary(). While the traditional discriminator’s output is (None, 1), distinguishing each image.</p>
</li>
<li><p>If traditional discriminator, the last two line codes would be:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">&gt;   d5 = Flatten()(d4)</div>
<div class="line">&gt;   validity = layers.Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>)(d5)</div>
<div class="line">&gt;   <span class="keyword">return</span> Model([img_A, img_B], validity)</div>
<div class="line">&gt;</div>
</pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>  ​</p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/23/Keras/" rel="next" title="Keras">
                <i class="fa fa-chevron-left"></i> Keras
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/05/Linux命令/" rel="prev" title="Linux命令">
                Linux命令 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">85</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">63</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Preprocessing"><span class="nav-number">1.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-labels"><span class="nav-number">2.</span> <span class="nav-text">Soft labels</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-Rate"><span class="nav-number">3.</span> <span class="nav-text">Learning Rate</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">4.</span> <span class="nav-text">Weight Initialization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-with-initializing-all-weights-to-0"><span class="nav-number">4.1.</span> <span class="nav-text">Problem with initializing all weights to 0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Problems-with-initializing-weights-randomly-ref"><span class="nav-number">4.2.</span> <span class="nav-text">Problems with initializing weights randomly ref</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-gradients"><span class="nav-number">4.2.1.</span> <span class="nav-text">Vanishing gradients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exploding-gradients"><span class="nav-number">4.2.2.</span> <span class="nav-text">Exploding gradients</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#New-Initialization-techniques-ref"><span class="nav-number">4.3.</span> <span class="nav-text">New Initialization techniques ref</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Xavier-initialization"><span class="nav-number">4.3.1.</span> <span class="nav-text">Xavier initialization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#He-initialization"><span class="nav-number">4.3.2.</span> <span class="nav-text">He initialization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">5.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation"><span class="nav-number">5.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Method"><span class="nav-number">5.2.</span> <span class="nav-text">Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementation"><span class="nav-number">5.3.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward"><span class="nav-number">5.3.1.</span> <span class="nav-text">Forward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Navie-Backward"><span class="nav-number">5.3.2.</span> <span class="nav-text">Navie Backward</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-9"><span class="nav-number">5.3.2.1.</span> <span class="nav-text">Step 9</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-8"><span class="nav-number">5.3.2.2.</span> <span class="nav-text">Step 8</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-7"><span class="nav-number">5.3.2.3.</span> <span class="nav-text">Step 7</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-6"><span class="nav-number">5.3.2.4.</span> <span class="nav-text">Step 6</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-5"><span class="nav-number">5.3.2.5.</span> <span class="nav-text">Step 5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-4"><span class="nav-number">5.3.2.6.</span> <span class="nav-text">Step 4</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-3"><span class="nav-number">5.3.2.7.</span> <span class="nav-text">Step 3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-2"><span class="nav-number">5.3.2.8.</span> <span class="nav-text">Step 2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-1"><span class="nav-number">5.3.2.9.</span> <span class="nav-text">Step 1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step-0"><span class="nav-number">5.3.2.10.</span> <span class="nav-text">Step 0</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Alternative-Backward"><span class="nav-number">5.3.3.</span> <span class="nav-text">Alternative Backward</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激活函数"><span class="nav-number">6.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid函数"><span class="nav-number">6.1.</span> <span class="nav-text">Sigmoid函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tanh函数"><span class="nav-number">6.2.</span> <span class="nav-text">Tanh函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLu函数"><span class="nav-number">6.3.</span> <span class="nav-text">ReLu函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeakyReLu"><span class="nav-number">6.4.</span> <span class="nav-text">LeakyReLu</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ELU"><span class="nav-number">6.5.</span> <span class="nav-text">ELU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Maxout"><span class="nav-number">6.6.</span> <span class="nav-text">Maxout</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Optimizer-ref"><span class="nav-number">7.</span> <span class="nav-text">Optimizer ref</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD"><span class="nav-number">7.1.</span> <span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD-Momentum-ref"><span class="nav-number">7.2.</span> <span class="nav-text">SGD+Momentum ref</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nesterov"><span class="nav-number">7.3.</span> <span class="nav-text">Nesterov</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">7.4.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSprop"><span class="nav-number">7.5.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">7.6.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout"><span class="nav-number">8.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Numpy-Implementation"><span class="nav-number">8.1.</span> <span class="nav-text">Numpy Implementation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Residual-Network"><span class="nav-number">9.</span> <span class="nav-text">Residual Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation-1"><span class="nav-number">9.1.</span> <span class="nav-text">Motivation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Models"><span class="nav-number">9.2.</span> <span class="nav-text">Models</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PatchGan"><span class="nav-number">10.</span> <span class="nav-text">PatchGan</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
