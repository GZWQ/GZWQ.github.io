<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,Support Vector Machine," />










<meta name="description" content="支持向量机(SVM)是一个用于分类和回归的线性模型，它可以解决线性和非线性问题。 支持向量机的思想很简单：我们尽量找到一条直线或者一个超平面将数据集分成不同的类。 在逻辑斯蒂回归中，我们用了sigmoid激活函数将$y=wx+b$的计算结果压缩到$[0,1]$范围内，这样，如果我们的压缩值大于临界值0.5，我们就将该样例分类到正例；否则我们将之分类到负例。在SVM中，我们将临界值设置为-1和+1，">
<meta name="keywords" content="Machine Learning,Support Vector Machine">
<meta property="og:type" content="article">
<meta property="og:title" content="Support Vector Machine">
<meta property="og:url" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="支持向量机(SVM)是一个用于分类和回归的线性模型，它可以解决线性和非线性问题。 支持向量机的思想很简单：我们尽量找到一条直线或者一个超平面将数据集分成不同的类。 在逻辑斯蒂回归中，我们用了sigmoid激活函数将$y=wx+b$的计算结果压缩到$[0,1]$范围内，这样，如果我们的压缩值大于临界值0.5，我们就将该样例分类到正例；否则我们将之分类到负例。在SVM中，我们将临界值设置为-1和+1，">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/1_VDATmWG1E1ZNg7hdasOh5g.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/1_AMR3v-jCvUMXPUtQskzxmQ.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202018-09-13%20at%206.07.12%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202018-09-13%20at%206.07.01%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/image-20180918214002918.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202019-06-03%20at%204.04.02%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202019-06-03%20at%204.27.50%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202018-09-15%20at%2010.42.27%20PM%20(2">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/1260px-Kernel_trick_idea.svg.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202018-09-11%20at%2010.16.55%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/Screen%20Shot%202018-09-11%20at%209.44.08%20AM.png">
<meta property="og:updated_time" content="2019-06-05T15:59:40.882Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Support Vector Machine">
<meta name="twitter:description" content="支持向量机(SVM)是一个用于分类和回归的线性模型，它可以解决线性和非线性问题。 支持向量机的思想很简单：我们尽量找到一条直线或者一个超平面将数据集分成不同的类。 在逻辑斯蒂回归中，我们用了sigmoid激活函数将$y=wx+b$的计算结果压缩到$[0,1]$范围内，这样，如果我们的压缩值大于临界值0.5，我们就将该样例分类到正例；否则我们将之分类到负例。在SVM中，我们将临界值设置为-1和+1，">
<meta name="twitter:image" content="http://yoursite.com/2018/09/11/Support-Vector-Machine/1_VDATmWG1E1ZNg7hdasOh5g.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/09/11/Support-Vector-Machine/"/>





  <title>Support Vector Machine | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/11/Support-Vector-Machine/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Support Vector Machine</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-09-11T12:11:24-05:00">
                2018-09-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>支持向量机(SVM)是一个用于分类和回归的线性模型，它可以解决线性和非线性问题。</p>
<p>支持向量机的思想很简单：我们尽量找到一条直线或者一个超平面将数据集分成不同的类。</p>
<p>在逻辑斯蒂回归中，我们用了sigmoid激活函数将$y=wx+b$的计算结果压缩到$[0,1]$范围内，这样，如果我们的压缩值大于临界值0.5，我们就将该样例分类到正例；否则我们将之分类到负例。在SVM中，我们将临界值设置为-1和+1，即如果$y=wx+b$的计算结果大于1，我们分类至正例；如果$y=wx+b$的计算结果小于-1，我们分类至负例，这样我们就得到了一个区间$[-1,1]$，称之为间隔。</p>
<a id="more"></a>
<p><a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" target="_blank" rel="noopener">ref1</a> <a href="https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989" target="_blank" rel="noopener">ref2</a> <a href="https://blog.csdn.net/v_july_v/article/details/7624837" target="_blank" rel="noopener">支持向量机通俗导论（理解SVM的三层境界</a> </p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>给定数据集$D=\{(x_1,y_1), (x_2,y_2),…,(x_m,y_m)\}, y_i\in{\{-1,+1\}}$, 欲找到一个划分超平面, 将不同类别的样本分开. </p>
<p><img src="/2018/09/11/Support-Vector-Machine/1_VDATmWG1E1ZNg7hdasOh5g.png" alt="1_VDATmWG1E1ZNg7hdasOh5g"></p>
<p>但是能将训练样本分开的划分超平面有多个, 直观上, 去找位于两类样本”正中间”的划分超平面,  即下图中的黄线是比较好的分割线。可用线性方程描述:</p>
<blockquote>
<script type="math/tex; mode=display">
w^Tx+b=0</script></blockquote>
<p><img src="/2018/09/11/Support-Vector-Machine/1_AMR3v-jCvUMXPUtQskzxmQ.png" alt="1_AMR3v-jCvUMXPUtQskzxmQ"></p>
<p>那么如何用数学方法描述黄色的那条直线是比较好的分割线呢？使用点到直线的距离，我们欲使直线两边的点离直线的距离越远越好，这样不同类的点也距离越远，说明我们的分割线比较明确地分类出正类和负类。</p>
<p>我们介绍两种衡量距离的方法：函数间隔和几何间隔。</p>
<p><strong>函数间隔</strong>      对于给定的数据集和超平面$(w,b)$, 定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:</p>
<script type="math/tex; mode=display">
functional\_margin_i=y_i(w\dot{}x_i+b)</script><p>定义样本点关于超平面最小的函数间隔为:</p>
<script type="math/tex; mode=display">
functianal\_margin(min)=\min\limits_{i=1,2,...,m}functional\_margin_i</script><blockquote>
<p><a href="https://www.zhihu.com/question/20466147/answer/28469993" target="_blank" rel="noopener">ref</a> </p>
<p>在超平面确定的情况下，即$w\cdot x+b=0$，对于二分类问题，如果$w\cdot x_i+b&gt;0$，则$x_i$的类别被判别为1；否则判定为-1。所以，$y_i(w\cdot x_i+b)&gt;0$意味着$x_i$的分类结果是正确的，而且$y_i(w\cdot x_i+b)$的值越大，分类结果的确信度越大，反之亦然。</p>
</blockquote>
<p><strong>几何间隔（点到平面距离）</strong>    对于给定的数据集和超平面$(w,b)$, 定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为:</p>
<script type="math/tex; mode=display">
geometric\_margin=y_i\frac{w\dot{}x_i+b}{||w||}</script><p>定义样本点关于超平面最小的几何间隔为:</p>
<script type="math/tex; mode=display">
geometric\_margin(min)=\min\limits_{i=1,2,...,m}geometric\_margin_i</script><blockquote>
<p>但是如果成比例的改变$w$和$b$, 超平面并没有改变, 但是函数间隔随之改变, 故对超平面的法向量进行规范化，使得间隔是确定的，这时函数间隔成为了几何间隔。</p>
</blockquote>
<p><strong>间隔最大化</strong>      </p>
<p>间隔最大化的直观解释: 对训练数据找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类.  即不仅将正负实例点分开, 而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开. 这样, 对未知的新实例有很好的分类预测能力. 具体地, 这个问题可以表示为以下的约束最优化问题:</p>
<script type="math/tex; mode=display">
\begin{align*}&\max\limits_{w,b} geometric\_margin\\&s.t. y_i(\frac{w\dot{}x_i+b}{||w||})\ge{geometric\_margin, i=1,2,..,N} \end{align*}</script><p>即希望最大化超平面$(w,b)$关于数据集的几个间隔$geometric_margin$, 约束条件表示的是超平面$(w,b)$关于每个样本点的几个间隔至少是$geometric_margin$. 超平面两边分别是正例和负例, 故会存在正例对超平面的最小几何间隔$geometric_margin+$, 负例对超平面的最小几何间隔$geometric_margin-$, 选其中最小的, 如果$geometric_margin+$&lt;$geometric_margin-$, 则一定有正例样本点落在间隔面上; $geometric_margin+$&gt;$geometric_margin-$, 则一定有负例点落在间隔面上; $geometric_margin+$=$geometric_margin-$, 则间隔面上有正负实例样本点.</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>SVM的目标就是寻找一个几何间隔最大化的分离超平面，该问题可以表示为下面的约束问题：</p>
<script type="math/tex; mode=display">
\begin{align*}&\max\limits_{w,b} geometric\_margin\\&s.t. y_i(\frac{w\dot{}x_i+b}{||w||})\ge{geometric\_margin, i=1,2,..,N} \end{align*}</script><p>即我们希望最大化超平面$(w,b)$关于训练集数据的几何间隔，约束条件表示每个样本点关于超平面的几何间隔至少是$geometric_margin$。</p>
<p>考虑几何间隔和函数间隔的关系, 即$\frac{1}{||w||}$大小，则</p>
<script type="math/tex; mode=display">
\begin{align*}&\max\limits_{w,b} \frac{functional\_margin}{||w||}\\&s.t. y_i({w\dot{}x_i+b})\ge{functional\_margin, i=1,2,..,N} \end{align*}</script><p>仍然最大化最小几何间隔, 即间隔临界样本点到超平面的距离, 约束变成了所有的样本点的函数间隔都要大于最小的函数间隔.</p>
<p>考虑到函数间隔$functional_margin$的取值并不影响最优化问题的解. 因为上面我们介绍过了，之所以采样几何间隔，就是因为成比例扩大/缩小$(w,b)$，函数间隔的值会改变，即使超平面不变。所以，对于同一个超平面，一个样本关于该平面的函数间隔有无穷多个，可以是1，可以是10等等，只需要成比例扩大/缩小$(w,b)$。故，为简化问题，我们领函数间隔为1，代入上式：</p>
<script type="math/tex; mode=display">
\max \limits_{w,b}\frac{1}{||w||}\\s.t. y_i(w^Tx_i+b)\ge1, i=1,2,...,m</script><p>又因为最大化$\frac{1}{||w||}$，等价于最小化$||w||$，等价于最小化$\frac{1}{2}||w||^2$，故上式等价于:</p>
<script type="math/tex; mode=display">
\min \limits_{w,b}\frac{1}{2}{||w||^2}=\frac{1}{2}\sum_{j=1}w_i^2\\
s.t. y_i(w^Tx_i+b)\ge1, i=1,2,...,m</script><h1 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h1><p>比较下面两张图：我们发现图二的决策边界更具有一般性，但是我们的模型得到的是图一中的结果，图一中蓝色的决策边界有点过拟合，因为它过于在意完全分类正确，对于左上角的一个异常样本，为了将它分类正确，导致最终的分割线严重偏斜。所以，我们希望允许SVM有错误分类，这样才能提高模型的泛化能力。</p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-13 at 6.07.12 PM.png" alt="Screen Shot 2018-09-13 at 6.07.12 PM"></p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-13 at 6.07.01 PM.png" alt="Screen Shot 2018-09-13 at 6.07.01 PM"></p>
<p>在上述SVM的定义中，因为我们要求所有样本点的函数间隔都要大于等于1，即每个样本点都分类正确；那么允许分类错误意味着某些样本点不满足函数间隔大于等于1的约束条件, 故可以针对每个样本点引进一个松弛变量$\xi_{i}\ge0$, 使函数间隔加上松弛变量大于等于1. 这样, 约束条件变为:</p>
<script type="math/tex; mode=display">
y_i(w\dot{x_i}+b)+\xi_{i}\ge1</script><p>同时, 对每个松弛变量$\xi_{i}$, 支付一个代价$\xi_{i}$, 则目标函数由原来的$\frac{1}{2}||w||^2$变成</p>
<script type="math/tex; mode=display">
\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i</script><p>$C\ge0$称为惩罚参数. 最小化目标函数包含两层含义: 使$\frac{1}{2}||w||^2$尽量小即间隔尽量打, 同时使误分类点的个数尽量小, C是调和二者的系数. C越大，则允许的分类错误越少；C越小，则允许多的分类误差。</p>
<p>令$z_i=y_i(w\dot{x_i}+b)-1$时，</p>
<script type="math/tex; mode=display">
l_{0/1}(z)=

\left\{

\begin{align} 

1, & \text{if z<0}\\

0, &\text{otherwise}

\end{align}

\right.</script><p>代入得：</p>
<script type="math/tex; mode=display">
\frac{1}{2}||w||^2+C\sum_{i=1}^{N}l_{0/1}(y_i(w\dot{x_i}+b)-1)</script><p>但是$l_{0/1}$是阶跃函数，非凸，非连续，所以我们采用其他函数来代替，替代损失函数一般选取凸的连续函数且是$l_{0/1}$的上界，常用的：</p>
<script type="math/tex; mode=display">
hinge损失：l_{hinge}(z)=max(0,1-z)\\
指数损失：l_{exp}(z)=e^{-z}\\
对率损失：l_{log}=log(1+e^{-z})</script><p><img src="/2018/09/11/Support-Vector-Machine/image-20180918214002918.png" alt="image-20180918214002918"></p>
<blockquote>
<p>以上函数的x正半轴几乎为0，与我们的事实符合：$z_i=y_i(w\dot{x_i}+b)-1&gt;0$，则没有任何损失。</p>
</blockquote>
<p>则线性支持向量机的学习问题变成了如下凸二次规划问题:</p>
<script type="math/tex; mode=display">
\min\limits_{w, b,\xi}\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i\\
\begin{align*}s.t. &y_i(w\dot{x_i}+b)\ge1-\xi_{i}, i=1,2,...,N\\&\xi_i\ge0, i=1,2,...,m\end{align*}</script><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>Say given an example $(x_i,y_i)$, and use the shorthand for the scores vector: $s=f(x_i,W)$, then the SVM loss has the following form:</p>
<script type="math/tex; mode=display">
L_i = \sum_{j\ne y_i}\left\{

\begin{align} 

0& & \text{if   }  s_{y_i}\ge s_j+1 \\

s_j-s_{y_i}+1& &\text{if otherwise}

\end{align}
=\sum_{j\ne y_i}max(0,s_j-s_{y_i}+1)
\right.</script><p>其中，$y_i$是groundtruth label，$s_{y_i}$是分类器给真实分类的分数，$s_j$是分类器给其他类的分数，该损失函数鼓励真实分类的分数大于其他分类的分数+1。</p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2019-06-03 at 4.04.02 PM.png" alt="creen Shot 2019-06-03 at 4.04.02 P"></p>
<p>The two given figures are plots of function $f(x)=max(0,1-x)$ .We can see from the second figure that when x is between 0 and 1, the loss is in range $[0,1]$. </p>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>Say we have an image classification and we use it to classify three images, like the following:</p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2019-06-03 at 4.27.50 PM.png" alt="creen Shot 2019-06-03 at 4.27.50 P"></p>
<p>The output of classification is a 3d vector, each element of the vector is the probability of image classification. For example, for the cat image, the vector is $[3.2, 5.1, -1.7]$, meaning this picture is cat with probability 3.2, is car with prob 5.1 and frog with prob -1.7.</p>
<p> Then, the $s_{y_i}=3.2$, $s_1=5.1$ and $s_2=-1.7$，so the loss is:</p>
<script type="math/tex; mode=display">
L_{cat}=max(0,5.1+1-3.2)+max(0,-1.7+1-3.2)=max(0,2.9)+max(0,-3.9)=2.9</script><p>Similarily, $L_{car}=0$, $L_{frog}=12.9$.</p>
<p>So the overall loss is:</p>
<script type="math/tex; mode=display">
L=\frac{1}{N}\sum_{i=1}^{N}L_i\\
L=\frac{(2.9+0+12.9)}{3}=5.27</script><blockquote>
<ol>
<li><p>What happens to loss if car scores change a bit?</p>
<p>The answer is the loss will not change. The SVM loss only cares about getting the correct score to be greater than one more than the incorrect scores. But in this case, the car score is already quite a bit large than the others. So if the scores for this class changes, this margin of one will still be retained and the loss will not change.</p>
</li>
<li><p>What is the min/max possible loss?</p>
<p>The min loss is 0 because across all the classes, if our correct score was much larger then we will incur zero loss across all the classes. </p>
<p>The max loss is infinite. According to the hinge loss figure, if correct score goes very very negative, then we could incur potentially infinite loss.</p>
</li>
<li><p>At initialization W is small so all $s\approx 0$. What is the loss?</p>
<p>The answer is the number of class minus one.</p>
</li>
<li><p>What if the sum was over all classes?(including $j=y_j$).</p>
<p>The loss increases by one.</p>
</li>
<li><p>What if we used mean instead of sum?</p>
<p>The answer is that it doesn’t change. We just rescale the whole loss function by a constant value and we don’t care about the true value of the loss.</p>
</li>
<li><p>What if we used $L_i=\sum_{j\ne y_i}max(0,s_j-s_{y_i}+1)^2$?</p>
<p>The answer is it is different. We are kind of changing the trade-offs between good and badness in kind of nonlinear way, and this would end up actually computing a different loss function.</p>
</li>
<li><p>Suppose we found a W such that $L=0$. Is this W unique?</p>
<p>The answer is no. $2W$ is also has $L=0$.</p>
</li>
</ol>
</blockquote>
<h1 id="Derivative-ref"><a href="#Derivative-ref" class="headerlink" title="Derivative ref"></a>Derivative <a href="https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html" target="_blank" rel="noopener">ref</a></h1><p>Say for a single datapoint $(x_i,y_i)$, we have the following hinge loss:</p>
<script type="math/tex; mode=display">
L_i=\sum_{j\ne y_i}^{c} max(0, s_j+1-s_i)</script><p>where $c$ is the class number and $s_j=w_j^T x_i$ is the score for the $j{th}$ class. What we do here is to iterate scores for all classes and compare them with the score of truth class.</p>
<p>To spread out, </p>
<script type="math/tex; mode=display">
\begin{align*}
L_i = &\max(0,1+w_1x_i-w_{y_i}x_i) + \\
 &\max(0, 1+w_2x_i-w_{y_i}x_i) + \\
& \quad \quad \quad \quad \quad \quad \vdots \\
&\max(0, 1+w_cx_i-w_{y_i}x_i)
\end{align*}</script><p>If $(w_jx_i+1-w_{y_i}x_i)&gt;0$, $\frac{dL_i}{dw_j}=x_i$.</p>
<p>But for $w_{y_i}$,</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{dL_i}{dw_{y_i}} &= - \sum_{j\neq y_i} \mathbb{1}(x_iw_j - x_iw_{y_i} + \Delta > 0) x_i \tag{3}
\end{align*}</script><h1 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h1><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div>
<div class="line">        self.W = <span class="keyword">None</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(self,W, X, y, reg)</span>:</span></div>
<div class="line">        <span class="string">'''</span></div>
<div class="line"><span class="string">        :param W:A numpy array of shape (D, C) containing weights.</span></div>
<div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing a minibatch of data.</span></div>
<div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c means</span></div>
<div class="line"><span class="string">        that X[i] has label c, where 0 &lt;= c &lt; C.</span></div>
<div class="line"><span class="string">        :param reg:(float) regularization strength</span></div>
<div class="line"><span class="string">        :return:a tuple of:</span></div>
<div class="line"><span class="string">                 - loss as single float</span></div>
<div class="line"><span class="string">                 - gradient with respect to weights W; an array of same shape as W</span></div>
<div class="line"><span class="string">        '''</span></div>
<div class="line">        dW = np.zeros(W.shape)</div>
<div class="line">        num_classes = W.shape[<span class="number">1</span>]</div>
<div class="line">        num_train = X.shape[<span class="number">0</span>]</div>
<div class="line">        loss = <span class="number">0.0</span></div>
<div class="line">        scores = np.dot(X, W)</div>
<div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</div>
<div class="line">            target = y[i]</div>
<div class="line">            score = scores[i]</div>
<div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</div>
<div class="line">                <span class="keyword">if</span> target == j:</div>
<div class="line">                    <span class="keyword">continue</span></div>
<div class="line">                <span class="keyword">if</span> score[j] - score[target] + <span class="number">1</span> &gt; <span class="number">0</span>:</div>
<div class="line">                    loss += score[j] - score[target] + <span class="number">1</span></div>
<div class="line">                    dW[:, j] += X[i]</div>
<div class="line">                    dW[:, target] -= X[i]</div>
<div class="line">        loss /= num_train</div>
<div class="line">        loss += reg * np.sum(W ** <span class="number">2</span>)</div>
<div class="line">        dW = dW / num_train + <span class="number">2</span> * reg * W</div>
<div class="line">        <span class="keyword">return</span> loss, dW</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(self,W, X, y, reg)</span>:</span></div>
<div class="line">        N = np.shape(X)[<span class="number">0</span>]</div>
<div class="line">        C = np.shape(W)[<span class="number">1</span>]</div>
<div class="line">        loss = <span class="number">0.0</span></div>
<div class="line">        dW = np.zeros(W.shape)</div>
<div class="line"></div>
<div class="line">        scores = np.dot(X, W)</div>
<div class="line">        target_scores = scores[np.arange(N), y].reshape(<span class="number">-1</span>, <span class="number">1</span>)</div>
<div class="line">        scores_pos = scores - target_scores + <span class="number">1</span></div>
<div class="line">        scores_pos[np.arange(N), y] = <span class="number">0</span></div>
<div class="line">        scores_pos = np.maximum(scores_pos, <span class="number">0</span>)</div>
<div class="line">        loss = np.sum(scores_pos) / N</div>
<div class="line"></div>
<div class="line">        mask = np.zeros(scores_pos.shape)</div>
<div class="line">        mask[scores_pos &gt; <span class="number">0</span>] = <span class="number">1</span></div>
<div class="line">        mask[np.arange(N), y] -= np.sum(mask, axis=<span class="number">1</span>)  <span class="comment"># attention:"-"</span></div>
<div class="line">        dW = np.dot(X.T, mask)</div>
<div class="line">        dW = dW / N + <span class="number">2</span> * reg * W</div>
<div class="line"></div>
<div class="line">        <span class="keyword">return</span> loss, dW</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></div>
<div class="line">        <span class="keyword">return</span> self.svm_loss_vectorized(self.W, X_batch, y_batch, reg)</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></div>
<div class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></div>
<div class="line">        <span class="string">'''</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing training data; there are N</span></div>
<div class="line"><span class="string">          training samples each of dimension D.</span></div>
<div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c</span></div>
<div class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></div>
<div class="line"><span class="string">        :param learning_rate:(float) learning rate for optimization.</span></div>
<div class="line"><span class="string">        :param reg:(float) regularization strength.</span></div>
<div class="line"><span class="string">        :param num_iters:(integer) number of steps to take when optimizing</span></div>
<div class="line"><span class="string">        :param batch_size:(integer) number of training examples to use at each step.</span></div>
<div class="line"><span class="string">        :param verbose:(boolean) If true, print progress during optimization.</span></div>
<div class="line"><span class="string">        :return:A list containing the value of the loss function at each training iteration.</span></div>
<div class="line"><span class="string">        '''</span></div>
<div class="line">        num_train, dim = X.shape</div>
<div class="line">        num_classes = np.max(y) + <span class="number">1</span>  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></div>
<div class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="keyword">None</span>:</div>
<div class="line">            <span class="comment"># lazily initialize W</span></div>
<div class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</div>
<div class="line"></div>
<div class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></div>
<div class="line">        loss_history = []</div>
<div class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</div>
<div class="line">            mask = np.random.choice(num_train, batch_size)</div>
<div class="line">            X_batch = X[mask]</div>
<div class="line">            y_batch = y[mask]</div>
<div class="line"></div>
<div class="line">            <span class="comment"># evaluate loss and gradient</span></div>
<div class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</div>
<div class="line">            loss_history.append(loss)</div>
<div class="line">            self.W -= learning_rate * grad</div>
<div class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</div>
<div class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</div>
<div class="line">        <span class="keyword">return</span> loss_history</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div>
<div class="line">        y_pred = np.zeros(X.shape[<span class="number">0</span>])</div>
<div class="line">        scores = X.dot(self.W)</div>
<div class="line">        y_pred = np.argmax(scores,axis=<span class="number">1</span>)</div>
<div class="line">        <span class="keyword">return</span> y_pred</div>
</pre></td></tr></table></figure>

</div></div>
<h1 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>通俗说，只要涉及空间的变换和內积的运算，我们就可以使用Kernel trick来简化运算。</p>
<p>一般的，在高维空间计算內积，我们需要分两步：</p>
<ol>
<li><p>将原始低维数据空间$X$映射到更高维的$Z$空间：</p>
<script type="math/tex; mode=display">
\phi(x):X\to Z</script></li>
<li><p>在$Z$空间里计算內积：</p>
<script type="math/tex; mode=display">
z_i = \phi(x_i)\\
z_j = \phi(x_j)\\
k = z_i^Tz_j</script></li>
</ol>
<p>而核方法：</p>
<script type="math/tex; mode=display">
K(x_i,x_j)=z_i^Tz_j=\phi(x_i)^T\phi(x_j)</script><p>即，$K(x_i,x_j)$计算得到的结果就是原始数据空间中的两点先升维$\phi(x)$再內积$\phi(x_i)^T\phi(x_j)$的结果，不必进行显示的升维操作。</p>
<h2 id="Kernel-Trick-in-SVM"><a href="#Kernel-Trick-in-SVM" class="headerlink" title="Kernel Trick in SVM"></a>Kernel Trick in SVM</h2><p><a href="https://www.youtube.com/watch?v=vMmG_7JcfIc" target="_blank" rel="noopener">kernel visualiztion</a> </p>
<p>那么SVM中如何利用核方法呢？且看下面的这个例子。</p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-15 at 10.42.27 PM (2" alt="Screen Shot 2018-09-15 at 10.42.27 PM (2)">.png)</p>
<p>在一维空间（直线上）我们有一系列样本点，蓝色为正例，红色为负例，显然我们不能找到一个线性的分割来分类它们。那么，如果将一维的数据点映射到二维平面呢？即对每个样本点，我们先进行映射：$(x)\to (x,y)$，其中$y=x^2$，这样就将蓝色和红色的样本点分开了，于是，我们就可以使用一条直线取划分样本点了。</p>
<p>再看一个二维空间的例子，如下图所示：</p>
<p><img src="/2018/09/11/Support-Vector-Machine/1260px-Kernel_trick_idea.svg.png" alt="1260px-Kernel_trick_idea.svg"></p>
<p>显然我们无法使用线性的分割去分类上述点，但是如果我们将二维平面上的点增加一个维度$z$，使平面上点映射到三维空间上，$(x,y)\to (x,y,z)$，如右边图所示，这样我们就可以使用一个平面进行划分了。</p>
<h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><p>通过上面的讨论，我们希望样本在新的特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。但是在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间，故选择合适的核函数很重要。</p>
<ul>
<li><p>线性核：$k(x_i,x_j)=x_i^Tx_j$</p>
</li>
<li><p>多项式核：$k(x_i,x_j)=(\gamma x_i^Tx_j+r)^d$</p>
<blockquote>
<p>当$d&gt;1$</p>
</blockquote>
</li>
<li><p>高斯核：$k(x_i,x_j)=e^{(-\frac{||x_i-x_j||^2}{2\sigma^2})}$</p>
<blockquote>
<p>亦称RBF核，$\sigma&gt;0$为高斯核的带宽</p>
<p>如果$\sigma$设的太小，方差会很小，方差很小的高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，(如果让方差无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)而测试准确率不高的可能，就是通常说的过训练；而如果设的过大，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率。<a href="https://xijunlee.github.io/2017/03/29/sklearn%E4%B8%ADSVM%E8%B0%83%E5%8F%82%E8%AF%B4%E6%98%8E%E5%8F%8A%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">ref</a> </p>
</blockquote>
</li>
<li><p>拉普拉斯核：$k(x_i,x_j)=e^{(-\frac{||x_i-x_j||^2}{\sigma^2})}$</p>
<blockquote>
<p>$\sigma&gt;0$</p>
</blockquote>
</li>
<li><p>Sigmoid核：$k(x_i,x_j)=tanh(\beta x_i^Tx_j+\theta)$</p>
<blockquote>
<p>$\beta&gt;0$，$\theta&lt;0$</p>
</blockquote>
</li>
</ul>
<h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line">X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>]])</div>
<div class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</div>
<div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div>
<div class="line">clf = SVC(kernel=<span class="string">'linear'</span>)</div>
<div class="line">clf = SVC.fit(X, y)</div>
<div class="line">prediction = clf.predict([[<span class="number">0</span>,<span class="number">6</span>]])</div>
</pre></td></tr></table></figure>
<h2 id="Exercise1"><a href="#Exercise1" class="headerlink" title="Exercise1"></a>Exercise1</h2><p><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/" target="_blank" rel="noopener">Machine Learning Exercises In Python, Part 6 - SVM</a> </p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-11 at 10.16.55 AM.png" alt="Screen Shot 2018-09-11 at 10.16.55 AM"></p>
<p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-11 at 9.44.08 AM.png" alt="Screen Shot 2018-09-11 at 9.44.08 AM"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Support-Vector-Machine/" rel="tag"># Support Vector Machine</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/03/Grid-Search/" rel="next" title="Grid Search">
                <i class="fa fa-chevron-left"></i> Grid Search
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/12/C-Programing-Language/" rel="prev" title="C++ Programing Language">
                C++ Programing Language <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">66</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型"><span class="nav-number">2.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#软间隔最大化"><span class="nav-number">3.</span> <span class="nav-text">软间隔最大化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#损失函数"><span class="nav-number">4.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Examples"><span class="nav-number">4.1.</span> <span class="nav-text">Examples</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Derivative-ref"><span class="nav-number">5.</span> <span class="nav-text">Derivative ref</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Numpy-Implementation"><span class="nav-number">6.</span> <span class="nav-text">Numpy Implementation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#核方法"><span class="nav-number">7.</span> <span class="nav-text">核方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#引言"><span class="nav-number">7.1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernel-Trick-in-SVM"><span class="nav-number">7.2.</span> <span class="nav-text">Kernel Trick in SVM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用核函数"><span class="nav-number">7.3.</span> <span class="nav-text">常用核函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Python"><span class="nav-number">8.</span> <span class="nav-text">Python</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Exercise1"><span class="nav-number">8.1.</span> <span class="nav-text">Exercise1</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
