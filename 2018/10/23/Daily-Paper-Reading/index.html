<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Paper Everyday,Generative adversarial networks," />










<meta name="description" content="Some interesting papers that I read or are about to read.">
<meta name="keywords" content="Deep Learning,Paper Everyday,Generative adversarial networks">
<meta property="og:type" content="article">
<meta property="og:title" content="Daily Paper Reading">
<meta property="og:url" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Some interesting papers that I read or are about to read.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-18%20at%201.48.00%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-18%20at%201.48.27%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-18%20at%201.51.32%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-18%20at%202.00.22%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-09%20at%2010.15.46%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-12%20at%209.19.33%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-06%20at%209.25.11%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-06%20at%209.38.23%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-04%20at%2011.17.55%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-04%20at%2011.22.15%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-05%20at%2010.58.10%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-02%20at%204.30.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-31%20at%2011.17.32%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-14%20at%2010.55.48%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%2010.55.28%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%2011.22.36%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%202.17.07%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%202.20.38%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%202.51.41%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-31%20at%206.12.16%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-31%20at%2010.45.04%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.46.55%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.45.51%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.06.30%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.33.18%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.53.41%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.54.11%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.57.45%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.08.43%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.19.36%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.23.53%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-30%20at%2010.00.26%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.40.42%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.22.19%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-28%20at%2011.13.33%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.15.10%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.17.51%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-28%20at%2010.05.47%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-23%20at%2011.02.50%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-26%20at%209.15.48%20AM.png">
<meta property="og:updated_time" content="2019-02-28T19:06:01.381Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Daily Paper Reading">
<meta name="twitter:description" content="Some interesting papers that I read or are about to read.">
<meta name="twitter:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-18%20at%201.48.00%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/23/Daily-Paper-Reading/"/>





  <title>Daily Paper Reading | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/23/Daily-Paper-Reading/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Daily Paper Reading</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-23T22:37:04-05:00">
                2018-10-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Some interesting papers that I read or are about to read.</p>
<a id="more"></a>
<h1 id="Videos"><a href="#Videos" class="headerlink" title="Videos"></a>Videos</h1><p><a href="https://arxiv.org/pdf/1505.06250v1.pdf" target="_blank" rel="noopener">Efficient Large Scale Video Classification</a> </p>
<p><a href="https://arxiv.org/pdf/1711.11217v2.pdf" target="_blank" rel="noopener">Future Person Localization in First-Person Videos</a> </p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p><a href="https://arxiv.org/pdf/1711.09151v1.pdf" target="_blank" rel="noopener">Convolutional Image Captioning</a> </p>
<p><a href="https://arxiv.org/pdf/1803.11438v1.pdf" target="_blank" rel="noopener">Reconstruction Network for Video Captioning</a></p>
<p><a href="https://arxiv.org/pdf/1712.09382v1.pdf" target="_blank" rel="noopener">Audio to Body Dynamics</a> </p>
<p><a href="https://arxiv.org/pdf/1712.02036v1.pdf" target="_blank" rel="noopener">Learning Semantic Concepts and Order for Image and Sentence Matching</a> </p>
<p><a href="https://arxiv.org/pdf/1709.07192v1.pdf" target="_blank" rel="noopener">Visual Question Generation as Dual Task of Visual Question Answering</a> </p>
<p><a href="https://arxiv.org/pdf/1711.07613v1.pdf" target="_blank" rel="noopener">Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning</a> </p>
<p><a href="https://arxiv.org/pdf/1712.01381v3.pdf" target="_blank" rel="noopener">A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts</a> </p>
<p><a href="https://arxiv.org/pdf/1803.10892v1.pdf" target="_blank" rel="noopener">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1803.07485v1.pdf" target="_blank" rel="noopener">Actor and Action Video Segmentation from a Sentence</a></p>
<p><a href="https://arxiv.org/pdf/1703.09529v3.pdf" target="_blank" rel="noopener">Objects as context for detecting their semantic parts</a> </p>
<p><a href="https://arxiv.org/pdf/1706.03872v1.pdf" target="_blank" rel="noopener">Six Challenges for Neural Machine Translation</a> </p>
<h1 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h1><p>[Learning Residual Im<a href="https://arxiv.org/pdf/1612.05363v2.pdf" target="_blank" rel="noopener">ages for Face Attribute Manipulation</a> </p>
<p><a href="https://arxiv.org/pdf/1711.07410v2.pdf" target="_blank" rel="noopener">Disentangling Factors of Variation by Mixing Them</a> </p>
<p><a href="https://arxiv.org/pdf/1803.03345v2.pdf" target="_blank" rel="noopener">Deep Semantic Face Deblurring</a> </p>
<p><a href="https://arxiv.org/pdf/1712.02330v1.pdf" target="_blank" rel="noopener">SGAN: An Alternative Training of Generative Adversarial Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1711.07064v4.pdf" target="_blank" rel="noopener">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1702.01983v2.pdf" target="_blank" rel="noopener">Face Aging With Conditional Generative Adversarial Networks</a> </p>
<p><a href="http://tongtianta.site/paper/1449" target="_blank" rel="noopener">Global versus Localized Generative Adversarial Nets</a> </p>
<p><a href="https://arxiv.org/pdf/1803.11182v1.pdf" target="_blank" rel="noopener">Towards Open-Set Identity Preserving Face Synthesis</a> </p>
<p><a href="https://arxiv.org/pdf/1711.08565v1.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a> </p>
<p><a href="https://arxiv.org/pdf/1711.06454v5.pdf" target="_blank" rel="noopener">Separating Style and Content for Generalized Style Transfer</a> </p>
<p><a href="https://arxiv.org/pdf/1803.00839v1.pdf" target="_blank" rel="noopener">Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</a> </p>
<p><a href="https://arxiv.org/pdf/1804.08882v1.pdf" target="_blank" rel="noopener">Mask-aware Photorealistic Face Attribute Manipulation</a> </p>
<p><a href="https://arxiv.org/pdf/1706.09138v1.pdf" target="_blank" rel="noopener">Perceptual Adversarial Networks for Image-to-Image Transformation</a> </p>
<p><a href="https://arxiv.org/pdf/1804.04412v1.pdf" target="_blank" rel="noopener">Unsupervised Discovery of Object Landmarks as Structural Representations</a> </p>
<p><a href="https://arxiv.org/pdf/1804.00819v1.pdf" target="_blank" rel="noopener">End-to-End Dense Video Captioning with Masked Transformer</a> </p>
<p><a href="https://arxiv.org/pdf/1712.04350v1.pdf" target="_blank" rel="noopener">Predicting Yelp Star Reviews Based on Network Structure with Deep Learning</a> </p>
<p><a href="https://arxiv.org/pdf/1711.06448v1.pdf" target="_blank" rel="noopener">Chinese Typeface Transformation with Hierarchical Adversarial Network</a> </p>
<p><a href="https://arxiv.org/pdf/1711.06420v1.pdf" target="_blank" rel="noopener">Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models</a> </p>
<p><a href="https://arxiv.org/pdf/1712.01928v2.pdf" target="_blank" rel="noopener">Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Networks</a></p>
<p><a href="https://arxiv.org/pdf/1804.09578v1.pdf" target="_blank" rel="noopener">Unsupervised Domain Adaptation with Adversarial Residual Transform Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1804.03390v2.pdf" target="_blank" rel="noopener">Learning Pose Specific Representations by Predicting Different Views</a> </p>
<p><a href="https://arxiv.org/pdf/1801.01415v1.pdf" target="_blank" rel="noopener">What have we learned from deep representations for action recognition?</a> </p>
<p><a href="https://arxiv.org/pdf/1511.02799v4.pdf" target="_blank" rel="noopener">Neural Module Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1801.04356v2.pdf" target="_blank" rel="noopener">Feature Space Transfer for Data Augmentation</a> </p>
<p><a href="https://arxiv.org/pdf/1706.04306v1.pdf" target="_blank" rel="noopener">Photo-realistic Facial Texture Transfer</a></p>
<p><a href="https://arxiv.org/pdf/1804.07455v1.pdf" target="_blank" rel="noopener">Generating a Fusion Image: One’s Identity and Another’s Shape</a> </p>
<p><a href="https://arxiv.org/pdf/1805.11202v1.pdf" target="_blank" rel="noopener">FairGAN: Fairness-aware Generative Adversarial Networks</a></p>
<p><a href="https://arxiv.org/pdf/1804.00582v1.pdf" target="_blank" rel="noopener">Learning Intrinsic Image Decomposition from Watching the World</a> </p>
<p><a href="https://arxiv.org/pdf/1802.08797v2.pdf" target="_blank" rel="noopener">Residual Dense Network for Image Super-Resolution</a> </p>
<p><a href="https://arxiv.org/pdf/1704.07333v3.pdf" target="_blank" rel="noopener">Detecting and Recognizing Human-Object Interactions</a> </p>
<p><a href="https://arxiv.org/pdf/1611.09961v1.pdf" target="_blank" rel="noopener">Semantic Facial Expression Editing using Autoencoded Flow</a> </p>
<p>2019-01-24</p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Sangkloy_Scribbler_Controlling_Deep_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Scribbler: Controlling Deep Image Synthesis with Sketch and Color</a> </p>
<p><a href="https://arxiv.org/pdf/1701.07274.pdf" target="_blank" rel="noopener">DEEP REINFORCEMENT LEARNING: AN OVERVIEW</a> </p>
<p><a href="https://arxiv.org/pdf/1611.06355.pdf" target="_blank" rel="noopener">Invertible Conditional GANs for image editing</a> </p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.pdf" target="_blank" rel="noopener">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</a> </p>
<p><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a> </p>
<p><a href="https://arxiv.org/pdf/1805.10416.pdf" target="_blank" rel="noopener">Human Action Generation with Generative Adversarial Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1711.08682.pdf" target="_blank" rel="noopener">Deep Video Generation, Prediction and Completion of Human Action Sequences</a> skeleton generation</p>
<p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ceyuan_Yang_Pose_Guided_Human_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Pose Guided Human Video Generation</a> skeleton generation</p>
<p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf" target="_blank" rel="noopener">SwapNet: Image Based Garment Transfer</a> </p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Esser_A_Variational_U-Net_CVPR_2018_paper.pdf" target="_blank" rel="noopener">A Variational U-Net for Conditional Appearance and Shape Generation</a> </p>
<p>2019-01-04</p>
<p><a href="https://arxiv.org/pdf/1711.10684.pdf" target="_blank" rel="noopener">Residual U-net</a></p>
<p><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1803.04469.pdf" target="_blank" rel="noopener">An Introduction to Image Synthesis with Generative Adversarial Nets</a> </p>
<p><a href="https://arxiv.org/pdf/1708.00315.pdf" target="_blank" rel="noopener">Generative Semantic Manipulation with Contrasting GAN</a> </p>
<p>In this paper, distance loss is used to measure the difference between real/fake images.</p>
<p><a href="https://arxiv.org/pdf/1808.06601.pdf" target="_blank" rel="noopener">Video-to-Video Synthesis</a></p>
<p>2018-11-16</p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Disentangled_Representation_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Disentangled Representation Learning GAN for Pose-Invariant Face Recognition</a> </p>
<p>The authors try to solve the problem of Pose-Invariant Face Recognition.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 1.48.00 PM.png" alt="creen Shot 2018-11-18 at 1.48.00 P"></p>
<blockquote>
<p>By controlling $c$ and $z$, we can diversify the generated images. </p>
</blockquote>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 1.48.27 PM.png" alt="creen Shot 2018-11-18 at 1.48.27 P"></p>
<blockquote>
<p>Discriminator has three tasks: classify real images as real ones, fake images as fake ones, the identities and poses.</p>
</blockquote>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 1.51.32 PM.png" alt="creen Shot 2018-11-18 at 1.51.32 P"></p>
<blockquote>
<p>Generator tries to maximize the accuracy of generated images being classified to the  true identities and poses.</p>
</blockquote>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 2.00.22 PM.png" alt="creen Shot 2018-11-18 at 2.00.22 P"></p>
<blockquote>
<p>There are $n$ input images and each one has a corresponding generated image; and there is one generated image. For each generated image, there are two losses.</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/1611.03383.pdf" target="_blank" rel="noopener">Disentangling factors of variation in deep representations using adversarial training</a></p>
<p>2018-11-15</p>
<p><a href="https://arxiv.org/pdf/1810.11610.pdf" target="_blank" rel="noopener">Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis</a> </p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Pose Transferrable Person Re-Identification</a> </p>
<p><a href="http://www.robots.ox.ac.uk/~tvg/publications/2018/W21P20.pdf" target="_blank" rel="noopener">A Semi-supervised Deep Generative Model for Human Body Analysis</a> </p>
<p><a href="https://arxiv.org/pdf/1808.06847.pdf?utm_campaign=Awesome%20Computer%20Science&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">Deep Video-Based Performance Cloning</a> </p>
<p><a href="http://www.cs.sfu.ca/~mori/research/papers/zhai-bmvc18.pdf" target="_blank" rel="noopener">Adaptive Appearance Rendering</a> </p>
<p><a href="https://arxiv.org/pdf/1804.04779.pdf" target="_blank" rel="noopener">A Hybrid Model for Identity Obfuscation by Face Replacement</a> </p>
<p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/XU_YANG_Shuffle-Then-Assemble_Learning_Object-Agnostic_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship Features</a> </p>
<p><a href="https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf" target="_blank" rel="noopener">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a> </p>
<p>2018-11-10</p>
<p><a href="https://arxiv.org/pdf/1612.03242.pdf" target="_blank" rel="noopener">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a> </p>
<p><a href="https://arxiv.org/pdf/1703.09695.pdf" target="_blank" rel="noopener">Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network</a> </p>
<p><a href="https://arxiv.org/pdf/1704.03414.pdf" target="_blank" rel="noopener">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</a> </p>
<p><a href="https://www.ijcai.org/proceedings/2017/0404.pdf" target="_blank" rel="noopener">Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering</a> </p>
<p><a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a> </p>
<h1 id="2018-11-08"><a href="#2018-11-08" class="headerlink" title="2018-11-08"></a>2018-11-08</h1><p><a href="https://arxiv.org/pdf/1706.02823v3.pdf" target="_blank" rel="noopener">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</a></p>
<p><a href="https://arxiv.org/pdf/1804.04273v1.pdf" target="_blank" rel="noopener">VITAL: VIsual Tracking via Adversarial Learning</a> </p>
<p>Problem setting : tracking-by-detection, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-09 at 10.15.46 AM.png" alt="creen Shot 2018-11-09 at 10.15.46 A"></p>
<p><a href="https://arxiv.org/pdf/1712.02478v1.pdf" target="_blank" rel="noopener">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal</a> </p>
<p><a href="https://arxiv.org/pdf/1708.00159v1.pdf" target="_blank" rel="noopener">Image Denoising via CNNs: An Adversarial Approach</a> </p>
<p>Image denoising is a fundamental image processing problem whose objective is to remove the noise while preserving the original image structure.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-12 at 9.19.33 AM.png" alt="creen Shot 2018-11-12 at 9.19.33 A"></p>
<p><a href="https://arxiv.org/pdf/1711.09554v2.pdf" target="_blank" rel="noopener">Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation</a> </p>
<p><a href="https://arxiv.org/pdf/1705.08824v2.pdf" target="_blank" rel="noopener">From source to target and back: symmetric bi-directional adaptive GAN</a> </p>
<p><a href="https://arxiv.org/pdf/1805.00251v1.pdf" target="_blank" rel="noopener">Conditional Image-to-Image Translation</a> </p>
<p><a href="https://arxiv.org/pdf/1712.00268v4.pdf" target="_blank" rel="noopener">Deformable Shape Completion with Graph Convolutional Autoencoders</a> </p>
<h1 id="2018-11-05"><a href="#2018-11-05" class="headerlink" title="2018-11-05"></a>2018-11-05</h1><p><a href="https://papers.nips.cc/paper/5845-deep-visual-analogy-making.pdf" target="_blank" rel="noopener">Deep Visual Analogy-Making</a> <a href="https://github.com/carpedm20/visual-analogy-tensorflow" target="_blank" rel="noopener">tf code</a> </p>
<p>Given a pair of images $(a,b)$ and a query image $c$, we try to generate $d$, the corresponding image of $c$, such that $a$ is to $b$ as $c$ is to $d$.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-06 at 9.25.11 PM.png" alt="creen Shot 2018-11-06 at 9.25.11 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-06 at 9.38.23 PM.png" alt="creen Shot 2018-11-06 at 9.38.23 P"></p>
<p><a href="https://arxiv.org/pdf/1711.08565v1.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a> <a href="https://github.com/pkuvmc/PTGAN" target="_blank" rel="noopener">code</a> </p>
<p>A new Multi-Scene Multi-Time person ReID dataset (MSMT17) is proposed. </p>
<p><strong>[transfer styles but keep identies]</strong> A method is proposed to bridge the domain gap by transferring persons in dataset A to another dataset B. The transferred persons from A are desired to keep their identities, meanwhile present similar styles, e.g., backgrounds, lightings, etc., with persons in B.</p>
<p>To keep identity, a identities loss is introduced where the mask region of generated images and gt images should be similar.</p>
<p><a href="https://arxiv.org/pdf/1711.06454v5.pdf" target="_blank" rel="noopener">Separating Style and Content for Generalized Style Transfer</a> </p>
<p><a href="https://arxiv.org/pdf/1712.01066v1.pdf" target="_blank" rel="noopener">Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images</a> <a href="https://github.com/tribhuvanesh/visual_redactions" target="_blank" rel="noopener">code</a> </p>
<p><a href="https://arxiv.org/pdf/1712.07262v2.pdf" target="_blank" rel="noopener">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</a> </p>
<p><a href="https://arxiv.org/pdf/1802.06713v3.pdf" target="_blank" rel="noopener">Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment</a></p>
<p><a href="https://arxiv.org/pdf/1703.03492v3.pdf" target="_blank" rel="noopener">A New Representation of Skeleton Sequences for 3D Action Recognition</a> </p>
<h1 id="2018-11-04"><a href="#2018-11-04" class="headerlink" title="2018-11-04"></a>2018-11-04</h1><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1978.pdf" target="_blank" rel="noopener">Synthesizing Images of Humans in Unseen Poses</a></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-04 at 11.17.55 PM.png" alt="creen Shot 2018-11-04 at 11.17.55 P"></p>
<p>Our model is trained on (example, label) tuples of the form $((I_s, p_s, p_t), I_t)$, where $I_s$, $p_s$ and $p_t$ are the source image, source 2D pose and target 2D pose, and $I_t$ is the target image.</p>
<p>our model first segments the scene into foreground and background layers. It further segments the person’s body into different part layers such as the arms and legs, allowing each part to then be moved independently of the others.<img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-04 at 11.22.15 PM.png" alt="creen Shot 2018-11-04 at 11.22.15 P"></p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3569.pdf" target="_blank" rel="noopener">Generating a Fusion Image: One’s Identity and Another’s Shape</a></p>
<p>Given two rgb images $x $ and $y$, we try to generate a new image which is the combination of the identity $x$ and the shape or pose of $y$.</p>
<p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lassner_A_Generative_Model_ICCV_2017_paper.pdf" target="_blank" rel="noopener">A Generative Model of People in Clothing</a> </p>
<p>The authors try to generate different people with different clothes but with the same specified shapes.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-05 at 10.58.10 AM.png" alt="creen Shot 2018-11-05 at 10.58.10 A"></p>
<h1 id="2018-11-2"><a href="#2018-11-2" class="headerlink" title="2018-11-2"></a>2018-11-2</h1><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.pdf" target="_blank" rel="noopener">5</a> </p>
<p><a href="https://arxiv.org/pdf/1507.06821.pdf" target="_blank" rel="noopener">Multimodal Deep Learning for Robust RGB-D Object Recognition</a> </p>
<p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Depth-aware CNN for RGB-D Segmentation</a> </p>
<p><a href="https://arxiv.org/pdf/1407.5736.pdf" target="_blank" rel="noopener">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</a> </p>
<h2 id="Deep-Bilinear-Learning-for-RGB-D-Action-Recognition"><a href="#Deep-Bilinear-Learning-for-RGB-D-Action-Recognition" class="headerlink" title="Deep Bilinear Learning for RGB-D Action Recognition"></a>Deep Bilinear Learning for RGB-D Action Recognition</h2><p><a href="http://isee.sysu.edu.cn/~hujianfang/pdfFiles/ECCV2018.pdf" target="_blank" rel="noopener">PAPER</a></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-02 at 4.30.19 PM.png" alt="creen Shot 2018-11-02 at 4.30.19 P"></p>
<p>In this paper, we present a novel tensor-structured cube feature【The multi-modal sequences with temporal information can be regarded as a tensor， structured with two different dimensions (temporal and modality)】, and propose to learn time-varying information from multi-modal action history sequences for RGB-D action recognition.</p>
<p>In this paper, we address this challenge by proposing a novel deep bilinear framework, where a bilinear block consisting of two linear pooling layers (modality pooling layer and temporal pooling layer) is defined to pool the input tensor along the modality and temporal directions, separately. In this way, the structures along the temporal and modal dimensions are both preserved. By stacking the proposed bilinear blocks and other network layers (e.g., Relu and softmax), we develop our deep bilinear model to jointly learn the action history and modality information in videos. Results have shown that learning modality-temporal mutual information is beneficial for the recognition of RGB-D actions.</p>
<h1 id="2018-10-31"><a href="#2018-10-31" class="headerlink" title="2018-10-31"></a>2018-10-31</h1><h5 id="A-Pose-Sensitive-Embedding-for-Person-Re-Identification-with-Expanded-Cross-Neighborhood-Re-Ranking"><a href="#A-Pose-Sensitive-Embedding-for-Person-Re-Identification-with-Expanded-Cross-Neighborhood-Re-Ranking" class="headerlink" title="A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1114.pdf" target="_blank" rel="noopener">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</a></h5><p>coarse pose : front, back, side of a person’s orientation to the camera.</p>
<p>fine pose : joint skeleton</p>
<h5 id="Everybody-Dance-Now"><a href="#Everybody-Dance-Now" class="headerlink" title="Everybody Dance Now"></a><a href="https://arxiv.org/pdf/1808.07371.pdf" target="_blank" rel="noopener">Everybody Dance Now</a></h5><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-31 at 11.17.32 PM.png" alt="creen Shot 2018-10-31 at 11.17.32 P"></p>
<p>The task is to generate a action video conditioned on the figure and source video. </p>
<p>The problem is you have to train a generate G model for each new person.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-14 at 10.55.48 PM.png" alt="creen Shot 2018-11-14 at 10.55.48 P"></p>
<p>$L_{VGG}$ loss: instead of using per-pixel loss functions depending only on low-level pixel information, we train our networks using perceptual loss functions that depend on high-level features from a pretrained loss network. During training, perceptual losses measure<br>image similarities more robustly than per-pixel losses</p>
<h5 id="CR-GAN-Learning-Complete-Representations-for-Multi-view-Generation"><a href="#CR-GAN-Learning-Complete-Representations-for-Multi-view-Generation" class="headerlink" title="CR-GAN: Learning Complete Representations for Multi-view Generation"></a><a href="https://www.ijcai.org/proceedings/2018/0131.pdf" target="_blank" rel="noopener">CR-GAN: Learning Complete Representations for Multi-view Generation</a></h5><ol>
<li>learn complete representation to handle unseen data problem.</li>
</ol>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%2010.55.28%20AM.png" alt="creen Shot 2018-11-01 at 10.55.28 A"></p>
<p>The authors aim to geneate multi-view images of a figure given one image of that person. </p>
<h5 id="Geometry-Contrastive-GAN-for-Facial-Expression-Transfer"><a href="#Geometry-Contrastive-GAN-for-Facial-Expression-Transfer" class="headerlink" title="Geometry-Contrastive GAN for Facial Expression Transfer"></a><a href="https://arxiv.org/pdf/1802.01822.pdf" target="_blank" rel="noopener">Geometry-Contrastive GAN for Facial Expression Transfer</a></h5><ol>
<li>handle the misalignment across different subjects or facial expressions.</li>
</ol>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%2011.22.36%20AM.png" alt="creen Shot 2018-11-01 at 11.22.36 A"></p>
<h5 id="Pose-Guided-Human-Video-Generation"><a href="#Pose-Guided-Human-Video-Generation" class="headerlink" title="Pose Guided Human Video Generation"></a><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ceyuan_Yang_Pose_Guided_Human_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Pose Guided Human Video Generation</a></h5><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-01 at 2.17.07 PM.png" alt="creen Shot 2018-11-01 at 2.17.07 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-01 at 2.20.38 PM.png" alt="creen Shot 2018-11-01 at 2.20.38 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-01 at 2.51.41 PM.png" alt="creen Shot 2018-11-01 at 2.51.41 P"></p>
<h5 id="DDDDDDDIFFICULTLearning-to-Forecast-and-Refine-Residual-Motion-for-Image-to-Video-Generation"><a href="#DDDDDDDIFFICULTLearning-to-Forecast-and-Refine-Residual-Motion-for-Image-to-Video-Generation" class="headerlink" title="DDDDDDDIFFICULTLearning to Forecast and Refine Residual Motion for Image-to-Video Generation"></a>DDDDDDDIFFICULT<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Long_Zhao_Learning_to_Forecast_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Learning to Forecast and Refine Residual Motion for Image-to-Video Generation</a></h5><p>we study a form of classic problems in video generation that can be framed as<br>image-to-video translation tasks, where a system receives one or more images<br>as the input and translates it into a video containing realistic motions of a<br>single object.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-31 at 6.12.16 PM.png" alt="creen Shot 2018-10-31 at 6.12.16 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-31 at 10.45.04 PM.png" alt="creen Shot 2018-10-31 at 10.45.04 P"></p>
<h1 id="2018-10-29"><a href="#2018-10-29" class="headerlink" title="2018-10-29"></a>2018-10-29</h1><h5 id="Pose-Normalized-Image-Generation-for-Person-Re-identification"><a href="#Pose-Normalized-Image-Generation-for-Person-Re-identification" class="headerlink" title="Pose-Normalized Image Generation for Person Re-identification"></a><a href="https://arxiv.org/pdf/1712.02225.pdf" target="_blank" rel="noopener">Pose-Normalized Image Generation for Person Re-identification</a></h5><p>Critically, once trained, the model can be applied to a new dataset without any model fine-tuning as long as the test image’s pose is also normalized.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.46.55 PM.png" alt="creen Shot 2018-10-29 at 9.46.55 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.45.51 PM.png" alt="creen Shot 2018-10-29 at 9.45.51 P"></p>
<p>we train two re-id models. One model is trained using the original images in a training set to extract identity-invariant features in the presence of pose variation. The other is trained using the synthesized images with normalized poses using our PN-GAN to compute re-id features free of pose variation. They are then fused as the final feature representat.</p>
<h5 id="VITON-An-Image-based-Virtual-Try-on-Network"><a href="#VITON-An-Image-based-Virtual-Try-on-Network" class="headerlink" title="VITON: An Image-based Virtual Try-on Network"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf" target="_blank" rel="noopener">VITON: An Image-based Virtual Try-on Network</a></h5><p>We present an image-based virtual try-on approach, relying merely on plain RGB images without leveraging any 3D information. we propose a virtual try-on network (VITON), a coarse-to-fine framework that seamlessly transfers a target clothing item in a product image to the corresponding region of a clothed person in a 2D image.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.06.30 PM.png" alt="creen Shot 2018-10-29 at 10.06.30 P"></p>
<p>The mask is then used as a guidance to warp the target clothing item to account for deformations.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.33.18 PM.png" alt="creen Shot 2018-10-29 at 10.33.18 P"></p>
<h5 id="Disentangled-Person-Image-Generation"><a href="#Disentangled-Person-Image-Generation" class="headerlink" title="Disentangled Person Image Generation"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Disentangled Person Image Generation</a></h5><p>Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.53.41 PM.png" alt="creen Shot 2018-10-29 at 10.53.41 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.54.11 PM.png" alt="creen Shot 2018-10-29 at 10.54.11 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.57.45 PM.png" alt="creen Shot 2018-10-29 at 10.57.45 P"></p>
<p>In stage one, a real image is used to train 3 independent encoders, i.e., Pose Encoder, Foreground Encoder, and Background Encoder. </p>
<p>In stage two, we can smaple features from 3 encoders respectively to get pose features, foreground features and background features. And combining these three features to generate imagse.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.08.43 PM.png" alt="creen Shot 2018-10-29 at 11.08.43 P"></p>
<p>In particular, we aim at sampling from a standard distribution, e.g. a Gaussian<br>distribution, to first generate new embedding features and from them generate new images</p>
<h5 id="Natural-and-Effective-Obfuscation-by-Head-Inpainting"><a href="#Natural-and-Effective-Obfuscation-by-Head-Inpainting" class="headerlink" title="Natural and Effective Obfuscation by Head Inpainting"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Natural_and_Effective_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Natural and Effective Obfuscation by Head Inpainting</a></h5><ol>
<li>detecting 68 facial keypoints using the python dlib toolbox <a href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf" target="_blank" rel="noopener">paper</a> </li>
<li>​</li>
</ol>
<p>We focus on the scenario where the user wants to obfuscate some identities in a social media photo by inpainting new heads for them. We use facial landmarks to provide strong guidance for the head inpainter. We factor the head inpainting task into two stages: (1) landmark detection or generation and (2) head inpainting conditioned on body context and landmarks.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.19.36 PM.png" alt="creen Shot 2018-10-29 at 11.19.36 P"></p>
<p>It takes <strong>either the original or blackhead image as input</strong>, in order to give flexibility to deal with cases where the original images are not available.</p>
<p>Given original or headobfuscated input, stage-I detects or generates landmarks,<br>respectively. Stage-II takes the blackhead image and landmarks as input, and outputs the generated image.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.23.53 PM.png" alt="creen Shot 2018-10-29 at 11.23.53 P"></p>
<h5 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Deformable GANs for Pose-based Human Image Generation</a></h5><p>Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with <strong>pixel-to-pixel misalignments caused by the pose differences</strong>, we introduce deformable skip connections in the generator of our Generative Adversarial Network. </p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-30 at 10.00.26 AM.png" alt="creen Shot 2018-10-30 at 10.00.26 A"></p>
<h1 id="2018-10-27"><a href="#2018-10-27" class="headerlink" title="2018-10-27"></a>2018-10-27</h1><h5 id="Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training"><a href="#Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training" class="headerlink" title="Cross-Modality Person Re-Identification with Generative Adversarial Training"></a><a href="https://www.ijcai.org/proceedings/2018/0094.pdf" target="_blank" rel="noopener">Cross-Modality Person Re-Identification with Generative Adversarial Training</a></h5><p>studied the Re-ID between infrared and RGB images, which is essentially a cross-modality problem and widely encountered in real-world scenarios.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.40.42 AM.png" alt="creen Shot 2018-10-29 at 9.40.42 A"></p>
<h5 id="Predicting-Human-Interaction-via-Relative-Attention-Model"><a href="#Predicting-Human-Interaction-via-Relative-Attention-Model" class="headerlink" title="Predicting Human Interaction via Relative Attention Model"></a><a href="https://arxiv.org/pdf/1705.09467.pdf" target="_blank" rel="noopener">Predicting Human Interaction via Relative Attention Model</a></h5><p>Essentially, a good algorithm should effectively model the mutual influence between the two interacting subjects. Also, only a small region in the scene is discriminative for identifying the on-going interaction.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.22.19 AM.png" alt="creen Shot 2018-10-29 at 10.22.19 A"></p>
<h5 id="An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data"><a href="#An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data" class="headerlink" title="An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data"></a><a href="https://arxiv.org/pdf/1611.06067.pdf" target="_blank" rel="noopener">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</a></h5><p>We build our model on top of the Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which learns to selectively focus on <strong>discriminative joints</strong> of skeleton <strong>within each frame</strong> of the inputs and pays <strong>different levels of attention</strong> to the outputs of <strong>different frames</strong>.</p>
<p>For spatial joints of skeleton, we propose a spatial attention module which conducts automatic mining of discriminative joints. A certain type of action is usually only associated with and characterized by the combinations of a subset of kinematic joints. </p>
<p>For a sequence, the amount of valuable information provided by different frames is in general not equal. Only some of the frames (key frames) contain the most discriminative information while the other frames provide context information.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-28 at 11.13.33 AM.png" alt="creen Shot 2018-10-28 at 11.13.33 A"></p>
<h5 id="Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation"><a href="#Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation" class="headerlink" title="Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation"></a><a href="https://www.ijcai.org/proceedings/2018/0109.pdf" target="_blank" rel="noopener">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</a></h5><p>focus on the problem of skeleton-based human action recognition and detection.</p>
<p>By investigating the convolution operation, we may decompose it into two steps, i.e. local feature aggregation across the spatial domain (width and height) and global feature aggregation across channels.</p>
<p>The input is skeleton sequences and skeleton temporal differences.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.15.10 AM.png" alt="creen Shot 2018-10-29 at 11.15.10 A"></p>
<p>For multiple persons, inputs of multiple persons go through the same subnetwork and their conv6 feature maps are merged with either concatenation along channels or element-wise maximum / mean operation.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.17.51 AM.png" alt="creen Shot 2018-10-29 at 11.17.51 A"></p>
<p>Action detection</p>
<h5 id="Pose-Guided-Person-Image-Generation"><a href="#Pose-Guided-Person-Image-Generation" class="headerlink" title="Pose Guided Person Image Generation"></a><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a></h5><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-28 at 10.05.47 PM.png" alt="creen Shot 2018-10-28 at 10.05.47 P"></p>
<p><a href="https://arxiv.org/pdf/1601.01006.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.01006.pdf</a></p>
<h5 id="A2g-GAN"><a href="#A2g-GAN" class="headerlink" title="A2g-GAN"></a><a href="https://shaoanlu.wordpress.com/2018/09/12/lets-train-gans-to-play-guitar/" target="_blank" rel="noopener">A2g-GAN</a></h5><h1 id="2018-10-26"><a href="#2018-10-26" class="headerlink" title="2018-10-26"></a>2018-10-26</h1><h5 id="IJCAI-2018"><a href="#IJCAI-2018" class="headerlink" title="IJCAI 2018"></a><a href="https://github.com/CSer-Tang-hao/Papers-Reading-Recording" target="_blank" rel="noopener">IJCAI 2018</a></h5><h5 id="Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks"><a href="#Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks" class="headerlink" title="Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks"></a><a href="https://arxiv.org/pdf/1805.04384.pdf" target="_blank" rel="noopener">Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks</a></h5><p>The two-level HiGAN is designed to have a low-level conditional GAN and a high-level conditional GAN. The low-level conditional GAN is built to connect videos and their corresponding video frames by learning a mapping function from frame features to video features in the target domain. The high-level conditional GAN, on the other hand, is modeled to bridge the gap between source images and target videos by formulating a mapping function from video features to image-frame features.</p>
<h5 id="Memory-Attention-Networks-for-Skeleton-based-Action-Recognition"><a href="#Memory-Attention-Networks-for-Skeleton-based-Action-Recognition" class="headerlink" title="Memory Attention Networks for Skeleton-based Action Recognition"></a><a href="https://arxiv.org/pdf/1804.08254.pdf" target="_blank" rel="noopener">Memory Attention Networks for Skeleton-based Action Recognition</a></h5><h1 id="2018-10-25"><a href="#2018-10-25" class="headerlink" title="2018-10-25"></a>2018-10-25</h1><h5 id="StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation"><a href="#StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"></a><a href="https://arxiv.org/pdf/1711.09020.pdf" target="_blank" rel="noopener">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></h5><h5 id="Pose-Guided-Person-Image-Generation-1"><a href="#Pose-Guided-Person-Image-Generation-1" class="headerlink" title="Pose Guided Person Image Generation"></a><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a></h5><h5 id="Disentangled-Person-Image-Generation-1"><a href="#Disentangled-Person-Image-Generation-1" class="headerlink" title="Disentangled Person Image Generation"></a><a href="https://arxiv.org/pdf/1712.02621.pdf" target="_blank" rel="noopener">Disentangled Person Image Generation</a></h5><h5 id="Deformable-GANs-for-Pose-based-Human-Image-Generation-1"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation-1" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a><a href="https://arxiv.org/pdf/1801.00055.pdf" target="_blank" rel="noopener">Deformable GANs for Pose-based Human Image Generation</a></h5><h1 id="2018-10-23"><a href="#2018-10-23" class="headerlink" title="2018-10-23"></a>2018-10-23</h1><h5 id="Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs"><a href="#Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs" class="headerlink" title="Generating Realistic Videos from Keyframes with Concatenated GANs"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8451971" target="_blank" rel="noopener">Generating Realistic Videos from Keyframes with Concatenated GANs</a></h5><p>Given two video frames X0 and Xn+1, we aim to generate a series of intermediate frames Y1, Y2, · · · , Yn, such that the resulting video consisting of frames X0, Y1-Yn, Xn+1 appears realistic to a human watcher.</p>
<h5 id="Human-Action-Generation-with-Generative-Adversarial-Networks"><a href="#Human-Action-Generation-with-Generative-Adversarial-Networks" class="headerlink" title="Human Action Generation with Generative Adversarial Networks"></a><a href="https://arxiv.org/pdf/1805.10416.pdf" target="_blank" rel="noopener">Human Action Generation with Generative Adversarial Networks</a></h5><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-23 at 11.02.50 PM.png" alt="creen Shot 2018-10-23 at 11.02.50 P"></p>
<h5 id="Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences"><a href="#Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences" class="headerlink" title="Deep Video Generation, Prediction and Completion of Human Action Sequences"></a><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chunyan_Bai_Deep_Video_Generation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Deep Video Generation, Prediction and Completion of Human Action Sequences</a></h5><p>The model itself</p>
<p>is originally desi gne d for video generation, i.e., generating human action videos</p>
<p>from random noise. We split the generation process into two stages: ﬁrst, we</p>
<p>generate human skeleton sequences from random noise, and then we t r an sf orm</p>
<p>from the skeleton images to the real pixel-level images.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-26 at 9.15.48 AM.png" alt="creen Shot 2018-10-26 at 9.15.48 A"></p>
<p>The model is independent of training subjests, where we train the model using some subjects but test it using totally different subjects.</p>
<h5 id="Multiple-Granularity-Group-Interaction-Prediction"><a href="#Multiple-Granularity-Group-Interaction-Prediction" class="headerlink" title="Multiple Granularity Group Interaction Prediction"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0721.pdf" target="_blank" rel="noopener">Multiple Granularity Group Interaction Prediction</a></h5><h5 id="GestureGAN-for-Hand-Gesture-to-Gesture-Translation"><a href="#GestureGAN-for-Hand-Gesture-to-Gesture-Translation" class="headerlink" title="GestureGAN for Hand Gesture-to-Gesture Translation"></a><a href="https://arxiv.org/pdf/1808.04859.pdf" target="_blank" rel="noopener">GestureGAN for Hand Gesture-to-Gesture Translation</a></h5><h5 id="Human-Motion-Generation-via-Cross-Space-Constrained-Sampling"><a href="#Human-Motion-Generation-via-Cross-Space-Constrained-Sampling" class="headerlink" title="Human Motion Generation via Cross-Space Constrained Sampling"></a><a href="https://www.ijcai.org/proceedings/2018/0105.pdf" target="_blank" rel="noopener">Human Motion Generation via Cross-Space Constrained Sampling</a></h5>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Paper-Everyday/" rel="tag"># Paper Everyday</a>
          
            <a href="/tags/Generative-adversarial-networks/" rel="tag"># Generative adversarial networks</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/20/Cooking/" rel="next" title="Cooking">
                <i class="fa fa-chevron-left"></i> Cooking
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/02/AlexNet/" rel="prev" title="AlexNet">
                AlexNet <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">67</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">51</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Videos"><span class="nav-number">1.</span> <span class="nav-text">Videos</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP"><span class="nav-number">2.</span> <span class="nav-text">NLP</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Images"><span class="nav-number">3.</span> <span class="nav-text">Images</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-11-08"><span class="nav-number">4.</span> <span class="nav-text">2018-11-08</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-11-05"><span class="nav-number">5.</span> <span class="nav-text">2018-11-05</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-11-04"><span class="nav-number">6.</span> <span class="nav-text">2018-11-04</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-11-2"><span class="nav-number">7.</span> <span class="nav-text">2018-11-2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Bilinear-Learning-for-RGB-D-Action-Recognition"><span class="nav-number">7.1.</span> <span class="nav-text">Deep Bilinear Learning for RGB-D Action Recognition</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-10-31"><span class="nav-number">8.</span> <span class="nav-text">2018-10-31</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#A-Pose-Sensitive-Embedding-for-Person-Re-Identification-with-Expanded-Cross-Neighborhood-Re-Ranking"><span class="nav-number">8.0.0.0.1.</span> <span class="nav-text">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Everybody-Dance-Now"><span class="nav-number">8.0.0.0.2.</span> <span class="nav-text">Everybody Dance Now</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CR-GAN-Learning-Complete-Representations-for-Multi-view-Generation"><span class="nav-number">8.0.0.0.3.</span> <span class="nav-text">CR-GAN: Learning Complete Representations for Multi-view Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Geometry-Contrastive-GAN-for-Facial-Expression-Transfer"><span class="nav-number">8.0.0.0.4.</span> <span class="nav-text">Geometry-Contrastive GAN for Facial Expression Transfer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Guided-Human-Video-Generation"><span class="nav-number">8.0.0.0.5.</span> <span class="nav-text">Pose Guided Human Video Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DDDDDDDIFFICULTLearning-to-Forecast-and-Refine-Residual-Motion-for-Image-to-Video-Generation"><span class="nav-number">8.0.0.0.6.</span> <span class="nav-text">DDDDDDDIFFICULTLearning to Forecast and Refine Residual Motion for Image-to-Video Generation</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-10-29"><span class="nav-number">9.</span> <span class="nav-text">2018-10-29</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Normalized-Image-Generation-for-Person-Re-identification"><span class="nav-number">9.0.0.0.1.</span> <span class="nav-text">Pose-Normalized Image Generation for Person Re-identification</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#VITON-An-Image-based-Virtual-Try-on-Network"><span class="nav-number">9.0.0.0.2.</span> <span class="nav-text">VITON: An Image-based Virtual Try-on Network</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Disentangled-Person-Image-Generation"><span class="nav-number">9.0.0.0.3.</span> <span class="nav-text">Disentangled Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Natural-and-Effective-Obfuscation-by-Head-Inpainting"><span class="nav-number">9.0.0.0.4.</span> <span class="nav-text">Natural and Effective Obfuscation by Head Inpainting</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deformable-GANs-for-Pose-based-Human-Image-Generation"><span class="nav-number">9.0.0.0.5.</span> <span class="nav-text">Deformable GANs for Pose-based Human Image Generation</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-10-27"><span class="nav-number">10.</span> <span class="nav-text">2018-10-27</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training"><span class="nav-number">10.0.0.0.1.</span> <span class="nav-text">Cross-Modality Person Re-Identification with Generative Adversarial Training</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Predicting-Human-Interaction-via-Relative-Attention-Model"><span class="nav-number">10.0.0.0.2.</span> <span class="nav-text">Predicting Human Interaction via Relative Attention Model</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data"><span class="nav-number">10.0.0.0.3.</span> <span class="nav-text">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation"><span class="nav-number">10.0.0.0.4.</span> <span class="nav-text">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Guided-Person-Image-Generation"><span class="nav-number">10.0.0.0.5.</span> <span class="nav-text">Pose Guided Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A2g-GAN"><span class="nav-number">10.0.0.0.6.</span> <span class="nav-text">A2g-GAN</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-10-26"><span class="nav-number">11.</span> <span class="nav-text">2018-10-26</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#IJCAI-2018"><span class="nav-number">11.0.0.0.1.</span> <span class="nav-text">IJCAI 2018</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks"><span class="nav-number">11.0.0.0.2.</span> <span class="nav-text">Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Memory-Attention-Networks-for-Skeleton-based-Action-Recognition"><span class="nav-number">11.0.0.0.3.</span> <span class="nav-text">Memory Attention Networks for Skeleton-based Action Recognition</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-10-25"><span class="nav-number">12.</span> <span class="nav-text">2018-10-25</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation"><span class="nav-number">12.0.0.0.1.</span> <span class="nav-text">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Guided-Person-Image-Generation-1"><span class="nav-number">12.0.0.0.2.</span> <span class="nav-text">Pose Guided Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Disentangled-Person-Image-Generation-1"><span class="nav-number">12.0.0.0.3.</span> <span class="nav-text">Disentangled Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deformable-GANs-for-Pose-based-Human-Image-Generation-1"><span class="nav-number">12.0.0.0.4.</span> <span class="nav-text">Deformable GANs for Pose-based Human Image Generation</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-10-23"><span class="nav-number">13.</span> <span class="nav-text">2018-10-23</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs"><span class="nav-number">13.0.0.0.1.</span> <span class="nav-text">Generating Realistic Videos from Keyframes with Concatenated GANs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Human-Action-Generation-with-Generative-Adversarial-Networks"><span class="nav-number">13.0.0.0.2.</span> <span class="nav-text">Human Action Generation with Generative Adversarial Networks</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences"><span class="nav-number">13.0.0.0.3.</span> <span class="nav-text">Deep Video Generation, Prediction and Completion of Human Action Sequences</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multiple-Granularity-Group-Interaction-Prediction"><span class="nav-number">13.0.0.0.4.</span> <span class="nav-text">Multiple Granularity Group Interaction Prediction</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GestureGAN-for-Hand-Gesture-to-Gesture-Translation"><span class="nav-number">13.0.0.0.5.</span> <span class="nav-text">GestureGAN for Hand Gesture-to-Gesture Translation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Human-Motion-Generation-via-Cross-Space-Constrained-Sampling"><span class="nav-number">13.0.0.0.6.</span> <span class="nav-text">Human Motion Generation via Cross-Space Constrained Sampling</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
