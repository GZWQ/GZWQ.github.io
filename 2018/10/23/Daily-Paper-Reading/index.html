<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Paper Everyday,Generative adversarial networks," />










<meta name="description" content="2018-10-29Pose-Normalized Image Generation for Person Re-identificationCritically, once trained, the model can be applied to a new dataset without any model fine-tuning as long as the test image’s pos">
<meta name="keywords" content="Deep Learning,Paper Everyday,Generative adversarial networks">
<meta property="og:type" content="article">
<meta property="og:title" content="Daily Paper Reading">
<meta property="og:url" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="2018-10-29Pose-Normalized Image Generation for Person Re-identificationCritically, once trained, the model can be applied to a new dataset without any model fine-tuning as long as the test image’s pos">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.46.55%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.45.51%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.06.30%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.33.18%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.53.41%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.54.11%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.57.45%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.08.43%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.19.36%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.23.53%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-30%20at%2010.00.26%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.40.42%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2010.22.19%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-28%20at%2011.13.33%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.15.10%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%2011.17.51%20AM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-28%20at%2010.05.47%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-23%20at%2011.02.50%20PM.png">
<meta property="og:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-26%20at%209.15.48%20AM.png">
<meta property="og:updated_time" content="2018-10-30T19:18:51.188Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Daily Paper Reading">
<meta name="twitter:description" content="2018-10-29Pose-Normalized Image Generation for Person Re-identificationCritically, once trained, the model can be applied to a new dataset without any model fine-tuning as long as the test image’s pos">
<meta name="twitter:image" content="http://yoursite.com/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-10-29%20at%209.46.55%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/23/Daily-Paper-Reading/"/>





  <title>Daily Paper Reading | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/23/Daily-Paper-Reading/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Daily Paper Reading</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-23T22:37:04-05:00">
                2018-10-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h4 id="2018-10-29"><a href="#2018-10-29" class="headerlink" title="2018-10-29"></a>2018-10-29</h4><h5 id="Pose-Normalized-Image-Generation-for-Person-Re-identification"><a href="#Pose-Normalized-Image-Generation-for-Person-Re-identification" class="headerlink" title="Pose-Normalized Image Generation for Person Re-identification"></a><a href="https://arxiv.org/pdf/1712.02225.pdf" target="_blank" rel="noopener">Pose-Normalized Image Generation for Person Re-identification</a></h5><p>Critically, once trained, the model can be applied to a new dataset without any model fine-tuning as long as the test image’s pose is also normalized.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.46.55 PM.png" alt="creen Shot 2018-10-29 at 9.46.55 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.45.51 PM.png" alt="creen Shot 2018-10-29 at 9.45.51 P"></p>
<h5 id="VITON-An-Image-based-Virtual-Try-on-Network"><a href="#VITON-An-Image-based-Virtual-Try-on-Network" class="headerlink" title="VITON: An Image-based Virtual Try-on Network"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf" target="_blank" rel="noopener">VITON: An Image-based Virtual Try-on Network</a></h5><p>We present an image-based virtual try-on approach, relying merely on plain RGB images without leveraging any 3D information. we propose a virtual try-on network (VITON), a coarse-to-fine framework that seamlessly transfers a target clothing item in a product image to the corresponding region of a clothed person in a 2D image.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.06.30 PM.png" alt="creen Shot 2018-10-29 at 10.06.30 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.33.18 PM.png" alt="creen Shot 2018-10-29 at 10.33.18 P"></p>
<h5 id="Disentangled-Person-Image-Generation"><a href="#Disentangled-Person-Image-Generation" class="headerlink" title="Disentangled Person Image Generation"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Disentangled Person Image Generation</a></h5><p>Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.53.41 PM.png" alt="creen Shot 2018-10-29 at 10.53.41 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.54.11 PM.png" alt="creen Shot 2018-10-29 at 10.54.11 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.57.45 PM.png" alt="creen Shot 2018-10-29 at 10.57.45 P"></p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.08.43 PM.png" alt="creen Shot 2018-10-29 at 11.08.43 P"></p>
<p>In particular, we aim at sampling from a standard distribution, e.g. a Gaussian<br>distribution, to first generate new embedding features and from them generate new images</p>
<h5 id="Natural-and-Effective-Obfuscation-by-Head-Inpainting"><a href="#Natural-and-Effective-Obfuscation-by-Head-Inpainting" class="headerlink" title="Natural and Effective Obfuscation by Head Inpainting"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Natural_and_Effective_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Natural and Effective Obfuscation by Head Inpainting</a></h5><p>We focus on the scenario where the user wants to obfuscate some identities in a social media photo by inpainting new heads for them. We use facial landmarks to provide strong guidance for the head inpainter. We factor the head inpainting task into two stages: (1) landmark detection or generation and (2) head inpainting conditioned on body context and landmarks.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.19.36 PM.png" alt="creen Shot 2018-10-29 at 11.19.36 P"></p>
<p>It takes <strong>either the original or blackhead image as input</strong>, in order to give flexibility to deal with cases where the original images are not available.</p>
<p>Given original or headobfuscated input, stage-I detects or generates landmarks,<br>respectively. Stage-II takes the blackhead image and landmarks as input, and outputs the generated image.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.23.53 PM.png" alt="creen Shot 2018-10-29 at 11.23.53 P"></p>
<h5 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Deformable GANs for Pose-based Human Image Generation</a></h5><p>Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with <strong>pixel-to-pixel misalignments caused by the pose differences</strong>, we introduce deformable skip connections in the generator of our Generative Adversarial Network. </p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-30 at 10.00.26 AM.png" alt="creen Shot 2018-10-30 at 10.00.26 A"></p>
<h4 id="2018-10-27"><a href="#2018-10-27" class="headerlink" title="2018-10-27"></a>2018-10-27</h4><h5 id="Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training"><a href="#Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training" class="headerlink" title="Cross-Modality Person Re-Identification with Generative Adversarial Training"></a><a href="https://www.ijcai.org/proceedings/2018/0094.pdf" target="_blank" rel="noopener">Cross-Modality Person Re-Identification with Generative Adversarial Training</a></h5><p>studied the Re-ID between infrared and RGB images, which is essentially a cross-modality problem and widely encountered in real-world scenarios.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.40.42 AM.png" alt="creen Shot 2018-10-29 at 9.40.42 A"></p>
<h5 id="Predicting-Human-Interaction-via-Relative-Attention-Model"><a href="#Predicting-Human-Interaction-via-Relative-Attention-Model" class="headerlink" title="Predicting Human Interaction via Relative Attention Model"></a><a href="https://arxiv.org/pdf/1705.09467.pdf" target="_blank" rel="noopener">Predicting Human Interaction via Relative Attention Model</a></h5><p>Essentially, a good algorithm should effectively model the mutual influence between the two interacting subjects. Also, only a small region in the scene is discriminative for identifying the on-going interaction.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.22.19 AM.png" alt="creen Shot 2018-10-29 at 10.22.19 A"></p>
<h5 id="An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data"><a href="#An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data" class="headerlink" title="An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data"></a><a href="https://arxiv.org/pdf/1611.06067.pdf" target="_blank" rel="noopener">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</a></h5><p>We build our model on top of the Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which learns to selectively focus on <strong>discriminative joints</strong> of skeleton <strong>within each frame</strong> of the inputs and pays <strong>different levels of attention</strong> to the outputs of <strong>different frames</strong>.</p>
<p>For spatial joints of skeleton, we propose a spatial attention module which conducts automatic mining of discriminative joints. A certain type of action is usually only associated with and characterized by the combinations of a subset of kinematic joints. </p>
<p>For a sequence, the amount of valuable information provided by different frames is in general not equal. Only some of the frames (key frames) contain the most discriminative information while the other frames provide context information.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-28 at 11.13.33 AM.png" alt="creen Shot 2018-10-28 at 11.13.33 A"></p>
<h5 id="Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation"><a href="#Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation" class="headerlink" title="Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation"></a><a href="https://www.ijcai.org/proceedings/2018/0109.pdf" target="_blank" rel="noopener">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</a></h5><p>focus on the problem of skeleton-based human action recognition and detection.</p>
<p>By investigating the convolution operation, we may decompose it into two steps, i.e. local feature aggregation across the spatial domain (width and height) and global feature aggregation across channels.</p>
<p>The input is skeleton sequences and skeleton temporal differences.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.15.10 AM.png" alt="creen Shot 2018-10-29 at 11.15.10 A"></p>
<p>For multiple persons, inputs of multiple persons go through the same subnetwork and their conv6 feature maps are merged with either concatenation along channels or element-wise maximum / mean operation.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.17.51 AM.png" alt="creen Shot 2018-10-29 at 11.17.51 A"></p>
<p>Action detection</p>
<h5 id="Pose-Guided-Person-Image-Generation"><a href="#Pose-Guided-Person-Image-Generation" class="headerlink" title="Pose Guided Person Image Generation"></a><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a></h5><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-28 at 10.05.47 PM.png" alt="creen Shot 2018-10-28 at 10.05.47 P"></p>
<p><a href="https://arxiv.org/pdf/1601.01006.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.01006.pdf</a></p>
<h5 id="A2g-GAN"><a href="#A2g-GAN" class="headerlink" title="A2g-GAN"></a><a href="https://shaoanlu.wordpress.com/2018/09/12/lets-train-gans-to-play-guitar/" target="_blank" rel="noopener">A2g-GAN</a></h5><h4 id="2018-10-26"><a href="#2018-10-26" class="headerlink" title="2018-10-26"></a>2018-10-26</h4><h5 id="IJCAI-2018"><a href="#IJCAI-2018" class="headerlink" title="IJCAI 2018"></a><a href="https://github.com/CSer-Tang-hao/Papers-Reading-Recording" target="_blank" rel="noopener">IJCAI 2018</a></h5><h5 id="Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks"><a href="#Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks" class="headerlink" title="Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks"></a><a href="https://arxiv.org/pdf/1805.04384.pdf" target="_blank" rel="noopener">Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks</a></h5><p>The two-level HiGAN is designed to have a low-level conditional GAN and a high-level conditional GAN. The low-level conditional GAN is built to connect videos and their corresponding video frames by learning a mapping function from frame features to video features in the target domain. The high-level conditional GAN, on the other hand, is modeled to bridge the gap between source images and target videos by formulating a mapping function from video features to image-frame features.</p>
<h5 id="Memory-Attention-Networks-for-Skeleton-based-Action-Recognition"><a href="#Memory-Attention-Networks-for-Skeleton-based-Action-Recognition" class="headerlink" title="Memory Attention Networks for Skeleton-based Action Recognition"></a><a href="https://arxiv.org/pdf/1804.08254.pdf" target="_blank" rel="noopener">Memory Attention Networks for Skeleton-based Action Recognition</a></h5><h4 id="2018-10-25"><a href="#2018-10-25" class="headerlink" title="2018-10-25"></a>2018-10-25</h4><h5 id="StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation"><a href="#StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"></a><a href="https://arxiv.org/pdf/1711.09020.pdf" target="_blank" rel="noopener">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></h5><h5 id="Pose-Guided-Person-Image-Generation-1"><a href="#Pose-Guided-Person-Image-Generation-1" class="headerlink" title="Pose Guided Person Image Generation"></a><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a></h5><h5 id="Disentangled-Person-Image-Generation-1"><a href="#Disentangled-Person-Image-Generation-1" class="headerlink" title="Disentangled Person Image Generation"></a><a href="https://arxiv.org/pdf/1712.02621.pdf" target="_blank" rel="noopener">Disentangled Person Image Generation</a></h5><h5 id="Deformable-GANs-for-Pose-based-Human-Image-Generation-1"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation-1" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a><a href="https://arxiv.org/pdf/1801.00055.pdf" target="_blank" rel="noopener">Deformable GANs for Pose-based Human Image Generation</a></h5><h4 id="2018-10-23"><a href="#2018-10-23" class="headerlink" title="2018-10-23"></a>2018-10-23</h4><h5 id="Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs"><a href="#Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs" class="headerlink" title="Generating Realistic Videos from Keyframes with Concatenated GANs"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8451971" target="_blank" rel="noopener">Generating Realistic Videos from Keyframes with Concatenated GANs</a></h5><p>Given two video frames X0 and Xn+1, we aim to generate a series of intermediate frames Y1, Y2, · · · , Yn, such that the resulting video consisting of frames X0, Y1-Yn, Xn+1 appears realistic to a human watcher.</p>
<h5 id="Human-Action-Generation-with-Generative-Adversarial-Networks"><a href="#Human-Action-Generation-with-Generative-Adversarial-Networks" class="headerlink" title="Human Action Generation with Generative Adversarial Networks"></a><a href="https://arxiv.org/pdf/1805.10416.pdf" target="_blank" rel="noopener">Human Action Generation with Generative Adversarial Networks</a></h5><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-23 at 11.02.50 PM.png" alt="creen Shot 2018-10-23 at 11.02.50 P"></p>
<h5 id="Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences"><a href="#Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences" class="headerlink" title="Deep Video Generation, Prediction and Completion of Human Action Sequences"></a><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chunyan_Bai_Deep_Video_Generation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Deep Video Generation, Prediction and Completion of Human Action Sequences</a></h5><p>The model itself</p>
<p>is originally desi gne d for video generation, i.e., generating human action videos</p>
<p>from random noise. We split the generation process into two stages: ﬁrst, we</p>
<p>generate human skeleton sequences from random noise, and then we t r an sf orm</p>
<p>from the skeleton images to the real pixel-level images.</p>
<p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-26 at 9.15.48 AM.png" alt="creen Shot 2018-10-26 at 9.15.48 A"></p>
<p>The model is independent of training subjests, where we train the model using some subjects but test it using totally different subjects.</p>
<h5 id="Multiple-Granularity-Group-Interaction-Prediction"><a href="#Multiple-Granularity-Group-Interaction-Prediction" class="headerlink" title="Multiple Granularity Group Interaction Prediction"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0721.pdf" target="_blank" rel="noopener">Multiple Granularity Group Interaction Prediction</a></h5><h5 id="GestureGAN-for-Hand-Gesture-to-Gesture-Translation"><a href="#GestureGAN-for-Hand-Gesture-to-Gesture-Translation" class="headerlink" title="GestureGAN for Hand Gesture-to-Gesture Translation"></a><a href="https://arxiv.org/pdf/1808.04859.pdf" target="_blank" rel="noopener">GestureGAN for Hand Gesture-to-Gesture Translation</a></h5><h5 id="Human-Motion-Generation-via-Cross-Space-Constrained-Sampling"><a href="#Human-Motion-Generation-via-Cross-Space-Constrained-Sampling" class="headerlink" title="Human Motion Generation via Cross-Space Constrained Sampling"></a><a href="https://www.ijcai.org/proceedings/2018/0105.pdf" target="_blank" rel="noopener">Human Motion Generation via Cross-Space Constrained Sampling</a></h5>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Paper-Everyday/" rel="tag"># Paper Everyday</a>
          
            <a href="/tags/Generative-adversarial-networks/" rel="tag"># Generative adversarial networks</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/20/Cooking/" rel="next" title="Cooking">
                <i class="fa fa-chevron-left"></i> Cooking
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">51</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#2018-10-29"><span class="nav-number">1.</span> <span class="nav-text">2018-10-29</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Normalized-Image-Generation-for-Person-Re-identification"><span class="nav-number">1.1.</span> <span class="nav-text">Pose-Normalized Image Generation for Person Re-identification</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#VITON-An-Image-based-Virtual-Try-on-Network"><span class="nav-number">1.2.</span> <span class="nav-text">VITON: An Image-based Virtual Try-on Network</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Disentangled-Person-Image-Generation"><span class="nav-number">1.3.</span> <span class="nav-text">Disentangled Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Natural-and-Effective-Obfuscation-by-Head-Inpainting"><span class="nav-number">1.4.</span> <span class="nav-text">Natural and Effective Obfuscation by Head Inpainting</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deformable-GANs-for-Pose-based-Human-Image-Generation"><span class="nav-number">1.5.</span> <span class="nav-text">Deformable GANs for Pose-based Human Image Generation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2018-10-27"><span class="nav-number">2.</span> <span class="nav-text">2018-10-27</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training"><span class="nav-number">2.1.</span> <span class="nav-text">Cross-Modality Person Re-Identification with Generative Adversarial Training</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Predicting-Human-Interaction-via-Relative-Attention-Model"><span class="nav-number">2.2.</span> <span class="nav-text">Predicting Human Interaction via Relative Attention Model</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data"><span class="nav-number">2.3.</span> <span class="nav-text">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation"><span class="nav-number">2.4.</span> <span class="nav-text">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Guided-Person-Image-Generation"><span class="nav-number">2.5.</span> <span class="nav-text">Pose Guided Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A2g-GAN"><span class="nav-number">2.6.</span> <span class="nav-text">A2g-GAN</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2018-10-26"><span class="nav-number">3.</span> <span class="nav-text">2018-10-26</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#IJCAI-2018"><span class="nav-number">3.1.</span> <span class="nav-text">IJCAI 2018</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks"><span class="nav-number">3.2.</span> <span class="nav-text">Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Memory-Attention-Networks-for-Skeleton-based-Action-Recognition"><span class="nav-number">3.3.</span> <span class="nav-text">Memory Attention Networks for Skeleton-based Action Recognition</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2018-10-25"><span class="nav-number">4.</span> <span class="nav-text">2018-10-25</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation"><span class="nav-number">4.1.</span> <span class="nav-text">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pose-Guided-Person-Image-Generation-1"><span class="nav-number">4.2.</span> <span class="nav-text">Pose Guided Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Disentangled-Person-Image-Generation-1"><span class="nav-number">4.3.</span> <span class="nav-text">Disentangled Person Image Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deformable-GANs-for-Pose-based-Human-Image-Generation-1"><span class="nav-number">4.4.</span> <span class="nav-text">Deformable GANs for Pose-based Human Image Generation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2018-10-23"><span class="nav-number">5.</span> <span class="nav-text">2018-10-23</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs"><span class="nav-number">5.1.</span> <span class="nav-text">Generating Realistic Videos from Keyframes with Concatenated GANs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Human-Action-Generation-with-Generative-Adversarial-Networks"><span class="nav-number">5.2.</span> <span class="nav-text">Human Action Generation with Generative Adversarial Networks</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences"><span class="nav-number">5.3.</span> <span class="nav-text">Deep Video Generation, Prediction and Completion of Human Action Sequences</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multiple-Granularity-Group-Interaction-Prediction"><span class="nav-number">5.4.</span> <span class="nav-text">Multiple Granularity Group Interaction Prediction</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GestureGAN-for-Hand-Gesture-to-Gesture-Translation"><span class="nav-number">5.5.</span> <span class="nav-text">GestureGAN for Hand Gesture-to-Gesture Translation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Human-Motion-Generation-via-Cross-Space-Constrained-Sampling"><span class="nav-number">5.6.</span> <span class="nav-text">Human Motion Generation via Cross-Space Constrained Sampling</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
