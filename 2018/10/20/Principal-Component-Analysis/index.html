<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,Principal Component Analysis," />










<meta name="description" content="机器学习中，在高维情形下出现的数据样本稀疏、距离计算困难等问题，被称为“维数灾难”（curse of dimensionality）。而缓解该问题的一个重要途径是降维（dimension reduction），即通过某种数学变换将原始高维空间转换为一个低维“子空间”（subspace），在这个子空间中样本密度大幅度提高，距离计算也变得更加容易。 之所以可以进行降维，是因为在许多时候，人们观测或者收">
<meta name="keywords" content="Machine Learning,Principal Component Analysis">
<meta property="og:type" content="article">
<meta property="og:title" content="Principal Component Analysis">
<meta property="og:url" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="机器学习中，在高维情形下出现的数据样本稀疏、距离计算困难等问题，被称为“维数灾难”（curse of dimensionality）。而缓解该问题的一个重要途径是降维（dimension reduction），即通过某种数学变换将原始高维空间转换为一个低维“子空间”（subspace），在这个子空间中样本密度大幅度提高，距离计算也变得更加容易。 之所以可以进行降维，是因为在许多时候，人们观测或者收">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/Screen%20Shot%202018-10-24%20at%2012.56.10%20PM-0403807.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810261921009.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810241415598.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810202030142.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810252123309.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810252140521.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810261632428.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810272228567.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810272231320.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810272239305.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810272307483.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810272310030.png">
<meta property="og:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/image-201810272311402.png">
<meta property="og:updated_time" content="2018-10-28T04:11:45.815Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Principal Component Analysis">
<meta name="twitter:description" content="机器学习中，在高维情形下出现的数据样本稀疏、距离计算困难等问题，被称为“维数灾难”（curse of dimensionality）。而缓解该问题的一个重要途径是降维（dimension reduction），即通过某种数学变换将原始高维空间转换为一个低维“子空间”（subspace），在这个子空间中样本密度大幅度提高，距离计算也变得更加容易。 之所以可以进行降维，是因为在许多时候，人们观测或者收">
<meta name="twitter:image" content="http://yoursite.com/2018/10/20/Principal-Component-Analysis/Screen%20Shot%202018-10-24%20at%2012.56.10%20PM-0403807.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/20/Principal-Component-Analysis/"/>





  <title>Principal Component Analysis | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/20/Principal-Component-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Principal Component Analysis</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-20T20:16:00-05:00">
                2018-10-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>机器学习中，在高维情形下出现的数据样本稀疏、距离计算困难等问题，被称为“维数灾难”（curse of dimensionality）。而缓解该问题的一个重要途径是降维（dimension reduction），即通过某种数学变换将原始高维空间转换为一个低维“子空间”（subspace），在这个子空间中样本密度大幅度提高，距离计算也变得更加容易。</p>
<p>之所以可以进行降维，是因为在许多时候，人们观测或者收集到的数据样本虽然是高维的，但是与学习任务相关的也许仅仅是某个低维分布。</p>
<a id="more"></a>
<p><img src="/2018/10/20/Principal-Component-Analysis/Screen Shot 2018-10-24 at 12.56.10 PM-0403807.png" alt="creen Shot 2018-10-24 at 12.56.10 PM-040380"></p>
<p>成分数目选择</p>
<p>PCA降维一个重要的步骤是预先确定成分数目来描述数据，一种方法是计算cumulative explained variance ratio,</p>
<script type="math/tex; mode=display">
\frac{\sum_{i=1}^{d'}\lambda_i}{\sum_{i=1}^{d}\lambda_i}</script><p>其中，$d$是原始数据维度，$d’$是数据降解之后的维度，$\lambda_i$是第$i$个特征值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">component_number</span><span class="params">(self)</span>:</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div>
<div class="line">    digits = load_digits()  <span class="comment"># digits.data.shape=(1797, 64)</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">    pca = PCA()</div>
<div class="line">    component = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">64</span>)]</div>
<div class="line">    pca.fit(digits.data)</div>
<div class="line">    plt.plot(component,np.cumsum(pca.explained_variance_ratio_))</div>
<div class="line">    plt.xlabel(<span class="string">"number of components"</span>)</div>
<div class="line">    plt.ylabel(<span class="string">"cumulative explained variance"</span>)</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810261921009.png" alt="mage-20181026192100"></p>
<blockquote>
<p>This curve quantifies how much of the total, 64-dimensional variance is contained within the first NN components. For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.</p>
<p>Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we’d need about 20 components to retain 90% of the variance. Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.</p>
</blockquote>
<p><a href="https://github.com/makang101/machinelearning/blob/master/chapter10dimred/PCA.ipynb" target="_blank" rel="noopener">PCA from Scratch</a> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(self)</span>:</span></div>
<div class="line">    self.data = pd.read_csv(<span class="string">'testSet.txt'</span>,sep=<span class="string">'\t'</span>,header=<span class="keyword">None</span>,names=[<span class="string">'x1'</span>,<span class="string">'x2'</span>]).values</div>
<div class="line">    self.data = np.array(self.data)</div>
<div class="line">    self.ori_data = pd.read_csv(<span class="string">'testSet.txt'</span>,sep=<span class="string">'\t'</span>,header=<span class="keyword">None</span>,names=[<span class="string">'x1'</span>,<span class="string">'x2'</span>]).values</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(self)</span>:</span></div>
<div class="line">    self.d = <span class="number">1</span> <span class="comment"># dimension after reduction</span></div>
<div class="line">    self.load_data()</div>
<div class="line">    row,col = self.data.shape</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Step 1: data centralization</span></div>
<div class="line">    col_sum = np.sum(self.data,axis=<span class="number">0</span>)</div>
<div class="line">    col_mean = col_sum/row</div>
<div class="line">    self.data -= col_mean</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Step 2: calculating covariance matrix</span></div>
<div class="line">    cov = self.data.T.dot(self.data)</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Step 3: feature factorization</span></div>
<div class="line">    feature_val, feature_vec = np.linalg.eig(cov)</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Step 4: top n feature value and vector</span></div>
<div class="line">    sorted_eig = np.argsort(feature_val)</div>
<div class="line">    sorted_eig = sorted_eig[:-(self.d+<span class="number">1</span>):<span class="number">-1</span>] <span class="comment"># pic d values from the end to the front</span></div>
<div class="line">    w = feature_vec[:,sorted_eig]</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Step 5: calculate low-dimension data</span></div>
<div class="line">    self.lowdim_data = self.data.dot(w)+col_mean</div>
<div class="line">    self.vis()</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis</span><span class="params">(self)</span>:</span></div>
<div class="line">    x = self.ori_data[:,<span class="number">0</span>] <span class="comment">#(1000,2)</span></div>
<div class="line">    y = self.ori_data[:,<span class="number">1</span>]</div>
<div class="line">    fig = plt.figure()</div>
<div class="line">    ax = fig.add_subplot(<span class="number">111</span>)</div>
<div class="line">    ax.scatter(x, y)</div>
<div class="line">    x0 = self.lowdim_data[:, <span class="number">0</span>] <span class="comment">#(1000,2)</span></div>
<div class="line">    y0 = self.lowdim_data[:, <span class="number">1</span>]</div>
<div class="line">    ax.scatter(x0, y0, marker=<span class="string">'o'</span>, c=<span class="string">'r'</span>)</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810241415598.png" alt="mage-20181024141559"></p>
<p><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html" target="_blank" rel="noopener">PCA Exploration</a> </p>
<p>我们利用<code>sklearn</code>中的方法来探索pca，首先生成样本数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(self)</span>:</span></div>
<div class="line">        rng = np.random.RandomState(<span class="number">1</span>)</div>
<div class="line">        <span class="comment"># rng.rand(2,2) - sample 2*2 from uniform distribution</span></div>
<div class="line">        <span class="comment"># rng.randn(2,200) - sample 2*200 from normal distribution</span></div>
<div class="line">        x = np.dot(rng.rand(<span class="number">2</span>,<span class="number">2</span>),rng.randn(<span class="number">2</span>,<span class="number">200</span>)).T <span class="comment"># get 2*200 mixture sample</span></div>
<div class="line">        plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>])</div>
<div class="line">        plt.axis(<span class="string">'equal'</span>)</div>
<div class="line">        plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810202030142.png" alt="mage-20181020203014"></p>
<p>明显的，<code>x</code>和<code>y</code>存在类似线性的关系，但是不像线性回归去建模得到一个从<code>x</code>到<code>y</code>的预测函数，<code>PCA</code>尝试学习<code>x</code>与<code>y</code>的关系，这种关系是由一组特征向量和特征值表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sk_pca</span><span class="params">(self)</span>:</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">    pca = PCA(n_components=<span class="number">2</span>)</div>
<div class="line">    pca.fit(self.x)</div>
<div class="line">    print(pca.components_)</div>
<div class="line">    <span class="comment">### [[ 0.94446029  0.32862557]</span></div>
<div class="line">    <span class="comment">### [ 0.32862557 -0.94446029]]</span></div>
<div class="line">    print(pca.explained_variance_)</div>
<div class="line">    <span class="comment">### [ 0.75871884  0.01838551]</span></div>
</pre></td></tr></table></figure>
<p>其中<code>pca.components_</code>是特征向量，<code>pca.explained_variance_</code>是特征值。为了了解它们的意义，我们将之可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_component</span><span class="params">(self,v0,v1)</span>:</span></div>
<div class="line">    ax = plt.gca()</div>
<div class="line">    arrowprops = dict(arrowstyle=<span class="string">'-&gt;'</span>,linewidth=<span class="number">2</span>)</div>
<div class="line">    ax.annotate(<span class="string">'mean point'</span>,v1,v0,arrowprops=arrowprops) <span class="comment">#x1是箭头的终点</span></div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sk_pca</span><span class="params">(self)</span>:</span></div>
<div class="line">    rng = np.random.RandomState(<span class="number">1</span>)</div>
<div class="line">    self.x = np.dot(rng.rand(<span class="number">2</span>,<span class="number">2</span>),rng.randn(<span class="number">2</span>,<span class="number">200</span>)).T <span class="comment"># get 2*200 mixture sample</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">    pca = PCA(n_components=<span class="number">2</span>)</div>
<div class="line">    pca.fit(self.x)</div>
<div class="line">    plt.scatter(self.x[:,<span class="number">0</span>],self.x[:,<span class="number">1</span>])</div>
<div class="line">    <span class="keyword">for</span> length,vector <span class="keyword">in</span> zip(pca.explained_variance_,pca.components_):</div>
<div class="line">        v = vector*<span class="number">3</span>*np.sqrt(length)</div>
<div class="line">        self.visualize_component(pca.mean_,pca.mean_+v)</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810252123309.png" alt="mage-20181025212330"></p>
<p>这些向量表征数据的主轴，而向量的长度描述了数据在该方向分布的重要性。</p>
<blockquote>
<p>More precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the “principal components” of the data.</p>
</blockquote>
<h5 id="PCA-in-Dimension-Reducation"><a href="#PCA-in-Dimension-Reducation" class="headerlink" title="PCA in Dimension Reducation"></a>PCA in Dimension Reducation</h5><p>在维度削减中，PCA剔除一个或多个最小的主成分，使数据在低维上的映射尽可能保存数据的多样性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dim_reduction</span><span class="params">(self)</span>:</span></div>
<div class="line">    rng = np.random.RandomState(<span class="number">1</span>)</div>
<div class="line">    x = np.dot(rng.rand(<span class="number">2</span>, <span class="number">2</span>), rng.randn(<span class="number">2</span>, <span class="number">200</span>)).T  <span class="comment"># get 2*200 mixture sample</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">    pca = PCA(n_components=<span class="number">1</span>)</div>
<div class="line">    pca.fit(x)</div>
<div class="line">    x_pca = pca.transform(x)</div>
<div class="line">    print(<span class="string">"original shape:   "</span>, x.shape) <span class="comment">#original shape:    (200, 2)</span></div>
<div class="line">    print(<span class="string">"transformed shape:"</span>, x_pca.shape) <span class="comment">#transformed shape: (200, 1)</span></div>
</pre></td></tr></table></figure>
<p>数据被降维到1维空间，为了更加直观的理解降维效果，我们将降维后的数据还原到初始空间，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dim_reduction</span><span class="params">(self)</span>:</span></div>
<div class="line">    rng = np.random.RandomState(<span class="number">1</span>)</div>
<div class="line">    x = np.dot(rng.rand(<span class="number">2</span>, <span class="number">2</span>), rng.randn(<span class="number">2</span>, <span class="number">200</span>)).T  <span class="comment"># get 2*200 mixture sample</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">    pca = PCA(n_components=<span class="number">1</span>)</div>
<div class="line">    pca.fit(x)</div>
<div class="line">    x_pca = pca.transform(x)</div>
<div class="line">    x_new = pca.inverse_transform(x_pca)</div>
<div class="line">    plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],label=<span class="string">'original data'</span>)</div>
<div class="line">    plt.scatter(x_new[:,<span class="number">0</span>],x_new[:,<span class="number">1</span>],label=<span class="string">'inverse data'</span>)</div>
<div class="line">    plt.legend()</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810252140521.png" alt="mage-20181025214052"></p>
<p>从图中可以看出PCA降维的原理：分布在比较不重要的轴的信息被移除，留下分布在重要轴的信息。</p>
<p>PCA for Visualization</p>
<p>这一次我们使用高维的数据进行可视化，先载入数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div>
<div class="line">digits = load_digits() <span class="comment">#digits.data.shape=(1797, 64)</span></div>
</pre></td></tr></table></figure>
<p>手写体数字是<code>8*8</code>图片，故有64个维度，为了理解这些数据点之间的关系，我们将之降维到平面，即2维空间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">pca = PCA(n_components=<span class="number">2</span>)</div>
<div class="line">pca.fit(digits.data)</div>
<div class="line">projected = pca.transform(digits.data) <span class="comment">#projected.shape = (1797, 2)</span></div>
</pre></td></tr></table></figure>
<p>最终将它们可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_digits</span><span class="params">(self)</span>:</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div>
<div class="line">    digits = load_digits() <span class="comment">#digits.data.shape=(1797, 64)</span></div>
<div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">    pca = PCA(n_components=<span class="number">2</span>)</div>
<div class="line">    pca.fit(digits.data)</div>
<div class="line">    projected = pca.transform(digits.data) <span class="comment">#projected.shape = (1797, 2)</span></div>
<div class="line">    plt.scatter(projected[:,<span class="number">0</span>],projected[:,<span class="number">1</span>],</div>
<div class="line">                c=digits.target,cmap=<span class="string">'jet'</span>)</div>
<div class="line">    plt.xlabel(<span class="string">'component 1'</span>)</div>
<div class="line">    plt.ylabel(<span class="string">'component 2'</span>)</div>
<div class="line">    plt.colorbar()</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810261632428.png" alt="mage-20181026163242"></p>
<p>PCA as Noise Filtering</p>
<blockquote>
<p>PCA can also be used as a filtering approach for noisy data. The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise. So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.</p>
</blockquote>
<p>下面我们以手写体数字为例，探索PCA作为噪声过滤器的效果，首先我们画出无噪声的手写体数字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div>
<div class="line">digits = load_digits().data</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(data)</span>:</span></div>
<div class="line">    fig,axes = plt.subplots(<span class="number">4</span>,<span class="number">10</span>,figsize=(<span class="number">10</span>,<span class="number">4</span>),sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>)</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</div>
<div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div>
<div class="line">            axes[i][j].imshow(data[i*<span class="number">10</span>+j].reshape(<span class="number">8</span>,<span class="number">8</span>),cmap=<span class="string">'binary'</span>)</div>
<div class="line">    plt.show()</div>
<div class="line">plot_digits(digits)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272228567.png" alt="mage-20181027222856"></p>
<p>现在我们队手写体数字添加噪声：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">np.random.seed(<span class="number">42</span>)</div>
<div class="line">noisy = np.random.normal(digits,<span class="number">4</span>)</div>
<div class="line">plot_digits(noisy)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272231320.png" alt="mage-20181027223132"></p>
<p>显然，添加了噪声的数据很难辨别，现在我们就去训练一个PCA进行去噪：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div>
<div class="line">pca = PCA(<span class="number">0.5</span>)</div>
<div class="line">pca.fit(noisy)</div>
<div class="line">print(pca.n_components_) <span class="comment">#12</span></div>
</pre></td></tr></table></figure>
<p>我们想要模型解释至少50%的变化，根据模型显示，需要12个维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">components = pca.transform(noisy)</div>
<div class="line">filtered = pca.inverse_transform(components)</div>
<div class="line">plot_digits(filtered)</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272239305.png" alt="mage-20181027223930"></p>
<blockquote>
<p>This signal preserving/noise filtering property makes PCA a very useful feature selection routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</div>
<div class="line">faces = fetch_lfw_people(min_faces_per_person=<span class="number">60</span>)</div>
<div class="line">print(faces.data.shape) <span class="comment">#(1348, 2914)</span></div>
</pre></td></tr></table></figure>
<p>可以看到每一张图片的维度是2941=62*47，由于数据维度比较大，这次我们使用<code>RandomizedPCA</code>而非标准<code>PCA</code></p>
<blockquote>
<p>In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as “eigenvectors,” so these types of images are often called “eigenfaces”). As you can see in this figure, they are as creepy as they sound:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> RandomizedPCA</div>
<div class="line">pca = RandomizedPCA(<span class="number">150</span>)</div>
<div class="line">pca.fit(faces.data)</div>
<div class="line">fig,axes = plt.subplots(<span class="number">3</span>,<span class="number">8</span>,figsize=(<span class="number">9</span>,<span class="number">4</span>),sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>)</div>
<div class="line"><span class="keyword">for</span> i,ax <span class="keyword">in</span> enumerate(axes.flat):</div>
<div class="line">    ax.imshow(pca.components_[i].reshape(<span class="number">62</span>,<span class="number">47</span>),cmap=<span class="string">'bone'</span>)</div>
<div class="line">plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272307483.png" alt="mage-20181027230748"></p>
<blockquote>
<p>The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let’s take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">plt.plot(range(<span class="number">150</span>),np.cumsum(pca.explained_variance_ratio_))</div>
<div class="line">plt.xlabel(<span class="string">'number of components'</span>)</div>
<div class="line">plt.ylabel(<span class="string">'cumulative explained variance'</span>)</div>
<div class="line">plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272310030.png" alt="mage-20181027231003"></p>
<blockquote>
<p>We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line">components = pca.transform(faces.data)</div>
<div class="line">projected = pca.inverse_transform(components)</div>
<div class="line">fig, ax = plt.subplots(<span class="number">2</span>, <span class="number">10</span>, figsize=(<span class="number">10</span>, <span class="number">2.5</span>),</div>
<div class="line">                       subplot_kw=&#123;<span class="string">'xticks'</span>: [], <span class="string">'yticks'</span>: []&#125;,</div>
<div class="line">                       gridspec_kw=dict(hspace=<span class="number">0.1</span>, wspace=<span class="number">0.1</span>))</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div>
<div class="line">    ax[<span class="number">0</span>, i].imshow(faces.data[i].reshape(<span class="number">62</span>, <span class="number">47</span>), cmap=<span class="string">'binary_r'</span>)</div>
<div class="line">    ax[<span class="number">1</span>, i].imshow(projected[i].reshape(<span class="number">62</span>, <span class="number">47</span>), cmap=<span class="string">'binary_r'</span>)</div>
<div class="line"></div>
<div class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_ylabel(<span class="string">'full-dim\ninput'</span>)</div>
<div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].set_ylabel(<span class="string">'150-dim\nreconstruction'</span>)</div>
<div class="line">plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272311402.png" alt="mage-20181027231140"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Principal-Component-Analysis/" rel="tag"># Principal Component Analysis</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/12/Clustering/" rel="next" title="Clustering">
                <i class="fa fa-chevron-left"></i> Clustering
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/20/Cooking/" rel="prev" title="Cooking">
                Cooking <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">82</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">63</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#PCA-in-Dimension-Reducation"><span class="nav-number">1.</span> <span class="nav-text">PCA in Dimension Reducation</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
