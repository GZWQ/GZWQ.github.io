<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Math,Linear Algebra," />










<meta name="description" content="This is my study notes for MIT Linear Algebra.">
<meta name="keywords" content="Math,Linear Algebra">
<meta property="og:type" content="article">
<meta property="og:title" content="Math-Linear Algebra">
<meta property="og:url" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="This is my study notes for MIT Linear Algebra.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-16%20at%206.53.24%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-17%20at%206.28.03%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-18%20at%206.44.46%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-18%20at%206.48.06%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-19%20at%209.55.56%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-19%20at%209.56.12%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-20%20at%2011.28.04%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-06%20at%2010.06.53%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-07%20at%2011.36.52%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-08%20at%2012.57.32%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-07%20at%202.34.28%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-09%20at%2011.08.57%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-09%20at%2011.12.20%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-10%20at%204.05.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-12%20at%2011.52.58%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-12%20at%2011.55.36%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-12%20at%2012.06.56%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-12%20at%2012.09.35%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-13%20at%2010.53.07%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-14%20at%205.42.50%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202020-01-14%20at%205.41.25%20PM.png">
<meta property="og:updated_time" content="2020-01-15T21:08:33.433Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Math-Linear Algebra">
<meta name="twitter:description" content="This is my study notes for MIT Linear Algebra.">
<meta name="twitter:image" content="http://yoursite.com/2019/12/11/Math-Linear-Algebra/Screen%20Shot%202019-12-16%20at%206.53.24%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/12/11/Math-Linear-Algebra/"/>





  <title>Math-Linear Algebra | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/11/Math-Linear-Algebra/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Math-Linear Algebra</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-11T11:23:44-06:00">
                2019-12-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This is my study notes for <a href="https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/" target="_blank" rel="noopener">MIT Linear Algebra</a>.</p>
<a id="more"></a>
<h1 id="The-geometry-of-linear-equations"><a href="#The-geometry-of-linear-equations" class="headerlink" title="The geometry of linear equations"></a>The geometry of linear equations</h1><p>The fundamental problem of linear algebra is to solve $n$ linear equations in $n$ unknowns, for example</p>
<script type="math/tex; mode=display">
\begin{aligned} 2 x-y &=0 \\-x+2 y &=3 \end{aligned}</script><p>Here, we are going to view this problem in three ways.</p>
<h2 id="Row-Picture"><a href="#Row-Picture" class="headerlink" title="Row Picture"></a>Row Picture</h2><p>Plot the points that satisfy each equation. The intersection of the plots (if they do intersect) represents the solution to the system of equations. Looking at the following figure we see that the solution to this system of equations is $x=1,y=2$</p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-16 at 6.53.24 PM.png" alt="creen Shot 2019-12-16 at 6.53.24 P"></p>
<h2 id="Column-Picture"><a href="#Column-Picture" class="headerlink" title="Column Picture"></a>Column Picture</h2><p>In the column picture we rewrite the system of linear equations as a single<br>equation by turning the coefficients in the columns of the system into vectors:</p>
<script type="math/tex; mode=display">
x\left[\begin{array}{r}{2} \\ {-1}\end{array}\right]+y\left[\begin{array}{r}{-1} \\ {2}\end{array}\right]=\left[\begin{array}{l}{0} \\ {3}\end{array}\right]</script><p>Given two vectors $c$ and $d$ and scalars $x$ and $y$, then sum $xc+yd$ is called a linear combination of $c$ and $d$. </p>
<h2 id="Matrix-Picture"><a href="#Matrix-Picture" class="headerlink" title="Matrix Picture"></a>Matrix Picture</h2><p>We can write the system of equations as a single equation by using matrices and vectors:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{rr}{2} & {-1} \\ {-1} & {2}\end{array}\right]\left[\begin{array}{l}{x} \\ {y}\end{array}\right]=\left[\begin{array}{l}{0} \\ {3}\end{array}\right]</script><p>The matrix $A=\left[\begin{array}{rr}{2} &amp; {-1} \\ {-1} &amp; {2}\end{array}\right]$ is called the coefficient matrix. The vector $x=\left[\begin{array}{l}{x} \\ {y}\end{array}\right]$ is the vector of unknowns. The values on the right hand side of the equations form the vector $b$</p>
<script type="math/tex; mode=display">
Ax=b</script><p><strong>Matrix Multiplication</strong></p>
<p>One method to multiply a matrix $A$ by a vector $x$ is to think of $x$ as the coefficients of a linear combination of the column vectors of the matrix</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}{2} & {5} \\ {1} & {3}\end{array}\right]\left[\begin{array}{l}{1} \\ {2}\end{array}\right]=1\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+2\left[\begin{array}{l}{5} \\ {3}\end{array}\right]=\left[\begin{array}{c}{12} \\ {7}\end{array}\right]</script><p>or if</p>
<script type="math/tex; mode=display">
\begin{bmatrix}1&2\end{bmatrix}\left[\begin{array}{ll}{2} & {5} \\ {1} & {3}\end{array}\right]=1\begin{bmatrix}2&5\end{bmatrix}+2\begin{bmatrix}1&3\end{bmatrix}</script><h2 id="Linear-Independence"><a href="#Linear-Independence" class="headerlink" title="Linear Independence"></a>Linear Independence</h2><p>Given a matrix $A$, can we solve</p>
<script type="math/tex; mode=display">
Ax=b</script><p>for every possible vector $b$? In other words, do the linear combinations of the column vectors fill the $xy$-plane (or space, in the three dimensional case)?</p>
<p>If the answer is “no”, we say that A is a singular matrix. In this singular<br>case its column vectors are linearly dependent; all linear combinations of those vectors lie on a point or line (in two dimensions) or on a point, line or plane (in three dimensions). The combinations don’t fill the whole space. </p>
<h1 id="Elimination-with-Matrices"><a href="#Elimination-with-Matrices" class="headerlink" title="Elimination with Matrices"></a>Elimination with Matrices</h1><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-17 at 6.28.03 PM.png" alt="creen Shot 2019-12-17 at 6.28.03 P"></p>
<p>Let’s look at the result matrix, i.e., the third one. For the first row,  $\begin{bmatrix}1&amp;2&amp;1\end{bmatrix}$, it is computed by only using the first row of the second matrix, which means the elimination matrix, i.e., the first matrix, has $\begin{bmatrix}1&amp;0&amp;0\end{bmatrix}$ as its first row. It is the same situation with the third row. So we have elimination matrix $\begin{bmatrix}1 &amp; 0 &amp; 0\\ &amp;  &amp; \\ 0&amp;0&amp;1\end{bmatrix}$. As for the second row, we can see that in the second matrix, its first row contributes -3 and its second row contributes 1. But there is no contribution from third row. Therefore, the elimination matrix is $\begin{bmatrix}1 &amp; 0 &amp; 0\\ -3&amp;  1&amp; 0\\ 0&amp;0&amp;1\end{bmatrix}$. </p>
<h2 id="Inverses"><a href="#Inverses" class="headerlink" title="Inverses"></a>Inverses</h2><p>If $A$ is a square matrix, the most important question you can ask about it is whether it has an inverse $A^{-1}$. If it does, then $ A^{-1}A=I=AA^{-1}$ (This is just a special form of the equation Ax = b). And we say that $A$ is invertible or nonsingular.</p>
<p>If A is singular – i.e. A does not have an inverse – its determinant is zero and we can find some non-zero vector x for which Ax = 0. For example:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}{1} & {3} \\ {2} & {6}\end{array}\right]\left[\begin{array}{r}{3} \\ {-1}\end{array}\right]=\left[\begin{array}{l}{0} \\ {0}\end{array}\right]</script><p>We can use the method of elimination to solve two or more linear equations at<br>the same time. Just augment the matrix with the whole identity matrix I:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll|ll}{1} & {3} & {1} & {0} \\ {2} & {7} & {0} & {1}\end{array}\right] \rightarrow\left[\begin{array}{rr|rr}{1} & {3} & {1} & {0} \\ {0} & {1} & {-2} & {1}\end{array}\right] \rightarrow\left[\begin{array}{rr|rr}{1} & {0} & {7} & {-3} \\ {0} & {1} & {-2} & {1}\end{array}\right]</script><script type="math/tex; mode=display">
A^{-1}=\left[\begin{array}{rr}{7} & {-3} \\ {-2} & {1}\end{array}\right]</script><blockquote>
<p>Use GaussJordan elimination on $\begin{bmatrix}U&amp;I\end{bmatrix}$to find the upper triangular $U^{-1}$.</p>
<script type="math/tex; mode=display">
U U^{-1}=I\left[\begin{array}{ccc}{1} & {a} & {b} \\ {0} & {1} & {c} \\ {0} & {0} & {1}\end{array}\right]\left[\begin{array}{lll}{x_{1}} & {x_{2}} & {x_{3}} \\ {} & {} & {}\end{array}\right]=\left[\begin{array}{lll}{1} & {0} & {0} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right]</script><p>Solution: Row reduce $\begin{bmatrix}U&amp;I\end{bmatrix}$ to get $\begin{bmatrix}I&amp;U^{-1}\end{bmatrix}$ as follows:</p>
<script type="math/tex; mode=display">
</script><script type="math/tex; mode=display">
\left.\left[\begin{array}{cccccc}{1} & {a} & {b} & {1} & {0} & {0} \\ {0} & {1} & {c} & {0} & {1} & {0} \\ {0} & {0} & {1} & {0} & {0} & {1}\end{array}\right] \longrightarrow \begin{array}{l}{\left(R_{1}=R_{1}-a R_{2}\right)} \\ {R_{2}=R_{2}-c R_{2}}\end{array}\right)\left[\begin{array}{cccccc}{1} & {0} & {b-a c} & {1} & {-a} & {0} \\ {0} & {1} & {} & {0} & {0} & {1} & {-c} \\ {0} & {0} & {1} & {0} & {0} & {1}\end{array}\right]\\
\left(R_{1}=R_{1}-(b-a c) R_{3}\right)\left[\begin{array}{ccccc}{1} & {0} & {0} & {1} & {-a} & {a c-b} \\ {0} & {1} & {0} & {0} & {1} & {-c} \\ {0} & {0} & {1} & {0} & {0} & {1}\end{array}\right]=\left[\begin{array}{cc}{I} & {L^{-1}}\end{array}\right]</script></blockquote>
<h2 id="Permutations"><a href="#Permutations" class="headerlink" title="Permutations"></a>Permutations</h2><p>Multiplication by a permutation matrix $P$ swaps the rows of a matrix; when applying the method of elimination we use permutation matrices to move zeros out of pivot positions. Our factorization $A=LU$ then becomes $PA=LU$, where $P$ is a permutation matrix which recorders any number of rows of $A$. Recall that $P^{-1}=P^{T}$, i.e., that $P^{T}P=I$.</p>
<h1 id="Vector-Space"><a href="#Vector-Space" class="headerlink" title="Vector Space"></a>Vector Space</h1><p>We can add vectors and multiply them by numbers, which means we can discuss linear combinations of vectors. These combinations follow the rules of a vector space.</p>
<p><strong>Closure</strong></p>
<p>The collection of vectors with exactly two positive real valued components is not a vector space. The sum of any two vectors in that collection is again in the collection, but multiplying any vector by, say, −5, gives a vector that’s not in the collection. We say that this collection of positive vectors is closed under addition but not under multiplication.<br>If a collection of vectors is closed under linear combinations (i.e. under addition and multiplication by any real numbers), and if multiplication and addition behave in a reasonable way, then we call that collection a vector space.</p>
<p>A vector space must contain origin, i.e., it has zero vector. Because it should allow multiply by 0, or addition of negative and positive identity vectors.</p>
<p><strong>A vector space is a collection of vectors which is closed under linear combinations. In other words, for any two vectors v and w in the space and any two real numbers c and d, the vector cv + dw is also in the vector space.</strong></p>
<p>A plane $P$ containing $\begin{bmatrix}0\\0\\0\end{bmatrix}$ and a line $L$ containing $\begin{bmatrix}0\\0\\0\end{bmatrix}$ are both subspace of $R^3$. The union $P \cup L$ of these two subspaces is generally not a subspace, because the sum of a vector in $P$ and a vector in $L$ is probably not contained in $P \cup L$. The intersection $S \cap T$ of two subspaces $S$ and $T$ is a subspace. To prove this, use the fact that both $S$ and $T$ are closed under linear combinations to show that their intersection is closed under linear combinations.  </p>
<h2 id="Subspace"><a href="#Subspace" class="headerlink" title="Subspace"></a>Subspace</h2><p>A vector space that is contained inside of another vector space is called a subspace of that space. For example, take any non-zero vector $v$ in $R^2$. Then the set of all vectors $cv$, where $c$ is a real number, forms a subspace of $R^2    $. This collection of vectors describes a line through $\begin{bmatrix}0\\0\end{bmatrix}$ in $R^2$ and is closed under addition.</p>
<p>A line in $R^2$ that does not pass through the origin is not a subspace of $R^2$. Multiplying any vector on that line by 0 gives the zero vector, whcih does not line on the line. Every subspace must contain the zero vector because vector spaces are closed under multiplication.</p>
<p>The subspace of $R^2$ are:</p>
<ol>
<li>all of $R^2$</li>
<li>any line through $\begin{bmatrix}0\\0\end{bmatrix}$ and</li>
<li>the zero vector alone</li>
</ol>
<p>The subspace of $R^3$ are:</p>
<ol>
<li>all of $R^3$</li>
<li>any plane through the origin</li>
<li>any line through the origin, and</li>
<li>the zero vector alone</li>
</ol>
<blockquote>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-18 at 6.44.46 PM.png" alt="creen Shot 2019-12-18 at 6.44.46 P"></p>
<p>First of all, here $M$ is a set containing all symmetric matrices. According to the definition of subspace, as long as the vectors in that set is closed, then it is a space. So, we use two symmetric matrix $A$ and $B$ and prove whether $(A+B)$ and $cA$ are symmetric.</p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-18 at 6.48.06 PM.png" alt="creen Shot 2019-12-18 at 6.48.06 P"></p>
</blockquote>
<h2 id="Column-space"><a href="#Column-space" class="headerlink" title="Column space"></a>Column space</h2><p>Given a matrix $A$ with columns in $R^3$, these columns and all their linear combinations form a subspace of $R^3$. This is the column space $C(A)$. If $A=\begin{bmatrix}1&amp;2\\2&amp;3\\4&amp;1\end{bmatrix}$, the column space of $A$ is the plane through the origin in $R^3$ containing $A=\begin{bmatrix}1\\2\\4\end{bmatrix}$ and $A=\begin{bmatrix}2\\3\\1\end{bmatrix}$</p>
<p>In a nutshell, the column space of a matrix $A$ is the vector space made up of all linear combinations of the columns of $A$. </p>
<p><strong>Solving $Ax=b$</strong></p>
<p>Given a matrix $A$, for what vectors $b$ does $Ax=b$ have a solution?</p>
<script type="math/tex; mode=display">
\text { Let } A=\left[\begin{array}{lll}{1} & {1} & {2} \\ {2} & {1} & {3} \\ {3} & {1} & {4} \\ {4} & {1} & {5}\end{array}\right]</script><p>Then $Ax = b$ does not have a solution for every choice of b because solving $Ax = b$ is equivalent to solving four linear equations in three unknowns. If there is a solution to $Ax = b$, then $b$ must be a linear combination of the columns of A. <strong>Only three columns cannot fill the entire four dimensional vector space</strong> – some vectors b cannot be expressed as linear combinations of columns of A. </p>
<p>Big question: what $b$’s allow $Ax=b$ to be solved?</p>
<p>The system of linear equations $Ax=b$ is solvable exactly when b is a vector in the column space of $A$.</p>
<p>For our example matrix $A$, what can we say about the column space of $A$? Are the columns of $A$ independent? In other words, does each column contribute something new to the subspace? The third column of $A$ is the sum of the first two columns, so does not add anything to the subspace. The column space of our matrix $A$ is a two dimensional subspace of $R^4$.</p>
<h2 id="Nullspace-of-Matrix"><a href="#Nullspace-of-Matrix" class="headerlink" title="Nullspace of Matrix"></a>Nullspace of Matrix</h2><p>The nullspace of a matrix $A$ is the collection of all solutions $x=\begin{bmatrix}x_1 \\x_2 \\x_3 \end{bmatrix}$ to the equation $Ax=0$. The column space of the matrix in our example was a subpace of $R^4$. The nullspace of $A$ is a subspace of $R^3$. To see that it’s a vector space, check that any sum or multiple of solutions to $Ax=0$ is a also solution: $A(x_1+x_2)=Ax_1+Ax_2=0+0$ and $A(cx)=cAx=c(0)$.</p>
<p>In the example,</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}{1} & {1} & {2} \\ {2} & {1} & {3} \\ {3} & {1} & {4} \\ {4} & {1} & {5}\end{array}\right]\left[\begin{array}{l}{x_{1}} \\ {x_{2}} \\ {x_{3}}\end{array}\right]=\left[\begin{array}{l}{0} \\ {0} \\ {0} \\ {0}\end{array}\right]</script><p>the nullspace $N(A)$ consists of all multiples of $\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}$. column 1 plus column 2 minus column 3 equals the zero vector. This nullspace is a line in $R^3$.</p>
<p><strong>Other values of b</strong></p>
<p>The solutions to the equations:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}{1} & {1} & {2} \\ {2} & {1} & {3} \\ {3} & {1} & {4} \\ {4} & {1} & {5}\end{array}\right]\left[\begin{array}{l}{x_{1}} \\ {x_{2}} \\ {x_{3}}\end{array}\right]=\left[\begin{array}{l}{1} \\ {2} \\ {3} \\ {4}\end{array}\right]</script><p>do not form a subspace. The zero vector is not a solution to this equation. The set of solutions forms a line in $R^3$ that passes through the points $\begin{bmatrix}1 \\0 \\0 \end{bmatrix}$ and $\begin{bmatrix}0 \\ -1 \\1 \end{bmatrix}$ but not $\begin{bmatrix}0 \\0 \\0 \end{bmatrix}$</p>
<blockquote>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-19 at 9.55.56 AM.png" alt="creen Shot 2019-12-19 at 9.55.56 A"></p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-19 at 9.56.12 AM.png" alt="creen Shot 2019-12-19 at 9.56.12 A"></p>
</blockquote>
<h2 id="computing-the-nullspace"><a href="#computing-the-nullspace" class="headerlink" title="computing the nullspace"></a>computing the nullspace</h2><p>Suppose </p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {2} & {4} & {6} & {8} \\ {3} & {6} & {8} & {10}\end{array}\right]</script><p>Our algorithm for computing the nullspace of this matrix uses the method of elimination, despite the fact that A is not invertible. We don’t need to use an augmented matrix because the right side (the vector b) is 0 in this computation. </p>
<p>The first step of elimination gives us: </p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {2} & {4} & {6} & {8} \\ {3} & {6} & {8} & {10}\end{array}\right] \quad \longrightarrow \quad\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {2} & {4}\end{array}\right]</script><p>We don’t find a pivot in the second column, so our next pivot is the 2 in the third column of the second row: </p>
<script type="math/tex; mode=display">
\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {2} & {4}\end{array}\right] \quad \longrightarrow \quad\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {0} & {0}\end{array}\right]=U</script><p>The matrix U is in echelon (staircase) form. The third row is zero because row 3 was a linear combination of rows 1 and 2; it was eliminated. <strong>The rank of a matrix A equals the number of pivots it has</strong>. In this example, the rank of A (and of U) is 2. </p>
<p><strong>special solutions</strong></p>
<p>Once we’ve found U we can use back-substitution to find the solutions x to the equation Ux = 0. In our example, columns 1 and 3 are pivot columns containing pivots, and columns 2 and 4 are free columns. We can assign any value to x2 and x4; we call these free variables. Suppose x2 = 1 and x4 = 0. Then: </p>
<script type="math/tex; mode=display">
2x_3+4x_4=0 => x_3=0</script><p>and </p>
<script type="math/tex; mode=display">
x_1+2x_2+2x_3+2x_4=0  =>x_1=-2</script><p>so one solution is $x=\begin{bmatrix}-2\\1\\0\\0\end{bmatrix}$. Any multiple of this vector is in the nullspace. </p>
<p>Letting a different free variable equal 1 and setting the other free variables equal to 0 gives us other vectors in the nullspace. For example:</p>
<script type="math/tex; mode=display">
x=\begin{bmatrix}2\\0\\-2\\1\end{bmatrix}</script><p><strong>The rank $r$ of A equals the number of pivot columns, so the number of free columns is $n − r$: the number of columns (variables) minus the number of pivot columns. This equals the number of special solution vectors and the dimension of the nullspace. </strong></p>
<h2 id="Reduced-row-echelon-form"><a href="#Reduced-row-echelon-form" class="headerlink" title="Reduced row echelon form"></a>Reduced row echelon form</h2><p>By continuing to use the method of elimination we can convert U to a matrix R in reduced row echelon form (rref form), with pivots equal to 1 and zeros above and below the pivots. </p>
<script type="math/tex; mode=display">
U=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {0} & {0}\end{array}\right] \rightarrow\left[\begin{array}{rrrr}{1} & {2} & {0} & {-2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {0} & {0}\end{array}\right] \rightarrow\left[\begin{array}{rrrr}{1} & {2} & {0} & {-2} \\ {0} & {0} & {1} & {2} \\ {0} & {0} & {0} & {0}\end{array}\right]=R</script><p>By exchanging some columns, R can be rewritten with a copy of the identity matrix in the upper left corner, possibly followed by some free columns on the right. If some rows of A are linearly dependent, the lower rows of the matrix R will be filled with zeros: </p>
<script type="math/tex; mode=display">
R=\left[\begin{array}{cc}{I} & {F} \\ {0} & {0}\end{array}\right]</script><p>(Here I is an r by r square matrix.) </p>
<p>If N is a nullspace matrix $N=\begin{bmatrix}-F\\I\end{bmatrix}$ then $RN=0$.  (Here I is an n − r by n − r square matrix and 0 is an m by n − r matrix.) The columns of N are the special solutions.</p>
<p>In this example, we have $-F=\begin{bmatrix}0&amp;-2\-1&amp;-2\\1&amp;0\\0&amp;1\end{bmatrix}$. So the special solutions to $Ax=0$ are:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{r}{0} \\ {-1 } \\ {1} \\ {0}\end{array}\right] \text { and }\left[\begin{array}{r}{-2 } \\ {-2} \\ {0} \\ {1}\end{array}\right]</script><h2 id="Solving-Ax-b"><a href="#Solving-Ax-b" class="headerlink" title="Solving Ax=b"></a>Solving Ax=b</h2><h3 id="Solvability-conditions-on-b"><a href="#Solvability-conditions-on-b" class="headerlink" title="Solvability conditions on b"></a>Solvability conditions on b</h3><p>when does $Ax=b$ have solutions and how can we describe those solutions?</p>
<p>We again use the example:</p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {2} & {4} & {6} & {8} \\ {3} & {6} & {8} & {10}\end{array}\right]</script><p>The third row of A is the sum of its first and second rows, so we know that if Ax = b the third component of b equals the sum of its first and second components. If b does not satisfy b3 = b1 + b2 the system has no solution. If a combination of the rows of A gives the zero row, then the same combination of the entries of b must equal zero. </p>
<p>One way to find out whether Ax = b is solvable is to use elimination on the augmented matrix. If a row of A is completely eliminated, so is the corresponding entry in b. In our example, row 3 of A is completely eliminated: </p>
<script type="math/tex; mode=display">
\left[\begin{array}{ccccc}{1} & {2} & {2} & {2} & {b_{1}} \\ {2} & {4} & {6} & {8} & {b_{2}} \\ {3} & {6} & {8} & {10} & {b_{3}}\end{array}\right] \rightarrow \cdots \rightarrow\left[\begin{array}{ccccc}{1} & {2} & {2} & {2} & {b_{1}} \\ {0} & {0} & {2} & {4} & {b_{2}-2 b_{1}} \\ {0} & {0} & {0} & {0} & {b_{3}-b_{2}-b_{1}}\end{array}\right]</script><p>If $Ax=b$ has a solution, then $b_3-b_2-b_1=0$. For example, we could choose $b=\begin{bmatrix}1\\5\\6\end{bmatrix}$. From an earlier lecture, we know that Ax = b is solvable exactly when b is in the column space C(A). We have these two conditions on b; in fact they are equivalent. </p>
<h3 id="Complete-solution"><a href="#Complete-solution" class="headerlink" title="Complete solution"></a>Complete solution</h3><p>In order to find all solutions to Ax = b we first check that the equation is solvable, then find a particular solution. We get the complete solution of the equation by adding the particular solution to all the vectors in the nullspace. </p>
<p><strong>A particular solution</strong></p>
<p>One way to find a particular solution to the equation Ax = b is to set all free variables to zero, then solve for the pivot variables.</p>
<p>For our example matrix $A$, we let $x_2=x_4=0$ to get the system of equations:</p>
<script type="math/tex; mode=display">
\begin{aligned} x_{1}+2 x_{3} &=1 \\ 2 x_{3} &=3 \end{aligned}</script><p>which has the solution $x_3=\frac{3}{2}, x_1=-2$. Our particular solution is:</p>
<script type="math/tex; mode=display">
\mathbf{x}_{p}=\left[\begin{array}{r}{-2} \\ {0} \\ {3 / 2} \\ {0}\end{array}\right]</script><p><strong>Combined with the nullspace</strong></p>
<p>The general solution to $Ax=b$ is given by $X_{complete}=x_p+x_n$, where $x_n$ is a generic vector in the nullspace. To see this, we add $Ax_p=b$ to $Ax_n=0$ and get $A(x_p+x_n)=b$ for every vector in the nullspace.</p>
<p>We know that the nullspace of $A$ is the collection of all combinations of the special solutions  $\begin{bmatrix}-2\\1\\0\\0\end{bmatrix}$ and $\begin{bmatrix}2\\0\-2\\1\end{bmatrix}$. So the complete solution to the equation $Ax=\begin{bmatrix}1\\5\\6\end{bmatrix}$ is:</p>
<script type="math/tex; mode=display">
\mathbf{x}_{\text {complete }}=\left[\begin{array}{r}{-2} \\ {0} \\ {3 / 2} \\ {0}\end{array}\right]+c_1\left[\begin{array}{r}{-2} \\ {1} \\ {0} \\ {0}\end{array}\right]+c_{2}\left[\begin{array}{r}{2} \\ {0} \\ {-2} \\ {1}\end{array}\right]</script><p>where $c_1$ and $c_2$ are real numbers.</p>
<p>The nullspace of $A$ is a two dimensional subspace of $R^4$, and the solutions to the equation $Ax=b$ form a plane parallel to that through $x_p=\left[\begin{array}{r}{-2} \\ {0} \\ {3 / 2} \\ {0}\end{array}\right]$.</p>
<h2 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h2><p>The rank of a matrix equals the number of pivots of that matrix. If $A$ is an $m$ by $n$ matrix of rank $r$, we know that $r\le m$ and $r\le n$.</p>
<h3 id="Full-column-rank"><a href="#Full-column-rank" class="headerlink" title="Full column rank"></a>Full column rank</h3><p>If $r=n$, then the nullspace has dimension $n-r=0$ and contains only zero vector. There are no free variables or special solutions.</p>
<p>Since there are going to have zero rows after row reduction, so the combinations of $b$ should be zero too. In this case, $Ax=b$ has a solution and it is unique. Otherwise, there is no solution. </p>
<p>The row reduced echelon form of the matrix will look like $R=\begin{bmatrix}I\\0\end{bmatrix}$. </p>
<h3 id="Full-row-rank"><a href="#Full-row-rank" class="headerlink" title="Full row rank"></a>Full row rank</h3><p>If $r=m$, then the reduced matrix $R=\begin{bmatrix}I&amp;F\end{bmatrix}$ has no rows of zeros and so there are no requirements for the entries of $b$ to satisfy. The equation $Ax=b$ is solvable for every $b$. There are $n-m$ free variables, so there are $n-m$ special solutions to $Ax=0$</p>
<h3 id="Full-row-and-column-rank"><a href="#Full-row-and-column-rank" class="headerlink" title="Full row and column rank"></a>Full row and column rank</h3><p>If $r=m=n$ is the number of pivots of $A$, then $A$ is an invertible square matrix and $R$ is the identity matrix. The nullspace has dimension zero and $Ax=b$ has a unique solution for every $b$.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-20 at 11.28.04 AM.png" alt="creen Shot 2019-12-20 at 11.28.04 A"></p>
<h1 id="Linear-independence"><a href="#Linear-independence" class="headerlink" title="Linear independence"></a>Linear independence</h1><p>Suppose $A$ is $m$ by $n$ with $m &lt; n$. Then there are non-zero solutions to $Ax=0$. This is a equation with more unknowns than equations. So there will be free variables. A combination of the columns is zero, so the columns of this $A$ are dependent.</p>
<p>We say vectors $x_1,x_2,…x_n$ are linearly indepent if $c_1x_1+c_2x_2+…+c_nx_n=0$ only when $c_1,c_2,…,c_n$ are all 0. When those vectors are the columns of $A$, the only solution to $Ax=0$ is $x=0$. </p>
<p>Two vectors are independent if they do not lie on the same line. Three vectors are independent if they do npt lie in the same plane. Thinking of $Ax$ as a linear combination of the column vctors of $A$, we see that the column vectors of $A$ are independent exactly when the nullspace of $A$ contains only the zero vector.</p>
<blockquote>
<p>Say we have three vectors in the $x-y$ plane. So each vector is a $\begin{bmatrix}a\\b \end{bmatrix}$. But we have three unknowns, meaning there must be free variables according to the first paragraph.</p>
</blockquote>
<p>If the columns of $A$ are independent then all columns are pivot columns, the rank of $A$ is $n$, and there are no free variables. If the columns of $A$ are dependent then the rank of $A$ is less than $n$ and there are free variables.</p>
<h2 id="Spanning-a-space"><a href="#Spanning-a-space" class="headerlink" title="Spanning a space"></a>Spanning a space</h2><p>Vectors $v_1,v_2,…,v_k$ span a space when the space consists of all combinations of those vectors. For example, the column vectors of $A$ span the column space of $A$. If vectors $v_1,v_2,…,v_k$ span a space $S$, then $S$ is the smalles space containing those vectors.</p>
<h2 id="Basis-and-dimension"><a href="#Basis-and-dimension" class="headerlink" title="Basis and dimension"></a>Basis and dimension</h2><p>A basis for a vector space is a sequence of vectors $v_1,v_2,…,v_d$ with two properties:</p>
<ul>
<li>$v_1,v_2,…,v_d$ are independent</li>
<li>$v_1,v_2,…,v_d$ span the vector space</li>
</ul>
<p><strong>Example $R^3$:</strong></p>
<p>One basis for $R^3$ is $\left\{\left[\begin{array}{l}{1} \\ {0} \\ {0}\end{array}\right],\left[\begin{array}{l}{0} \\ {1} \\ {0}\end{array}\right],\left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right]\right\}$.However, the vectors $\begin{bmatrix}1\\1\\2\end{bmatrix}$, $\begin{bmatrix}2\\2\\5\end{bmatrix}$ and $\begin{bmatrix}3\\3\\8\end{bmatrix}$ do not form a basis for $R^3$ because the third vector is in the space of the first two vectors. In general, for a square matrix, i.e., $n$ vectors in $R^n$, form a basis if they are the column vectors of an invertible matrix.</p>
<p>The vectors $\begin{bmatrix}1\\1\\2\end{bmatrix}$ and $\begin{bmatrix}2\\2\\5\end{bmatrix}$ span a plane in $R^3$ but they cannot form a basis for $R^3$. However, they form a basis of column space for matrix $\begin{bmatrix}1 ,2,3\\1,2,3\\2,5,8\end{bmatrix}$.</p>
<p>Given a space, every basis for that space has the same number of vectors; that number is the dimension of the space, So there are exactly $n$ vectors in every basis for $R^n$.</p>
<h2 id="Bases-of-a-column-space-and-nullspace"><a href="#Bases-of-a-column-space-and-nullspace" class="headerlink" title="Bases of a column space and nullspace"></a>Bases of a column space and nullspace</h2><p>Suppose:</p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{llll}{1} & {2} & {3} & {1} \\ {1} & {1} & {2} & {1} \\ {1} & {2} & {3} & {1}\end{array}\right]</script><p>By definition, the four column vectors of $A$ span the column space of $A$. The third and fourth column vectors are dependent on the first and second, and the first two columns are independent. Therefore, the first two column vectors are the pivot columns. They form a basis for the column space $C(A)$. The matrix has rank 2. </p>
<script type="math/tex; mode=display">
\text{rank}(A)=\text{number of pivot columns of }A=\text{dimension of } C(A)</script><p>The column vectors of this $A$ are not independent, so the nullspace $N(A)$ contains more than just zero vectors. Because the third column is the sum of the first two, we know that the vector $\begin{bmatrix}-1\\ -1\\1\\0\end{bmatrix}$ is in the nullspace. Similarly, $\begin{bmatrix}-1\\0\\0\\ -1\end{bmatrix}$ is also in $N(A)$. These are the two special solutions to $Ax=0$.</p>
<script type="math/tex; mode=display">
\text{dimension of }N(A)=\text{number of free variables}=n-r</script><p>so we know that the dimension of $N(A)$ is 4-2=2. These two special solutions form a basis for the nullspace.</p>
<h2 id="The-Four-Fundamental-Subspaces"><a href="#The-Four-Fundamental-Subspaces" class="headerlink" title="The Four Fundamental Subspaces"></a>The Four Fundamental Subspaces</h2><p>Any  $m$ by $n$ matrix $A$ determines four subspaces</p>
<p><strong>column space</strong> $C(A)$: consists of all combinations of the columns of $A$ and is a vector space in $R^m$.</p>
<p>The $r$ pivot columns form a basis for $C(A)$: $\text{dim} C(A)=r$.</p>
<p><strong>nullspace</strong> $N(A)$ consists of all solutions $x$ of the equation $Ax=0$ and lies in $R^n$.</p>
<p>The special solutions to $Ax=0$ correspond to free variables and form a basis for $N(A)$. An $m$ by $n$ matrix has $n-r$ free variables: $\text{dim} N(A)=n-r$. </p>
<p><strong>row space</strong> $C(A^T)$, the combinations of the row vectors of $A$ form a subspace of $R^n$. We equate this with $C(A^T)$, the column space of the transpose of $A$.</p>
<p>We could perform row reduction on $A^T$, but insted we make use of $R$, the row reduced echelon form of $A$.</p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{llll}{1} & {2} & {3} & {1} \\ {1} & {1} & {2} & {1} \\ {1} & {2} & {3} & {1}\end{array}\right] \rightarrow \cdots \rightarrow\left[\begin{array}{llll}{1} & {0} & {1} & {1} \\ {0} & {1} & {1} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right]=\left[\begin{array}{ll}{I} & {F} \\ {0} & {0}\end{array}\right]=R</script><p>Although the column spaces of $A$ and $R$ are different, the row space of $R$ is the same as the row space of $A$. The rows of $R$ are combinations of the rows of $A$, and because reduction is reversible the rows of $A$ are combinations of the rows of $R$. The first $r$ rows of $R$ are the echelon basis for the row space of $A$: $\text{dim}C(A^T)=r$. In this exampe, there is a basis consisting of two vectors: $\begin{bmatrix}1\ 0\ 1\ 1\end{bmatrix}$ and $\begin{bmatrix}0\ 1\ 1\ 0\end{bmatrix}$.</p>
<p><strong>left nullspace</strong>, $N(A^T)$, is a subspace of $R^m$.</p>
<p>The matrix $A^T$ has $m$ columns. We just saw that $r$ is the rank of $A^T$, so the number of free columns of $A^T$ must be $m-r$: $\text{dim}N(A^T)=m-r$. The left nullspace is the collection of vectors $y$ for which $A^Ty=0$. Equivalently, $y^T A=0$; here $y$ and 0 are row vectors. We say left nullspace because $y^T$ is on the left of $A$ in this equation. To find a basis for the left nullspace we reduce an augmented version of $A$:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}{A_{m \times n}} & {I_{m \times n}}\end{array}\right] \longrightarrow\left[\begin{array}{ll}{R_{m \times n}} & {E_{m \times n}}\end{array}\right]</script><p>From this we get the matrix $E$ for which $EA=R$. (If $A$ is a square, invertible matrix then $E=A^{-1}$). In our example,</p>
<script type="math/tex; mode=display">
E A=\left[\begin{array}{rrr}{-1} & {2} & {0} \\ {1} & {-1} & {0} \\ {-1} & {0} & {1}\end{array}\right]\left[\begin{array}{llll}{1} & {2} & {3} & {1} \\ {1} & {1} & {2} & {1} \\ {1} & {2} & {3} & {1}\end{array}\right]=\left[\begin{array}{llll}{1} & {0} & {1} & {1} \\ {0} & {1} & {1} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right]=R</script><p>the bottom $m-r$ rows of $E$ describe linear dependencies of rows of $A$, because the bottome $m-r$ rows of $R$ are zero. Here $m-r=1$, the bottom $m-r$ rows of $E$ satisfy the equation $y^TA=0$ and form a basis for the left nullspace of $A$.</p>
<h2 id="Rank-one-matrices"><a href="#Rank-one-matrices" class="headerlink" title="Rank one matrices"></a>Rank one matrices</h2><p>The rank of a matrix is the dimension of its column (or row) space. The matrix </p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{ccc}{1} & {4} & {5} \\ {2} & {8} & {10}\end{array}\right]</script><p>has rank 1 because each of its columns is a multiple of the first column. </p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{l}{1} \\ {2}\end{array}\right]\left[\begin{array}{lll}{1} & {4} & {5}\end{array}\right]</script><p>Every rank 1 matrix $A$ can be written $A=UV^T$, where $U$ and $V$ are column vectors. We’ll use rank 1 matrices as building blocks for more complex matrices.</p>
<h1 id="Orthogonal-amp-Projection"><a href="#Orthogonal-amp-Projection" class="headerlink" title="Orthogonal&amp;Projection"></a>Orthogonal&amp;Projection</h1><p>If two vectors are orthogonal, they form a right triangle whose hypotenuse is the sum of the vectors. Thus, we can use the Pythagorean theorem to prove that the dot product $x^Ty=y^Tx$ is zero exactly when $x$ and $y$ are orthogonal. The length squared $||x||^2$ equals $x^Tx$. Note that all vectors are orthogonal to the zero vector. </p>
<p>Subspace S is orthogonal to subspace T means: every vector in S is orthogonal to every vector in T.</p>
<h2 id="Nullspace-is-perpendicular-to-row-space"><a href="#Nullspace-is-perpendicular-to-row-space" class="headerlink" title="Nullspace is perpendicular to row space"></a>Nullspace is perpendicular to row space</h2><p>The row space of a matrix is orthogonal to the nullspace, because Ax = 0 means the dot product of x with each row of A is 0. Then the product of x with any combination of rows of A must be 0. The column space is orthogonal to the left nullspace of A because the row<br>space of $A^T$ is perpendicular to the nullspace of $A^T$.  </p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-06 at 10.06.53 AM.png" alt="creen Shot 2020-01-06 at 10.06.53 A"></p>
<p>$A^TA$ is square and symmetric. In fact:</p>
<script type="math/tex; mode=display">
\begin{aligned} N\left(A^{T} A\right) &=N(A) \\ \text { rank of } A^{T} A &=\operatorname{rank} \text { of } A \end{aligned}</script><p>We conclude that $A^TA$ is invertible exactly when A has independent columns. </p>
<h2 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h2><p>If we have a vector $b$ and a line determined by a vector $a$, how do we find the point on the line that is closest to $b$?</p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-07 at 11.36.52 AM.png" alt="creen Shot 2020-01-07 at 11.36.52 A"></p>
<p>We can see from Figure 1 that this closest point $p$ is at the intersection formed by a line through $b$ that is orthogonal to $a$. If we think of $p$ as an approximation of $b$, then the length of $e=b-p$ is the error in that approximation. Since $p$ lies on the line through $a$, we know $p=xa$ for some number $x$. We also know that $a$ is perpendicular to $e=b-xa$:</p>
<script type="math/tex; mode=display">
a^T(b-ax)=0\\
a^Txa=a^Tb\\
x=\frac{a^Tb}{a^Ta}</script><p>Remember that $a^Ta$ is a number. $p=ax=a\frac{a^Tb}{a^Ta}$. Doubling $b$ doubles $p$. Doubling $a$ does not affect $p$.</p>
<h3 id="Projection-matrix"><a href="#Projection-matrix" class="headerlink" title="Projection matrix"></a>Projection matrix</h3><p>We’d like to write this projection in terms of a projection matrix $P:p=Pb$.</p>
<script type="math/tex; mode=display">
p=ax=a\frac{a^Tb}{a^Ta}</script><p>so the matrix is:</p>
<script type="math/tex; mode=display">
P=\frac{aa^T}{a^Ta}</script><p>Note that $aa^T$ is a matrix, not a number. Since for any $b$, $Pb$ lines on the line $a$, thus, the column space of $P$ is spanned by $a$. The rank of $P$ is 1. $P$ is symmetric. $P^2b=Pb$ because the projection of a vector already on the line through $a$ is just that vector. In general, projection matrices have the properties:</p>
<script type="math/tex; mode=display">
P^T=P \ \text{and} \ P^2=P</script><h3 id="why-projection"><a href="#why-projection" class="headerlink" title="why projection"></a>why projection</h3><p>As we know, the equation $Ax=b$ may have no solution. The vector $Ax$ is always in the column space of $A$, and $b$ is unlikely to be in the column space. So we project $b$ onto a vector $p$ in the column space of $A$ and solve $A \hat x=p$. </p>
<h3 id="Projection-in-higher-dimensions"><a href="#Projection-in-higher-dimensions" class="headerlink" title="Projection in higher dimensions"></a>Projection in higher dimensions</h3><p> In $R^3$,how do we project a vector $b$ onto the closest point $p$ in a plane? If $a_1$ and $a_2$ form a basis for the plane, then that plane is the column space of the matrix $A=\begin{bmatrix}a_1 \ a_2 \end{bmatrix}$. We know that $p=\hat x_1 a_1+\hat x_2a_2=A \hat x$. We want to find $\hat x$. There are many ways to show that $e=b-p=b-A \hat x$ is orthogonal to the planewe are projecting onto, after which we can use the fact that $e$ is perpendicular to $a_1$ and $a_2$:</p>
<script type="math/tex; mode=display">
\mathbf{a}_{1}^{T}(\mathbf{b}-A \hat{\mathbf{x}})=0 \quad \text { and } \quad \mathbf{a}_{2}^{T}(\mathbf{b}-A \hat{\mathbf{x}})=0</script><p>In matrix form, $A^T(b-A \hat x)=0$. When we were projecting onto a line, $A$ only had one column and so this equation looked like: $a^T (b-xa)=0$. Note that $e=b-A \hat x$ is in the nullspace of $A^T$ and so is in the left nullspace of $A$. We know that everything in the left nullspace of $A$ is perpendicular to the column space of $A$, so this is another confirmation that our calculation are correct.</p>
<p>We can rewrite the equation $A^T(b-A \hat x)=0$ as:</p>
<script type="math/tex; mode=display">
A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b}</script><p>When projecting onto a line, $A^TA$ was just a number; now it is a square matrix. So insted of dividing by $a^Ta$ we now have to multiply by $(A^TA)^{-1}$, in $n$ dimensions:</p>
<script type="math/tex; mode=display">
\begin{aligned} \hat{\mathbf{x}} &=\left(A^{T} A\right)^{-1} A^{T} \mathbf{b} \\ \mathbf{p}=A \hat{\mathbf{x}} &=A\left(A^{T} A\right)^{-1} A^{T} \mathbf{b} \\ P &=A\left(A^{T} A\right)^{-1} A^{T} \end{aligned}</script><p>It is tempting to try to simplify these expressions, but if $A$ isn’t a square matrix we can’t say that $(A^TA)^{-1}=A^{-1}(A^T)^{-1}$. If $A$ does happen to be a square and invertible matrix, then its column space is the whole space and contains $b$. In this case $P$ is the identity, as we find when we simplify. It is still true that:</p>
<script type="math/tex; mode=display">
P^T=P \ \text{and} \ P^2=P</script><p>If $b$ is perpendicular to the column space, then it’s in the left nullspace $N(A^T)$ of $A$ and $Pb=0$. </p>
<blockquote>
<p>According to b perpendicular to the column space of $A$, we have $A^Tb=0$. And we have $Pb$ as follows:</p>
<script type="math/tex; mode=display">
Pb = A(A^TA)^{-1}A^T b= A(A^TA)^{-1}\cdot 0 = 0</script></blockquote>
<p>If $b$ is in the column space, then $b=Ax$ for some $x$, and $Pb=b$.</p>
<blockquote>
<p>Similarly, $Pb = A(A^TA)^{-1}A^T b$. Substitute $b$ with $Ax$, we have $Pb=A(A^TA)^{-1}A^T Ax$. $A^TA$ are cancelled by its inverse matrix $(A^TA)^{-1}$ and results in $I$. So we have $Pb=Ax=b$.</p>
</blockquote>
<p>For a typical vector $b$, it will have a component $p$ in the column space and a component $e$ perpendicular to the column space, which menas $e$ is in the left nullspace. The matrix projecting $b$ onto $N(A^T)$ is $I-P$:</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\mathbf{e}=\mathbf{b}-\mathbf{p}} \\ {\mathbf{e}=(I-P) \mathbf{b}}\end{array}</script><p>Naturally, $I-P$ has all the properties of a projection matrix.</p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-08 at 12.57.32 PM.png" alt="creen Shot 2020-01-08 at 12.57.32 P"></p>
<h3 id="Least-Squares"><a href="#Least-Squares" class="headerlink" title="Least Squares"></a>Least Squares</h3><p>Suppose we’re given a collection of data points $(t,b)$:</p>
<script type="math/tex; mode=display">
\{(1,1),(2,2),(3,2)\}</script><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-07 at 2.34.28 PM.png" alt="creen Shot 2020-01-07 at 2.34.28 P"></p>
<p>and we want to find the closest line $b=C+Dt$ to that collection. By “closest” line we mean one that minimizes the error represented by the distance from the points to the line. We measure that error by adding up the squares of these distances. In other words, we want to minimize $||Ax-b||^2=||e||^2$. </p>
<p>If the line went through all three points, we’d have:</p>
<script type="math/tex; mode=display">
C+D=1\\
C+2D=2\\
C+3D=2</script><p>which is equivalent to:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}{1} & {1} \\ {1} & {2} \\ {1} & {3}\end{array}\right] \quad\left[\begin{array}{l}{C} \\ {D}\end{array}\right]=\left[\begin{array}{l}{1} \\ {2} \\ {2} \\ {b}\end{array}\right]</script><p>There are two ways of viewing this. In the space of the line we’re trying to find, $e_1,e_2$ and $e_3$ are the vertical distances from the data points to the line. The components $p_1, p_2$ and $p_3$ are the values of $C+Dt$ near each data point; $p \approx b$.</p>
<p>In the other view we have a vector $b$ in $R^3$, its projection $p$ onto the column space of $A$, and its projection $e$ onto $N(A^T)$. We will now find $\hat x=\begin{bmatrix}\hat C \ \hat D \end{bmatrix}$ and $p$. We know:</p>
<script type="math/tex; mode=display">
A^TA \hat x=A^T b\\
\left[\begin{array}{cc}{3} & {6} \\ {6} & {14}\end{array}\right]\left[\begin{array}{c}{\hat{\mathrm{C}}} \\ {\hat{D}}\end{array}\right]=\left[\begin{array}{c}{5} \\ {11}\end{array}\right]</script><p>From this equation we get the normal equations:</p>
<script type="math/tex; mode=display">
\begin{aligned} 3 \hat C+6 \hat D &=5 \\ 6 \hat{C}+14 \hat D &=11 \end{aligned}</script><p>We solve these to find $\hat D=\frac{1}{2}$ and $\hat C=\frac{2}{3}$.</p>
<p>By using these closest line $b=\frac{2}{3}+\frac{1}{2}t$, we can find the $p$ by plugging three points into the line:</p>
<script type="math/tex; mode=display">
p=\begin{bmatrix}\frac{2}{3}+\frac{1}{2}*1 \\ \frac{2}{3}+\frac{1}{2}*2 \\ \frac{2}{3}+\frac{1}{2}*3 \end{bmatrix} =\begin{bmatrix}\frac{7}{6} \\ \frac{5}{3} \\ \frac{13}{6} \end{bmatrix}</script><p>So the error is:</p>
<script type="math/tex; mode=display">
p=\begin{bmatrix}1-\frac{7}{6} \\ 2-\frac{5}{3} \\ 2-\frac{13}{6} \end{bmatrix}=\begin{bmatrix}-\frac{1}{6} \\ \frac{1}{3} \\ -\frac{1}{6} \end{bmatrix}</script><p>Note that $p$ and $e$ are orthogonal, i.e., $p*e=0$ and $p+e=b$.</p>
<h3 id="The-matrix-A-TA"><a href="#The-matrix-A-TA" class="headerlink" title="The matrix $A^TA$"></a>The matrix $A^TA$</h3><p>In this example, the line does not go through all three points, so this equation is not solvable. Instead we’ll solve:</p>
<script type="math/tex; mode=display">
A^TA \hat x=A^Tb</script><p>But we assumed that the matrix $A^TA$ is invertible. Is this justified?</p>
<p>If $A$ has independent columns, then $A^TA$ is invertible. To prove this, we assume that $A^TAx=0$, then show that it must be true that $x=0$:</p>
<script type="math/tex; mode=display">
\begin{aligned} A^{T} A \mathbf{x} &=\mathbf{0} \\ \mathbf{x}^{T} A^{T} A \mathbf{x} &=\mathbf{x}^{T} \mathbf{0} \\(A \mathbf{x})^{T}(A \mathbf{x}) &=0 \\ A \mathbf{x} &=\mathbf{0} \end{aligned}</script><p>Since $A$ has independent columns, $Ax=0$ only when $x=0$.</p>
<p>As long as the columns of A are independent, we can use linear regression to find approximate solutions to unsolvable systems of linear equations. The columns of A are guaranteed to be independent if they are orthonormal, i.e., if they are perpendicular unit vectors like $\begin{bmatrix}1 \\ 0\\ 0\end{bmatrix}, \begin{bmatrix}0 \\ 1\\ 0\end{bmatrix}$ and $\begin{bmatrix}0 \\ 0\\ 1\end{bmatrix}$, or like $\begin{bmatrix} cos\theta \\ sin\theta \end{bmatrix}$ and $\begin{bmatrix} -sin \theta \\ cos \theta \end{bmatrix}$.</p>
<h2 id="Orthogonal"><a href="#Orthogonal" class="headerlink" title="Orthogonal"></a>Orthogonal</h2><h3 id="Orthonormal-vectors"><a href="#Orthonormal-vectors" class="headerlink" title="Orthonormal vectors"></a>Orthonormal vectors</h3><p>The vectors $q_1,q_2,…,q_n$ are orthonormal if:</p>
<script type="math/tex; mode=display">
\mathbf{q}_{i}^{T} \mathbf{q}_{j}=\left\{\begin{array}{ll}{0} & {\text { if } i \neq j} \\ {1} & {\text { if } i=j}\end{array}\right.</script><p>In other words, they all have (normal) length 1 and are perpendicular (ortho) to each other. Orthonormal vectors are always independent. </p>
<h3 id="Orthonormal-matrix"><a href="#Orthonormal-matrix" class="headerlink" title="Orthonormal matrix"></a>Orthonormal matrix</h3><p>If the columns of $Q=\begin{bmatrix}q_1 \ … \ q_n \end{bmatrix}$ are orthonormal, then $Q^TQ=I$ is the identity. Matrices with orthonormal columns are a new class of important matrices to add to those on our list: triangular, diagonal, permutation, symmetric, reduced row echelon, and projection matrices. We’ll call them “orthonormal matrices”. </p>
<p>A square orthonormal matrix $Q$ is called an orthogonal matrix. If $Q$ is square, then $Q^TQ$ tells us that $Q^T=Q^{-1}$. The matrix $\left[\begin{array}{rr}{1} &amp; {1} \\ {1} &amp; {-1}\end{array}\right]$ is not orthogonal, but we can adjust that matrix to get the orthogonal matrix $Q=\frac{1}{\sqrt 2}\left[\begin{array}{rr}{1} &amp; {1} \\ {1} &amp; {-1}\end{array}\right]$. </p>
<h3 id="Why-orthonormal"><a href="#Why-orthonormal" class="headerlink" title="Why orthonormal"></a>Why orthonormal</h3><p>Suppose Q has orthonormal columns. The matrix that projects onto the column space of Q is: </p>
<script type="math/tex; mode=display">
P=Q(Q^TQ)^{-1}Q^T</script><p>If the columns of $Q$ are orthonormal, then $Q^TQ=I$ and $P=QQ^T$.If $Q$ is square, then $P=I$ because the columns of $Q$ span the entire space. Many equations become trivial when using a matrix with orthonormal columns. If our basis is orthonormal, the projection component $\hat x_i$ is just $q_i^Tb$ because $A^TA \hat x=A^Tb$ becomes $\hat x=Q^T b$. </p>
<h3 id="Gram-Schmidt"><a href="#Gram-Schmidt" class="headerlink" title="Gram-Schmidt"></a>Gram-Schmidt</h3><p>With elimination, our goal was “make the matrix triangular”. Now our goal is “make the matrix orthonormal”. We start with two independent vectors a and b and want to find orthonormal vectors q1 and q2 that span the same plane. We start by finding orthogonal<br>vectors A and B that span the same space as a and b. Then the unit vectors $q_1 = \frac{A}{||A||}$ and $q_2=\frac{B}{||B||}$ from the desired orthonormal basis.</p>
<p>Let A = a. We get a vector orthogonal to A in the space spanned by a and b by projecting b onto a and letting B = b − p. (B is what we previously called e.) </p>
<script type="math/tex; mode=display">
\mathbf{B}=\mathbf{b}-\frac{\mathbf{A}^{T} \mathbf{b}}{\mathbf{A}^{T} \mathbf{A}} \mathbf{A}</script><p>If we multiply both sides of this equation by $A^T$, we see that $A^TB=0$.</p>
<p>What if we had started with three independent vectors, a, b and c? Then we’d find a vector C orthogonal to both A and B by subtracting from c its components in the A and B directions: </p>
<script type="math/tex; mode=display">
\mathbf{C}=\mathbf{c}-\frac{\mathbf{A}^{T} \mathbf{c}}{\mathbf{A}^{T} \mathbf{A}} \mathbf{A}-\frac{\mathbf{B}^{T} \mathbf{c}}{\mathbf{B}^{T} \mathbf{B}} \mathbf{B}</script><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-09 at 11.08.57 AM.png" alt="creen Shot 2020-01-09 at 11.08.57 A"></p>
<p>When we studied elimination, we wrote the process in terms of matrices and found A = LU. A similar equation A = QR relates our starting matrix A to the result Q of the Gram-Schmidt process. Where L was lower triangular, R is upper triangular. </p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-09 at 11.12.20 AM.png" alt="creen Shot 2020-01-09 at 11.12.20 A"></p>
<p>If R is upper triangular, then it should be true that $a_1^Tq_2 = 0$. This must be true<br>because we chose $q_1$ to be a unit vector in the direction of $a_1$. All the later $q_i$<br>were chosen to be perpendicular to the earlier ones. Notice that $R = Q^TA$. This makes sense; $Q^TQ = I$.  </p>
<h1 id="Determinants-and-Eigenvalues"><a href="#Determinants-and-Eigenvalues" class="headerlink" title="Determinants and Eigenvalues"></a>Determinants and Eigenvalues</h1><h2 id="Properties-of-Determinants"><a href="#Properties-of-Determinants" class="headerlink" title="Properties of Determinants"></a>Properties of Determinants</h2><p>The determinant is a number associated with any square matrix; we’ll write it as det A or |A|. The determinant encodes a lot of information about the matrix; the matrix is invertible exactly when the determinant is non-zero. We know that $\left|\begin{array}{ll}{a} &amp; {b} \\ {c} &amp; {d}\end{array}\right|=a d-b c$; but the following properties will give us a formula for the determinant of square matrices of all size.</p>
<ol>
<li><p>$\text{det} I=1$</p>
</li>
<li><p>If you exchange two rows of a matrix, you reverse the sign of its determinant from positive to negative or from negative to positive. </p>
</li>
<li><p><strong>(A)</strong> If we multiply one row of a matrix by $t$, the determinant is multiplied by $t$: $\left|\begin{array}{ll}{ta} &amp; {tb} \\ {c} &amp; {d}\end{array}\right|=t\left|\begin{array}{ll}{a} &amp; {b} \\ {c} &amp; {d}\end{array}\right|$.</p>
<p><strong>(B)</strong> The determinant behaves like a linear function on the rows of the matrix:</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ll}{a+a'} & {b+b'} \\ {c} & {d}\end{array}\right|=\left|\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right|+\left|\begin{array}{ll}{a'} & {b'} \\ {c} & {d}\end{array}\right|</script></li>
<li><p>If two rows of a matrix are equal, its determinant is zero. </p>
<p>This is because of property 2, the exchange rule. On the one hand, exchanging the two identical rows does not change the determinant. On the other hand, exchanging the two rows changes the sign of the determinant. Therefore the determinant must be 0. </p>
</li>
<li><p>If $i \ne j$, subtracting $t$ times row $i$ from row $j$ doesn’t change the determinant.</p>
<script type="math/tex; mode=display">
\begin{aligned}\left|\begin{array}{cc}{a} & {b} \\ {c-t a} & {d-t b}\end{array}\right| &=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right|-\left|\begin{array}{cc}{a} & {b} \\ {t a} & {t b}\end{array}\right| & \text { property } 3(b) \\ &=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right|-t\left|\begin{array}{cc}{a} & {b} \\ {a} & {b}\end{array}\right| \quad \text { property } 3(a) \\ &=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right| & \text { property } 4 \end{aligned}</script></li>
<li><p>If $A$ has a row that is all zeros, then $\text{det} A=0$. We get this from property 3(A) by letting $t=0$.</p>
</li>
<li><p>The determinant of a triangular matrix is the product of the diagonal<br>entries (pivots) $d_1,d_2,…,d_n$.</p>
<p>Property 5 tells us that the determinant of the triangular matrix won’t change if we use elimination to convert it to a diagonal matrix with the entries di on its diagonal. Then property 3 (a) tells us that the determinant of this diagonal matrix is the product $d_1d_2 · · · d_n$ times the determinant of the identity matrix. Property 1 completes the argument. </p>
<p>Note that we cannot use elimination to get a diagonal matrix if one of the $d_i$ is zero. In that case elimination will give us a row of zeros and property 6 gives us the conclusion we want. </p>
</li>
<li><p>$\text{det} A=0$ exactly when $A$ is singular.</p>
<p>If A is singular, then we can use elimination to get a row of zeros, and property 6 tells us that the determinant is zero. If A is not singular, then elimination produces a full set of pivots $d_1, d_2, …, d_n$ and the determinant is $d_1d_2 · · · d_n \ne 0$ (with minus signs from row exchanges). </p>
</li>
<li><p>$\text{det}AB=(\text{det}A)(\text{det}B)$</p>
<p>This is very useful. Although the determinant of a sum does not equal the sum of the determinants, it is true that the determinant of a product equals the product of the determinants. </p>
<p>For example:</p>
<script type="math/tex; mode=display">
\text{det}A^{-1}=\frac{1}{\text{det}A}</script><p>because $A^{-1}A=1.$ (Note that if $A$ is singular then $A^{-1}$ does not exist and $\text{det}A^{-1}$ is undefined.) Also,  $\text{det}A^2=(\text{det}A)^2$ and $\text{det}2A=2^n\text{det}A$ (applying property 3 to each row of the matrix). This reminds us of volume – if we double the length, width and height of a three dimensional box, we increase its volume by a multiple of $2^3=8$.</p>
</li>
<li><p>$\text{det}A^T=\text{det}A$.</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right|=\left|\begin{array}{cc}{a} & {c} \\ {b} & {d}\end{array}\right|=a d-b c</script><p>To see why $|A^T|=|A|$, use elimination to write $A=LU$. The statement becomes $|U^TL^T|=|LU|$. Rule 9 then tells us $|U^T||L^T|=|L||U|$. Matrix $L$ is a lower triangular matrix with 1’s on the diagonal, so rule 5 tells us that $|L|=|L^t|=1$. Because $U$ is upper triangular, rule 5 tells us that $|U|=|U^T|$. Therefore $|U^T||L^T|=|L||U|$ and $|A^T|=|A|$.</p>
</li>
</ol>
<h2 id="Determinant-Formulas"><a href="#Determinant-Formulas" class="headerlink" title="Determinant Formulas"></a>Determinant Formulas</h2><p>Firstly, we try to find a formula for the $2 \times 2$ matrix:</p>
<script type="math/tex; mode=display">
\begin{aligned}\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right| &=\left|\begin{array}{cc}{a} & {0} \\ {c} & {d}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {c} & {d}\end{array}\right| \\ &=\left|\begin{array}{cc}{a} & {0} \\ {c} & {0}\end{array}\right|+\left|\begin{array}{cc}{a} & {0} \\ {0} & {0}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {c} & {0}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {c} & {0}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {0} & {d}\end{array}\right| \\ &=0+a d+(-c b)+0 \\ &=a d-b c \end{aligned}</script><p>By applying property 3 (The determinant is linear in each row separately) to separate the individual entries of each row we could get a formula for any other square matrix. However, for a 3 by 3 matrix we’ll have to add the determinants of twenty seven different matrices! Many of those determinants are zero. The non-zero pieces are: </p>
<script type="math/tex; mode=display">
\left|\begin{array}{ccc}{a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ {a_{31}} & {a_{32}} & {a_{33}}\end{array}\right|=\left|\begin{array}{ccc}{a_{11}} & {0} & {0} \\ {0} & {a_{22}} & {0} \\ {0} & {0} & {a_{33}}\end{array}\right|+\left|\begin{array}{ccc}{a_{11}} & {0} & {0} \\ {0} & {0} & {a_{23}} \\ {0} & {a_{32}} & {0}\end{array}\right|+\left|\begin{array}{ccc}{0} & {a_{12}} & {0} \\ {a_{21}} & {0} & {0} \\ {0} & {0} & {a_{33}}\end{array}\right| \\
+\left|\begin{array}{ccc}{0} & {a_{12}} & {0} \\ {0} & {0} & {a_{23}} \\ {a_{31}} & {0} & {0}\end{array}\right|+\left|\begin{array}{ccc}{0} & {0} & {a_{13}} \\ {a_{21}} & {0} & {0} \\ {0} & {a_{32}} & {0}\end{array}\right|+\left|\begin{array}{ccc}{0} & {0} & {a_{13}} \\ {0} & {a_{22}} & {0} \\ {a_{31}} & {0} & {0}\end{array}\right|\\
=a_{11}a_{22}a_{33}-a_{11}a_{23}a_{33}-a_{12}a_{21}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}</script><p>Each of the non-zero pieces has one entry from each row in each column, as in a permutation matrix. Since the determinant of a permutation matrix is either 1 or -1, we can again use property 3 to find the determinants of each of these summands and obtain our formula. </p>
<p>The number of parts with non-zero determinants was 2 in the 2 by 2 case, 6 in the 3 by 3 case, and will be $24=4!$ in the 4 by 4 case. This is because there are n ways to choose an element from the first row, after which there are only n − 1 ways to choose an element from the second row that avoids a zero determinant. Then there are n − 2 choices from the third<br>row, n − 3 from the fourth, and so on. The big formula for computing the determinant of any square matrix is: </p>
<script type="math/tex; mode=display">
\operatorname{det} A=\sum_{n ! \text { terms }} \pm a_{1 \alpha} a_{2 \beta} a_{3 \gamma} \ldots a_{n \omega}</script><p>where $(\alpha, \beta,\gamma, …, \varpi )$ is some permutation of $(1,2,3,..,n)$. </p>
<h2 id="Cofactor-formula"><a href="#Cofactor-formula" class="headerlink" title="Cofactor formula"></a>Cofactor formula</h2><p>The cofactor formula rewrites the big formula for the determinant of an n by n matrix in terms of the determinants of smaller matrices. In the $3 \times 3$ case, the formula looks like:</p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{det} A &=a_{11}\left(a_{22} a_{33}-a_{23} a_{32}\right)+a_{12}\left(-a_{21} a_{33}+a_{23} a_{31}\right)+a_{13}\left(a_{21} a_{32}-a_{22} a_{31}\right) \\ &=\left|\begin{array}{ccc}{a_{11}} & {0} & {0} \\ {0} & {a_{22}} & {a_{23}} \\ {0} & {a_{32}} & {a_{33}}\end{array}\right|+\left|\begin{array}{ccc}{0} & {a_{12}} & {0} \\ {a_{21}} & {0} & {a_{23}} \\ {a_{31}} & {0} & {a_{33}}\end{array}\right|+\left|\begin{array}{ccc}{0} & {0} & {a_{13}} \\ {a_{21}} & {a_{22}} & {0} \\ {a_{31}} & {a_{32}} & {0}\end{array}\right| \end{aligned}</script><p>Each element is multiplied by the cofactors in the parentheses following it. Note that each cofactor is (plus or minus) the determinant of a two by two matrix. Thatdeterminant is made up of products of elements in the rows and columns NOT containing $a_{1j}$. </p>
<p>In general, the cofactor $C_{ij}$ of $a_{ij}$ can be found by looking at all the terms in the big formula that contain $a_{ij}$. $C_{ij}$ equals $(-1)^{i+j}$ times the determinant of the $n-1$ by $n-1$ square matrix obtained by removing row $i$ and column $j$. ($C_{ij}$ is positive if $i+j$ is even.)</p>
<p>For $n \times n$ matrices, the cofactor formula is:</p>
<script type="math/tex; mode=display">
\text{det}A=a_{11}C_{11}+a_{12}C_{12}+...+a_{1n}C_{1n}</script><blockquote>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-10 at 4.05.19 PM.png" alt="creen Shot 2020-01-10 at 4.05.19 P"></p>
</blockquote>
<h2 id="Inverse-matrix-formula"><a href="#Inverse-matrix-formula" class="headerlink" title="Inverse matrix formula"></a>Inverse matrix formula</h2><p>We know:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right]^{-1}=\frac{1}{a d-b c}\left[\begin{array}{rr}{d} & {-b} \\ {-c} & {a}\end{array}\right]</script><p>In fact: $A^{-1}=\frac{1}{\text{det}A}C^T$, where $C$ is the matrix of cofactors.</p>
<p>To more formally verify the formula, we’ll check that $AC^T=\text{(detA)}I$.</p>
<script type="math/tex; mode=display">
A C^{T}=\left[\begin{array}{ccc}{a_{11}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {\ddots} & {\vdots} \\ {a_{n 1}} & {\cdots} & {a_{n n}}\end{array}\right]\left[\begin{array}{ccc}{C_{11}} & {\cdots} & {C_{n 1}} \\ {\vdots} & {\ddots} & {\vdots} \\ {C_{1 n}} & {\cdots} & {C_{n n}}\end{array}\right]</script><p>The entry in the first row and first column of the product matrix is: </p>
<script type="math/tex; mode=display">
\sum_{j=1}^na_{1j}C_{j1}=\text{det}A</script><p>(This is just the cofactor formula for the determinant.) This happens for every entry on the diagonal of $AC^T$. To finish proving that $AC^T=(\text{det}A)I$, we just need to check that the offdiagonal entries of $AC^T$ are zero. In the two by two case, multiplying the entries in row 1 of $A$ by the entries in column 2 of $C^T$ gives $a(-b)+b(a)=0$. This is the determinant of $A_{S}=\left[\begin{array}{ll}{a} &amp; {b} \\ {a} &amp; {b}\end{array}\right]$. In higher dimensions, the product of the first row of $A$ and the last column of $C^T$ equals the the determinant of a matrix whose first and last rows are identical. This happens with all the off diagonal matrices, which confirms that $A^{-1}=\frac{1}{\text{det}A}C^T$. </p>
<h3 id="Cramer’s-rule-for-x-A-1-b"><a href="#Cramer’s-rule-for-x-A-1-b" class="headerlink" title="Cramer’s rule for $x=A^{-1}b$"></a>Cramer’s rule for $x=A^{-1}b$</h3><p>We know that if $Ax=b$ and $A$ is nonsingular, then $x=A^{-1}b$. Applying the formula $A^{-1}=\frac{C^T}{\text{det}A}$ gives us:</p>
<script type="math/tex; mode=display">
x=\frac{C^Tb}{\text{det}A}</script><p>Cramer’s rule gives us another way of looking at this equation. To derive this rule we break x down into its components. Because the i’th component of $C^Tb$ is a sum of cofactors times some number, it is the determinant of some matrix $B_j$.</p>
<script type="math/tex; mode=display">
x=\frac{\text{det}B_J}{\text{det}A}</script><p>where $B_j$ is the matrix created by starting with $A$ and then replacing column $j$ with $b$, so</p>
<script type="math/tex; mode=display">
\begin{aligned} B_{1} &=\left[\begin{array}{c}{\text { last } \mathrm{n}-1} \\ {\mathbf{b}} & {\text { columns }} \\ {\text { of } A}\end{array}\right] \\ B_{n} &=\left[\begin{array}{cc}{\text { first } \mathrm{n}-1} \\ {\text { columns }} & {\mathbf{b}} \\ {\text { of } A} & {}\end{array}\right] \end{aligned}</script><p>Computing inverses using Cramer’s rule is usually less efficient than using elimination. </p>
<h3 id="text-det-A-volume-of-box"><a href="#text-det-A-volume-of-box" class="headerlink" title="$|\text{det}A|=$ volume of box"></a>$|\text{det}A|=$ volume of box</h3><p>Claim: | det A| is the volume of the box (parallelepiped) whose edges are the column vectors of A. (We could equally well use the row vectors, forming a different box with the same volume.) </p>
<p>If A = I, then the box is a unit cube and its volume is 1. Because this agrees with our claim, we can conclude that the volume obeys determinant property 1.</p>
<p>If A = Q is an orthogonal matrix then the box is a unit cube in a different orientation with volume 1 = det Q . (Because Q is an orthogonal matrix, $Q^TQ=I$ and so det Q = ±1.) </p>
<p>Swapping two columns of A does not change the volume of the box  (remembering that det A = det AT) or the absolute value of the determinant (property 2).  If we show that the volume of the box also obeys property 3 we’ll have proven | det A| equals the volume of the box.</p>
<p>If we double the length of one column of A, we double the volume of the box formed by its columns. Volume satisfies property 3(a). Property 3(b) says that the determinant is linear in the rows of the matrix: </p>
<script type="math/tex; mode=display">
\left|\begin{array}{cc}{a+a^{\prime}} & {b+b^{\prime}} \\ {c} & {d}\end{array}\right|=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right|+\left|\begin{array}{cc}{a^{\prime}} & {b^{\prime}} \\ {c} & {d}\end{array}\right|</script><p>Although it’s not needed for our proof, we can also see that determinants obey property 4. If two edges of a box are equal, the box flattens out and has no volume. </p>
<p>Important note: If you know the coordinates for the corners of a box, then computing the volume of the box is as easy as calculating a determinant. In particular, the area of a parallelogram with edges $\begin{bmatrix}a\\b\end{bmatrix}$ and $\begin{bmatrix}c\\d\end{bmatrix}$is $ad-bc$. The area of a triangle with edges $\begin{bmatrix}a\\b\end{bmatrix}$ and $\begin{bmatrix}c\\d\end{bmatrix}$is $\frac{1}{2}(ad-bc)$.</p>
<h1 id="Eigenvalues-and-eigenvectors"><a href="#Eigenvalues-and-eigenvectors" class="headerlink" title="Eigenvalues and eigenvectors"></a>Eigenvalues and eigenvectors</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>A matrix $A$ acts on vectors $x$ like a function does, with input $x$ and output $Ax$. Eigenvectors are vectors for which $Ax$ is parallel to $x$. In other words:</p>
<script type="math/tex; mode=display">
Ax=\lambda x</script><p>In this equation, $x$ is an eigenvector of $A$ and $\lambda $ is an eigenvalue of $A$.</p>
<p><strong>Eignevalue 0</strong> </p>
<p>If the eigenvalue $\lambda $ equals 0 then $Ax = 0x = 0$. Vectors with eigenvalue 0 make<br>up the nullspace of $A$; if A is singular, then $\lambda = 0$ is an eigenvalue of A. </p>
<p><strong>Example</strong></p>
<p>Suppose P is the matrix of a projection onto a plane. For any x in the plane Px = x, so x is an eigenvector with eigenvalue 1. A vector x perpendicular to the plane has Px = 0, so this is an eigenvector with eigenvalue $\lambda = 0$. The eigenvectors of P span the whole space (but this is not true for every matrix). </p>
<p>The matrix $B=\begin{bmatrix}0 \ 1 \\ 1 \ 0 \end{bmatrix}$ has an eigenvector $x=\begin{bmatrix} 1 \\ 1  \end{bmatrix}$ with eigenvalue 1 and another eigenvector $x=\begin{bmatrix} 1 \\ -1  \end{bmatrix}$ with eigenvalue -1. These eigenvectors span the space. They are perpendicular because $B=B^T$ (as we will prove).</p>
<p>$\text{det}(A-\lambda I)=0$</p>
<p>An n by n matrix will have n eigenvalues, and their sum will be the sum of the diagonal entries of the matrix: $a_{11}+a_{22}+…+a_{nn}$. This sum is the trace of the matrix. For a two by two matrix, if we know one eigenvalue we can use this fact to find the second. </p>
<p>In order to solve $Ax=\lambda x$ for the eigenvalues and eigenvectors of $A$, we need to be clever to solve this problem:</p>
<script type="math/tex; mode=display">
Ax=\lambda x\\
(A-\lambda I)x=0</script><p>In order for $\lambda $ to be an eigenvector, $A-\lambda I$ must be singular. In other words, $\text{det}(A-\lambda I)=0$. Let $A=\begin{bmatrix}3 \ 1 \\ 1 \ 3 \end{bmatrix}$, then:</p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 11.52.58 AM.png" alt="creen Shot 2020-01-12 at 11.52.58 A"></p>
<p>Note that the coefficient 6 is the trace (sum of diagonal entries) and 8 is the determinant of A. In general, the eigenvalues of a two by two matrix are the solutions to: </p>
<script type="math/tex; mode=display">
\lambda ^2-\text{trace}(A) \cdot \lambda + \text{det} A=0</script><p>Just as the trace is the sum of the eigenvalues of a matrix, the product of the eigenvalues of any matrix equals its determinant. </p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 11.55.36 AM.png" alt="creen Shot 2020-01-12 at 11.55.36 A"><br><strong>A caution</strong></p>
<p>Similarly, if $Ax=\lambda x$ and $Bx=\alpha x$, $(A+B)x=(\lambda + \alpha)x$. It would be nice if the eigenvalues of a matrix sum were always the sums of the eigenvelues, but this is only true if A and B have the same eigenvectors. The eigenvalues of the product $AB$ are not usually equal to the products $\lambda (A) \lambda (B)$, either.</p>
<p><strong>Complex eigenvalues</strong></p>
<p>The matrix $Q=\begin{bmatrix}0 \ -1 \\ 1 \ 0 \end{bmatrix}$ rotates every vector in the plane by $90^o$. It has trace $0=\lambda_1 +\lambda_2$ and determinant $0=\lambda_1 \cdot \lambda_2$. Its only real eigenvector is the zero vector; any other vector’s direction changes when it is multiplied by $Q$. How will this affect our eigenvalue calculation? </p>
<script type="math/tex; mode=display">
\begin{aligned} \operatorname{det}(A-\lambda I) &=\left|\begin{array}{cc}{-\lambda} & {-1} \\ {1} & {-\lambda}\end{array}\right| \\ &=\lambda^{2}+1 \end{aligned}</script><p>$\text{det}(A-\lambda I)=0$ has solutions $\lambda_1=i$ and $\lambda_2=-i$. If a matrix has a complex eigenvalue $a+bi$ then the complex conjugate $a-bi$ is also en eigenvalue of that matrix. </p>
<p>Symmetric matrices have real eigenvalues. For antisymmetric matrices like $Q$, for which $A^T=-A$, all eigenvalues are imaginary $(\lambda =bi)$.</p>
<p><strong>Triangular matrices and repeated eigenvalues</strong></p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 12.06.56 PM.png" alt="creen Shot 2020-01-12 at 12.06.56 P"></p>
<blockquote>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 12.09.35 PM.png" alt="creen Shot 2020-01-12 at 12.09.35 P"></p>
</blockquote>
<h2 id="Diagonalization-and-powers-of-A"><a href="#Diagonalization-and-powers-of-A" class="headerlink" title="Diagonalization and powers of $A$"></a>Diagonalization and powers of $A$</h2><p><strong>Diagonalizing a matrix</strong></p>
<p>If $A$ has $n$ linearly independent eigenvectors, we can put those vectors in the columns of a matrix $S$. Then,</p>
<script type="math/tex; mode=display">
\begin{aligned} A S &=A\left[\begin{array}{llll}{\mathbf{x}_{1}} & {\mathbf{x}_{2}} & {\cdots} & {\mathbf{x}_{n}}\end{array}\right] \\ &=\left[\begin{array}{llll}{\mathbf{\lambda}_{1} \mathbf{x}_{1}} & {\lambda_{2} \mathbf{x}_{2}} & {\cdots} & {\lambda_{n} \mathbf{x}_{n}}\end{array}\right] \\ &=\left[\begin{array}{cccc}{\lambda_{1} \mathbf{x}_{1}} & {0} & {\cdots} & {0} \\ {0} & {\lambda_{2}} & {} & {0} \\ {\vdots} & {} & {\ddots} & {\vdots} \\ {0} & {\cdots} & {0} & {\lambda_{n}}\end{array}\right]=S \Lambda \end{aligned}</script><p>Note that $\Lambda$ is a diagonal matrix whose non-zero entries are the eigenvalues of $A$. Because the columns of $S$ are indepedent, $S^{-1}$ exists and we can multiply both sides of $AS=S\Lambda$ by $S^{-1}$:</p>
<script type="math/tex; mode=display">
S^{-1}AS=\Lambda \\
A=S\Lambda S^{-1}</script><p><strong>Power of</strong> $A$</p>
<p>If $Ax=\lambda x$, then $A^2 x = A\lambda x = \lambda Ax = \lambda ^2 x$.</p>
<p>The eigenvalues of $A^2$ are the squares of the eigenvalues of $A$. The eigenvectors of $A^2$ are the same as the eigenvectors of $A$. If we write $A=S\Lambda S^{-1}$ then:</p>
<script type="math/tex; mode=display">
A^2=S\Lambda S^{-1}S\Lambda S^{-1}=S\Lambda ^2 S^{-1}</script><p>Similarly, $A^k=S\Lambda^k S^{-1}$ tells us that raising the eigenvalues of $A$ to the $k$th power gives us the eigenvalues of $A^k$, and that the eigenvectors of $A^k$ are the same as those of $A$.</p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-13 at 10.53.07 AM.png" alt="creen Shot 2020-01-13 at 10.53.07 A"></p>
<p>A is guaranteed to have n independent eigenvectors (and be diagonalizable) if all its eigenvalues are different. Most matrices do have distinct eigenvalues.</p>
<p><strong>Repeated eigenvalues</strong></p>
<p>If A has repeated eigenvalues, it may or may not have n independent eigenvectors. For example, the eigenvalues of the identity matrix are all 1, but that matrix still has n independent eigenvectors. </p>
<p>If $A$ is the triangular matrix $\begin{bmatrix}2 &amp;1\\ 0&amp;2\end{bmatrix}$, its eigenvalues are 2 and 2. Its eigenvectors are in the nullspace of $A-\lambda I=\begin{bmatrix}0 &amp;1\\ 0&amp;0\end{bmatrix}$ which is spanned by $x=\begin{bmatrix}1\\0 \end{bmatrix}$. This particular A does not have two independent eigenvectors. </p>
<h2 id="Difference-equations"><a href="#Difference-equations" class="headerlink" title="Difference equations"></a>Difference equations</h2><p>Start with a given vector $u_0$. We can create a sequence of vectors in which each new vector is A times the previous vector: $u_{k+1}=Au_k$. $u_{k+1}=Au_k$ is a first order difference equation, and $u_k=A^ku_0$ is a solution to this system. We can get a more satisfying solution if we write $u_0$ as a combination of eigenvectors of $A$:</p>
<script type="math/tex; mode=display">
u_0=c_1x_1+c_2x_2+\cdot \cdot \cdot +c_nx_n=Sc</script><p>Then:</p>
<script type="math/tex; mode=display">
Au_0=c_1\lambda_1x_1+c_2\lambda_2x_2+\cdot \cdot \cdot +c_n\lambda_nx_n</script><p>and:</p>
<script type="math/tex; mode=display">
u_k=A^ku_0=c_1\lambda_1^kx_1+c_2\lambda_2^kx_2+\cdot \cdot \cdot +c_n\lambda_n^kx_n=\Lambda^kSc</script><h3 id="Fibonacci-sequence"><a href="#Fibonacci-sequence" class="headerlink" title="Fibonacci sequence"></a>Fibonacci sequence</h3><p>The Fibonacci sequence is $0,1,1,2,3,5,8,13,\cdot\cdot \cdot$ In general, $F_{k+2}=F_{k+1}+F_{k}$. If we could understand this in terms of matrices, the eigenvalues of the matrices would tell us how fast the numbers in the sequence are increasing.</p>
<p>$u_{k+1}=Au_k$ was a first order system. $F_{k+2}=F_{k+1}+F_k$ is a second order scalar equation, but we can convert it to first order linear system by using a clever trick. If $u_k=\begin{bmatrix}F_{k+1} \\ F_k\end{bmatrix}$, then:</p>
<script type="math/tex; mode=display">
F_{k+2}=F_{k+1}+F_k\\
F_{k+1}=F_{k+1}</script><p>is equivalent to the first order system $u_{k+1}=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}u_k$. </p>
<p>Because $A=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}$ is symmetric, its eigenvalues will be real and its eigenvectors will be orthogonal.Because A is a two by two matrix we know its eigenvalues sum to 1 (the trace)<br>and their product is −1 (the determinant). </p>
<script type="math/tex; mode=display">
\begin{equation}
|A-\lambda I|=\left|\begin{array}{cc}{1-\lambda} & {1} \\ {1} & {-\lambda}\end{array}\right|=\lambda^{2}-\lambda-1
\end{equation}</script><p>Setting this to zero we find $\lambda = \frac{1\pm \sqrt{1+4}}{2}$; i.e. $\lambda_1 = \frac{1}{2}(1+\sqrt5)\approx 1.618$ and $\lambda_2 = \frac{1}{2}(1-\sqrt5)\approx -0.618$. The growth rate of the $F_k$ is controlled by $\lambda_1$, the only eigenvalue with absolute value greater than 1. This tells us that for large $k$, $F_k \approx c_1 \frac{1+\sqrt{5}}{2}$ for some constant $c_1$. To find the eigenvectors of $A$:</p>
<script type="math/tex; mode=display">
\begin{equation}
(A-\lambda I) \mathbf{x}=\left[\begin{array}{cc}{1-\lambda} & {1} \\ {1} & {-\lambda}\end{array}\right] \mathbf{x}
\end{equation}</script><p>equals $0$ when $x=\begin{bmatrix}\lambda \\ 1\end{bmatrix}$, so $x_1=\begin{bmatrix}\lambda_1 \\ 1\end{bmatrix}$ and $x_2=\begin{bmatrix}\lambda_2 \\ 1\end{bmatrix}$. </p>
<p>Finally, $u_0=\begin{bmatrix}F_{1} \\ F_0 \end{bmatrix}=\begin{bmatrix}1 \\ 0\end{bmatrix}=c_1x_1+c_2x_x$ tells us that $c_1=-c_2=\frac{1}{\sqrt{5}}$. Because $\begin{bmatrix}F_{k+1} \\ F_k\end{bmatrix}=u_k=c_1\lambda^k_1x_1+c_2\lambda_2^kx_2$, we get:</p>
<script type="math/tex; mode=display">
\begin{equation}
F_{k}=\frac{1}{\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{k}-\frac{1}{\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^{k}
\end{equation}</script><p>Using eigenvalues and eigenvectors, we have found a closed form expression for the Fibonacci numbers.    </p>
<p><strong>Summary</strong>: When a sequence evolves over time according to the rules of a<br>first order system, the eigenvalues of the matrix of that system determine the<br>long term behavior of the series. To get an exact formula for the series we find<br>the eigenvectors of the matrix and then solve for the coefficients $c_1, c_2,…$</p>
<h3 id="e-At-in-differential-equations"><a href="#e-At-in-differential-equations" class="headerlink" title="$e^{At}$ in differential equations"></a>$e^{At}$ in differential equations</h3><p>The system of equations below describes how the values of variables $u_1$ and $u_2$ affect each other over time:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned} \frac{d u_{1}}{d t} &=-u_{1}+2 u_{2} \\ \frac{d u_{2}}{d t} &=u_{1}-2 u_{2} \end{aligned}
\end{equation}</script><p>Just as we applied linear algebra to solve a difference equation, we can use it to solve this differential equation. For example, the initial condition $u_1=1,u_2=0$ can written $u(0)=\begin{bmatrix}1\\0\end{bmatrix}$.</p>
<p>By looking at the eigenvalues of the matrix $A=\begin{bmatrix}-1&amp;2\\1&amp;-2\end{bmatrix}$, we can see that $A$ is singular and its trace is $-3$ we know that its eigenvalues are $\lambda_1=0$ and $\lambda_2=-3$. The solution will turn out to include $e^{-3t}$ and $e^{0t}$. As $t$ increases, $e^{-3t}$ vanishes and $e^{0t}=1$ remains constant. Eigenvalues equal to zero have eigenvectors that are steady state solutions.</p>
<p>$x_1=\begin{bmatrix}2\\1\end{bmatrix}$ is an eigenvector for which $Ax_1=0x_1$. To find an eigenvector corresponding to $\lambda_2=-3$ we solve $(A-\lambda_2I)x_2=0$:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}2&2\\1&1\end{bmatrix}x_2=0 \ \text{so} \ x_2=\begin{bmatrix}1\\-1\end{bmatrix}</script><p>The general solution to this system of differential equations will be:</p>
<script type="math/tex; mode=display">
u(t)=c_1e^{\lambda_1t}x_1+c_2e^{\lambda_2t}x_2</script><p>To find out whether $e^{\lambda_1t}x_1$ really a solution to $\frac{du}{dt}=Au$, plug in $u=e^{\lambda_1t}x_1$:</p>
<script type="math/tex; mode=display">
\frac{du}{dt}=\lambda_1e^{\lambda_1t}x_1</script><p>whcih agrees with:</p>
<script type="math/tex; mode=display">
Au=e^{\lambda_1t}Ax_1=\lambda_1e^{\lambda_1t}x_1</script><p>The two pure terms $e^{\lambda_1t}x_1$ and $e^{\lambda_2t}x_2$ are analogous to the terms ${\lambda_i^k}x_i$ we saw in the solution $c_1\lambda_1^kx_1+c_2\lambda_2^kx_2+\cdot \cdot \cdot +c_n\lambda_n^kx_n$ to the difference equation $u_{k+1}=Au_k$.</p>
<p>Plugging in the values of the eigenvectors, we get:</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{u}(t)=c_{1} e^{\lambda_{1} t} \mathbf{x}_{1}+c_{2} e^{\lambda_{2} t} \mathbf{x}_{2}=c_{1}\left[\begin{array}{c}{2} \\ {1}\end{array}\right]+c_{2} e^{-3 t}\left[\begin{array}{r}{1} \\ {-1}\end{array}\right]
\end{equation}</script><p>we know  $u(0)=\begin{bmatrix}1\\0\end{bmatrix}$, so at $t=0$:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}1\\0\end{bmatrix} = c_1\begin{bmatrix}2\\1\end{bmatrix}+c_2\begin{bmatrix}1\\-1\end{bmatrix}</script><p>$c_1=c_2=\frac{1}{3}$ and $u(t)=\frac{1}{3}\begin{bmatrix}2\\1\end{bmatrix}+\frac{1}{3}e^{-3t}\begin{bmatrix}1\-1\end{bmatrix}$ . This tells us that the system starts with $u_1 = 1$ and $u_2 = 0$ but that as t approaches infinity, $u_1$ decays to 2/3 and $u_2$ increases to 1/3. This might describe stuff moving from $u_1$ to $u_2$. The steady state of this system is $u(\infin)=\begin{bmatrix}2/3\\1/3\end{bmatrix}$ </p>
<h3 id="Stability"><a href="#Stability" class="headerlink" title="Stability"></a>Stability</h3><p>Not all systems have a steady state. The eigenvalues of A will tell us what sort of solutions to expect: </p>
<ol>
<li>Stability: $u(t) \to 0$ when $R(\lambda) &lt; 0$</li>
<li>Steady state: One eigenvalue is 0 and all other eigenvalues have negative real part.</li>
<li>Blow up: if $R(\lambda )&gt;0$ for any eigenvalue $\lambda$</li>
</ol>
<p>If a two by two matrix $A=\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}$ has two eigenvalues with negative real part, its trace $a+d$ is negative. If $A$ has a positive determinant and negative trace then the corresponding solutions must be stable. </p>
<p><strong>Applying $S$</strong></p>
<p>The final step of our solution to the system $\frac{du}{dt}=Au$ was to solve:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{array}{l}{c_{1}\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+c_{2}\left[\begin{array}{r}{1} \\ {-1}\end{array}\right]=\left[\begin{array}{l}{1} \\ {0}\end{array}\right]} \\ {\left[\begin{array}{rr}{2} & {1} \\ {1} & {-1}\end{array}\right]\left[\begin{array}{l}{c_{1}} \\ {c_{2}}\end{array}\right]=\left[\begin{array}{l}{1} \\ {0}\end{array}\right]}\end{array}
\end{equation}</script><p>or $Sc=u(0)$, where $S$ is the eigenvector matrix. The components of c determine the contribution from each pure exponential solution, based on the initial conditions of the system. In the equation $\frac{du}{dt}=Au$, the matrix $A$ couples the pure solutions. We set $u=Sv$, where S is the matrix of eigenvectors of A, to get: </p>
<script type="math/tex; mode=display">
S\frac{dv}{dt}=ASv</script><p>or:</p>
<script type="math/tex; mode=display">
\frac{dv}{dt}=S^{-1}ASv=\Lambda v</script><p>This diagonalizes the system: $\frac{dv_i}{dt}=\lambda_i v_i$. The general solution is then:</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned} \mathbf{v}(t) &=e^{\Lambda t} \mathbf{v}(0), \quad \text { and } \\ \mathbf{u}(t) &=S e^{\Lambda t} S^{-1} \mathbf{v}(0)=e^{A t} \mathbf{u}(0) \end{aligned}
\end{equation}</script><p><strong>Matrix exponential $e^{At}$</strong></p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-14 at 5.42.50 PM.png" alt="creen Shot 2020-01-14 at 5.42.50 P"></p>
<p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-14 at 5.41.25 PM.png" alt="creen Shot 2020-01-14 at 5.41.25 P"></p>
<h2 id="Markov-matrices"><a href="#Markov-matrices" class="headerlink" title="Markov matrices"></a>Markov matrices</h2><p>The eigenvalues of $A$ and the eigenvalues of $A^T$ are the same:</p>
<script type="math/tex; mode=display">
(A-\lambda I)^T=A^T-\lambda I</script><p>so property 10 of determinates tells us that $\text{det}(A-\lambda I)=\text{det}(A^T-\lambda I)$. If $\lambda $ is an eigenvalue of $A$ then:</p>
<script type="math/tex; mode=display">
\text{det}(A^T-\lambda I)=0</script><p>which means $\lambda $ is an eigenvalue of $A^T$.</p>
<p>For a matrix like:</p>
<script type="math/tex; mode=display">
A=\begin{bmatrix}0.1&0.01&0.3\\ 0.2&0.99&0.3\\ 0.7&0&0.4\end{bmatrix}</script><p>in which all entries are non-negative and each column adds to 1 is called a Markov matrix. These requirements come from Markov matrices’ use in probability. Squaring or raising a Markov matrix to a power gives us another Markov matrix. </p>
<p>When dealing with systems of differential equations, eigenvectors with the eigenvalue 0 represented steady states. Here we’re dealing with powers of matrices and get a steady state when $\lambda = 1$ is an eigenvalue. </p>
<p>The constraint that the columns add to 1 guarantees that 1 is an eigenvalue. All other eigenvalues will be less than 1. Remember that (if $A$ has $n$ independent eigenvectors) the solution to $u_k=A^k u_0$ is $u_k = c_1 \lambda_1^k x_1+c_2 \lambda_2^k x_2+\cdot \cdot \cdot +c_n \lambda_n^k x_n$. If $\lambda_1=1$ and all other eigenvalues are less than one, then the system approaches the steady state $c_1x-1$. This is the $x_1$ component of $u_0$. </p>
<p>Why does the fact that the columns sum to 1 guarantee that 1 is an eigenvalue? If 1 is an eigenvalue of A, then: </p>
<script type="math/tex; mode=display">
A-1I=\begin{bmatrix}-0.9&0.01&0.3\\ 0.2&-0.01&0.3\\ 0.7&0&-0.6\end{bmatrix}</script><p>should be singular. Since we’ve subtracted 1 from each diagonal entry, the sum of the entries in each column of A − I is zero. But then the sum of the rows of A-I must be the zero row, and so A-I is singular. The eigenvector $x_1$ is in the nullspace of $A-I$ and has eigenvalue 1. It is not hard to find $x_1=\begin{bmatrix}0.6\\ 0.33\\ 0.7\end{bmatrix}$. </p>
<p>We’re studying the equation $u_{k+1} = Au_k$ where A is a Markov matrix. For example $u_1$ might be the population of (number of people in) Massachusetts and $u_2$ might be the population of California. A might describe what fraction of the population moves from state to state, or the probability of a single personmoving. We can’t have negative numbers of people, so the entries of A will always be positive. We want to account for all the people in our model, so the columns of A add to 1 = 100%. For example:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=k+1}=\begin{bmatrix}0.9&0.2  \\ 0.1&0.8 \end{bmatrix}\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=k}</script><p>assumes that there’s a 90% chance that a person in California will stay in California and only a 10% chance that she or he will move, while there’s a 20% percent chance that a Massachusetts resident will move to California. If our initial conditions are $\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=1}=\begin{bmatrix}0 \\ 1000 \end{bmatrix}$, then after one move $u_1=Au_0$ is:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=1}=\begin{bmatrix}0.9&0.2  \\ 0.1&0.8 \end{bmatrix}\begin{bmatrix}0\\1000 \end{bmatrix}=\begin{bmatrix}200\\800 \end{bmatrix}</script><p>For the next few values of $k$,  the Massachusetts population will decrease and the California population will increase while the total population remains constant at 1000. </p>
<p>To understand the long term behavior of this system we’ll need the eigen­vectors and eigenvalues of $\begin{bmatrix}0.9&amp;0.2  \\ 0.1&amp;0.8 \end{bmatrix}$. We know that one eigenvalue is $\lambda_1=1$. Becasue the trace 0.9+0.8=1.7 is the sum of the eigenvalues, we see that $\lambda_2=0.7$. Next we calculate the eigenvectors: $x_1=\begin{bmatrix}2  \\1 \end{bmatrix}$ . The eigenvalue 1 corresponds to the steady state solution, and $\lambda_2=0.7 &lt;1$, so the system approaches a limit in which $\frac{2}{3}$ of 1000 people live in Californina and $\frac{1}{3}$ of 1000 people are in Massachusetts. This will be the limit frim any starting vector $u_0$.</p>
<p>To know how the population is distributed after a finite number of steps we look for an eigenvector corresponding to $\lambda_2=0.7$ is $x_2=\begin{bmatrix}1  \-1 \end{bmatrix}$. From what we learned about difference equation we know that:</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{u}_{k}=c_{1} 1^{k}\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+c_{2}(.7)^{k}\left[\begin{array}{r}{-1} \\ {1}\end{array}\right]
\end{equation}</script><p>when $k=0$ we have:</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{u}_{0}=\left[\begin{array}{r}{0} \\ {1000}\end{array}\right]=c_{1}\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+c_{2}\left[\begin{array}{r}{-1} \\ {1}\end{array}\right]
\end{equation}</script><p>so $c_1=\frac{1000}{3}$ and $c_2=\frac{2000}{3}$</p>
<h2 id="Fourier-series"><a href="#Fourier-series" class="headerlink" title="Fourier series"></a>Fourier series</h2><p><strong>Expansion with an orthonormal basis</strong></p>
<p>If we have an orthonormal basis $q_1, q_2,…,q_n$ then we can write any vector $v$ as $v+x_1q_1+x_2q_x+\cdot \cdot \cdot +x_nq_n$, where:</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathbf{q}_{i}^{T} \mathbf{v}=x_{1} \mathbf{q}_{i}^{T} \mathbf{q}_{1}+x_{2} \mathbf{q}_{i}^{T} \mathbf{q}_{2}+\cdots+x_{n} \mathbf{q}_{i}^{T} \mathbf{q}_{n}=x_{i}
\end{equation}</script><p>since $q_i^Tq_j=0$ unless $i=j$, this equation gives $x_i=q_i^Tv$.</p>
<p>In terms of matrices, $\begin{bmatrix}q_1 &amp;\cdot \cdot \cdot &amp; q_n\end{bmatrix}\begin{bmatrix}x_1 \\ \cdot \\ \cdot \\ \cdot \\ q_n\end{bmatrix}=v$, or $Qx=v$. So $x=Q^{-1}v$. Because the $q_i$ form an orthonormal basis, $Q^{-1}=Q^T$ and $x=Q^Tv$. This another way to see that $x_i=q_i^Tv$.</p>
<p><strong>Fourier series</strong></p>
<p>The key idea above was that the basis of vectors $q_i$ was orthonormal. Fourier series are built on this idea. We can describe a function $f(x)$ in terms of trigonometric functions: </p>
<script type="math/tex; mode=display">
f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2sin2x+\cdot \cdot \cdot</script><p>This Fourier series is an infinite sum and the previous example was finite, but the two are related by the fact that the cosines and sines in the Fourier series are orthogonal. We’re now working in an infinite dimensional vector space. The vectors in this space are functions and the (orthogonal) basis vectors are $1, cosx, sinx, cos2x, sin2x, \cdot \cdot \cdot$</p>
<p>What does “orthogonal” mean in this context? How do we compute a dot product or inner product in this vector space? For vectors in $R^n$ the inner product is $v^Tw=v_1w_1+v_2w_2+ \cdot \cdot \cdot+v_nw_n$. Functions are described by a continuing of values $f(x)$ rather than by a discrete collection of components $v_i$. The best parallel to the vector dot product is:</p>
<script type="math/tex; mode=display">
f^Tg=\int_{0}^{2\pi} f(x)g(x)dx</script><p>We integrate from 0 to 2π because Fourier series are periodic: </p>
<script type="math/tex; mode=display">
f(x)=f(x+2\pi)</script><p>The inner product of two basis vectors is zero, as desired. For example, </p>
<script type="math/tex; mode=display">
\begin{equation}
\int_{0}^{2 \pi} \sin x \cos x d x=\left.\frac{1}{2}(\sin x)^{2}\right|_{0} ^{2 \pi}=0
\end{equation}</script><p>How do we find $a_0, a_1$, etc. to find the coordinates or Fourier coefficients of a function in this space? The constant term $a_0$ is the average value of the function. Because we’re working with an orthonormal basis, we can use the inner product to find the coefficients ai. </p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned} \int_{0}^{2 \pi} f(x) \cos x d x &=\int_{0}^{2 \pi}\left(a_{0}+a_{1} \cos x+b_{1} \sin x+a_{2} \cos 2 x+\cdots\right) \cos x d x \\ &=0+\int_{0}^{2 \pi} a_{1} \cos ^{2} x d x+0+0+\cdots \\ &=a_{1} \pi \end{aligned}
\end{equation}</script><p>we conclude that $a_1=\frac{1}{\pi} \int_0^{2\pi}f(x)cosxdx$. We can use the same technique to find any of the values $a_i$.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Math/" rel="tag"># Math</a>
          
            <a href="/tags/Linear-Algebra/" rel="tag"># Linear Algebra</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/16/NLP-TorchText/" rel="next" title="NLP-TorchText">
                <i class="fa fa-chevron-left"></i> NLP-TorchText
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/13/ML-Knowledge/" rel="prev" title="ML-Knowledge">
                ML-Knowledge <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">90</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">68</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#The-geometry-of-linear-equations"><span class="nav-number">1.</span> <span class="nav-text">The geometry of linear equations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Row-Picture"><span class="nav-number">1.1.</span> <span class="nav-text">Row Picture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Column-Picture"><span class="nav-number">1.2.</span> <span class="nav-text">Column Picture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix-Picture"><span class="nav-number">1.3.</span> <span class="nav-text">Matrix Picture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Independence"><span class="nav-number">1.4.</span> <span class="nav-text">Linear Independence</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Elimination-with-Matrices"><span class="nav-number">2.</span> <span class="nav-text">Elimination with Matrices</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverses"><span class="nav-number">2.1.</span> <span class="nav-text">Inverses</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Permutations"><span class="nav-number">2.2.</span> <span class="nav-text">Permutations</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vector-Space"><span class="nav-number">3.</span> <span class="nav-text">Vector Space</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Subspace"><span class="nav-number">3.1.</span> <span class="nav-text">Subspace</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Column-space"><span class="nav-number">3.2.</span> <span class="nav-text">Column space</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nullspace-of-Matrix"><span class="nav-number">3.3.</span> <span class="nav-text">Nullspace of Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#computing-the-nullspace"><span class="nav-number">3.4.</span> <span class="nav-text">computing the nullspace</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reduced-row-echelon-form"><span class="nav-number">3.5.</span> <span class="nav-text">Reduced row echelon form</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Solving-Ax-b"><span class="nav-number">3.6.</span> <span class="nav-text">Solving Ax=b</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Solvability-conditions-on-b"><span class="nav-number">3.6.1.</span> <span class="nav-text">Solvability conditions on b</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Complete-solution"><span class="nav-number">3.6.2.</span> <span class="nav-text">Complete solution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Rank"><span class="nav-number">3.7.</span> <span class="nav-text">Rank</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Full-column-rank"><span class="nav-number">3.7.1.</span> <span class="nav-text">Full column rank</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Full-row-rank"><span class="nav-number">3.7.2.</span> <span class="nav-text">Full row rank</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Full-row-and-column-rank"><span class="nav-number">3.7.3.</span> <span class="nav-text">Full row and column rank</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">3.7.4.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-independence"><span class="nav-number">4.</span> <span class="nav-text">Linear independence</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spanning-a-space"><span class="nav-number">4.1.</span> <span class="nav-text">Spanning a space</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basis-and-dimension"><span class="nav-number">4.2.</span> <span class="nav-text">Basis and dimension</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bases-of-a-column-space-and-nullspace"><span class="nav-number">4.3.</span> <span class="nav-text">Bases of a column space and nullspace</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Four-Fundamental-Subspaces"><span class="nav-number">4.4.</span> <span class="nav-text">The Four Fundamental Subspaces</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Rank-one-matrices"><span class="nav-number">4.5.</span> <span class="nav-text">Rank one matrices</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Orthogonal-amp-Projection"><span class="nav-number">5.</span> <span class="nav-text">Orthogonal&amp;Projection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Nullspace-is-perpendicular-to-row-space"><span class="nav-number">5.1.</span> <span class="nav-text">Nullspace is perpendicular to row space</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Projection"><span class="nav-number">5.2.</span> <span class="nav-text">Projection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Projection-matrix"><span class="nav-number">5.2.1.</span> <span class="nav-text">Projection matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-projection"><span class="nav-number">5.2.2.</span> <span class="nav-text">why projection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Projection-in-higher-dimensions"><span class="nav-number">5.2.3.</span> <span class="nav-text">Projection in higher dimensions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Least-Squares"><span class="nav-number">5.2.4.</span> <span class="nav-text">Least Squares</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-matrix-A-TA"><span class="nav-number">5.2.5.</span> <span class="nav-text">The matrix $A^TA$</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Orthogonal"><span class="nav-number">5.3.</span> <span class="nav-text">Orthogonal</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Orthonormal-vectors"><span class="nav-number">5.3.1.</span> <span class="nav-text">Orthonormal vectors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Orthonormal-matrix"><span class="nav-number">5.3.2.</span> <span class="nav-text">Orthonormal matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-orthonormal"><span class="nav-number">5.3.3.</span> <span class="nav-text">Why orthonormal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gram-Schmidt"><span class="nav-number">5.3.4.</span> <span class="nav-text">Gram-Schmidt</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Determinants-and-Eigenvalues"><span class="nav-number">6.</span> <span class="nav-text">Determinants and Eigenvalues</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Properties-of-Determinants"><span class="nav-number">6.1.</span> <span class="nav-text">Properties of Determinants</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Determinant-Formulas"><span class="nav-number">6.2.</span> <span class="nav-text">Determinant Formulas</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cofactor-formula"><span class="nav-number">6.3.</span> <span class="nav-text">Cofactor formula</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverse-matrix-formula"><span class="nav-number">6.4.</span> <span class="nav-text">Inverse matrix formula</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cramer’s-rule-for-x-A-1-b"><span class="nav-number">6.4.1.</span> <span class="nav-text">Cramer’s rule for $x=A^{-1}b$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#text-det-A-volume-of-box"><span class="nav-number">6.4.2.</span> <span class="nav-text">$|\text{det}A|=$ volume of box</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Eigenvalues-and-eigenvectors"><span class="nav-number">7.</span> <span class="nav-text">Eigenvalues and eigenvectors</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Definition"><span class="nav-number">7.1.</span> <span class="nav-text">Definition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diagonalization-and-powers-of-A"><span class="nav-number">7.2.</span> <span class="nav-text">Diagonalization and powers of $A$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Difference-equations"><span class="nav-number">7.3.</span> <span class="nav-text">Difference equations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fibonacci-sequence"><span class="nav-number">7.3.1.</span> <span class="nav-text">Fibonacci sequence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#e-At-in-differential-equations"><span class="nav-number">7.3.2.</span> <span class="nav-text">$e^{At}$ in differential equations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stability"><span class="nav-number">7.3.3.</span> <span class="nav-text">Stability</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Markov-matrices"><span class="nav-number">7.4.</span> <span class="nav-text">Markov matrices</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fourier-series"><span class="nav-number">7.5.</span> <span class="nav-text">Fourier series</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
