<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,NLP," />










<meta name="description" content="If you’ve ever worked on a project for deep learning for NLP, you’ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to:  Read the data from disk">
<meta name="keywords" content="Deep Learning,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP-TorchText">
<meta property="og:url" content="http://yoursite.com/2019/07/16/NLP-TorchText/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="If you’ve ever worked on a project for deep learning for NLP, you’ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to:  Read the data from disk">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-10.32.59.png">
<meta property="og:image" content="http://yoursite.com/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-12.19.35.png">
<meta property="og:updated_time" content="2019-09-07T15:56:54.726Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP-TorchText">
<meta name="twitter:description" content="If you’ve ever worked on a project for deep learning for NLP, you’ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to:  Read the data from disk">
<meta name="twitter:image" content="http://yoursite.com/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-10.32.59.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/16/NLP-TorchText/"/>





  <title>NLP-TorchText | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/16/NLP-TorchText/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NLP-TorchText</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-16T13:48:30-05:00">
                2019-07-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>If you’ve ever worked on a project for deep learning for NLP, you’ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to:</p>
<ol>
<li>Read the data from disk</li>
<li>Tokenize the text</li>
<li>Create a mapping from word to a unique integer</li>
<li>Convert the text into lists of integers</li>
<li>Load the data in whatever format your deep learning framework requires</li>
<li>Pad the text so that all the sequences are the same length, so you can process them in batch</li>
</ol>
<p><a href="https://github.com/pytorch/text" target="_blank" rel="noopener">Torchtext</a> is a library that makes all the above processing much easier. </p>
<a id="more"></a>
<p>In this post, I’ll demonstrate how torchtext can be used to build and train a text classifier from scratch.To make this tutorial realistic, I’m going to use a small sample of data from <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" target="_blank" rel="noopener">this Kaggle competition</a>. The data and code are available in <a href="https://github.com/keitakurita/practical-torchtext" target="_blank" rel="noopener">GitHub repo</a>, so feel free to clone it and follow along. Or, if you just want to see the minimal working example, feel free to skip the rest of this tutorial and just read the <a href="https://github.com/keitakurita/practical-torchtext/blob/master/Lesson%201:%20intro%20to%20torchtext%20with%20text%20classification.ipynb" target="_blank" rel="noopener">notebook</a>.</p>
<h1 id="The-Overview"><a href="#The-Overview" class="headerlink" title="The Overview"></a>The Overview</h1><p>Torchtext follows the following basic formula for transforming data into working input for your neural network:</p>
<p><img src="/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-10.32.59.png" alt="クリーンショット-2018-02-07-10.32.5"></p>
<p>Torchtext takes in <strong>raw data</strong> in the form of text files, csv/tsv files, json files, and directories (as of now) and <strong>converts them to Datasets</strong>. Datasets are simply preprocessed blocks of data read into memory with various fields.</p>
<p>Torchtext then passes the <strong>Dataset to an Iterator</strong>. Iterators handle <strong>numericalizing, batching, packaging, and moving the data to the GPU</strong>. Basically, it does all the heavy lifting necessary to pass the data to a neural network.</p>
<p>In the following sections, we’ll see how each of these processes plays out in an actual working example.</p>
<h1 id="Declaring-the-Fields"><a href="#Declaring-the-Fields" class="headerlink" title="Declaring the Fields"></a>Declaring the Fields</h1><p>Torchtext takes a declarative approach to loading its data: you tell torchtext how you want the data to look like, and torchtext handles it for you.</p>
<p>The way you do this is by declaring a Field. The Field specifies how you want a certain (you guessed it) field to be processed. Let’s look at an example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field</div>
<div class="line">tokenize = <span class="keyword">lambda</span> x: x.split()</div>
<div class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=tokenize, lower=<span class="keyword">True</span>)</div>
<div class="line"></div>
<div class="line">LABEL = Field(sequential=<span class="keyword">False</span>, use_vocab=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<p>In the toxic comment classification dataset, there are two kinds of fields: the comment text and the labels (toxic, severe toxic, obscene, threat, insult, and identity hate).</p>
<p><img src="/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-12.19.35.png" alt="クリーンショット-2018-02-07-12.19.3"></p>
<p>Let’s look at the LABEL field first, since it’s simpler. All fields, by default, expect a sequence of words to come in, and they expect to build a mapping from the words to integers later on (this mapping is called the vocab, and we will see how it is created later). If you are passing a field that is already numericalized by default and is not sequential, you should pass use_vocab=False and sequential=False.</p>
<p>For the comment text, we pass in the preprocessing we want the field to do as keyword arguments. We give it the tokenizer we want the field to use, tell it to convert the input to lowercase, and also tell it the input is sequential.</p>
<p>In addition to the keyword arguments mentioned above, the Field class also allows the user to specify special tokens (the <code>unk_token</code> for out-of-vocabulary words, the <code>pad_token</code> for padding, the <code>eos_token</code> for the end of a sentence, and an optional <code>init_token</code> for the start of the sentence), choose whether to make the first dimension the batch or the sequence (<strong>the first dimension is the sequence by default</strong>), and choose whether to allow the sequence lengths to be decided at runtime or decided in advance. Fortunately, <a href="https://github.com/pytorch/text/blob/c839a7934930819be7e240ea972e4d600966afdc/torchtext/data/field.py#L61" target="_blank" rel="noopener">the docstrings</a> for the Field class are relatively well written, so if you need some advanced preprocessing you should refer to them for more information.</p>
<p>The field class is at the center of torchtext and is what makes preprocessing such an ease. Aside from the standard field class, here’s a list of the fields that are currently available (along with their use cases):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Field</td>
<td>A regular field that defines preprocessing and postprocessing</td>
<td>Non-text fields and text fields where you don’t need to map integers back to words</td>
</tr>
<tr>
<td>ReversibleField</td>
<td>An extension of the field that allows reverse mapping of word ids to words</td>
<td>Text fields if you want to map the integers back to natural language (such as in the case of language modeling)</td>
</tr>
<tr>
<td>NestedField</td>
<td>A field that takes processes non-tokenized text into a set of smaller fields</td>
<td>Char-based models</td>
</tr>
<tr>
<td>LabelField (New!)</td>
<td>A regular field with sequential=False and no <unk> token. Newly added on the master branch of the torchtext github repo, not yet available for release.</unk></td>
<td>Label fields in text classification.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Constructing-the-Dataset"><a href="#Constructing-the-Dataset" class="headerlink" title="Constructing the Dataset"></a>Constructing the Dataset</h1><p>The fields know what to do when given raw data. Now, we need to tell the fields what data it should work on. This is where we use Datasets.</p>
<p>There are various built-in Datasets in torchtext that handle common data formats. For csv/tsv files, the TabularDataset class is convenient. Here’s how we would read data from a csv file using the TabularDataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</div>
<div class="line"></div>
<div class="line">tv_datafields = [(<span class="string">"id"</span>, <span class="keyword">None</span>), <span class="comment"># we won't be needing the id, so we pass in None as the field</span></div>
<div class="line">                 (<span class="string">"comment_text"</span>, TEXT), (<span class="string">"toxic"</span>, LABEL),</div>
<div class="line">                 (<span class="string">"severe_toxic"</span>, LABEL), (<span class="string">"threat"</span>, LABEL),</div>
<div class="line">                 (<span class="string">"obscene"</span>, LABEL), (<span class="string">"insult"</span>, LABEL),</div>
<div class="line">                 (<span class="string">"identity_hate"</span>, LABEL)]</div>
<div class="line">trn, vld = TabularDataset.splits(</div>
<div class="line">               path=<span class="string">"data"</span>, <span class="comment"># the root directory where the data lies</span></div>
<div class="line">               train=<span class="string">'train.csv'</span>, validation=<span class="string">"valid.csv"</span>,</div>
<div class="line">               format=<span class="string">'csv'</span>,</div>
<div class="line">               skip_header=<span class="keyword">True</span>, <span class="comment"># if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!</span></div>
<div class="line">               fields=tv_datafields)</div>
<div class="line"></div>
<div class="line">tst_datafields = [(<span class="string">"id"</span>, <span class="keyword">None</span>), <span class="comment"># we won't be needing the id, so we pass in None as the field</span></div>
<div class="line">                  (<span class="string">"comment_text"</span>, TEXT)]</div>
<div class="line">tst = TabularDataset(</div>
<div class="line">           path=<span class="string">"data/test.csv"</span>, <span class="comment"># the file path</span></div>
<div class="line">           format=<span class="string">'csv'</span>,</div>
<div class="line">           skip_header=<span class="keyword">True</span>, <span class="comment"># if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!</span></div>
<div class="line">           fields=tst_datafields)</div>
</pre></td></tr></table></figure>
<p>For the TabularDataset, we pass in a list of (name, field) pairs as the fields argument. The fields we pass in must be in the same order as the columns. For the columns we don’t use, we pass in a tuple where the field element is None</p>
<p>The splits method creates a dataset for the train and validation data by applying the same processing. It can also handle the test data, but since out test data has a different format from the train and validation data, we create a different dataset.</p>
<p>Datasets can mostly be treated in the same way as lists. To understand this, it’s instructive to take a look inside our Dataset. Datasets can be indexed and iterated over like normal lists, so let’s see what the first element looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>trn[<span class="number">0</span>]</div>
<div class="line">torchtext.data.example.Example at <span class="number">0x10d3ed3c8</span></div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>trn[<span class="number">0</span>].__dict__.keys()</div>
<div class="line">dict_keys([<span class="string">'comment_text'</span>, <span class="string">'toxic'</span>, <span class="string">'severe_toxic'</span>, <span class="string">'threat'</span>, <span class="string">'obscene'</span>, <span class="string">'insult'</span>, <span class="string">'identity_hate'</span>])</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>trn[<span class="number">0</span>].comment_text[:<span class="number">3</span>]</div>
<div class="line">[<span class="string">'explanation'</span>, <span class="string">'why'</span>, <span class="string">'the'</span>]</div>
</pre></td></tr></table></figure>
<p>we get an Example object. The Example object bundles the attributes of a single data point together. We also see that the text has already been tokenized for us, but has not yet been converted to integers. This makes sense since we have not yet constructed the mapping from words to ids. Constructing this mapping is our next step.</p>
<p>Torchtext handles mapping words to integers, but it has to be told the full range of words it should handle. In our case, we probably want to build the vocabulary on the training set only, so we run the following code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">TEXT.build_vocab(trn)</div>
</pre></td></tr></table></figure>
<p>This makes torchtext go through all the elements in the training set, check the contents corresponding to the <code>TEXT</code> field, and register the words in its vocabulary. Torchtext has its own class called Vocab for handling the vocabulary. The Vocab class holds a mapping from word to id in its <code>stoi</code>attribute and a reverse mapping in its <code>itos</code> attribute. In addition to this, it can automatically build an embedding matrix for you using various pretrained embeddings like word2vec. The Vocab class can also take options like <code>max_size</code> and <code>min_freq</code> that dictate how many words are in the vocabulary or how many times a word has to appear to be registered in the vocabulary. Words that are not included in the vocabulary will be converted into <unk>, a token standing for “unknown”.</unk></p>
<p>Here is a list of the currently available set of datasets and the format of data they take in:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabularDataset</td>
<td>Takes paths to csv/tsv files and json files or Python dictionaries as inputs.</td>
<td>Any problem that involves a label (or labels) for each piece of text</td>
</tr>
<tr>
<td>LanguageModelingDataset</td>
<td>Takes the path to a text file as input.</td>
<td>Language modeling</td>
</tr>
<tr>
<td>TranslationDataset</td>
<td>Takes a path and extensions to a file for each language. e.g. If the files are English: “hoge.en”, French: “hoge.fr”, path=”hoge”, exts=(“en”,”fr”)</td>
<td>Translation</td>
</tr>
<tr>
<td>SequenceTaggingDataset</td>
<td>Takes a path to a file with the input sequence and output sequence separated by tabs. <a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/#easy-footnote-bottom-2-310" target="_blank" rel="noopener">2</a></td>
<td>Sequence taggingNow that we have our data formatted and read into memory, we turn to the next step: creating an Iterator to pass the data to our model.</td>
</tr>
</tbody>
</table>
</div>
<p>Now that we have our data formatted and read into memory, we turn to the next step: creating an Iterator to pass the data to our model.</p>
<h1 id="Constructing-the-Iterator"><a href="#Constructing-the-Iterator" class="headerlink" title="Constructing the Iterator"></a>Constructing the Iterator</h1><p>In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.</p>
<p>Below is code for how you would initialize the Iterators for the train, validation, and test data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator, BucketIterator</div>
<div class="line"></div>
<div class="line">train_iter, val_iter = BucketIterator.splits(</div>
<div class="line"> (trn, vld), <span class="comment"># we pass in the datasets we want the iterator to draw data from</span></div>
<div class="line"> batch_sizes=(<span class="number">64</span>, <span class="number">64</span>),</div>
<div class="line"> device=<span class="number">-1</span>, <span class="comment"># if you want to use the GPU, specify the GPU number here</span></div>
<div class="line"> sort_key=<span class="keyword">lambda</span> x: len(x.comment_text), <span class="comment"># the BucketIterator needs to be told what function it should use to group the data.</span></div>
<div class="line"> sort_within_batch=<span class="keyword">False</span>,</div>
<div class="line"> repeat=<span class="keyword">False</span> <span class="comment"># we pass repeat=False because we want to wrap this Iterator layer.</span></div>
<div class="line">)</div>
<div class="line">test_iter = Iterator(tst, batch_size=<span class="number">64</span>, device=<span class="number">-1</span>, sort=<span class="keyword">False</span>, sort_within_batch=<span class="keyword">False</span>, repeat=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<p>Update: The <code>sort_within_batch</code> argument, when set to True, sorts the data within each minibatch in decreasing order according to the <code>sort_key</code>. This is necessary when you want to use <code>pack_padded_sequence</code> with the padded sequence data and convert the padded sequence tensor to a <code>PackedSequence</code>object.</p>
<p>The BucketIterator is one of the most powerful features of torchtext. It automatically shuffles and buckets the input sequences into sequences of similar length.</p>
<p>The reason this is powerful is that – as I mentioned earlier – we need to pad the input sequences to be of the same length to enable batch processing. For instance, the sequences</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">[ [<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>],</div>
<div class="line">  [<span class="number">4</span>, <span class="number">1</span>],</div>
<div class="line">  [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>] ]</div>
</pre></td></tr></table></figure>
<p>would need to be padded to become</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">[ [<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">0</span>],</div>
<div class="line">  [<span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</div>
<div class="line">  [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>] ]</div>
</pre></td></tr></table></figure>
<p>As you can see, the amount of padding necessary is determined by the longest sequence in the batch. Therefore, padding is most efficient when the sequences are of similar lengths. The BucketIterator does all this behind the scenes. As a word of caution, you need to tell the BucketIterator what attribute you want to bucket the data on. In our case, we want to bucket based on the lengths of the comment_text field, so we pass that in as a keyword argument. See the code above for details on the other arguments.</p>
<p>For the test data, we don’t want to shuffle the data since we’ll be outputting the predictions at the end of training. This is why we use a standard iterator.</p>
<p>Here’s a list of the Iterators that torchtext currently implements:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Iterator</td>
<td>Iterates over the data in the order of the dataset.</td>
<td>Test data, or any other data where the order is important.</td>
</tr>
<tr>
<td>BucketIterator</td>
<td>Buckets sequences of similar lengths together.</td>
<td>Text classification, sequence tagging, etc. (use cases where the input is of variable length)</td>
</tr>
<tr>
<td>BPTTIterator</td>
<td>An iterator built especially for language modeling that also generates the input sequence delayed by one timestep. It also varies the BPTT (backpropagation through time) length. This iterator <a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">deserves its own post</a>, so I’ll omit the details here.</td>
<td>Language modeling</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Wrapping-the-Iterator"><a href="#Wrapping-the-Iterator" class="headerlink" title="Wrapping the Iterator"></a>Wrapping the Iterator</h1><p>Currently, the iterator returns a custom datatype called torchtext.data.Batch. The Batch class has a similar API to the Example type, with a batch of data from each field as attributes. Unfortunately, this custom datatype makes code reuse difficult (since each time the column names change, we need to modify the code), and makes torchtext hard to use with other libraries for some use cases (like torchsample and fastai).</p>
<p>Concretely, we’ll convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data). Here’s the code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
</pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span>:</span></div>
<div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></div>
<div class="line">            self.dl, self.x_var, self.y_vars = dl, x_var, y_vars <span class="comment"># we pass in the list of attributes for x </span></div>
<div class="line"></div>
<div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></div>
<div class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</div>
<div class="line">                  x = getattr(batch, self.x_var) <span class="comment"># we assume only one input in this wrapper</span></div>
<div class="line"></div>
<div class="line">                  <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">None</span>: <span class="comment"># we will concatenate y into a single tensor</span></div>
<div class="line">                        y = torch.cat([getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars], dim=<span class="number">1</span>).float()</div>
<div class="line">                  <span class="keyword">else</span>:</div>
<div class="line">                        y = torch.zeros((<span class="number">1</span>))</div>
<div class="line"></div>
<div class="line">                  <span class="keyword">yield</span> (x, y)</div>
<div class="line"></div>
<div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div>
<div class="line">            <span class="keyword">return</span> len(self.dl)</div>
<div class="line"></div>
<div class="line">train_dl = BatchWrapper(train_iter, <span class="string">"comment_text"</span>, [<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>])</div>
<div class="line">valid_dl = BatchWrapper(val_iter, <span class="string">"comment_text"</span>, [<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>])</div>
<div class="line">test_dl = BatchWrapper(test_iter, <span class="string">"comment_text"</span>, <span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<p>All we’re doing here is converting the batch object to a tuple of inputs and outputs.</p>
<h1 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h1><p>We’ll use a simple LSTM to demonstrate how to train the text classifier on the data we’ve built:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div>
<div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div>
<div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div>
<div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLSTMBaseline</span><span class="params">(nn.Module)</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dim, emb_dim=<span class="number">300</span>, num_linear=<span class="number">1</span>)</span>:</span></div>
<div class="line">        super().__init__() <span class="comment"># don't forget to call this!</span></div>
<div class="line">        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)</div>
<div class="line">        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=<span class="number">1</span>)</div>
<div class="line">        self.linear_layers = []</div>
<div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_linear - <span class="number">1</span>):</div>
<div class="line">            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))</div>
<div class="line">            self.linear_layers = nn.ModuleList(self.linear_layers)</div>
<div class="line">        self.predictor = nn.Linear(hidden_dim, <span class="number">6</span>)</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, seq)</span>:</span></div>
<div class="line">        hdn, _ = self.encoder(self.embedding(seq))</div>
<div class="line">        feature = hdn[<span class="number">-1</span>, :, :]</div>
<div class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linear_layers:</div>
<div class="line">          feature = layer(feature)</div>
<div class="line">          preds = self.predictor(feature)</div>
<div class="line">        <span class="keyword">return</span> preds</div>
<div class="line"></div>
<div class="line">em_sz = <span class="number">100</span></div>
<div class="line">nh = <span class="number">500</span></div>
<div class="line">nl = <span class="number">3</span></div>
<div class="line">model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)</div>
</pre></td></tr></table></figure>
<p>Now, we’ll write the training loop. Thanks to all our preprocessing, this is very simple. We can iterate using our wrapped Iterator, and the data will automatically be passed to us after being moved to the GPU and numericalized appropriately.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tqdm</div>
<div class="line"></div>
<div class="line">opt = optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)</div>
<div class="line">loss_func = nn.BCEWithLogitsLoss()</div>
<div class="line"></div>
<div class="line">epochs = <span class="number">2</span></div>
<div class="line"></div>
<div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</div>
<div class="line">    running_loss = <span class="number">0.0</span></div>
<div class="line">    running_corrects = <span class="number">0</span></div>
<div class="line">    model.train() <span class="comment"># turn on training mode</span></div>
<div class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(train_dl): <span class="comment"># thanks to our wrapper, we can intuitively iterate over our data!</span></div>
<div class="line">        opt.zero_grad()</div>
<div class="line"></div>
<div class="line">        preds = model(x)</div>
<div class="line">        loss = loss_func(y, preds)</div>
<div class="line">        loss.backward()</div>
<div class="line">        opt.step()</div>
<div class="line"></div>
<div class="line">        running_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</div>
<div class="line"></div>
<div class="line">    epoch_loss = running_loss / len(trn)</div>
<div class="line"></div>
<div class="line">    <span class="comment"># calculate the validation loss for this epoch</span></div>
<div class="line">    val_loss = <span class="number">0.0</span></div>
<div class="line">    model.eval() <span class="comment"># turn on evaluation mode</span></div>
<div class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_dl:</div>
<div class="line">        preds = model(x)</div>
<div class="line">        loss = loss_func(y, preds)</div>
<div class="line">        val_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</div>
<div class="line"></div>
<div class="line">    val_loss /= len(vld)</div>
<div class="line">    print(<span class="string">'Epoch: &#123;&#125;, Training Loss: &#123;:.4f&#125;, Validation Loss: &#123;:.4f&#125;'</span>.format(epoch, epoch_loss, val_loss))</div>
</pre></td></tr></table></figure>
<p>There’s not much to explain here: this is just a standard training loop. Now, let’s generate our predictions</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line">test_preds = []</div>
<div class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(test_dl):</div>
<div class="line">    preds = model(x)</div>
<div class="line">    preds = preds.data.numpy()</div>
<div class="line">    <span class="comment"># the actual outputs of the model are logits, so we need to pass these values to the sigmoid function</span></div>
<div class="line">    preds = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-preds))</div>
<div class="line">    test_preds.append(preds)</div>
<div class="line">    test_preds = np.hstack(test_preds)</div>
</pre></td></tr></table></figure>
<p>Finally, we can write our predictions to a csv file.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div>
<div class="line">df = pd.read_csv(<span class="string">"data/test.csv"</span>)</div>
<div class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate([<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>]):</div>
<div class="line">    df[col] = test_preds[:, i]</div>
<div class="line"></div>
<div class="line">df.drop(<span class="string">"comment_text"</span>, axis=<span class="number">1</span>).to_csv(<span class="string">"submission.csv"</span>, index=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<p>And we’re done! We can submit this file to Kaggle, try refining our model, changing the tokenizer, or whatever we feel like, and it will only take a few changes in the code above.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/" target="_blank" rel="noopener">A Comprehensive Introduction to Torchtext (Practical Torchtext part 1)</a> </p>
<p><a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">Language modeling tutorial in torchtext (Practical Torchtext part 2)</a> </p>
<p><a href="http://anie.me/On-Torchtext/" target="_blank" rel="noopener">A Tutorial on Torchtext</a> </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/07/DP-Clasical-CNNs/" rel="next" title="DP-Clasical CNNs">
                <i class="fa fa-chevron-left"></i> DP-Clasical CNNs
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/12/11/Math-Linear-Algebra/" rel="prev" title="Math-Linear Algebra">
                Math-Linear Algebra <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">87</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Overview"><span class="nav-number">1.</span> <span class="nav-text">The Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Declaring-the-Fields"><span class="nav-number">2.</span> <span class="nav-text">Declaring the Fields</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Constructing-the-Dataset"><span class="nav-number">3.</span> <span class="nav-text">Constructing the Dataset</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Constructing-the-Iterator"><span class="nav-number">4.</span> <span class="nav-text">Constructing the Iterator</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Wrapping-the-Iterator"><span class="nav-number">5.</span> <span class="nav-text">Wrapping the Iterator</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training-the-Model"><span class="nav-number">6.</span> <span class="nav-text">Training the Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
