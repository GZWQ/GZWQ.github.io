<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,NLP," />










<meta name="description" content="There’s a whole class of NLP tasks that rely on sequential output, or outputs that are sequences of potentially varying length. For example,  Translation: taking a sentence in one language as input an">
<meta name="keywords" content="Deep Learning,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP-Seq2Seq">
<meta property="og:url" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="There’s a whole class of NLP tasks that rely on sequential output, or outputs that are sequences of potentially varying length. For example,  Translation: taking a sentence in one language as input an">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%209.59.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.04.18%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.24.32%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.15.39%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.16.44%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.18.38%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.18.54%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.19.21%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.35.49%20PM-5062577.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.39.50%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.43.43%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.44.10%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.44.43%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.46.38%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.48.26%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.50.52%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.51.04%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.52.05%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.52.34%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%2010.52.43%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-07-13%20at%205.50.09%20PM.png">
<meta property="og:updated_time" content="2019-08-06T15:27:45.818Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP-Seq2Seq">
<meta name="twitter:description" content="There’s a whole class of NLP tasks that rely on sequential output, or outputs that are sequences of potentially varying length. For example,  Translation: taking a sentence in one language as input an">
<meta name="twitter:image" content="http://yoursite.com/2019/07/01/NLP-Seq2Seq/Screen%20Shot%202019-08-05%20at%209.59.19%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/01/NLP-Seq2Seq/"/>





  <title>NLP-Seq2Seq | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/01/NLP-Seq2Seq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NLP-Seq2Seq</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-01T16:43:32-05:00">
                2019-07-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>There’s a whole class of NLP tasks that rely on sequential output, or outputs that are sequences of potentially varying length. For example,</p>
<ul>
<li>Translation: taking a sentence in one language as input and outputting the same sentence in another language.</li>
<li>Conversation: taking a statement or question as input and responding to it.</li>
<li>Summarization: taking a large body of text as input and outputting a summary of it.</li>
</ul>
<p>Therefore, in this post, sequence-to-sequence models are introduced, a deep learning-based framework for handling these types of problems.</p>
<a id="more"></a>
<h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>The sequence-to-sequence model is an example of a Conditional Language Model.</p>
<ul>
<li><strong>Language Model</strong> because the decoder is predicting the next word of the target sentence y</li>
<li><strong>Conditional</strong> because its predictions are also conditioned on the source sentence x</li>
</ul>
<p>NMT directly calculates $P(y | x)$, </p>
<script type="math/tex; mode=display">
P(y | x)=P\left(y_{1} | x\right) P\left(y_{2} | y_{1}, x\right) P\left(y_{3} | y_{1}, y_{2}, x\right) \ldots P\left(y_{T} | y_{1}, \ldots, y_{T-1}, x\right)</script><p>Here $P\left(y_{T} | y_{1}, \ldots, y_{T-1}, x\right)$ is the probability of next target word given target words so far and source sentence $x$. </p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 9.59.19 PM.png" alt="creen Shot 2019-08-05 at 9.59.19 P"></p>
<h2 id="Output-decoding"><a href="#Output-decoding" class="headerlink" title="Output decoding"></a>Output decoding</h2><h3 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h3><p>When generating next word, we always take the most probable word on next step  and we stop untl the model produces a <code>&lt;END&gt;</code> token.</p>
<p>But this method has no way to undo decisions.</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.04.18 PM.png" alt="creen Shot 2019-08-05 at 10.04.18 P"></p>
<h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><h4 id="Core-idea"><a href="#Core-idea" class="headerlink" title="Core idea"></a>Core idea</h4><p>On each step of decoder, keep track of the <code>k</code> most probable partial translations (which we call hypotheses). Here <code>k</code>  is the beam size.</p>
<p>A hypothesis has a <strong>score</strong> which is its log probability:</p>
<script type="math/tex; mode=display">
\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\log P_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} | x\right)=\sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><ul>
<li>Scores are all negative, and higher score is better</li>
<li>We search for high-scoring hypotheses, tracking top k on each step</li>
</ul>
<h4 id="Stop-criterion"><a href="#Stop-criterion" class="headerlink" title="Stop criterion"></a>Stop criterion</h4><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.24.32 PM.png" alt="creen Shot 2019-08-05 at 10.24.32 P"></p>
<h4 id="Selection-criterion"><a href="#Selection-criterion" class="headerlink" title="Selection criterion"></a>Selection criterion</h4><p>We have our list of completed hypotheses, but How to select top one with highest score?</p>
<p>Say if we choose the hypothese with highest score, which is computed by:</p>
<script type="math/tex; mode=display">
\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\log P_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} | x\right)=\sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><p>Then it will tend to choose the shorter sentence, which usually has a higher score.</p>
<p>In this case, we fix it by normalizing by length:</p>
<script type="math/tex; mode=display">
\frac{1}{t} \sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><p>and use this to select top one.</p>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>Say we set <code>k=2</code>. We start generation with <code>&lt;START&gt;</code> token. Suppose the top two candidates are <code>he</code> and <code>I</code>:</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.15.39 PM.png" alt="creen Shot 2019-08-05 at 10.15.39 P"></p>
<p>Then we keep track of the two lines to generate next words:</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.16.44 PM.png" alt="creen Shot 2019-08-05 at 10.16.44 P"></p>
<p>The new score is parent’s score plus current probablity. Here we have four candidates, but since our <code>k</code> is 2, we choose top 2 among these 4 candidates.</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.18.38 PM.png" alt="creen Shot 2019-08-05 at 10.18.38 P"></p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.18.54 PM.png" alt="creen Shot 2019-08-05 at 10.18.54 P"></p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.19.21 PM.png" alt="creen Shot 2019-08-05 at 10.19.21 P"></p>
<h2 id="Machine-Translation-Evaluation"><a href="#Machine-Translation-Evaluation" class="headerlink" title="Machine Translation Evaluation"></a>Machine Translation Evaluation</h2><p>BLEU (Bilingual Evaluation Understudy) is the one we use for evaluation. BLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on:</p>
<ul>
<li>n-gram precision (usually for 1, 2, 3 and 4-grams)</li>
<li>Plus a penalty for too-short system translations</li>
</ul>
<p>BLEU is useful but imperfect:</p>
<ul>
<li>There are many valid ways to translate a sentence</li>
<li>So a good translation can get a poor BLEU score because it<br>has low n-gram overlap with the human translation </li>
</ul>
<h1 id="The-Attention-Mechanism"><a href="#The-Attention-Mechanism" class="headerlink" title="The Attention Mechanism"></a>The Attention Mechanism</h1><h2 id="Problem-in-Seq2Seq"><a href="#Problem-in-Seq2Seq" class="headerlink" title="Problem in Seq2Seq"></a>Problem in Seq2Seq</h2><p>The fixed-length vector carries the burden of encoding the the entire “meaning” of the input sequence, no matter how long that may be. With all the variance in language, this is a very hard problem. </p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.35.49 PM-5062577.png" alt="creen Shot 2019-08-05 at 10.35.49 PM-506257"></p>
<h2 id="Core-ideas"><a href="#Core-ideas" class="headerlink" title="Core ideas"></a>Core ideas</h2><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.39.50 PM.png" alt="creen Shot 2019-08-05 at 10.39.50 P"></p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.43.43 PM.png" alt="creen Shot 2019-08-05 at 10.43.43 P"></p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.44.10 PM.png" alt="creen Shot 2019-08-05 at 10.44.10 P"></p>
<p>In decoder, insted of just use the hidden state at the first step of the decoder, we operate the dot product between the decoder hidden state with the encoder hidden state of each step in encoder. So that we can get several scalar scores. </p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.44.43 PM.png" alt="creen Shot 2019-08-05 at 10.44.43 P"></p>
<p>Once we have these scores, we use a softmax function to turn the scores into a distribution.</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.46.38 PM.png" alt="creen Shot 2019-08-05 at 10.46.38 P"></p>
<p>Once we have the attention output, we use the attention distribution to take a weighted sum of the encoder hidden states.<img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.48.26 PM.png" alt="creen Shot 2019-08-05 at 10.48.26 P"></p>
<p>Finally Concatenate weighted sum of the encoder hidden states with decoder hidden state, then use to compute $\widehat{y}_{1}$ as before.</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.50.52 PM.png" alt="creen Shot 2019-08-05 at 10.50.52 P"></p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.51.04 PM.png" alt="creen Shot 2019-08-05 at 10.51.04 P"></p>
<h2 id="Attention-in-Equations"><a href="#Attention-in-Equations" class="headerlink" title="Attention in Equations"></a>Attention in Equations</h2><p>Remember that our seqzseq model is made of two parts, an encoder that encodes the input sentence, and a decoder that leverages the information extracted by the decoder to produce the translated sentence. Basically, our input is a sequence of words $x_1,…, x_n$ that we want to translate, and our target sentence is a sequence of words $y_1,…,y_m$.</p>
<ol>
<li><p>Encoder</p>
<p>Let $(h_1,., h_n)$ be the hidden vectors representing the input sentence. These vectors are the output of a BI-LSTM for instance, ane capture contextual representation of each word in the sentence</p>
</li>
<li><p>Decoder</p>
<p>We want to compute the hidden states $s_i$ of the decoder using a recursive formula of the form </p>
</li>
</ol>
<script type="math/tex; mode=display">
   s_i =f (s_{i-1}, y_{i-1}, c_i)</script><p>   where $s_{i-1}$ is the previous hidden vector, $y_{i-1}$ is the generated word at the previous step, and $c_i$ is a context vector that capture the context from the original sentence that is relevant to the time step i of the decoder. The context vector $c_i$ captures relevant information for the i-th decoding time step (unlike the standard Seg2 seq in which theres only one context vector). For each hidden vector from the original sentence hi, compute a score</p>
<script type="math/tex; mode=display">
    e_{i,i}= a (s_{i-1}, h_i)</script><p>   where a is any function, for instance a single layer fully-connected neural network. Then we end up with a sequence of scalar value $e_{i,1},…, e_{i,n}$. Normalize these scores into a vector $\alpha_{i}=\left(\alpha_{i, 1}, \ldots, \alpha_{i, n}\right)$, using a softmax layer.</p>
<script type="math/tex; mode=display">
   \alpha_{i, j}=\frac{\exp \left(e_{i, j}\right)}{\sum_{k=1}^{n} \exp \left(e_{i, k}\right)}</script><p>   Then, compute the context vector $c_i$ as the weighted average of the hidden vectors from the original sentence</p>
<script type="math/tex; mode=display">
   c_{i}=\sum_{j=1}^{n} \alpha_{i, j} h_{j}</script><p>   Intuitively, this vector captures the relevant contextual information from the original sentence for the i-th step of the decoder.</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.52.05 PM.png" alt="creen Shot 2019-08-05 at 10.52.05 P"></p>
<h2 id="Attention-Variants"><a href="#Attention-Variants" class="headerlink" title="Attention Variants"></a>Attention Variants</h2><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.52.34 PM.png" alt="creen Shot 2019-08-05 at 10.52.34 P"></p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.52.43 PM.png" alt="creen Shot 2019-08-05 at 10.52.43 P"></p>
<h2 id="Conncetion-with-translation-alignment"><a href="#Conncetion-with-translation-alignment" class="headerlink" title="Conncetion with translation alignment"></a>Conncetion with translation alignment</h2><p>The attention-based model learns to assign significance to different parts of the input for each step of the output. In the context of translation, attention can be thought of as “alignment.” Bahdanau et al. argue that the attention scores aij at decoding step i signify the words in the source sentence that align with word i in the target. Noting this, we can use attention scores to build an alignment table – a table mapping words in the source to corresponding words in the target sentence – based on the learned encoder and decoder from our Seq2Seq NMT system.</p>
<p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-07-13 at 5.50.09 PM.png" alt="creen Shot 2019-07-13 at 5.50.09 P"></p>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a><a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank" rel="noopener">Keras</a></h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/12/NLP-Dependency-Parser/" rel="next" title="NLP-Dependency Parser">
                <i class="fa fa-chevron-left"></i> NLP-Dependency Parser
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/07/DP-Clasical-CNNs/" rel="prev" title="DP-Clasical CNNs">
                DP-Clasical CNNs <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Seq2Seq-Model"><span class="nav-number">1.</span> <span class="nav-text">The Seq2Seq Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Output-decoding"><span class="nav-number">1.1.</span> <span class="nav-text">Output decoding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-decoding"><span class="nav-number">1.1.1.</span> <span class="nav-text">Greedy decoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Beam-search-decoding"><span class="nav-number">1.1.2.</span> <span class="nav-text">Beam search decoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Core-idea"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Core idea</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Stop-criterion"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Stop criterion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selection-criterion"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">Selection criterion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">Example</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Machine-Translation-Evaluation"><span class="nav-number">1.2.</span> <span class="nav-text">Machine Translation Evaluation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Attention-Mechanism"><span class="nav-number">2.</span> <span class="nav-text">The Attention Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-in-Seq2Seq"><span class="nav-number">2.1.</span> <span class="nav-text">Problem in Seq2Seq</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Core-ideas"><span class="nav-number">2.2.</span> <span class="nav-text">Core ideas</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-in-Equations"><span class="nav-number">2.3.</span> <span class="nav-text">Attention in Equations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-Variants"><span class="nav-number">2.4.</span> <span class="nav-text">Attention Variants</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conncetion-with-translation-alignment"><span class="nav-number">2.5.</span> <span class="nav-text">Conncetion with translation alignment</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Implementation"><span class="nav-number">3.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras"><span class="nav-number">3.1.</span> <span class="nav-text">Keras</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
