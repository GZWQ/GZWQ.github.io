<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Neural Network," />










<meta name="description" content="本节介绍最基本的神经网络。">
<meta name="keywords" content="Deep Learning,Neural Network">
<meta property="og:type" content="article">
<meta property="og:title" content="DP-Neural Network">
<meta property="og:url" content="http://yoursite.com/2019/06/11/DP-Neural-Network/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="本节介绍最基本的神经网络。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/tikz0.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/perceptron-0271060.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/tikz2.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/Screen%20Shot%202018-08-23%20at%208.45.15%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/Screen%20Shot%202018-08-23%20at%208.46.24%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/tikz11.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/2256672-bfbb364740f898d1.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/2256672-c1388dc8fdcce427.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/2256672-6f27ced45cf5c0d8.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/2256672-6f27ced45cf5c0d8-5514755.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/Screen%20Shot%202019-02-13%20at%2010.26.59%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/Screen%20Shot%202018-08-29%20at%2010.22.09%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/Screen%20Shot%202018-09-02%20at%204.03.40%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/Screen%20Shot%202019-03-07%20at%204.24.27%20PM.png">
<meta property="og:updated_time" content="2019-06-11T16:41:35.104Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DP-Neural Network">
<meta name="twitter:description" content="本节介绍最基本的神经网络。">
<meta name="twitter:image" content="http://yoursite.com/2019/06/11/DP-Neural-Network/tikz0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/06/11/DP-Neural-Network/"/>





  <title>DP-Neural Network | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/11/DP-Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DP-Neural Network</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-11T11:30:52-05:00">
                2019-06-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本节介绍最基本的神经网络。</p>
<a id="more"></a>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><p><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap1/c1s1.html" target="_blank" rel="noopener">ref1</a> <a href="https://www.leiphone.com/news/201706/QFydbeV7FXQtRIOl.html" target="_blank" rel="noopener">ref2</a> </p>
<h3 id="二分类线性模型"><a href="#二分类线性模型" class="headerlink" title="二分类线性模型"></a>二分类线性模型</h3><p>感知机作为一种二元线性分类模型，能（且一定能）将线性可分的数据集分开。什么叫线性可分？在二维平面上、线性可分意味着能用一条线将正负样本分开，在三维空间中、线性可分意味着能用一个平面将正负样本分开。</p>
<p>虽然简单，但它既可以发展成支持向量机（通过简单地修改损失函数），又可以发展为神经网络（通过简单地叠加），它的模型如下：</p>
<p><img src="/2019/06/11/DP-Neural-Network/tikz0.png" alt="ikz"></p>
<p>感知机接受几个输入$x_i$，对每一个权重赋予一个权重衡量该输入对输入的重要性，其输出为0或者1，由加权和$\sum_{j}{w_jx_j}$是否小于或者大于某一个阈值决定。和权重一样，阈值也是一个实数，同时它是神经元的一个参数。使用更严密的代数形式来表示：</p>
<script type="math/tex; mode=display">
\begin{eqnarray} \mbox{output} & = & \left\{ \begin{array}{ll} 0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\ 1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}</script><p>以上为感知机的工作方式。鉴于上述表示方法过于繁琐，我们通过使用两个新记法来简化它。</p>
<p>第一个是使用点乘代替$\sum_{j}w_jx_j$：$w\cdot x \equiv \sum_{j}w_jx_j$</p>
<p>第二个是将阈值移到不等式的另一侧，并使用偏置(bias)来代替阈值：$bias \equiv -threshold$ </p>
<script type="math/tex; mode=display">
\begin{eqnarray} \mbox{output} = \left\{ \begin{array}{ll} 0 & \mbox{if } w\cdot x + b \leq 0 \\ 1 & \mbox{if } w\cdot x + b > 0 \end{array} \right. \tag{2}\end{eqnarray}</script><p>如下图所示。</p>
<p><img src="/2019/06/11/DP-Neural-Network/perceptron-0271060.png" alt="erceptron-027106"></p>
<p>可以将偏置理解为感知机为了得到输出为1的容易度的度量，如果一个感知机的偏置非常大，那么这个感知机的输出很容易为1，相反如果偏置非常小，那么输出1就很困难。</p>
<h3 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><blockquote>
<p>为确定感知机模型参数$w$和$b$,需要定义一个损失函数并将损失函数最小化。样本点到超平面的距离:</p>
</blockquote>
<script type="math/tex; mode=display">
\frac{1}{||w||}{|w\times{x}+b|}</script><p>$||w||$是$w$的$L_2$范数，为保计算结果为正，对$w\times x+b$进行绝对值运算。</p>
<blockquote>
<p>对于误分类的数据$(x_i,y_i)$来说，$-y_i(w\times{x_i}+b)&gt;0$，假设超平面$S$的误分类点集合为$M$，那么损失函数为所有误分类点到平面距离:</p>
</blockquote>
 
$$
L(w,b)=-\frac{1}{||w||}\sum_{x_i\in{M}}{{y_i(w\times{x_i}+b)}}
$$
$\frac{1}{||w||}$是常量，不影响，$L(w,b)=-\sum_{x_i\in{M}}{{y_i(w\times{x_i}+b)}}$


<blockquote>
<p>采用梯度下降法，对于一个误分类样本点$(x_i,y_i)$，$\frac{\partial L}{\partial w}=-\eta{y_ix_i}$，$\frac{\partial{L}}{\partial{b}}=-\eta{y_i}$。</p>
<p>则参数更新公式为：</p>
<p>$w=w+\eta y_ix_i, b = b+\eta y_i$ </p>
</blockquote>
<h3 id="逻辑计算"><a href="#逻辑计算" class="headerlink" title="逻辑计算"></a>逻辑计算</h3><p>感知机可以用于计算初等逻辑函数：与、或、非。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">AND</th>
<th style="text-align:center">OR</th>
<th style="text-align:center">NOT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>感知机的输入是两个取值0、1的变量，</p>
<p>对于$AND$，它们对输出的贡献是相同的，则权重相同，我们取权重为-2和-2，但是发现4个里面才一个1，得到1比较困难，故有较高的阈值，设置阈值为3。</p>
<p><img src="/2019/06/11/DP-Neural-Network/tikz2.png" alt="ikz"></p>
<p>对于$OR$，权重相同，但是得到1比较容易，所以取较小的阈值，设置阈值为0.5。</p>
<p>对于$NOT$，发现只有一个输入，则只有一个输入对输出有影响，所以权重不同，$w_1=-1, w_2=0$，阈值为$-0.5$。</p>
<h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。</p>
<p>（1）M-P神经元模型：</p>
<p>神经元接收来自$n$个其他神经元传递过来的输入信号，这些输入信号通过待权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过”激活函数”处理以产生神经元的输出。</p>
<p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-08-23 at 8.45.15 PM.png" alt="creen Shot 2018-08-23 at 8.45.15 P"></p>
<p>（2）激活函数</p>
<p>采用（a）阶跃函数作为激活函数，它将输入值映射为输出值“1”（对应于神经元兴奋）或“0”（神经元抑制），可是阶跃函数不连续、不光滑，故采用sigmoid函数作为激活函数。</p>
<p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-08-23 at 8.46.24 PM.png" alt="creen Shot 2018-08-23 at 8.46.24 P"></p>
<p>故我们称上述神经元为sigmoid神经元。</p>
<blockquote>
<p>感知机到sigmoid神经元的motivation：</p>
<p>We want a samll change in a weight (or bias) to cause a small change in output.</p>
<p>Compared to perceptron, both inputs and outputs of sigmoid neural can take on any values between 0 and 1 instead of just 0 or 1.</p>
</blockquote>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><p>多个sigmoid神经元构成复杂的神经网络。网络的最左边的一层被称为输入层，其中的神经元被称为输入神经元(input neurons)；最右边的一层是输出层(output layer)，包含的神经元被称为输出神经元(output neurons)。下图中我们的输出层只有一个神经元，网络的中间一层被称为隐层(hidden layer)，因为它既不是输入层，也不是输出层。</p>
<p><img src="/2019/06/11/DP-Neural-Network/tikz11.png" alt="ikz1"></p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p><a href="https://ilewseu.github.io/2017/12/17/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/" target="_blank" rel="noopener">ref1</a> <a href="https://zhuanlan.zhihu.com/p/34378516" target="_blank" rel="noopener">ref2</a> <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">gotta</a> </p>
<p>神经网络的前向传播实质就是一个输入向量$\vec x$到输出向量$\vec y$的函数。下面我们使用一个例子来说明这个过程</p>
<p><img src="/2019/06/11/DP-Neural-Network/2256672-bfbb364740f898d1.png" alt="256672-bfbb364740f898d"></p>
<p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$w_{41}, w_{42},w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？</p>
<script type="math/tex; mode=display">
\begin{align}
a_4&=sigmoid(\vec{w}^T\centerdot\vec{x})\\
&=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})
\end{align}</script><p>其中$w_{4b}$是节点4的偏置项，图中没有画出来。而$w_{41}, w_{42},w_{43}$分别为节点1、2、3到节点4连接的权重，在给权重编号时，我们把目标节点的编号$j$放在前面，把源节点的编号$i$放在后面。</p>
<p>同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$：</p>
<script type="math/tex; mode=display">
\begin{align}
y_1&=sigmoid(\vec{w}^T\centerdot\vec{a})\\
&=sigmoid(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})
\end{align}</script><p>同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$时，神经网络的输出向量$\vec{y}=\begin{bmatrix}y_1\\y_2\end{bmatrix}$。这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong>。 </p>
<h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便，我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：</p>
<script type="math/tex; mode=display">
a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\
a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\
a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\
a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\</script><p>接着，定义网络的输入向量和隐藏层每个节点的权重向量$\vec w_j$，</p>
<script type="math/tex; mode=display">
\begin{align}
\vec{x}&=\begin{bmatrix}x_1\\x_2\\x_3\\1\end{bmatrix}\\
\vec{w}_4&=[w_{41},w_{42},w_{43},w_{4b}]\\
\vec{w}_5&=[w_{51},w_{52},w_{53},w_{5b}]\\
\vec{w}_6&=[w_{61},w_{62},w_{63},w_{6b}]\\
\vec{w}_7&=[w_{71},w_{72},w_{73},w_{7b}]\\
f&=sigmoid\\

\end{align}</script><p>代入得：</p>
<script type="math/tex; mode=display">
\begin{align}
a_4&=f(\vec{w_4}\centerdot\vec{x})\\
a_5&=f(\vec{w_5}\centerdot\vec{x})\\
a_6&=f(\vec{w_6}\centerdot\vec{x})\\
a_7&=f(\vec{w_7}\centerdot\vec{x})
\end{align}</script><p>如果把$a_4,a_5,a_6,a_7$表示成一个矩阵，</p>
<script type="math/tex; mode=display">
\vec{a}=
\begin{bmatrix}
a_4 \\
a_5 \\
a_6 \\
a_7 \\
\end{bmatrix},\qquad W=
\begin{bmatrix}
\vec{w}_4 \\
\vec{w}_5 \\
\vec{w}_6 \\
\vec{w}_7 \\
\end{bmatrix}=
\begin{bmatrix}
w_{41},w_{42},w_{43},w_{4b} \\
w_{51},w_{52},w_{53},w_{5b} \\
w_{61},w_{62},w_{63},w_{6b} \\
w_{71},w_{72},w_{73},w_{7b} \\
\end{bmatrix}
,\qquad f(
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
.\\
.\\
.\\
\end{bmatrix})=
\begin{bmatrix}
f(x_1)\\
f(x_2)\\
f(x_3)\\
.\\
.\\
.\\
\end{bmatrix}</script><p>代入得：</p>
<script type="math/tex; mode=display">
\vec{a}=f(W\centerdot\vec{x})\qquad</script><p>上式中，$f$是激活函数；$W$是某一层的权重矩阵；$\vec x$是某层的输入向量；$\vec a$是某层的输出向量。上式说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p>
<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$W_1, W_2,W_3,W_4$，每个隐藏层的输出分别是$\vec a_1, \vec a_2, \vec a_3$，神经网络的输入为$\vec x$，神经网络的输出为$\vec y$，如下图所示：</p>
<p><img src="/2019/06/11/DP-Neural-Network/2256672-c1388dc8fdcce427.png" alt="256672-c1388dc8fdcce42"></p>
<p>则每一层的输出向量的计算可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
&\vec{a}_1=f(W_1\centerdot\vec{x})\\
&\vec{a}_2=f(W_2\centerdot\vec{a}_1)\\
&\vec{a}_3=f(W_3\centerdot\vec{a}_2)\\
&\vec{y}=f(W_4\centerdot\vec{a}_3)\\
\end{align}</script><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p>
<p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<p>我们假设每个训练样本为$(\vec{x},\vec{t})$，其中向量$\vec x$是训练样本的特征，而$\vec t$是样本的目标值。</p>
<p><img src="/2019/06/11/DP-Neural-Network/2256672-6f27ced45cf5c0d8.png" alt="256672-6f27ced45cf5c0d"></p>
<p>首先，我们根据前向传播算法，用样本的特征$\vec x$，计算出神经网络中每个隐藏层节点的输出$a_i$，以及输出层每个节点的输出$y_i$。</p>
<p>然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$：</p>
<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>对于输出层的节点$i$ </p>
<script type="math/tex; mode=display">
\delta_i=y_i(1-y_i)(y_i-t_i)\qquad</script><p>其中$\delta_i$是节点$i$的误差项，$y_i$是节点$i$的输出值，$t_i$是样本对应于节点$i$的目标值。比如：节点8的误差是$\delta_8=y_1(1-y_1)(t_1-y_1)$。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p>按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用<strong>随机梯度下降</strong>优化算法去求目标函数最小值时的参数值。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：</p>
<script type="math/tex; mode=display">
E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2</script><p>其中，$E_d$表示是样本$d$的误差，展开得到：</p>
<script type="math/tex; mode=display">
E_d = E_{y_1}+E_{y_2}=\frac{1}{2}(t{_1}-{y_1})^2+\frac{1}{2}(t{_2}-{y_2})^2</script><p>然后我们用<strong>随机梯度下降</strong>算法对目标函数进行优化：</p>
<script type="math/tex; mode=display">
w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}</script><p>随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？</p>
<p><img src="/2019/06/11/DP-Neural-Network/2256672-6f27ced45cf5c0d8-5514755.png" alt="256672-6f27ced45cf5c0d8-551475"></p>
<p>对于隐藏层与输出层间的权值，我们需要计算$\frac{\partial E_d}{\partial w_{ji} }$，由链式法则：</p>
<script type="math/tex; mode=display">
\frac{\partial E_d}{\partial w_{ji} }=\frac{\partial E_d}{y_j}\frac{y_i}{net_j}\frac{net_j}{w_{ji}}</script><p>其中，$net_j$是是节点$j$的<strong>加权输入</strong>，即$ net_j=\vec{w_j}\centerdot\vec{x_j}=\sum_{i}{w_{ji}}x_{ji} $ </p>
<p>$y_i$是激活函数处理之后的输出值。</p>
<p>考虑第一项：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial{E_d}}{\partial{y_j}}&=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2\\
&=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2\\
&=y_j-t_j
\end{align}</script><p>考虑第二项：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial{y_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\
&=y_j(1-y_j)\\
\end{align}</script><blockquote>
<p><a href="https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/" target="_blank" rel="noopener">source: Derivation: Derivatives for Common Neural Network Activation Functions</a>  </p>
<p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2019-02-13 at 10.26.59 AM.png" alt="creen Shot 2019-02-13 at 10.26.59 A"></p>
</blockquote>
<p>将第一项和第二项带入，得到：</p>
<script type="math/tex; mode=display">
\frac{\partial{E_d}}{\partial{net_j}}=(y_j-t_j)y_j(1-y_j)</script><p>如果令$\sigma_j=\frac{\partial{E_d}}{\partial{net_j}}$来表示该节点的误差，则</p>
<script type="math/tex; mode=display">
\sigma_j =(y_j-t_j)y_j(1-y_j)</script><p>将上述推导带入随机梯度下降公式，得到：</p>
<script type="math/tex; mode=display">
\begin{align}
w_{ji}&\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}\\
&=w_{ji}+\eta(t_j-y_j)y_j(1-y_j)x_{ji}\\
&=w_{ji}+\eta\delta_jx_{ji}
\end{align}</script><p>这样我们就得到了隐藏层权重的更新公式。</p>

</div></div> 
<h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><script type="math/tex; mode=display">
\delta_i=a_i(1-a_i)\sum_{k\in{outputs}}w_{ki}\delta_k\qquad</script><p>其中，$a_i$是节点的输出值，$w_{ki}$是节点$i$到它的下一层节点$k$的连接的权重，$\sigma_k$是节点$i$的下一层节点$k$的误差项。例如，对于隐藏层节点4来说，计算方法为：$\delta_4=a_4(1-a_4)(w_{84}\delta_8+w_{94}\delta_9)$ </p>
<p>最后更新每个连接上的权值：</p>
<script type="math/tex; mode=display">
w_{ji}\gets w_{ji}+\eta\delta_jx_{ji}\qquad</script><p>其中，$w_ji$是节点$i$到节点$j$的权重，$\eta$是学习速率，$\sigma_j$是节点$j$的误差项，$x_{ji}$是节点$i$传递给节点$j$的输入。例如，权重的更新方法如下：$w_{84}\gets w_{84}+\eta\delta_8 a_4$；类似的，权重$w_{41}$的更新方法如下：$w_{41}\gets w_{41}+\eta\delta_4 x_1$；</p>
<p>偏置项的输入值永远为1。例如，节点4的偏置项$w_{4b}$应该按照下面的方法计算： $w_{4b}\gets w_{4b}+\eta\delta_4$ </p>
<p>显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<p>同样地，我们需要计算$\frac{\partial E_d } {\partial w_{ji} }$,由链式法则：</p>
<script type="math/tex; mode=display">
\frac{\partial{E_d}}{\partial{w_{ji}}}={\frac{\partial{E_d}}{\partial{a_j}}}{\frac{\partial{a_j}}{\partial{net_j}}}{\frac{\partial{net_j}}{\partial{w_{ji}}}}</script><p>考虑第一项：</p>
<p>首先，我们需要定义节点$j$的所有直接下游节点的集合$Downstream(j)$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$a_j$只能通过影响$Downstream(j)$再影响$E_d$。设$net_j$是节点$j$的下游节点的输入，则</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
{\frac{\partial{E_d}}{\partial{a_j}}}
&=\sum_{k\in Downstream(j)}{\frac{\partial{E_d}}{\partial{net_k}}}{\frac{\partial{net_k}}{\partial{a_j}}}\\
&=\sum_{k\in Downstream(j)}\sigma_k{\frac{\partial{net_k}}{\partial{a_j}}}\\
&=\sum_{k\in Downstream(j)}\sigma_k w_{kj}\\
\end{aligned}
\end{equation}</script><p>考虑第二项：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial{a_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\
&=a_j(1-a_j)\\
\end{align}</script><p>综合第一项和第二项：</p>
<script type="math/tex; mode=display">
{\frac{\partial{E_d}}{\partial{net_j}}}=a_j(1-a_j)\sum_{k\in Downstream(j)}\sigma_k w_{kj}</script><p>令$\delta_j=\frac{\partial E_d}{ \partial net_j }$，则</p>
<script type="math/tex; mode=display">
\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}</script>
</div></div> 
<h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><p>假设我们三层神经网络，即输入层、隐藏层和输出层，使用$L_2$损失函数。</p>
<ul>
<li>输入是<code>x</code>，令<code>a1=x</code>。</li>
<li>前向传播<ul>
<li>输入层-隐藏层的前向传播：<code>z2=a1*w1</code></li>
<li>输入层-隐藏层的激活函数：<code>a2=sigmoid(z2)</code></li>
<li>隐藏层-输出层的前向传播：<code>z3=a2*w2</code></li>
<li>隐藏层-输出层的激活函数：<code>h=sigmoid(z3)</code></li>
</ul>
</li>
<li>后向传播<ul>
<li>输出层-隐藏层的损失：<code>h_err=(h-y)*sigmoid_prime(h)</code></li>
<li>输出层-隐藏层的梯度：<code>h_delta=h_err*a2</code></li>
<li>隐藏层-输入层的损失：<code>z2_err=(h_err*w2)*sigmoid_prime(a2)</code></li>
<li>隐藏层-输入层的梯度：<code>z2_delta=z2_err*a1</code></li>
<li>更新梯度：<code>w1 -= z2_delta</code>,<code>w2 -= h_delta</code></li>
</ul>
</li>
</ul>
<h2 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h2><h3 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a><a href="https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python" target="_blank" rel="noopener">Toy Example</a></h3><p>在该例子中，我们将建模一个具有二输入、一输出和一隐藏层的神经网络，该网络用于预测考试成绩基于两个输入：学习时间和睡觉时间。以下为训练样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">学习时间</th>
<th style="text-align:center">睡觉时间</th>
<th style="text-align:center">考试成绩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">9</td>
<td style="text-align:center">92</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">5</td>
<td style="text-align:center">86</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">6</td>
<td style="text-align:center">89</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">8</td>
<td style="text-align:center">？</td>
</tr>
</tbody>
</table>
</div>
<h4 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h4><p>考虑到时间是小时制，而考试成绩的百分制，所以我们想要将数据标准化，故对每一个变量我们都除以它的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line">x = np.array(([<span class="number">2</span>,<span class="number">9</span>],[<span class="number">1</span>,<span class="number">5</span>],[<span class="number">3</span>,<span class="number">6</span>]),dtype=float)</div>
<div class="line">y = np.array(([<span class="number">92</span>],[<span class="number">86</span>],[<span class="number">89</span>]),dtype=float)</div>
<div class="line">x = x/np.max(x,axis=<span class="number">0</span>)</div>
<div class="line">y = y/<span class="number">100</span></div>
</pre></td></tr></table></figure>
<p>然后，我们定义一个类<code>class</code>和初始化函数<code>init</code>，在<code>init</code>函数中我们定义神经网络参数，如输入层，隐藏层和输出层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neural_Network</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div>
<div class="line">        self.input_size = <span class="number">2</span></div>
<div class="line">        self.output_size = <span class="number">1</span></div>
<div class="line">        self.hidden_size = <span class="number">3</span></div>
<div class="line">        self.w1 = np.random.randn(self.hidden_size,self.input_size)</div>
<div class="line">        self.w2 = np.random.randn(self.output_size,self.hidden_size)</div>
</pre></td></tr></table></figure>
<p>变量定义好之后，我们便可以写前向传播函数了，在前向传播中，我们要完成的工作有输入与权重的点乘，再应用激活函数；隐层与权重的点乘，再应用一个激活函数得到输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></div>
<div class="line">    self.z2 = x.dot(self.w1.T) <span class="comment"># x.shape=(n,2) , w1.shape=(3,2),z1.shape=(n,3)</span></div>
<div class="line">    self.a2 = self.sigmoid(self.z2)</div>
<div class="line">    self.z3 = self.a2.dot(self.w2.T)</div>
<div class="line">    o = self.sigmoid(self.z3)</div>
<div class="line">    <span class="keyword">return</span> o</div>
</pre></td></tr></table></figure>
<p>所以，我们还需要定义一个<code>sigmoid</code>激活函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</div>
</pre></td></tr></table></figure>
<h4 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h4><p>第一步我们需要定义一个损失函数，我们使用<code>MSE</code>作为损失函数</p>
<script type="math/tex; mode=display">
Loss=\frac{1}{2m}\sum_{i=1}^{m}(o_i-y_i)</script><p>其中，$o_i$是对$i_{th}$样本的预测，$y_i$是该样本的真实输出。</p>
<p>一旦定义好了损失函数，我们的目标就是使损失函数的值不断逼近0，即我们需要优化损失函数，我们需要激活函数的导数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(self,z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> z*(<span class="number">1</span>-z)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,x,y,o)</span>:</span></div>
<div class="line">    self.o_err = o-y <span class="comment">#shape=(n,output_size)</span></div>
<div class="line">    self.o_delta = self.o_err*self.sigmoid_prime(o) <span class="comment">#shape=(n,output_size)</span></div>
<div class="line"></div>
<div class="line">    self.z2_err = self.o_delta.dot(self.w2) <span class="comment">#(n,hidden_size)</span></div>
<div class="line">    self.z2_delta = self.z2_err*self.sigmoid_prime(self.a2) <span class="comment">#(n,hidden_size)</span></div>
<div class="line"></div>
<div class="line">    self.w1 -= self.z2_delta.T.dot(x)</div>
<div class="line">    self.w2 -= self.o_delta.T.dot(self.a2)</div>
</pre></td></tr></table></figure>
<p>最后，我们便可以定义一个训练函数，它完成前向传播和后向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,x,y)</span>:</span></div>
<div class="line">    o = self.forward(x)</div>
<div class="line">    self.backward(x,y,o)</div>
</pre></td></tr></table></figure>
<p>最后我们定义<code>main</code>函数，训练模型1000次</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    nn = Neural_Network()</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div>
<div class="line">        o = nn.forward(x)</div>
<div class="line">        <span class="comment"># print("Predicted Output:",o)</span></div>
<div class="line">        print(<span class="string">"LOSS:"</span>,np.mean(np.square(y-o)))</div>
<div class="line">        nn.train(x,y)</div>
</pre></td></tr></table></figure>
<h3 id="MNIST手写体数字分类"><a href="#MNIST手写体数字分类" class="headerlink" title="MNIST手写体数字分类"></a><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/" target="_blank" rel="noopener">MNIST手写体数字分类</a></h3><p>本例我们介绍如何使用神经网络进行多分类任务，数据集$ex3data1.mat$包含了5000个训练样本，每个样本都是$20\times 20$像素大小的灰度图片，这$20\times 20$大小的 图片被展开成一个400维的向量，也就是说，我们的训练数据集是一个$5000\times 400$的矩阵。同样地，图片的标签被记录在大小为5000维向量$y$里，范围从1-10。</p>
<p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-08-29 at 10.22.09 PM.png" alt="creen Shot 2018-08-29 at 10.22.09 P"></p>
<p>我们就构建一个最简单的神经网络：输入层-隐藏层-输出层。</p>
<p><strong>输入层神经元个数</strong>：$input_size=20\times 20 =400$</p>
<p><strong>隐藏层神经元个数</strong>：$hidden_size=25$</p>
<p><strong>输出层神经元个数</strong>：$output_size=10$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line">data = scipy.io.loadmat(<span class="string">'dataset/ex3data1.mat'</span>)</div>
<div class="line">x = data[<span class="string">'X'</span>]</div>
<div class="line">y = data[<span class="string">'y'</span>]</div>
<div class="line">encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</div>
<div class="line">y = encoder.fit_transform(y)</div>
<div class="line"></div>
<div class="line">input_size = <span class="number">400</span></div>
<div class="line">hidden_size = <span class="number">25</span></div>
<div class="line">output_size = <span class="number">10</span></div>
<div class="line">learning_rate = <span class="number">1</span></div>
</pre></td></tr></table></figure>
<p>那么<strong>输入层和隐藏层</strong>之间的参数个数：$\theta_1=hidden_size \times (input_size+1)=25\times 401=10025$</p>
<p>输出层和隐藏层之间的参数个数：</p>
<p>$\theta_2=output_size \times (hidden_size+1)=10\times 26=260$</p>
<blockquote>
<p>上式中的+1是偏置神经元。</p>
</blockquote>
<p>所以这个神经网络的参数总数为：</p>
<p>$\theta=10025+260=10285$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">theta = np.random.randn(hidden_size*(input_size+<span class="number">1</span>)+output_size*(hidden_size+<span class="number">1</span>))</div>
<div class="line">theta1 = theta[<span class="number">0</span>:hidden_size*(input_size+<span class="number">1</span>)]</div>
<div class="line">theta1 = theta1.reshape(hidden_size,input_size+<span class="number">1</span>)</div>
<div class="line">theta2 = theta[hidden_size*(input_size+<span class="number">1</span>):]</div>
<div class="line">theta2 = theta2.reshape(output_size,hidden_size+<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<blockquote>
<p>不要将权重参数初始化为0</p>
</blockquote>
<h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">displayData</span><span class="params">(self,example_width=None, figsize=<span class="params">(<span class="number">10</span>, <span class="number">10</span>)</span>)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Displays 2D data stored in X in a nice grid.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="comment"># Compute rows, cols</span></div>
<div class="line">    rand_indices = np.random.choice(len(self.y), <span class="number">100</span>, replace=<span class="keyword">False</span>)</div>
<div class="line">    X = self.x[rand_indices, :]</div>
<div class="line">    <span class="keyword">if</span> X.ndim == <span class="number">2</span>:</div>
<div class="line">        m, n = X.shape</div>
<div class="line">    <span class="keyword">elif</span> X.ndim == <span class="number">1</span>:</div>
<div class="line">        n = X.size</div>
<div class="line">        m = <span class="number">1</span></div>
<div class="line">        X = X[<span class="keyword">None</span>]  <span class="comment"># Promote to a 2 dimensional array</span></div>
<div class="line">    <span class="keyword">else</span>:</div>
<div class="line">        <span class="keyword">raise</span> IndexError(<span class="string">'Input X should be 1 or 2 dimensional.'</span>)</div>
<div class="line"></div>
<div class="line">    example_width = example_width <span class="keyword">or</span> int(np.round(np.sqrt(n)))</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Compute number of items to display</span></div>
<div class="line">    display_rows = int(np.floor(np.sqrt(m)))</div>
<div class="line">    display_cols = int(np.ceil(m / display_rows))</div>
<div class="line"></div>
<div class="line">    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)</div>
<div class="line">    fig.subplots_adjust(wspace=<span class="number">0.025</span>, hspace=<span class="number">0.025</span>)</div>
<div class="line"></div>
<div class="line">    ax_array = [ax_array] <span class="keyword">if</span> m == <span class="number">1</span> <span class="keyword">else</span> ax_array.ravel()</div>
<div class="line"></div>
<div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(ax_array):</div>
<div class="line">        <span class="comment"># Display Image</span></div>
<div class="line">        h = ax.imshow(X[i].reshape(example_width, example_width, order=<span class="string">'F'</span>),</div>
<div class="line">                      cmap=<span class="string">'Greys'</span>, extent=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div>
<div class="line">        ax.axis(<span class="string">'off'</span>)</div>
<div class="line">    plt.show()</div>
</pre></td></tr></table></figure>

</div></div>
<h4 id="前向传播-2"><a href="#前向传播-2" class="headerlink" title="前向传播"></a>前向传播</h4><p>输入是$x$，其维度是（$batch_size, 400$），我们需要对其插入一列全1偏置向量，得到新输入$a_1$变成（$batch_size, 401$）维：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">row,col = np.shape</div>
<div class="line">a1 = np.insert(x,np.ones(row),axis=<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<p>那么先全连接乘法，再对其使用激活函数：</p>
<p>$z_2=a1*theta1^T$</p>
<p>$a_2=sigmoid(z_2)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">z2 = a1.dot(theta1.T)</div>
<div class="line">a2 = sigmoid(z2)</div>
</pre></td></tr></table></figure>
<p>得到隐层输出$a_2$之后，我们计算输出层：</p>
<p>$a_2$的大小是（$batch_size,25$），我们需要插入偏置向量，变成$(batch_size,26)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">a2 = np.insert(a2,np.ones(row),axis=<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<p>$z_3=a2*theta2^T$</p>
<p>$ h=sigmoid(z_3)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">z3 = a2.dot(theta2.T)</div>
<div class="line">h = sigmoid(z3)</div>
</pre></td></tr></table></figure>
<p>这样我们就完成了前向传播的计算。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagate</span><span class="params">(x,theta1,theta2)</span>:</span></div>
<div class="line">    <span class="comment">#x_shape = (5000,400)</span></div>
<div class="line">    <span class="comment"># theta1_shape = [hidden_size, input_size+1]</span></div>
<div class="line">    <span class="comment"># theta1_shape = [num_labels,hidden_size+1]</span></div>
<div class="line">    row,col = np.shape(x)</div>
<div class="line">    a1 = np.insert(x,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>)</div>
<div class="line">    z2 = a1.dot(theta1.T)</div>
<div class="line">    a2 = sigmoid(z2)</div>
<div class="line">    a2 = np.insert(a2,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>)</div>
<div class="line">    z3 = a2.dot(theta2.T)</div>
<div class="line">    h = sigmoid(z3)</div>
<div class="line">    <span class="keyword">return</span> a1,z2,a2,z3,h</div>
</pre></td></tr></table></figure>

</div></div>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>我们利用神经网络进行分类任务，所以我们使用交叉熵来衡量损失：<a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" target="_blank" rel="noopener">ref</a> </p>
<p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-09-02 at 4.03.40 PM.png" alt="creen Shot 2018-09-02 at 4.03.40 P"></p>
<p>上式是增加了正则化项的损失函数，其中$m$是样本数，即$batch_{size}$；$K$是输出神经元数，即$output_{size}$；$\lambda$是学习速率；需要注意的是，正则化项没有偏置参数。</p>
<blockquote>
<p><a href="https://blog.csdn.net/sinat_29819401/article/details/60885679" target="_blank" rel="noopener">why regularization</a> </p>
<p>除了偏置神经元的权重之外，我们对网络的所有神经元的权重平方和作为正则化项。在求损失函数对权重的梯度时，在原来的梯度计算基础上，加上$\frac{\lambda}{m}\theta$。</p>
<p>回忆起$L_2$的梯度更新公式：</p>
<script type="math/tex; mode=display">
\frac{\partial Cost}{\partial \theta}=\frac{\partial Cost}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial \theta}=(h-y)\sigma'(h)x</script><p>其中$L_2$的$\frac{\partial Cost}{\partial h}=(h-y)$，但是对于交叉熵而言$\frac{\partial Cost}{\partial h}=\frac{h-y}{h(1-h)}=\frac{h-y}{\sigma’(h)}$</p>
<p>$\frac{\partial Cost}{\partial h}$代入上式：</p>
<script type="math/tex; mode=display">
\frac{\partial Cost}{\partial \theta}=\frac{\partial Cost}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial \theta}=\frac{h-y}{\sigma'(h)}\sigma'(h)x=(h-y)x</script><p>可以看出，偏导数是由$(h-y)$控制，模型的输出$h$与标签$y$之间的差异越大，偏导数就会越大，学习就会越快。</p>
</blockquote>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(thetas,x,y)</span>:</span></div>
<div class="line">    row, col = np.shape(x)</div>
<div class="line">    theta1 = thetas[:hidden_size*(input_size+<span class="number">1</span>)].reshape(hidden_size,input_size+<span class="number">1</span>)</div>
<div class="line">    theta2 = thetas[hidden_size*(input_size+<span class="number">1</span>   ):].reshape(num_labels,hidden_size+<span class="number">1</span>)</div>
<div class="line">    a1, z2, a2, z3, h = forward_propagate(x,theta1,theta2)</div>
<div class="line">    <span class="comment"># err = (h-y)**2</span></div>
<div class="line">    <span class="comment"># err = 0.5*sum(sum(err)/len(x))</span></div>
<div class="line">    l1 = -y*(np.log(h))</div>
<div class="line">    l2 = (y<span class="number">-1</span>)*np.log(<span class="number">1</span>-h)</div>
<div class="line">    err = l1+l2</div>
<div class="line">    reg_err = learning_rate/(<span class="number">2</span>*row)*(np.sum(theta1[:,<span class="number">1</span>:]**<span class="number">2</span>)+np.sum(theta2[:,<span class="number">1</span>:]**<span class="number">2</span>))</div>
<div class="line">    <span class="keyword">return</span> np.mean(err)+reg_err,a1,z2,a2,z3,h</div>
</pre></td></tr></table></figure>
<blockquote>
<p>初始时，err=0.6383642819248599；reg_err=0.016015625000000002</p>
</blockquote>

</div></div>
<h4 id="反向传播-2"><a href="#反向传播-2" class="headerlink" title="反向传播"></a>反向传播</h4><p>反向传播不断更新参数使样本的损失函数越来越小，不过我们需要一个工具计算激活函数的梯度：</p>
<script type="math/tex; mode=display">
sigmoid(z)=\frac{1}{1+e^{-z}}</script><p>其梯度为：</p>
<script type="math/tex; mode=display">
sigmoid'(z)=z(1-z)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span><span class="params">(z)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> z*(<span class="number">1</span>-z)</div>
</pre></td></tr></table></figure>
<p>接下来我们需要逐层计算损失：</p>
<p>输出层损失：</p>
<p>$err_3=(a_3-y)*sigmoid_{gradient}(a_3)$，$shape=(batch_size,output_size)$</p>
<p>隐藏层与输出层之间的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">z2 = np.insert(z2,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>) <span class="comment">#shape = (batch_size,hidden_size+1)</span></div>
</pre></td></tr></table></figure>
<p>$\delta_3=[err_3]^T.*z_2$，$shape=(output_size,hidden_size+1)$</p>
<p>隐藏层损失：</p>
<p>$err_2=sigmoid_{gradient}(a_2)<em>err_3.  </em>theta_2$,$shape=(batch_size,hidden_size+1)$</p>
<p>输入层与隐藏层之间的梯度：</p>
<p>$\delta_2=[err_2]^T.*a_1$，$shape=(hidden_size,input_size+1)$</p>
<h3 id="KERAS版本实现"><a href="#KERAS版本实现" class="headerlink" title="KERAS版本实现"></a>KERAS版本实现</h3><p><code>100epoch</code>训练之后的正确率为<code>94.56%</code>。</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</div>
<div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div>
<div class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</div>
<div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ann</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div>
<div class="line">        self.input_size = <span class="number">400</span></div>
<div class="line">        self.hidden_size = <span class="number">25</span></div>
<div class="line">        self.output_size = <span class="number">10</span></div>
<div class="line"></div>
<div class="line">        opt = Adam(lr=<span class="number">0.0002</span>,beta_1=<span class="number">0.5</span>)</div>
<div class="line"></div>
<div class="line">        self.ann = self.make_model()</div>
<div class="line">        self.ann.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div>
<div class="line">                         optimizer=opt,</div>
<div class="line">                         metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(self)</span>:</span></div>
<div class="line">        inputs = Input((self.input_size,))</div>
<div class="line">        h = Dense(self.hidden_size,activation=<span class="string">'sigmoid'</span>)(inputs)</div>
<div class="line">        outputs = Dense(self.output_size,activation=<span class="string">'sigmoid'</span>)(h)</div>
<div class="line">        model = Model(inputs,outputs)</div>
<div class="line">        model.summary()</div>
<div class="line">        <span class="keyword">return</span> model</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></div>
<div class="line">        data = loadmat(<span class="string">'dataset/ex3data1.mat'</span>)</div>
<div class="line">        X = data[<span class="string">'X'</span>]</div>
<div class="line">        y = data[<span class="string">'y'</span>]</div>
<div class="line">        encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</div>
<div class="line">        y = encoder.fit_transform(y)</div>
<div class="line"></div>
<div class="line">        results = self.ann.fit(X,y,epochs=<span class="number">100</span>)</div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    ann = Ann()</div>
<div class="line">    ann.train()</div>
</pre></td></tr></table></figure>

</div></div>
<h3 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h3><p>实现了神经网络的前向传播和后向传播之后，为保证我们的代码能正确运行，在进行神经网络训练数据之前，我们需要检查梯度是否正确。最简单的方法是将神经网络的梯度与我们手工计算得梯度进行比较。</p>
<script type="math/tex; mode=display">
\frac{\partial Cost}{\partial \theta}=\frac{Cost(\theta +\epsilon)-Cost(\theta -\epsilon)}{2\epsilon}</script><p>也就是说，反向传播计算出来的梯度应该近似上述式子计算出来的梯度，否则我们的代码有误。</p>
<p>一般的，我们设置$\epsilon=10^{-7}$，而两个梯度之间的差异应该小于$10^{-7}$。</p>
<ol>
<li><a href="https://www.leiphone.com/news/201711/MWEDFvRMdOyN7Evm.html" target="_blank" rel="noopener">神经网络中容易被忽视的基础知识</a> </li>
</ol>
<h1 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h1><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
<div class="line">104</div>
<div class="line">105</div>
<div class="line">106</div>
<div class="line">107</div>
<div class="line">108</div>
<div class="line">109</div>
<div class="line">110</div>
<div class="line">111</div>
<div class="line">112</div>
<div class="line">113</div>
<div class="line">114</div>
<div class="line">115</div>
<div class="line">116</div>
<div class="line">117</div>
<div class="line">118</div>
<div class="line">119</div>
<div class="line">120</div>
<div class="line">121</div>
<div class="line">122</div>
<div class="line">123</div>
<div class="line">124</div>
<div class="line">125</div>
<div class="line">126</div>
<div class="line">127</div>
<div class="line">128</div>
<div class="line">129</div>
<div class="line">130</div>
<div class="line">131</div>
<div class="line">132</div>
<div class="line">133</div>
<div class="line">134</div>
<div class="line">135</div>
<div class="line">136</div>
<div class="line">137</div>
<div class="line">138</div>
<div class="line">139</div>
<div class="line">140</div>
<div class="line">141</div>
<div class="line">142</div>
<div class="line">143</div>
<div class="line">144</div>
<div class="line">145</div>
<div class="line">146</div>
<div class="line">147</div>
<div class="line">148</div>
<div class="line">149</div>
<div class="line">150</div>
<div class="line">151</div>
<div class="line">152</div>
<div class="line">153</div>
<div class="line">154</div>
<div class="line">155</div>
<div class="line">156</div>
<div class="line">157</div>
<div class="line">158</div>
<div class="line">159</div>
<div class="line">160</div>
<div class="line">161</div>
<div class="line">162</div>
<div class="line">163</div>
<div class="line">164</div>
<div class="line">165</div>
<div class="line">166</div>
<div class="line">167</div>
<div class="line">168</div>
<div class="line">169</div>
<div class="line">170</div>
<div class="line">171</div>
<div class="line">172</div>
<div class="line">173</div>
<div class="line">174</div>
<div class="line">175</div>
<div class="line">176</div>
<div class="line">177</div>
<div class="line">178</div>
<div class="line">179</div>
<div class="line">180</div>
<div class="line">181</div>
<div class="line">182</div>
<div class="line">183</div>
<div class="line">184</div>
<div class="line">185</div>
<div class="line">186</div>
<div class="line">187</div>
<div class="line">188</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayersNetwork</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size, hidden_size, output_size, std=<span class="number">1e-4</span>)</span>:</span></div>
<div class="line">        <span class="string">"""</span></div>
<div class="line"><span class="string">        Initialize the model. Weights are initialized to small random values and</span></div>
<div class="line"><span class="string">        biases are initialized to zero. Weights and biases are stored in the</span></div>
<div class="line"><span class="string">        variable self.params, which is a dictionary with the following keys:</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        W1: First layer weights; has shape (D, H)</span></div>
<div class="line"><span class="string">        b1: First layer biases; has shape (H,)</span></div>
<div class="line"><span class="string">        W2: Second layer weights; has shape (H, C)</span></div>
<div class="line"><span class="string">        b2: Second layer biases; has shape (C,)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        Inputs:</span></div>
<div class="line"><span class="string">        - input_size: The dimension D of the input data.</span></div>
<div class="line"><span class="string">        - hidden_size: The number of neurons H in the hidden layer.</span></div>
<div class="line"><span class="string">        - output_size: The number of classes C.</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        self.params = &#123;&#125;</div>
<div class="line">        self.params[<span class="string">'W1'</span>] = std * np.random.randn(input_size, hidden_size)</div>
<div class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</div>
<div class="line">        self.params[<span class="string">'W2'</span>] = std * np.random.randn(hidden_size, output_size)</div>
<div class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></div>
<div class="line">        <span class="string">"""</span></div>
<div class="line"><span class="string">        Compute the loss and gradients for a two layer fully connected neural</span></div>
<div class="line"><span class="string">        network.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        Inputs:</span></div>
<div class="line"><span class="string">        - X: Input data of shape (N, D). Each X[i] is a training sample.</span></div>
<div class="line"><span class="string">        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></div>
<div class="line"><span class="string">          an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></div>
<div class="line"><span class="string">          is not passed then we only return scores, and if it is passed then we</span></div>
<div class="line"><span class="string">          instead return the loss and gradients.</span></div>
<div class="line"><span class="string">        - reg: Regularization strength.</span></div>
<div class="line"><span class="string">        - relu activation function</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        Returns:</span></div>
<div class="line"><span class="string">        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></div>
<div class="line"><span class="string">        the score for class c on input X[i].</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        If y is not None, instead return a tuple of:</span></div>
<div class="line"><span class="string">        - loss: Loss (data loss and regularization loss) for this batch of training</span></div>
<div class="line"><span class="string">          samples.</span></div>
<div class="line"><span class="string">        - grads: Dictionary mapping parameter names to gradients of those parameters</span></div>
<div class="line"><span class="string">          with respect to the loss function; has the same keys as self.params.</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        <span class="comment"># Unpack variables from the params dictionary</span></div>
<div class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</div>
<div class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</div>
<div class="line">        N, D = X.shape</div>
<div class="line"></div>
<div class="line">        <span class="comment"># Compute the forward pass</span></div>
<div class="line">        scores = <span class="keyword">None</span></div>
<div class="line"></div>
<div class="line">        s1 = X.dot(W1) + b1</div>
<div class="line">        a1 = np.maximum(s1, <span class="number">0</span>)</div>
<div class="line">        scores = a1.dot(W2) + b2</div>
<div class="line"></div>
<div class="line">        <span class="comment"># If the targets are not given then jump out, we're done</span></div>
<div class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</div>
<div class="line">            <span class="keyword">return</span> scores</div>
<div class="line"></div>
<div class="line">        <span class="comment"># Compute the loss</span></div>
<div class="line"></div>
<div class="line">        normalized_scores = scores - np.max(scores, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</div>
<div class="line">        exp_scores = np.exp(normalized_scores)</div>
<div class="line">        soft_scores = exp_scores / (np.sum(exp_scores, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>)))</div>
<div class="line">        target_scores = soft_scores[np.arange(N), y]</div>
<div class="line">        loss = <span class="number">-1</span> * np.sum(np.log(target_scores)) / N + reg * (np.sum(W1 ** <span class="number">2</span>) + np.sum(W2 ** <span class="number">2</span>))</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">        <span class="comment"># Backward pass: compute gradients</span></div>
<div class="line">        grads = &#123;&#125;</div>
<div class="line"></div>
<div class="line">        dSoftmax = soft_scores</div>
<div class="line">        dSoftmax[np.arange(N), y] -= <span class="number">1</span></div>
<div class="line">        dW2 = a1.T.dot(dSoftmax)</div>
<div class="line">        db2 = np.sum(dSoftmax, axis=<span class="number">0</span>)</div>
<div class="line"></div>
<div class="line">        dRelu = np.zeros_like(a1)</div>
<div class="line">        dRelu[a1 &gt; <span class="number">0</span>] = <span class="number">1</span></div>
<div class="line"></div>
<div class="line">        prev = dSoftmax.dot(W2.T)</div>
<div class="line">        prev *= dRelu</div>
<div class="line"></div>
<div class="line">        dW1 = X.T.dot(prev)</div>
<div class="line">        db1 = np.sum(prev, axis=<span class="number">0</span>)</div>
<div class="line"></div>
<div class="line">        grads[<span class="string">'W2'</span>] = dW2 / N + <span class="number">2</span> * reg * W2</div>
<div class="line">        grads[<span class="string">'b2'</span>] = db2 / N</div>
<div class="line">        grads[<span class="string">'W1'</span>] = dW1 / N + <span class="number">2</span> * reg * W1</div>
<div class="line">        grads[<span class="string">'b1'</span>] = db1 / N</div>
<div class="line"></div>
<div class="line">        <span class="keyword">return</span> loss, grads</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, X_val, y_val,</span></span></div>
<div class="line"><span class="function"><span class="params">              learning_rate=<span class="number">1e-3</span>, learning_rate_decay=<span class="number">0.95</span>,</span></span></div>
<div class="line"><span class="function"><span class="params">              reg=<span class="number">5e-6</span>, num_iters=<span class="number">100</span>,</span></span></div>
<div class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></div>
<div class="line">        <span class="string">"""</span></div>
<div class="line"><span class="string">        Train this neural network using stochastic gradient descent.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        Inputs:</span></div>
<div class="line"><span class="string">        - X: A numpy array of shape (N, D) giving training data.</span></div>
<div class="line"><span class="string">        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span></div>
<div class="line"><span class="string">          X[i] has label c, where 0 &lt;= c &lt; C.</span></div>
<div class="line"><span class="string">        - X_val: A numpy array of shape (N_val, D) giving validation data.</span></div>
<div class="line"><span class="string">        - y_val: A numpy array of shape (N_val,) giving validation labels.</span></div>
<div class="line"><span class="string">        - learning_rate: Scalar giving learning rate for optimization.</span></div>
<div class="line"><span class="string">        - learning_rate_decay: Scalar giving factor used to decay the learning rate</span></div>
<div class="line"><span class="string">          after each epoch.</span></div>
<div class="line"><span class="string">        - reg: Scalar giving regularization strength.</span></div>
<div class="line"><span class="string">        - num_iters: Number of steps to take when optimizing.</span></div>
<div class="line"><span class="string">        - batch_size: Number of training examples to use per step.</span></div>
<div class="line"><span class="string">        - verbose: boolean; if true print progress during optimization.</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        num_train = X.shape[<span class="number">0</span>]</div>
<div class="line">        iterations_per_epoch = max(num_train / batch_size, <span class="number">1</span>)</div>
<div class="line"></div>
<div class="line">        N, D = X.shape</div>
<div class="line"></div>
<div class="line">        <span class="comment"># Use SGD to optimize the parameters in self.model</span></div>
<div class="line">        loss_history = []</div>
<div class="line">        train_acc_history = []</div>
<div class="line">        val_acc_history = []</div>
<div class="line"></div>
<div class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</div>
<div class="line"></div>
<div class="line">            idx = np.random.choice(N, batch_size)</div>
<div class="line">            X_batch = X[idx]</div>
<div class="line">            y_batch = y[idx]</div>
<div class="line"></div>
<div class="line">            <span class="comment"># Compute loss and gradients using the current minibatch</span></div>
<div class="line">            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</div>
<div class="line">            loss_history.append(loss)</div>
<div class="line"></div>
<div class="line">            self.params[<span class="string">'W1'</span>] -= learning_rate * grads[<span class="string">'W1'</span>]</div>
<div class="line">            self.params[<span class="string">'b1'</span>] -= learning_rate * grads[<span class="string">'b1'</span>]</div>
<div class="line">            self.params[<span class="string">'W2'</span>] -= learning_rate * grads[<span class="string">'W2'</span>]</div>
<div class="line">            self.params[<span class="string">'b2'</span>] -= learning_rate * grads[<span class="string">'b2'</span>]</div>
<div class="line"></div>
<div class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</div>
<div class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</div>
<div class="line"></div>
<div class="line">            <span class="comment"># Every epoch, check train and val accuracy and decay learning rate.</span></div>
<div class="line">            <span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</div>
<div class="line">                <span class="comment"># Check accuracy</span></div>
<div class="line">                train_acc = (self.predict(X_batch) == y_batch).mean()</div>
<div class="line">                val_acc = (self.predict(X_val) == y_val).mean()</div>
<div class="line">                train_acc_history.append(train_acc)</div>
<div class="line">                val_acc_history.append(val_acc)</div>
<div class="line"></div>
<div class="line">                <span class="comment"># Decay learning rate</span></div>
<div class="line">                learning_rate *= learning_rate_decay</div>
<div class="line"></div>
<div class="line">        <span class="keyword">return</span> &#123;</div>
<div class="line">            <span class="string">'loss_history'</span>: loss_history,</div>
<div class="line">            <span class="string">'train_acc_history'</span>: train_acc_history,</div>
<div class="line">            <span class="string">'val_acc_history'</span>: val_acc_history,</div>
<div class="line">        &#125;</div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div>
<div class="line">        <span class="string">"""</span></div>
<div class="line"><span class="string">        Use the trained weights of this two-layer network to predict labels for</span></div>
<div class="line"><span class="string">        data points. For each data point we predict scores for each of the C</span></div>
<div class="line"><span class="string">        classes, and assign each data point to the class with the highest score.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        Inputs:</span></div>
<div class="line"><span class="string">        - X: A numpy array of shape (N, D) giving N D-dimensional data points to</span></div>
<div class="line"><span class="string">          classify.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        Returns:</span></div>
<div class="line"><span class="string">        - y_pred: A numpy array of shape (N,) giving predicted labels for each of</span></div>
<div class="line"><span class="string">          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span></div>
<div class="line"><span class="string">          to have class c, where 0 &lt;= c &lt; C.</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line"></div>
<div class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</div>
<div class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</div>
<div class="line">        s1 = X.dot(W1) + b1</div>
<div class="line">        a1 = np.maximum(s1, <span class="number">0</span>)</div>
<div class="line">        scores = a1.dot(W2) + b2</div>
<div class="line">        y_pred = np.argmax(scores, axis=<span class="number">1</span>)</div>
<div class="line"></div>
<div class="line">        <span class="keyword">return</span> y_pred</div>
</pre></td></tr></table></figure>

</div></div>
<h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p><a href="https://towardsdatascience.com/deep-learning-concepts-part-2-9aed45e5e7ed" target="_blank" rel="noopener">to do</a> </p>
<p>y为ground_truth, a为预测。</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>The whole idea of regularization is you try to penalize the complexity of the model, instead of explicitly trying to fit the training data.</p>
<script type="math/tex; mode=display">
L=\frac{1}{N}\sum_{i=1}^{N}\sum_{j\ne y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)+\lambda R(W)</script><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2019-03-07 at 4.24.27 PM.png" alt="creen Shot 2019-03-07 at 4.24.27 P"></p>
<blockquote>
<p>$L1$ Regularization prefer the sparse weight vector, which </p>
</blockquote>
<h1 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h1><p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">zhihu</a> <a href="https://blog.csdn.net/whitesilence/article/details/75667002" target="_blank" rel="noopener">过程</a> </p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">ref1</a> <a href="https://www.cnblogs.com/makefile/p/dropout.html" target="_blank" rel="noopener">ref2</a> <a href="https://www.cnblogs.com/santian/p/5457412.html" target="_blank" rel="noopener">ref3</a> <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">ref4</a> <a href="https://yq.aliyun.com/articles/68901" target="_blank" rel="noopener">ref4-ch</a> <a href="http://lib.csdn.net/article/deeplearning/51257" target="_blank" rel="noopener">regularization vs dropout</a> </p>
<p>dropout的引入是为了防止网络的过拟合，本质是正则化。</p>
<p>神经元以概率$p$被保留，当一个神经元被drop out，该神经元的输出就被置0</p>
<p>被drop out的神经元对训练阶段(包括前向传播和后向传播)没有任何贡献，</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Neural-Network/" rel="tag"># Neural Network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/05/29/NLP-Word2Vec/" rel="next" title="NLP-Word2Vec">
                <i class="fa fa-chevron-left"></i> NLP-Word2Vec
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/12/NLP-Dependency-Parser/" rel="prev" title="NLP-Dependency Parser">
                NLP-Dependency Parser <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络"><span class="nav-number">1.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#感知机模型"><span class="nav-number">1.1.</span> <span class="nav-text">感知机模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#二分类线性模型"><span class="nav-number">1.1.1.</span> <span class="nav-text">二分类线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机学习策略"><span class="nav-number">1.1.2.</span> <span class="nav-text">感知机学习策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑计算"><span class="nav-number">1.1.3.</span> <span class="nav-text">逻辑计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经元模型"><span class="nav-number">1.2.</span> <span class="nav-text">神经元模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络结构"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">1.3.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的矩阵表示"><span class="nav-number">1.3.2.</span> <span class="nav-text">神经网络的矩阵表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">1.3.3.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输出层"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">输出层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐藏层"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">隐藏层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法步骤"><span class="nav-number">1.4.</span> <span class="nav-text">算法步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Python-实现"><span class="nav-number">1.5.</span> <span class="nav-text">Python 实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Toy-Example"><span class="nav-number">1.5.1.</span> <span class="nav-text">Toy Example</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向传播-1"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播-1"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST手写体数字分类"><span class="nav-number">1.5.2.</span> <span class="nav-text">MNIST手写体数字分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据可视化"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">数据可视化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#前向传播-2"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播-2"><span class="nav-number">1.5.2.4.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KERAS版本实现"><span class="nav-number">1.5.3.</span> <span class="nav-text">KERAS版本实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度检查"><span class="nav-number">1.5.4.</span> <span class="nav-text">梯度检查</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Numpy-Implementation"><span class="nav-number">2.</span> <span class="nav-text">Numpy Implementation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#代价函数"><span class="nav-number">3.</span> <span class="nav-text">代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">3.0.1.</span> <span class="nav-text">Regularization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#BatchNormalization"><span class="nav-number">4.</span> <span class="nav-text">BatchNormalization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout"><span class="nav-number">5.</span> <span class="nav-text">Dropout</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
