<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,NLP," />










<meta name="description" content="Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depict">
<meta name="keywords" content="Deep Learning,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP-Dependency Parser">
<meta property="og:url" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depict">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/Screen%20Shot%202019-06-12%20at%205.32.04%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/Screen%20Shot%202019-06-12%20at%205.33.19%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/Screen%20Shot%202019-06-12%20at%205.34.07%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/Screen%20Shot%202019-06-12%20at%205.35.18%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/Screen%20Shot%202019-06-12%20at%205.36.31%20PM.png">
<meta property="og:updated_time" content="2019-06-13T16:55:46.169Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP-Dependency Parser">
<meta name="twitter:description" content="Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depict">
<meta name="twitter:image" content="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/Screen%20Shot%202019-06-12%20at%205.32.04%20PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/"/>





  <title>NLP-Dependency Parser | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/12/NLP-Dependency-Parser/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NLP-Dependency Parser</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-12T16:58:47-05:00">
                2019-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate).</p>
<a id="more"></a>
<h1 id="What-is-Dependency-Parsing"><a href="#What-is-Dependency-Parsing" class="headerlink" title="What is Dependency Parsing"></a>What is Dependency Parsing</h1><p>Dependency parsing is the task of analyzing the syntactic dependency structure of a given input sentence S. The output of a dependency parser is a dependency tree where the words of the input sentence are connected by typed dependency relations. Formally, the dependency parsing problem asks to create a mapping from the input sentence with words $S = w_0w_1…w_n$ (where $w_0$ is the ROOT) to its dependency tree graph G. </p>
<p>To be precise, there are two subproblems in dependency parsing:</p>
<ol>
<li>Learning: Given a training set D of sentences annotated with dependency graphs, induce a parsing model M that can be used to<br>parse new sentences.</li>
<li>Parsing: Given a parsing model M and a sentence S, derive the<br>optimal dependency graph D for S according to M.</li>
</ol>
<h1 id="How-to-train-a-Dependency-Parsing"><a href="#How-to-train-a-Dependency-Parsing" class="headerlink" title="How to train a Dependency Parsing"></a>How to train a Dependency Parsing</h1><h2 id="Gready-Deterministic-Transition-Based-Parsing"><a href="#Gready-Deterministic-Transition-Based-Parsing" class="headerlink" title="Gready Deterministic Transition-Based Parsing"></a>Gready Deterministic Transition-Based Parsing</h2><p>This transition system is a state machine, which consists of states and transitions between those states. The model induces a sequence of transitions from some initial state to one of several terminal states.</p>
<p><strong>States:</strong></p>
<p>For any sentence $S = w_0w_1…w_n$, a state can be described with a triple $c = (\alpha, \beta, A)$:</p>
<ol>
<li>a stack $\alpha$ of words $w_i$ from $S$</li>
<li>a buffer $\beta$ of words  $w_i$ from $S$</li>
<li>a set of dependency arcs $A$ of the form $(w_i,r,w_j)$, where $r$ describes a dependency relation</li>
</ol>
<p>It follows that for any sentence $S = w_0w_1…w_n$,</p>
<ol>
<li>an initial state $c_0$ is of the form $([w_0]_{\alpha},[w_1,…,w_n]_{beta},\empty)$ (only the ROOT is on the stack, all other words are in the buffer and no actions have been chosen yet)</li>
<li>a terminate state has the form $(\alpha, []_{\beta}, A)$</li>
</ol>
<p><strong>Transitions:</strong></p>
<p>There are three types of transitions between states:</p>
<ol>
<li><p>Shift: Remove the first word in the buffer and push it on top of the stack. (Pre-condition: buffer has to be non-empty.)</p>
</li>
<li><p>LEFT-ARC: Add a dependency arc $(w_j,r,w_i)$ to the arc set $A$, where $w_i$ is the word second to the top of the stack and $w_j$ is the word at the top of the stack. Remove $w_i$ from the stack.</p>
<blockquote>
<p>$[w_0, w_1,…,w_i \gets w_j]$ </p>
</blockquote>
</li>
<li><p>RIGHT-ARC: Add a dependency arc $(w_i,r,w_j)$ to the arc set $A$, where $w_i$ is the word second to the top of the stack and $w_j$ is the word at the top of the stack. Remove $w_j$ from the stack.</p>
<blockquote>
<p>$[w_0, w_1,…,w_i \to w_j]$ </p>
</blockquote>
</li>
</ol>
<h2 id="Neural-Dependency-Parsing"><a href="#Neural-Dependency-Parsing" class="headerlink" title="Neural Dependency Parsing"></a>Neural Dependency Parsing</h2><p>The network is to predict the transitions between words, i.e., ${Shift, Left-Arc, Right-Arc}$. </p>
<p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.32.04 PM.png" alt="creen Shot 2019-06-12 at 5.32.04 P"></p>
<p>For each feature type, we will have a corresponding embedding matrix, mapping from the feature’s one hot encoding, to a d-dimensional<br>dense vector representation. </p>
<p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.33.19 PM.png" alt="creen Shot 2019-06-12 at 5.33.19 P"></p>
<p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.34.07 PM.png" alt="creen Shot 2019-06-12 at 5.34.07 P"></p>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.35.18 PM.png" alt="creen Shot 2019-06-12 at 5.35.18 P"></p>
<p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.36.31 PM.png" alt="creen Shot 2019-06-12 at 5.36.31 P"></p>
<blockquote>
<p>A sentence containing $n$ words will be parsed in $2n$ steps. Because for each word, it will be pushed from buffer to stack, which result in n steps. Then each word in the stack has to be assigned a transition, i.e., LEFT-ARC or RIGHT-ARC, which results in another n steps. Therefore, the total steps are $2n$.</p>
</blockquote>
<h1 id="Pytorch-Implementation"><a href="#Pytorch-Implementation" class="headerlink" title="Pytorch Implementation"></a>Pytorch Implementation</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
<div class="line">104</div>
<div class="line">105</div>
<div class="line">106</div>
<div class="line">107</div>
<div class="line">108</div>
<div class="line">109</div>
<div class="line">110</div>
<div class="line">111</div>
<div class="line">112</div>
<div class="line">113</div>
<div class="line">114</div>
<div class="line">115</div>
<div class="line">116</div>
<div class="line">117</div>
</pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartialParse</span><span class="params">(object)</span>:</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence)</span>:</span></div>
<div class="line">        <span class="string">"""Initializes this partial parse.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @param sentence (list of str): The sentence to be parsed as a list of words.</span></div>
<div class="line"><span class="string">                                        Your code should not modify the sentence.</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        <span class="comment"># The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.</span></div>
<div class="line">        self.sentence = sentence</div>
<div class="line"></div>
<div class="line">        <span class="comment">### YOUR CODE HERE (3 Lines)</span></div>
<div class="line">        <span class="comment">### Your code should initialize the following fields:</span></div>
<div class="line">        <span class="comment">###     self.stack: The current stack represented as a list with the top of the stack as the</span></div>
<div class="line">        <span class="comment">###                 last element of the list.</span></div>
<div class="line">        <span class="comment">###     self.buffer: The current buffer represented as a list with the first item on the</span></div>
<div class="line">        <span class="comment">###                  buffer as the first item of the list</span></div>
<div class="line">        <span class="comment">###     self.dependencies: The list of dependencies produced so far. Represented as a list of</span></div>
<div class="line">        <span class="comment">###             tuples where each tuple is of the form (head, dependent).</span></div>
<div class="line">        <span class="comment">###             Order for this list doesn't matter.</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">### Note: The root token should be represented with the string "ROOT"</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        self.stack = [<span class="string">'ROOT'</span>]</div>
<div class="line">        self.buffer = sentence.copy()</div>
<div class="line">        self.dependencies = []</div>
<div class="line">        <span class="comment">### END YOUR CODE</span></div>
<div class="line"></div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_step</span><span class="params">(self, transition)</span>:</span></div>
<div class="line">        <span class="string">"""Performs a single parse step by applying the given transition to this partial parse</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @param transition (str): A string that equals "S", "LA", or "RA" representing the shift,</span></div>
<div class="line"><span class="string">                                left-arc, and right-arc transitions. You can assume the provided</span></div>
<div class="line"><span class="string">                                transition is a legal transition.</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        <span class="comment">### YOUR CODE HERE (~7-10 Lines)</span></div>
<div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">        <span class="comment">###     Implement a single parsing step, i.e. the logic for the following as</span></div>
<div class="line">        <span class="comment">###     described in the pdf handout:</span></div>
<div class="line">        <span class="comment">###         1. Shift</span></div>
<div class="line">        <span class="comment">###         2. Left Arc</span></div>
<div class="line">        <span class="comment">###         3. Right Arc</span></div>
<div class="line">        <span class="keyword">if</span> transition==<span class="string">'S'</span>:</div>
<div class="line">            self.stack.append(self.buffer.pop(<span class="number">0</span>))</div>
<div class="line">        <span class="keyword">elif</span> transition==<span class="string">'LA'</span>:</div>
<div class="line">            stack_second = self.stack.pop(<span class="number">-2</span>)</div>
<div class="line">            self.dependencies.append((self.stack[<span class="number">-1</span>],stack_second))</div>
<div class="line">        <span class="keyword">else</span>:</div>
<div class="line">            stack_top = self.stack.pop()</div>
<div class="line">            self.dependencies.append((self.stack[<span class="number">-1</span>],stack_top))</div>
<div class="line"></div>
<div class="line">        <span class="comment">### END YOUR CODE</span></div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, transitions)</span>:</span></div>
<div class="line">        <span class="string">"""Applies the provided transitions to this PartialParse</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @param transitions (list of str): The list of transitions in the order they should be applied</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @return dsependencies (list of string tuples): The list of dependencies produced when</span></div>
<div class="line"><span class="string">                                                        parsing the sentence. Represented as a list of</span></div>
<div class="line"><span class="string">                                                        tuples where each tuple is of the form (head, dependent).</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> transitions:</div>
<div class="line">            self.parse_step(transition)</div>
<div class="line">        <span class="keyword">return</span> self.dependencies</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">minibatch_parse</span><span class="params">(sentences, model, batch_size)</span>:</span></div>
<div class="line">    <span class="string">"""Parses a list of sentences in minibatches using a model.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    @param sentences (list of list of str): A list of sentences to be parsed</span></div>
<div class="line"><span class="string">                                            (each sentence is a list of words and each word is of type string)</span></div>
<div class="line"><span class="string">    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function</span></div>
<div class="line"><span class="string">                                model.predict(partial_parses) that takes in a list of PartialParses as input and</span></div>
<div class="line"><span class="string">                                returns a list of transitions predicted for each parse. That is, after calling</span></div>
<div class="line"><span class="string">                                    transitions = model.predict(partial_parses)</span></div>
<div class="line"><span class="string">                                transitions[i] will be the next transition to apply to partial_parses[i].</span></div>
<div class="line"><span class="string">    @param batch_size (int): The number of PartialParses to include in each minibatch</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    @return dependencies (list of dependency lists): A list where each element is the dependencies</span></div>
<div class="line"><span class="string">                                                    list for a parsed sentence. Ordering should be the</span></div>
<div class="line"><span class="string">                                                    same as in sentences (i.e., dependencies[i] should</span></div>
<div class="line"><span class="string">                                                    contain the parse for sentences[i]).</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    dependencies = []</div>
<div class="line"></div>
<div class="line">    <span class="comment">### YOUR CODE HERE (~8-10 Lines)</span></div>
<div class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">    <span class="comment">###     Implement the minibatch parse algorithm as described in the pdf handout</span></div>
<div class="line">    <span class="comment">###</span></div>
<div class="line">    <span class="comment">###     Note: A shallow copy (as denoted in the PDF) can be made with the "=" sign in python, e.g.</span></div>
<div class="line">    <span class="comment">###                 unfinished_parses = partial_parses[:].</span></div>
<div class="line">    <span class="comment">###             Here `unfinished_parses` is a shallow copy of `partial_parses`.</span></div>
<div class="line">    <span class="comment">###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances</span></div>
<div class="line">    <span class="comment">###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.</span></div>
<div class="line">    <span class="comment">###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`</span></div>
<div class="line">    <span class="comment">###             contains references to the same objects. Thus, you should NOT use the `del` operator</span></div>
<div class="line">    <span class="comment">###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that</span></div>
<div class="line">    <span class="comment">###             is being accessed by `partial_parses` and may cause your code to crash.</span></div>
<div class="line"></div>
<div class="line">    <span class="comment">#initialization</span></div>
<div class="line">    partial_parses = []</div>
<div class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</div>
<div class="line">        partial_parses.append(PartialParse(sentence))</div>
<div class="line">    unfinished_parses = partial_parses[:]</div>
<div class="line"></div>
<div class="line">    <span class="keyword">while</span> unfinished_parses:</div>
<div class="line">        minibatch = unfinished_parses[:batch_size]</div>
<div class="line">        transitions = model.predict(minibatch)</div>
<div class="line">        <span class="keyword">for</span> i,parses <span class="keyword">in</span> enumerate(minibatch):</div>
<div class="line">            parses.parse([transitions[i]])</div>
<div class="line">            <span class="keyword">if</span> len(parses.buffer)==<span class="number">0</span> <span class="keyword">and</span> len(parses.stack)==<span class="number">1</span>:</div>
<div class="line">                dependencies.append(parses.dependencies)</div>
<div class="line">                unfinished_parses.remove(parses)</div>
<div class="line">    <span class="comment">### END YOUR CODE</span></div>
<div class="line">    <span class="keyword">return</span> dependencies</div>
</pre></td></tr></table></figure>
<h2 id="Train-Parser-Network"><a href="#Train-Parser-Network" class="headerlink" title="Train Parser Network"></a>Train Parser Network</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
<div class="line">104</div>
<div class="line">105</div>
<div class="line">106</div>
<div class="line">107</div>
<div class="line">108</div>
<div class="line">109</div>
<div class="line">110</div>
<div class="line">111</div>
<div class="line">112</div>
<div class="line">113</div>
<div class="line">114</div>
<div class="line">115</div>
<div class="line">116</div>
<div class="line">117</div>
<div class="line">118</div>
<div class="line">119</div>
<div class="line">120</div>
<div class="line">121</div>
<div class="line">122</div>
<div class="line">123</div>
<div class="line">124</div>
<div class="line">125</div>
<div class="line">126</div>
<div class="line">127</div>
<div class="line">128</div>
<div class="line">129</div>
<div class="line">130</div>
<div class="line">131</div>
<div class="line">132</div>
<div class="line">133</div>
<div class="line">134</div>
<div class="line">135</div>
<div class="line">136</div>
<div class="line">137</div>
<div class="line">138</div>
<div class="line">139</div>
<div class="line">140</div>
<div class="line">141</div>
<div class="line">142</div>
<div class="line">143</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div>
<div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div>
<div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParserModel</span><span class="params">(nn.Module)</span>:</span></div>
<div class="line">    <span class="string">""" Feedforward neural network with an embedding layer and single hidden layer.</span></div>
<div class="line"><span class="string">    The ParserModel will predict which transition should be applied to a</span></div>
<div class="line"><span class="string">    given partial parse configuration.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    PyTorch Notes:</span></div>
<div class="line"><span class="string">        - Note that "ParserModel" is a subclass of the "nn.Module" class. In PyTorch all neural networks</span></div>
<div class="line"><span class="string">            are a subclass of this "nn.Module".</span></div>
<div class="line"><span class="string">        - The "__init__" method is where you define all the layers and their respective parameters</span></div>
<div class="line"><span class="string">            (embedding layers, linear layers, dropout layers, etc.).</span></div>
<div class="line"><span class="string">        - "__init__" gets automatically called when you create a new instance of your class, e.g.</span></div>
<div class="line"><span class="string">            when you write "m = ParserModel()".</span></div>
<div class="line"><span class="string">        - Other methods of ParserModel can access variables that have "self." prefix. Thus,</span></div>
<div class="line"><span class="string">            you should add the "self." prefix layers, values, etc. that you want to utilize</span></div>
<div class="line"><span class="string">            in other ParserModel methods.</span></div>
<div class="line"><span class="string">        - For further documentation on "nn.Module" please see https://pytorch.org/docs/stable/nn.html.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embeddings, n_features=<span class="number">36</span>,</span></span></div>
<div class="line"><span class="function"><span class="params">        hidden_size=<span class="number">200</span>, n_classes=<span class="number">3</span>, dropout_prob=<span class="number">0.5</span>)</span>:</span></div>
<div class="line">        <span class="string">""" Initialize the parser model.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @param embeddings (Tensor): word embeddings (num_words, embedding_size)</span></div>
<div class="line"><span class="string">        @param n_features (int): number of input features</span></div>
<div class="line"><span class="string">        @param hidden_size (int): number of hidden units</span></div>
<div class="line"><span class="string">        @param n_classes (int): number of output classes</span></div>
<div class="line"><span class="string">        @param dropout_prob (float): dropout probability</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        super(ParserModel, self).__init__()</div>
<div class="line">        self.n_features = n_features</div>
<div class="line">        self.n_classes = n_classes</div>
<div class="line">        self.dropout_prob = dropout_prob</div>
<div class="line">        self.embed_size = embeddings.shape[<span class="number">1</span>]</div>
<div class="line">        self.hidden_size = hidden_size</div>
<div class="line">        self.pretrained_embeddings = nn.Embedding(embeddings.shape[<span class="number">0</span>], self.embed_size)</div>
<div class="line">        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))</div>
<div class="line"></div>
<div class="line">        <span class="comment">### YOUR CODE HERE (~5 Lines)</span></div>
<div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">        <span class="comment">###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix</span></div>
<div class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></div>
<div class="line">        <span class="comment">###     2) Construct `self.dropout` layer.</span></div>
<div class="line">        <span class="comment">###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix</span></div>
<div class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.</span></div>
<div class="line">        <span class="comment">###         It has been shown empirically, that this provides better initial weights</span></div>
<div class="line">        <span class="comment">###         for training networks than random uniform initialization.</span></div>
<div class="line">        <span class="comment">###         For more details checkout this great blogpost:</span></div>
<div class="line">        <span class="comment">###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization </span></div>
<div class="line">        <span class="comment">### Hints:</span></div>
<div class="line">        <span class="comment">###     - After you create a linear layer you can access the weight</span></div>
<div class="line">        <span class="comment">###       matrix via:</span></div>
<div class="line">        <span class="comment">###         linear_layer.weight</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">### Please see the following docs for support:</span></div>
<div class="line">        <span class="comment">###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></div>
<div class="line">        <span class="comment">###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_</span></div>
<div class="line">        <span class="comment">###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></div>
<div class="line"></div>
<div class="line">        self.embed_to_hidden = nn.Linear(self.embed_size*self.n_features,self.hidden_size) <span class="comment"># input_size = n_features * embedding_size</span></div>
<div class="line">        nn.init.xavier_uniform_(self.embed_to_hidden.weight)</div>
<div class="line">        self.dropout = nn.Dropout(self.dropout_prob)</div>
<div class="line">        self.hidden_to_logits = nn.Linear(hidden_size,self.n_classes)</div>
<div class="line">        nn.init.xavier_uniform_(self.hidden_to_logits.weight)</div>
<div class="line"></div>
<div class="line">        <span class="comment">### END YOUR CODE</span></div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(self, t)</span>:</span></div>
<div class="line">        <span class="string">""" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)</span></div>
<div class="line"><span class="string">            to embedding vectors.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">            PyTorch Notes:</span></div>
<div class="line"><span class="string">                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__</span></div>
<div class="line"><span class="string">                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).</span></div>
<div class="line"><span class="string">                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to</span></div>
<div class="line"><span class="string">                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)</span></div>
<div class="line"><span class="string">                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">            @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">            @return x (Tensor): tensor of embeddings for words represented in t</span></div>
<div class="line"><span class="string">                                (batch_size, n_features * embed_size)</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        <span class="comment">### YOUR CODE HERE (~1-3 Lines)</span></div>
<div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">        <span class="comment">###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.</span></div>
<div class="line">        <span class="comment">###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).</span></div>
<div class="line">        <span class="comment">###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">### Note: In order to get batch_size, you may need use the tensor .size() function:</span></div>
<div class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">###  Please see the following docs for support:</span></div>
<div class="line">        <span class="comment">###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></div>
<div class="line">        <span class="comment">###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></div>
<div class="line">        batch_size = t.shape[<span class="number">0</span>]</div>
<div class="line">        x = self.pretrained_embeddings(t).view(batch_size,<span class="number">-1</span>)</div>
<div class="line">        <span class="comment">### END YOUR CODE</span></div>
<div class="line">        <span class="keyword">return</span> x</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></div>
<div class="line">        <span class="string">""" Run the model forward.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">            PyTorch Notes:</span></div>
<div class="line"><span class="string">                - Every nn.Module object (PyTorch model) has a `forward` function.</span></div>
<div class="line"><span class="string">                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.</span></div>
<div class="line"><span class="string">                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,</span></div>
<div class="line"><span class="string">                    the `forward` function would called on `t` and the result would be stored in the `output` variable:</span></div>
<div class="line"><span class="string">                        model = ParserModel()</span></div>
<div class="line"><span class="string">                        output = model(t) # this calls the forward function</span></div>
<div class="line"><span class="string">                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)</span></div>
<div class="line"><span class="string">                                 without applying softmax (batch_size, n_classes)</span></div>
<div class="line"><span class="string">        """</span></div>
<div class="line">        <span class="comment">###  YOUR CODE HERE (~3-5 lines)</span></div>
<div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">        <span class="comment">###     1) Apply `self.embedding_lookup` to `t` to get the embeddings</span></div>
<div class="line">        <span class="comment">###     2) Apply `embed_to_hidden` linear layer to the embeddings</span></div>
<div class="line">        <span class="comment">###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.</span></div>
<div class="line">        <span class="comment">###     4) Apply dropout layer to the output of step 3.</span></div>
<div class="line">        <span class="comment">###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">### Note: We do not apply the softmax to the logits here, because</span></div>
<div class="line">        <span class="comment">### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.</span></div>
<div class="line">        <span class="comment">###</span></div>
<div class="line">        <span class="comment">### Please see the following docs for support:</span></div>
<div class="line">        <span class="comment">###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu</span></div>
<div class="line">        features_embeddings = self.embedding_lookup(t)</div>
<div class="line">        o = F.relu(self.embed_to_hidden(features_embeddings))</div>
<div class="line">        o = self.dropout(o)</div>
<div class="line">        logits = self.hidden_to_logits(o)</div>
<div class="line">        <span class="comment">### END YOUR CODE</span></div>
<div class="line">        <span class="keyword">return</span> logits</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(parser, train_data, dev_data, output_path, batch_size=<span class="number">1024</span>, n_epochs=<span class="number">10</span>, lr=<span class="number">0.0005</span>)</span>:</span></div>
<div class="line">    <span class="string">""" Train the neural dependency parser.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></div>
<div class="line"><span class="string">    @param train_data ():</span></div>
<div class="line"><span class="string">    @param dev_data ():</span></div>
<div class="line"><span class="string">    @param output_path (str): Path to which model weights and results are written.</span></div>
<div class="line"><span class="string">    @param batch_size (int): Number of examples in a single batch</span></div>
<div class="line"><span class="string">    @param n_epochs (int): Number of training epochs</span></div>
<div class="line"><span class="string">    @param lr (float): Learning rate</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    best_dev_UAS = <span class="number">0</span></div>
<div class="line"></div>
<div class="line"></div>
<div class="line">    <span class="comment">### YOUR CODE HERE (~2-7 lines)</span></div>
<div class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">    <span class="comment">###      1) Construct Adam Optimizer in variable `optimizer`</span></div>
<div class="line">    <span class="comment">###      2) Construct the Cross Entropy Loss Function in variable `loss_func`</span></div>
<div class="line">    <span class="comment">###</span></div>
<div class="line">    <span class="comment">### Hint: Use `parser.model.parameters()` to pass optimizer</span></div>
<div class="line">    <span class="comment">###       necessary parameters to tune.</span></div>
<div class="line">    <span class="comment">### Please see the following docs for support:</span></div>
<div class="line">    <span class="comment">###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html</span></div>
<div class="line">    <span class="comment">###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss</span></div>
<div class="line">    optimizer = optim.Adam(parser.model.parameters(),lr=lr)</div>
<div class="line">    loss_func = nn.CrossEntropyLoss()</div>
<div class="line"></div>
<div class="line">    <span class="comment">### END YOUR CODE</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</div>
<div class="line">        print(<span class="string">"Epoch &#123;:&#125; out of &#123;:&#125;"</span>.format(epoch + <span class="number">1</span>, n_epochs))</div>
<div class="line">        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)</div>
<div class="line">        <span class="keyword">if</span> dev_UAS &gt; best_dev_UAS:</div>
<div class="line">            best_dev_UAS = dev_UAS</div>
<div class="line">            print(<span class="string">"New best dev UAS! Saving model."</span>)</div>
<div class="line">            torch.save(parser.model.state_dict(), output_path)</div>
<div class="line">        print(<span class="string">""</span>)</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_for_epoch</span><span class="params">(parser, train_data, dev_data, optimizer, loss_func, batch_size)</span>:</span></div>
<div class="line">    <span class="string">""" Train the neural dependency parser for single epoch.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Note: In PyTorch we can signify train versus test and automatically have</span></div>
<div class="line"><span class="string">    the Dropout Layer applied and removed, accordingly, by specifying</span></div>
<div class="line"><span class="string">    whether we are training, `model.train()`, or evaluating, `model.eval()`</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></div>
<div class="line"><span class="string">    @param train_data ():</span></div>
<div class="line"><span class="string">    @param dev_data ():</span></div>
<div class="line"><span class="string">    @param optimizer (nn.Optimizer): Adam Optimizer</span></div>
<div class="line"><span class="string">    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function</span></div>
<div class="line"><span class="string">    @param batch_size (int): batch size</span></div>
<div class="line"><span class="string">    @param lr (float): learning rate</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    parser.model.train() <span class="comment"># Places model in "train" mode, i.e. apply dropout layer</span></div>
<div class="line">    n_minibatches = math.ceil(len(train_data) / batch_size)</div>
<div class="line">    loss_meter = AverageMeter()</div>
<div class="line">    dev_UAS, _ = parser.parse(dev_data)</div>
<div class="line"></div>
<div class="line">    <span class="keyword">with</span> tqdm(total=(n_minibatches)) <span class="keyword">as</span> prog:</div>
<div class="line">        <span class="keyword">for</span> i, (train_x, train_y) <span class="keyword">in</span> enumerate(minibatches(train_data, batch_size)):</div>
<div class="line">            optimizer.zero_grad()   <span class="comment"># remove any baggage in the optimizer</span></div>
<div class="line">            loss = <span class="number">0.</span> <span class="comment"># store loss for this batch here</span></div>
<div class="line">            train_x = torch.from_numpy(train_x).long()</div>
<div class="line">            train_y = torch.from_numpy(train_y.nonzero()[<span class="number">1</span>]).long()</div>
<div class="line"></div>
<div class="line">            <span class="comment">### YOUR CODE HERE (~5-10 lines)</span></div>
<div class="line">            <span class="comment">### <span class="doctag">TODO:</span></span></div>
<div class="line">            <span class="comment">###      1) Run train_x forward through model to produce `logits`</span></div>
<div class="line">            <span class="comment">###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.</span></div>
<div class="line">            <span class="comment">###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss</span></div>
<div class="line">            <span class="comment">###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)</span></div>
<div class="line">            <span class="comment">###         are the predictions (y^ from the PDF).</span></div>
<div class="line">            <span class="comment">###      3) Backprop losses</span></div>
<div class="line">            <span class="comment">###      4) Take step with the optimizer</span></div>
<div class="line">            <span class="comment">### Please see the following docs for support:</span></div>
<div class="line">            <span class="comment">###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step</span></div>
<div class="line">            logits = parser.model(train_x)</div>
<div class="line">            loss = loss_func(logits,train_y)</div>
<div class="line">            loss.backward()</div>
<div class="line">            optimizer.step()</div>
<div class="line">            <span class="comment">### END YOUR CODE</span></div>
<div class="line">            prog.update(<span class="number">1</span>)</div>
<div class="line">            loss_meter.update(loss.item())</div>
<div class="line"></div>
<div class="line">    <span class="keyword">print</span> (<span class="string">"Average Train Loss: &#123;&#125;"</span>.format(loss_meter.avg))</div>
<div class="line"></div>
<div class="line">    print(<span class="string">"Evaluating on dev set"</span>,)</div>
<div class="line">    parser.model.eval() <span class="comment"># Places model in "eval" mode, i.e. don't apply dropout layer</span></div>
<div class="line">    dev_UAS, _ = parser.parse(dev_data)</div>
<div class="line">    print(<span class="string">"- dev UAS: &#123;:.2f&#125;"</span>.format(dev_UAS * <span class="number">100.0</span>))</div>
<div class="line">    <span class="keyword">return</span> dev_UAS</div>
</pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/11/DP-Neural-Network/" rel="next" title="DP-Neural Network">
                <i class="fa fa-chevron-left"></i> DP-Neural Network
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/01/NLP-Seq2Seq/" rel="prev" title="NLP-Seq2Seq">
                NLP-Seq2Seq <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">66</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Dependency-Parsing"><span class="nav-number">1.</span> <span class="nav-text">What is Dependency Parsing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#How-to-train-a-Dependency-Parsing"><span class="nav-number">2.</span> <span class="nav-text">How to train a Dependency Parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gready-Deterministic-Transition-Based-Parsing"><span class="nav-number">2.1.</span> <span class="nav-text">Gready Deterministic Transition-Based Parsing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Dependency-Parsing"><span class="nav-number">2.2.</span> <span class="nav-text">Neural Dependency Parsing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example"><span class="nav-number">2.3.</span> <span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch-Implementation"><span class="nav-number">3.</span> <span class="nav-text">Pytorch Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-Parser-Network"><span class="nav-number">3.1.</span> <span class="nav-text">Train Parser Network</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
