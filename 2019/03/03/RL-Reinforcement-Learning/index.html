<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Reinforcement Learning," />










<meta name="description" content="Reinforcement learning is useful when you have no training data or specific enough expertise about the problem. On a high level, you know WHAT you want, but not really HOW to get there. Luckily, all y">
<meta name="keywords" content="Deep Learning,Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="RL-Reinforcement Learning">
<meta property="og:url" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Reinforcement learning is useful when you have no training data or specific enough expertise about the problem. On a high level, you know WHAT you want, but not really HOW to get there. Luckily, all y">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/0_ylz4lplMffGQR_g3.gif">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_AFAuM1Y8zmso4yB5mOApZA.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_tciNrjN6pW60-h0PiQRiXg.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_zrzRTXt8rtWF5fX__kZ-yQ.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_RLLzQl4YadpbhPlxpa5f6A.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_tciNrjN6pW60-h0PiQRiXg-2067468.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_LLfj11fivpkKZkwQ8uPi3A.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_APLmZ8CVgu0oY3sQBVYIuw.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/0_kvtRAhBZO-h77Iw1.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_2_JRk-4O523bcOcSy1u31g.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/0_8B4cAhvM-K4y9a5U.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/0_DNiQGeUl1FKunRbb.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_fii7Z01laRGateAJDvloAQ.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/1_r-F8AfutP0a8gPWs_5BBLQ.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/Reinforcement_Learning_Taxi_Env.width-1200.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/Screen%20Shot%202019-03-04%20at%204.50.53%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/Screen%20Shot%202019-03-04%20at%204.56.32%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/Screen%20Shot%202019-03-04%20at%205.07.33%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/OpenAI_Gym_Taxi_-Animation-1830599.gif">
<meta property="og:updated_time" content="2019-03-13T20:02:40.315Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RL-Reinforcement Learning">
<meta name="twitter:description" content="Reinforcement learning is useful when you have no training data or specific enough expertise about the problem. On a high level, you know WHAT you want, but not really HOW to get there. Luckily, all y">
<meta name="twitter:image" content="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/0_ylz4lplMffGQR_g3.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/"/>





  <title>RL-Reinforcement Learning | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/03/RL-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">RL-Reinforcement Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-03T09:55:02+08:00">
                2019-03-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Reinforcement learning is useful when you have no training data or specific enough expertise about the problem. On a high level, you know WHAT you want, but not really HOW to get there. Luckily, all you need is a reward mechanism, and the reinforcement learning model will figure out how to maximize the reward, if you just let it “play” long enough. This is analogous to teaching a dog to sit down using treats. At first the dog is clueless and tries random things on your command. At some point, it accidentally lands on its butt and gets a sudden reward. As time goes by, and given enough iterations, it’ll figure out the expert strategy of sitting down on cue.</p>
<a id="more"></a>
<h1 id="Introduction-link"><a href="#Introduction-link" class="headerlink" title="Introduction link"></a>Introduction <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419" target="_blank" rel="noopener">link</a></h1><p>The idea behind Reinforcement Learning is that an agent will learn from the environment by interacting with it and receiving rewards for performing actions. </p>
<h2 id="The-Reinforcement-Learning-Process"><a href="#The-Reinforcement-Learning-Process" class="headerlink" title="The Reinforcement Learning Process"></a>The Reinforcement Learning Process</h2><p>Let’s imagine an agent learning to play Super Mario Bros as a working example. The Reinforcement Learning (RL) process can be modeled as a loop that works like this:</p>
<ul>
<li>Our Agent receives <strong>state S0</strong> from the <strong>Environment</strong> (In our case we receive the first frame of our game (state) from Super Mario Bros (environment))</li>
<li>Based on that <strong>state S0,</strong> agent takes an <strong>action A0</strong> (our agent will move right)</li>
<li>Environment transitions to a <strong>new</strong> <strong>state S1</strong> (new frame)</li>
<li>Environment gives some <strong>reward R1</strong> to the agent (not dead: +1)</li>
</ul>
<p>This RL loop outputs a sequence of <strong>state, action and reward.</strong></p>
<p>The goal of the agent is to maximize the expected cumulative reward.</p>
<h2 id="The-central-idea-of-the-Reward-Hypothesis"><a href="#The-central-idea-of-the-Reward-Hypothesis" class="headerlink" title="The central idea of the Reward Hypothesis"></a>The central idea of the Reward Hypothesis</h2><p>Why is the goal of the agent to maximize the expected cumulative reward?</p>
<p>Well, Reinforcement Learning is based on the idea of the reward hypothesis. All goals can be described by the maximization of the expected cumulative reward.</p>
<p><strong>That’s why in Reinforcement Learning, to have the best behavior, we need to maximize the expected cumulative reward.</strong></p>
<p>The cumulative reward at each time step t can be written as:</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/0_ylz4lplMffGQR_g3.gif" alt="_ylz4lplMffGQR_g"></p>
<p>which is equivalent to:</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_AFAuM1Y8zmso4yB5mOApZA.png" alt="_AFAuM1Y8zmso4yB5mOApZ"></p>
<p>However, in reality, we can’t just add the rewards like that. The rewards that come sooner (in the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_tciNrjN6pW60-h0PiQRiXg.png" alt="_tciNrjN6pW60-h0PiQRiX"></p>
<p>Let say your agent is this small mouse and your opponent is the cat. Your goal is to eat the maximum amount of cheese before being eaten by the cat.</p>
<p>As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).</p>
<p>As a consequence, the reward near the cat, even if it is bigger (more cheese), will be discounted. We’re not really sure we’ll be able to eat it.</p>
<p>To discount the rewards, we proceed like this:</p>
<p>We define a discount rate called gamma. It must be between 0 and 1.</p>
<ul>
<li>The larger the gamma, the smaller the discount. This means the learning agent cares more about the long term reward.</li>
<li>On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).</li>
</ul>
<p>Our discounted cumulative expected rewards is:</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_zrzRTXt8rtWF5fX__kZ-yQ.png" alt="_zrzRTXt8rtWF5fX__kZ-y"></p>
<p>To be simple, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less probable to happen.</p>
<h2 id="Episodic-or-Continuing-tasks"><a href="#Episodic-or-Continuing-tasks" class="headerlink" title="Episodic or Continuing tasks"></a>Episodic or Continuing tasks</h2><p>A task is an instance of a Reinforcement Learning problem. We can have two types of tasks: episodic and continuous.</p>
<h3 id="Episodic-task"><a href="#Episodic-task" class="headerlink" title="Episodic task"></a>Episodic task</h3><p>In this case, we have a starting point and an ending point <strong>(a terminal state). This creates an episode</strong>: a list of States, Actions, Rewards, and New States.</p>
<p>For instance think about Super Mario Bros, an episode begin at the launch of a new Mario and ending: when you’re killed or you’re reach the end of the level.</p>
<h3 id="Continuing-task"><a href="#Continuing-task" class="headerlink" title="Continuing task"></a>Continuing task</h3><p><strong>These are tasks that continue forever (no terminal state).</strong> In this case, the agent has to learn how to choose the best actions and simultaneously interacts with the environment.</p>
<p>For instance, an agent that do automated stock trading. For this task, there is no starting point and terminal state. <strong>The agent keeps running until we decide to stop him.</strong></p>
<h2 id="Monte-Carlo-vs-TD-Learning-methods"><a href="#Monte-Carlo-vs-TD-Learning-methods" class="headerlink" title="Monte Carlo vs TD Learning methods"></a>Monte Carlo vs TD Learning methods</h2><p>We have two ways of learning:</p>
<ul>
<li>Collecting the rewards <strong>at the end of the episode</strong> and then calculating the <strong>maximum expected future reward</strong>: <em>Monte Carlo Approach</em></li>
<li>Estimate <strong>the rewards at each step</strong>: <em>Temporal Difference Learning</em></li>
</ul>
<h3 id="Monte-Carlo"><a href="#Monte-Carlo" class="headerlink" title="Monte Carlo"></a>Monte Carlo</h3><p>When the episode ends (the agent reaches a “terminal state”), <strong>the agent looks at the total cumulative reward to see how well it did.</strong> In Monte Carlo approach, rewards are only <strong>received at the end of the game.</strong></p>
<p>Then, we start a new game with the added knowledge. <strong>The agent makes better decisions with each iteration.</strong></p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_RLLzQl4YadpbhPlxpa5f6A.png" alt="_RLLzQl4YadpbhPlxpa5f6"></p>
<p>Let’s take an example:</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_tciNrjN6pW60-h0PiQRiXg-2067468.png" alt="_tciNrjN6pW60-h0PiQRiXg-206746"></p>
<p>If we take the maze environment:</p>
<ul>
<li>We always start at the same starting point.</li>
<li>We terminate the episode if the cat eats us or if we move &gt; 20 steps.</li>
<li>At the end of the episode, we have a list of State, Actions, Rewards, and New States.</li>
<li>The agent will sum the total rewards Gt (to see how well it did).</li>
<li>It will then update V(st) based on the formula above.</li>
<li>Then start a new game with this new knowledge.</li>
</ul>
<p>By running more and more episodes, <strong>the agent will learn to play better and better.</strong></p>
<h3 id="Temporal-Difference-Learning-learning-at-each-time-step"><a href="#Temporal-Difference-Learning-learning-at-each-time-step" class="headerlink" title="Temporal Difference Learning : learning at each time step"></a>Temporal Difference Learning : learning at each time step</h3><p>TD Learning, on the other hand, will not wait until the end of the episode to update <strong>the maximum expected future reward estimation: it will update its value estimation V for the non-terminal states St occurring at that experience.</strong></p>
<p>This method is called TD(0) or <strong>one step TD (update the value function after any individual step).</strong></p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_LLfj11fivpkKZkwQ8uPi3A.png" alt="_LLfj11fivpkKZkwQ8uPi3"></p>
<p>TD methods <strong>only wait until the next time step to update the value estimates.</strong> At time t+1 they immediately <strong>form a TD target using the observed reward Rt+1 and the current estimate V(St+1).</strong></p>
<p>TD target is an estimation: in fact you update the previous estimate V(St) <strong>by updating it towards a one-step target.</strong></p>
<h3 id="Exploration-Exploitation-trade-off"><a href="#Exploration-Exploitation-trade-off" class="headerlink" title="Exploration/Exploitation trade-off"></a>Exploration/Exploitation trade-off</h3><p>Before looking at the different strategies to solve Reinforcement Learning problems, we must cover one more very important topic: the exploration/exploitation trade-off.</p>
<ul>
<li>Exploration is finding more information about the environment.</li>
<li>Exploitation is exploiting known information to maximize the reward.</li>
</ul>
<p>Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap.</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_APLmZ8CVgu0oY3sQBVYIuw.png" alt="_APLmZ8CVgu0oY3sQBVYIu"></p>
<p>In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze there is a gigantic sum of cheese (+1000).</p>
<p>However, if we only focus on reward, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).</p>
<p>But if our agent does a little bit of exploration, it can find the big reward.</p>
<p>This is what we call the exploration/exploitation trade off. We must define a rule that helps to handle this trade-off. We’ll see in future articles different ways to handle it.</p>
<h2 id="Three-approaches-to-RL"><a href="#Three-approaches-to-RL" class="headerlink" title="Three approaches to RL"></a>Three approaches to RL</h2><p>Now that we defined the main elements of Reinforcement Learning, let’s move on to the three approaches to solve a Reinforcement Learning problem. These are value-based, policy-based, and model-based.</p>
<h3 id="Value-Based"><a href="#Value-Based" class="headerlink" title="Value Based"></a>Value Based</h3><p>In value-based RL, the goal is to optimize the value function <em>V(s)</em>.</p>
<p>The value function is a function that tells us the maximum expected future reward the agent will get at each state.</p>
<p><strong>The value of each state is the total amount of the reward an agent can expect to accumulate over the future, starting at that state.</strong></p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/0_kvtRAhBZO-h77Iw1.png" alt="_kvtRAhBZO-h77Iw"></p>
<p>The agent will use this value function to select which state to choose at each step. The agent takes the state with the biggest value.<img src="/2019/03/03/RL-Reinforcement-Learning/1_2_JRk-4O523bcOcSy1u31g.png" alt="_2_JRk-4O523bcOcSy1u31"></p>
<p>In the maze example, at each step we will take the biggest value: -7, then -6, then -5 (and so on) to attain the goal.</p>
<h3 id="Policy-Based"><a href="#Policy-Based" class="headerlink" title="Policy Based"></a>Policy Based</h3><p>In the maze example, at each step we will take the biggest value: -7, then -6, then -5 (and so on) to attain the goal.</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/0_8B4cAhvM-K4y9a5U.png" alt="_8B4cAhvM-K4y9a5"></p>
<p>We learn a policy function. This lets us map each state to the best corresponding action.</p>
<p>We have two types of policy:</p>
<ul>
<li>Deterministic: a policy at a given state will always return the same action.</li>
<li>Stochastic: output a distribution probability over actions.</li>
</ul>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/0_DNiQGeUl1FKunRbb.png" alt="_DNiQGeUl1FKunRb"></p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/1_fii7Z01laRGateAJDvloAQ.png" alt="_fii7Z01laRGateAJDvloA"></p>
<p>We learn a policy function. This lets us map each state to the best corresponding action.</p>
<p>We have two types of policy:</p>
<ul>
<li>Deterministic: a policy at a given state will always return the same action.</li>
<li>Stochastic: output a distribution probability over actions<strong>.</strong></li>
</ul>
<h3 id="Model-Based"><a href="#Model-Based" class="headerlink" title="Model Based"></a>Model Based</h3><p>In model-based RL, we model the environment. This means we create a model of the behavior of the environment. The problem is each environment will need a different model representation. </p>
<h1 id="The-Q-learning-algorithm"><a href="#The-Q-learning-algorithm" class="headerlink" title="The Q-learning algorithm"></a>The Q-learning algorithm</h1><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_r-F8AfutP0a8gPWs_5BBLQ.png" alt="_r-F8AfutP0a8gPWs_5BBL"></p>
<blockquote>
<p>$\lambda$ - determines how much importance we want to give to future rewards. A high value for the discount factor (close to <strong>1</strong>) captures the long-term effective award, whereas, a discount factor of <strong>0</strong> makes our agent consider only immediate reward, hence making it greedy.</p>
</blockquote>
<p>It looks a bit intimidating, but what it does is quite simple. We can summarize it as:</p>
<p><strong>Update the value estimation of an action based on the reward we got and the reward we expect next.</strong></p>
<p>This is the fundamental thing we are doing. The <strong>learning rate</strong> and <strong>discount</strong>, while required, are just there to tweak the behavior. The discount will define how much we weigh future expected action values over the one we just experienced. The learning rate is sort of an overall gas pedal. Go too fast and you’ll drive past the optimal, go too slow and you’ll never get there.</p>
<blockquote>
<p>Why do we need to gamble and take random actions? For the same reason that the accountant got stuck. Since our default strategy is still greedy, that is we take the most lucrative option by default, we need to introduce some stochasticity to ensure all possible \<state, action="">pairs are explored.</state,></p>
</blockquote>
<h1 id="Example-Design-Self-Driving-Cab"><a href="#Example-Design-Self-Driving-Cab" class="headerlink" title="Example Design: Self-Driving Cab"></a>Example Design: Self-Driving Cab</h1><h2 id="Problem-illustration"><a href="#Problem-illustration" class="headerlink" title="Problem illustration"></a>Problem illustration</h2><p>In this problem, we try to solve solve a problem with Q-Learning in python with OpenAI Gym. Let’s design a simulation of a self-driving cab. The major goal is to demonstrate, in a simplified environment, how you can use RL techniques to develop an efficient and safe approach for tackling this problem. The Smartcab’s job is to pick up the passenger at one location and drop them off in another. Here are a few things that we’d love our Smartcab to take care of:</p>
<ul>
<li>Drop off the passenger to the right location.</li>
<li>Save passenger’s time by taking minimum time possible to drop off</li>
<li>Take care of passenger’s safety and traffic rules</li>
</ul>
<p>There are different aspects that need to be considered here while modeling an RL solution to this problem: rewards, states, and actions.</p>
<h3 id="Rewards"><a href="#Rewards" class="headerlink" title="Rewards"></a>Rewards</h3><ul>
<li>The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired</li>
<li>The agent should be penalized if it tries to drop off a passenger in wrong locations</li>
<li>The agent should get a slight negative reward for not making it to the destination after every time-step. “Slight” negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible</li>
</ul>
<h3 id="State-Space"><a href="#State-Space" class="headerlink" title="State Space"></a>State Space</h3><p>In Reinforcement Learning, the agent encounters a state, and then takes action according to the state it’s in.</p>
<p>The <strong>State Space</strong> is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.</p>
<p>Let’s say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):</p>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/Reinforcement_Learning_Taxi_Env.width-1200.png" alt="einforcement_Learning_Taxi_Env.width-120"></p>
<p>Let’s assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).</p>
<p>You’ll also notice there are four locations that we can pick up and drop off a passenger: R, G, Y, B or <code>[(0,0), (0,4), (4,0), (4,3)]</code> in (row, col) coordinates. Our illustrated passenger is in location <strong>Y</strong> and they wish to go to location <strong>R</strong>.</p>
<p>When we also account for one additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there’s four destinations and five passenger locations.</p>
<p>So, our taxi environment has $5\times 5 \times 5 \times 4=500$ total possible states.</p>
<p>The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.</p>
<p>In other words, we have six possible actions:</p>
<ol>
<li><code>south</code></li>
<li><code>north</code></li>
<li><code>east</code></li>
<li><code>west</code></li>
<li><code>pickup</code></li>
<li><code>dropoff</code></li>
</ol>
<p>This is the <strong>action space</strong>: the set of all the actions that our agent can take in a given state.</p>
<p>You’ll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment’s code, we will simply provide a -1 penalty for every wall hit and the taxi won’t move anywhere. This will just rack up penalties causing the taxi to consider going around the wall.</p>
<h3 id="Action-Space"><a href="#Action-Space" class="headerlink" title="Action Space"></a>Action Space</h3><p>The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.</p>
<p>In other words, we have six possible actions:</p>
<ol>
<li><code>south</code></li>
<li><code>north</code></li>
<li><code>east</code></li>
<li><code>west</code></li>
<li><code>pickup</code></li>
<li><code>dropoff</code></li>
</ol>
<p>This is the <strong>action space</strong>: the set of all the actions that our agent can take in a given state.</p>
<p>You’ll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment’s code, we will simply provide a -1 penalty for every wall hit and the taxi won’t move anywhere. This will just rack up penalties causing the taxi to consider going around the wall.</p>
<h2 id="Implementation-with-Python"><a href="#Implementation-with-Python" class="headerlink" title="Implementation with Python"></a>Implementation with Python</h2><p>Fortunately, <a href="https://gym.openai.com/" target="_blank" rel="noopener">OpenAI Gym</a> has this exact environment already built for us.</p>
<p>Gym provides different game environments which we can plug into our code and test an agent. The library takes care of API for providing all the information that our agent would require, like possible actions, score, and current state. We just need to focus just on the algorithm part for our agent.</p>
<p>We’ll be using the Gym environment called <code>Taxi-V2</code>, which all of the details explained above were pulled from. The objectives, rewards, and actions are all the same.</p>
<h3 id="Gym’s-interface"><a href="#Gym’s-interface" class="headerlink" title="Gym’s interface"></a>Gym’s interface</h3><p>Firstly, we need to load the game environment and render what it looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div>
<div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>).env</div>
<div class="line">env.render()</div>
</pre></td></tr></table></figure>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/Screen Shot 2019-03-04 at 4.50.53 PM.png" alt="creen Shot 2019-03-04 at 4.50.53 P"></p>
<p>The core gym interface is <code>env</code>, which is the unified environment interface. The following are the <code>env</code>methods that would be quite helpful to us:</p>
<ul>
<li><code>env.reset</code>: Resets the environment and returns a random initial state.</li>
<li><code>env.step(action)</code>: Step the environment by one timestep. Returns<ul>
<li><strong>observation</strong>: Observations of the environment</li>
<li><strong>reward</strong>: If your action was beneficial or not</li>
<li><strong>done</strong>: Indicates if we have successfully picked up and dropped off a passenger, also called one <em>episode</em></li>
<li><strong>info</strong>: Additional info such as performance and latency for debugging purposes</li>
</ul>
</li>
<li><code>env.render</code>: Renders one frame of the environment (helpful in visualizing the environment)</li>
</ul>
<blockquote>
<p>Note: We are using the <code>.env</code> on the end of <code>make</code> to avoid training stopping at 200 iterations, which is the default for the new version of Gym</p>
</blockquote>
<h3 id="Reminder-of-the-problem"><a href="#Reminder-of-the-problem" class="headerlink" title="Reminder of the problem"></a>Reminder of the problem</h3><p>Here’s our restructured problem statement (from Gym docs):</p>
<p><em>“There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.”</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">env.reset() <span class="comment"># rest environment to a new, random state</span></div>
<div class="line">env.render()</div>
<div class="line">print(<span class="string">"Action Space &#123;&#125;"</span>.format(env.action_space))</div>
<div class="line">print(<span class="string">"State Space &#123;&#125;"</span>.format(env.observation_space))</div>
</pre></td></tr></table></figure>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/Screen Shot 2019-03-04 at 4.56.32 PM.png" alt="creen Shot 2019-03-04 at 4.56.32 P"></p>
<ul>
<li>The <strong>filled square</strong> represents the taxi, which is yellow without a passenger and green with a passenger.</li>
<li>The <strong>pipe (“|”)</strong> represents a wall which the taxi cannot cross.</li>
<li><strong>R, G, Y, B</strong> are the possible pickup and destination locations. The <strong>blue letter</strong> represents the current passenger pick-up location, and the <strong>purple letter</strong> is the current destination.</li>
</ul>
<p>As verified by the prints, we have an <strong>Action Space</strong> of size 6 and a <strong>State Space</strong> of size 500. As you’ll see, our RL algorithm won’t need any more information than these two things. All we need is a way to identify a state uniquely by assigning a unique number to every possible state, and RL learns to choose an action number from 0-5 where:</p>
<ul>
<li>0 = south</li>
<li>1 = north</li>
<li>2 = east</li>
<li>3 = west</li>
<li>4 = pickup</li>
<li>5 = dropoff</li>
</ul>
<p><strong>Recall that the 500 states correspond to a encoding of the taxi’s location, the passenger’s location, and the destination location.</strong></p>
<p>Reinforcement Learning will learn a mapping of <strong>states</strong> to the optimal <strong>action</strong> to perform in that state by <em>exploration</em>, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.</p>
<p>The optimal action for each state is the action that has the <strong>highest cumulative long-term reward</strong>.</p>
<p>We can actually take our illustration above, encode its state, and give it to the environment to render in Gym. Recall that we have the taxi at row 3, column 1, our passenger is at location 2, and our destination is location 0. Using the Taxi-v2 state encoding method, we can do the following:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">state = env.encode(<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) <span class="comment">#(taxi_row,taxi_column,passenger_position,destination_index)</span></div>
<div class="line">print(<span class="string">"State: "</span>,state)</div>
<div class="line">env.s = state</div>
<div class="line">env.render()</div>
</pre></td></tr></table></figure>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/Screen Shot 2019-03-04 at 5.07.33 PM.png" alt="creen Shot 2019-03-04 at 5.07.33 P"></p>
<p>We are using our illustration’s coordinates to generate a number corresponding to a state between 0 and 499, which turns out to be <strong>328</strong> for our illustration’s state.</p>
<p>Then we can set the environment’s state manually with <code>env.env.s</code> using that encoded number. You can play around with the numbers and you’ll see the taxi, passenger, and destination move around.</p>
<h3 id="The-reward-table"><a href="#The-reward-table" class="headerlink" title="The reward table"></a>The reward table</h3><p>When the Taxi environment is created, there is an initial Reward table that’s also created, called <code>P</code>. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a $states \times actions$.</p>
<p>Since every state is in this matrix, we can see the default reward values assigned to our illustration’s state:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">env.P[<span class="number">328</span>]</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line">&#123;<span class="number">0</span>: [(<span class="number">1.0</span>, <span class="number">428</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div>
<div class="line"> <span class="number">1</span>: [(<span class="number">1.0</span>, <span class="number">228</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div>
<div class="line"> <span class="number">2</span>: [(<span class="number">1.0</span>, <span class="number">348</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div>
<div class="line"> <span class="number">3</span>: [(<span class="number">1.0</span>, <span class="number">328</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div>
<div class="line"> <span class="number">4</span>: [(<span class="number">1.0</span>, <span class="number">328</span>, <span class="number">-10</span>, <span class="keyword">False</span>)],</div>
<div class="line"> <span class="number">5</span>: [(<span class="number">1.0</span>, <span class="number">328</span>, <span class="number">-10</span>, <span class="keyword">False</span>)]&#125;</div>
</pre></td></tr></table></figure>
<p>This dictionary has the structure <code>{action: [(probability, nextstate, reward, done)]}</code>.</p>
<p>A few things to note:</p>
<ul>
<li>The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.</li>
<li>In this env, <code>probability</code> is always 1.0.</li>
<li>The <code>nextstate</code> is the state we would be in if we take the action at this index of the dict</li>
<li>All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)</li>
<li><code>done</code> is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an <strong>episode</strong></li>
</ul>
<p>Note that if our agent chose to explore action two in this state it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the <strong>long-term reward</strong>.</p>
<h3 id="Without-Reinforcement-Learning"><a href="#Without-Reinforcement-Learning" class="headerlink" title="Without Reinforcement Learning"></a>Without Reinforcement Learning</h3><p>Let’s see what would happen if we try to brute-force our way to solving the problem without RL.</p>
<p>Since we have our <code>P</code> table for default rewards in each state, we can try to have our taxi navigate just using that.</p>
<p>We’ll create an infinite loop which runs until one passenger reaches one destination (one <strong>episode</strong>), or in other words, when the received reward is 20. The <code>env.action_space.sample()</code> method automatically selects one random action from set of all possible actions.</p>
<p>Let’s see what happens:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
</pre></td><td class="code"><pre><div class="line">env.s = <span class="number">328</span> <span class="comment"># set environment to illustration's state</span></div>
<div class="line">epochs = <span class="number">0</span></div>
<div class="line">penalties, reward = <span class="number">0</span>,<span class="number">0</span></div>
<div class="line">frames = [] <span class="comment"># store each step for visualization</span></div>
<div class="line">done = <span class="keyword">False</span></div>
<div class="line"><span class="keyword">while</span> <span class="keyword">not</span> done:</div>
<div class="line">    action = env.action_space.sample() <span class="comment"># choose next step randomly </span></div>
<div class="line">    state,reward,done,info = env.step(action)</div>
<div class="line">    <span class="keyword">if</span> reward == <span class="number">-10</span>: <span class="comment"># illegal pick-up and drop-off actions</span></div>
<div class="line">        penalties += <span class="number">1</span></div>
<div class="line">    <span class="comment"># put each rendered frame into dict for visualization    </span></div>
<div class="line">    frames.append(&#123;</div>
<div class="line">            <span class="string">'frame'</span>:env.render(mode=<span class="string">'ansi'</span>),</div>
<div class="line">            <span class="string">'state'</span>:state,</div>
<div class="line">            <span class="string">'action'</span>:action,</div>
<div class="line">            <span class="string">'reward'</span>:reward</div>
<div class="line">        &#125;)</div>
<div class="line">    epochs += <span class="number">1</span></div>
<div class="line">print(<span class="string">"Timesteps taken: &#123;&#125;"</span>.format(epochs))</div>
<div class="line">print(<span class="string">"Penalties incurred: &#123;&#125;"</span>.format(penalties))</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">Timesteps taken: <span class="number">1348</span></div>
<div class="line">Penalties incurred: <span class="number">431</span></div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</div>
<div class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_frames</span><span class="params">(frames)</span>:</span></div>
<div class="line">    <span class="keyword">for</span> i,frame <span class="keyword">in</span> enumerate(frames):</div>
<div class="line">        clear_output(wait=<span class="keyword">True</span>)</div>
<div class="line">        print(frame[<span class="string">'frame'</span>])</div>
<div class="line">        print(<span class="string">f"Timestep: <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>"</span>)</div>
<div class="line">        print(<span class="string">f"State: <span class="subst">&#123;frame[<span class="string">'state'</span>]&#125;</span>"</span>)</div>
<div class="line">        print(<span class="string">f"Action: <span class="subst">&#123;frame[<span class="string">'action'</span>]&#125;</span>"</span>)</div>
<div class="line">        print(<span class="string">f"Reward: <span class="subst">&#123;frame[<span class="string">'reward'</span>]&#125;</span>"</span>)</div>
<div class="line">        sleep(<span class="number">.1</span>)</div>
<div class="line">print_frames(frames)</div>
</pre></td></tr></table></figure>
<p><img src="/2019/03/03/RL-Reinforcement-Learning/OpenAI_Gym_Taxi_-Animation-1830599.gif" alt="penAI_Gym_Taxi_-Animation-183059"></p>
<p>Not good. Our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.</p>
<p>This is because we aren’t <em>learning</em> from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us.</p>
<h3 id="Enter-Reinforcement-Learning"><a href="#Enter-Reinforcement-Learning" class="headerlink" title="Enter Reinforcement Learning"></a>Enter Reinforcement Learning</h3><h4 id="Summing-up-the-Q-Learning-Process"><a href="#Summing-up-the-Q-Learning-Process" class="headerlink" title="Summing up the Q-Learning Process"></a>Summing up the Q-Learning Process</h4><p>Breaking it down into steps, we get</p>
<ul>
<li>Initialize the Q-table by all zeros.</li>
<li>Start exploring actions: For each state, select any one among all possible actions for the current state (S).</li>
<li>Travel to the next state (S’) as a result of that action (a).</li>
<li>For all possible actions from the state (S’) select the one with the highest Q-value.</li>
<li>Update Q-table values using the equation.</li>
<li>Set the next state as the current state.</li>
<li>If goal state is reached, then end and repeat the process.</li>
</ul>
<p>After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.</p>
<p>There’s a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we’ll be introducing another parameter called $\epsilon$ “epsilon” to cater to this during training.</p>
<p>Instead of just selecting the best learned Q-value action, we’ll sometimes favor exploring the action space further. Larger epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions.</p>
<h4 id="Training-the-agent"><a href="#Training-the-agent" class="headerlink" title="Training the agent"></a>Training the agent</h4><p>First, we’ll initialize the Q-table to a 500×6500×6 matrix of zeros:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line">q_table = np.zeros([env.observation_space.n,env.action_space.n])</div>
</pre></td></tr></table></figure>
<p>In the first part of <code>while not done</code>, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the <code>epsilon</code> value and comparing it to the <code>random.uniform(0, 1)</code> function, which returns an arbitrary number between 0 and 1.</p>
<p>We execute the chosen action in the environment to obtain the <code>next_state</code> and the <code>reward</code> from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the <code>next_state</code>, and with that, we can easily update our Q-value to the <code>new_q_value</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div>
<div class="line">alpha = <span class="number">0.1</span> <span class="comment">#learning rate</span></div>
<div class="line">lambda_ = <span class="number">0.6</span> <span class="comment"># discount</span></div>
<div class="line">epsilon = <span class="number">0.1</span></div>
<div class="line">all_epochs = []</div>
<div class="line">all_penalties = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">100001</span>):</div>
<div class="line">    state = env.reset()</div>
<div class="line">    epochs,penalties,reward = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></div>
<div class="line">    done = <span class="keyword">False</span></div>
<div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</div>
<div class="line">        <span class="keyword">if</span> random.uniform(<span class="number">0</span>,<span class="number">1</span>)&lt;epsilon:</div>
<div class="line">            action = env.action_space.sample() <span class="comment">#explore action space</span></div>
<div class="line">        <span class="keyword">else</span>:</div>
<div class="line">            action = np.argmax(q_table[state]) <span class="comment">#exploit learned values</span></div>
<div class="line">        next_state, reward, done, info = env.step(action)</div>
<div class="line">        old_value = q_table[state,action]</div>
<div class="line">        next_max = np.max(q_table[next_state])</div>
<div class="line">        new_value = (<span class="number">1</span>-alpha)*old_value + alpha*(reward+lambda_*next_max)</div>
<div class="line">        q_table[state,action] = new_value</div>
<div class="line">        <span class="keyword">if</span> reward == <span class="number">-10</span>:</div>
<div class="line">            penalties += <span class="number">1</span></div>
<div class="line">        state = next_state</div>
<div class="line">        epochs += <span class="number">1</span></div>
<div class="line">    <span class="keyword">if</span> i%<span class="number">100</span>==<span class="number">0</span>:</div>
<div class="line">        clear_output(wait=<span class="keyword">True</span>)</div>
<div class="line">        print(<span class="string">f"Episode:<span class="subst">&#123;i&#125;</span>"</span>)</div>
<div class="line">print(<span class="string">"Training finished.\n"</span>)</div>
</pre></td></tr></table></figure>
<p>Now that the Q-table has been established over 100,000 episodes, let’s see what the Q-values are at our illustration’s state:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">q_table[<span class="number">328</span>]</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">array([ <span class="number">-2.41338735</span>,  <span class="number">-2.27325184</span>,  <span class="number">-2.41222668</span>,  <span class="number">-2.36038871</span>,</div>
<div class="line">       <span class="number">-11.15090102</span>, <span class="number">-10.99517141</span>])</div>
</pre></td></tr></table></figure>
<p>The max Q-value is “north” (-2.27325184), so it looks like Q-learning has effectively learned the best action to take in our illustration’s state!</p>
<h4 id="Evaluating-the-agent"><a href="#Evaluating-the-agent" class="headerlink" title="Evaluating the agent"></a>Evaluating the agent</h4><p>Let’s evaluate the performance of our agent. We don’t need to explore actions any further, so now the next action is always selected using the best Q-value:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
</pre></td><td class="code"><pre><div class="line">total_epochs, total_penalties = <span class="number">0</span>, <span class="number">0</span></div>
<div class="line">episodes = <span class="number">100</span></div>
<div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(episodes):</div>
<div class="line">    state = env.reset()</div>
<div class="line">    epochs, penalties, reward = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></div>
<div class="line">    done = <span class="keyword">False</span></div>
<div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</div>
<div class="line">        action = np.argmax(q_table[state])</div>
<div class="line">        state, reward, done, info = env.step(action)</div>
<div class="line">        <span class="keyword">if</span> reward == <span class="number">-10</span>:</div>
<div class="line">            penalties += <span class="number">1</span></div>
<div class="line">        epochs += <span class="number">1</span></div>
<div class="line">    total_penalties += penalties</div>
<div class="line">    total_epochs += epochs</div>
<div class="line">print(<span class="string">f"Results after <span class="subst">&#123;episodes&#125;</span> episodes:"</span>)</div>
<div class="line">print(<span class="string">f"Average timesteps per episode: <span class="subst">&#123;total_epochs / episodes&#125;</span>"</span>)</div>
<div class="line">print(<span class="string">f"Average penalties per episode: <span class="subst">&#123;total_penalties / episodes&#125;</span>"</span>)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">Results after <span class="number">100</span> episodes:</div>
<div class="line">Average timesteps per episode: <span class="number">12.55</span></div>
<div class="line">Average penalties per episode: <span class="number">0.0</span></div>
</pre></td></tr></table></figure>
<p>We can see from the evaluation, the agent’s performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers.</p>
<h4 id="Comparison-between-learning-w-o-rl-and-w-rl"><a href="#Comparison-between-learning-w-o-rl-and-w-rl" class="headerlink" title="Comparison between learning w/o rl and w rl"></a>Comparison between learning w/o rl and w rl</h4><p>With Q-learning agent commits errors initially during exploration but once it has explored enough (seen most of the states), it can act wisely maximizing the rewards making smart moves. Let’s see how much better our Q-learning solution is when compared to the agent making just random moves.</p>
<p>We evaluate our agents according to the following metrics,</p>
<ul>
<li><strong>Average number of penalties per episode:</strong> The smaller the number, the better the performance of our agent. Ideally, we would like this metric to be zero or very close to zero.</li>
<li><strong>Average number of timesteps per trip:</strong> We want a small number of timesteps per episode as well since we want our agent to take minimum steps(i.e. the shortest path) to reach the destination.</li>
<li><strong>Average rewards per move:</strong> The larger the reward means the agent is doing the right thing. That’s why deciding rewards is a crucial part of Reinforcement Learning. In our case, as both timesteps and penalties are negatively rewarded, a higher average reward would mean that the agent reaches the destination as fast as possible with the least penalties”</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Measure</th>
<th style="text-align:center">Random agent’s performance</th>
<th style="text-align:center">Random agent’s performance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Average rewards per move</td>
<td style="text-align:center">-3.9012092102214075</td>
<td style="text-align:center">0.6962843295638126</td>
</tr>
<tr>
<td style="text-align:center">Average number of penalties per episode</td>
<td style="text-align:center">920.45</td>
<td style="text-align:center">0.0</td>
</tr>
<tr>
<td style="text-align:center">Average number of timesteps per trip</td>
<td style="text-align:center">2848.14</td>
<td style="text-align:center">12.38</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Hyperparameters-and-optimizations"><a href="#Hyperparameters-and-optimizations" class="headerlink" title="Hyperparameters and optimizations"></a>Hyperparameters and optimizations</h4><p>The values of <code>alpha</code>, <code>lambda</code>, and <code>epsilon</code> were mostly based on intuition and some “hit and trial”, but there are better ways to come up with good values.</p>
<p>Ideally, all three should decrease over time because as the agent continues to learn, it actually builds up more resilient priors;</p>
<ul>
<li>$\alpha$: (the learning rate) should decrease as you continue to gain a larger and larger knowledge base.</li>
<li>$\lambda$: as you get closer and closer to the deadline, your preference for near-term reward should increase, as you won’t be around long enough to get the long-term reward, which means your gamma should decrease.</li>
<li>$\epsilon$: as we develop our strategy, we have less need of exploration and more exploitation to get more utility from our policy, so as trials increase, epsilon should decrease.</li>
</ul>
<h1 id="OpenAI-GYM"><a href="#OpenAI-GYM" class="headerlink" title="OpenAI-GYM"></a>OpenAI-GYM</h1><p><strong>Gym is a toolkit for developing and comparing reinforcement learning algorithms.</strong> The <a href="https://github.com/openai/gym" target="_blank" rel="noopener">gym</a> library is a collection of test problems — <strong>environments</strong> — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.</p>
<h2 id="Environments"><a href="#Environments" class="headerlink" title="Environments"></a>Environments</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div>
<div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>)</div>
<div class="line">env.reset()</div>
<div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div>
<div class="line">    env.render()</div>
<div class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></div>
</pre></td></tr></table></figure>
<ul>
<li><code>gym.make()</code>: create an environment</li>
<li><code>reset()</code>: reset the environment and returns a random initial state.</li>
<li><code>render()</code>: print out the current environmnet</li>
<li><code>env.step(action)</code>: step the environment by one timestep. returns<ul>
<li><strong>observation(object)</strong>: an environment-specific object representing your observation of the environment. </li>
<li><strong>reward(float)</strong>: amount of reward achieved by the previous action.</li>
<li><strong>done(boolean)</strong>: whether it’s time to <code>reset</code> the environment again. Most (but not all) tasks are divided up into well-defined episodes, and <code>done</code> being <code>True</code> indicates the episode has terminated. </li>
<li><strong>info(dict)</strong>: Additional info such as performance and latency for debugging purposes</li>
</ul>
</li>
</ul>
<p>This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an <code>action</code>, and the environment returns an <code>observation</code> and a <code>reward</code>. The process gets started by calling <code>reset()</code>, which returns an initial <code>observation</code>. So a more proper way of writing the previous code would be to respect the <code>done</code> flag:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div>
<div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>)</div>
<div class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</div>
<div class="line">    observation =  env.reset()</div>
<div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</div>
<div class="line">        env.render()</div>
<div class="line">        print(observation)</div>
<div class="line">        action = env.action_space.sample()</div>
<div class="line">        observation,reward,done,info = env.step(action)</div>
<div class="line">        <span class="keyword">if</span> done:</div>
<div class="line">            print(print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>)))</div>
<div class="line">            <span class="keyword">break</span></div>
</pre></td></tr></table></figure>
<h2 id="Spaces"><a href="#Spaces" class="headerlink" title="Spaces"></a>Spaces</h2><p>In the examples above, we’ve been sampling random actions from the environment’s action space. But what actually are those actions? Every environment comes with an <code>action_space</code> and an <code>observation_space</code>. These attributes are of type <a href="https://github.com/openai/gym/blob/master/gym/core.py" target="_blank" rel="noopener"><code>Space</code></a>, and they describe the format of valid actions and observations:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div>
<div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>)</div>
<div class="line">print(env.action_space)</div>
<div class="line"><span class="comment">#&gt; Discrete(6)</span></div>
<div class="line">print(env.observation_space)</div>
<div class="line"><span class="comment">#&gt; Discrete(500)</span></div>
</pre></td></tr></table></figure>
<p>The <a href="https://github.com/openai/gym/blob/master/gym/spaces/discrete.py" target="_blank" rel="noopener"><code>Discrete</code></a> space allows a fixed range of non-negative numbers.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://towardsdatascience.com/reinforcement-learning-tutorial-part-1-q-learning-cadb36998b28" target="_blank" rel="noopener">Reinforcement Learning Tutorial Part 1: Q-Learning</a> </p>
<p><a href="https://medium.freecodecamp.org/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc" target="_blank" rel="noopener">An introduction to Q-Learning: reinforcement learning</a> </p>
<p><a href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/#" target="_blank" rel="noopener">Reinforcement Q-Learning from Scratch in Python with OpenAI Gym</a> </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/03/RL-Double-Q-Learning/" rel="next" title="RL-Double Q-Learning">
                <i class="fa fa-chevron-left"></i> RL-Double Q-Learning
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/04/DP-Data-Preprocessing/" rel="prev" title="DP-Data Preprocessing">
                DP-Data Preprocessing <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">90</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">68</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-link"><span class="nav-number">1.</span> <span class="nav-text">Introduction link</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Reinforcement-Learning-Process"><span class="nav-number">1.1.</span> <span class="nav-text">The Reinforcement Learning Process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-central-idea-of-the-Reward-Hypothesis"><span class="nav-number">1.2.</span> <span class="nav-text">The central idea of the Reward Hypothesis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Episodic-or-Continuing-tasks"><span class="nav-number">1.3.</span> <span class="nav-text">Episodic or Continuing tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Episodic-task"><span class="nav-number">1.3.1.</span> <span class="nav-text">Episodic task</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Continuing-task"><span class="nav-number">1.3.2.</span> <span class="nav-text">Continuing task</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Monte-Carlo-vs-TD-Learning-methods"><span class="nav-number">1.4.</span> <span class="nav-text">Monte Carlo vs TD Learning methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Monte-Carlo"><span class="nav-number">1.4.1.</span> <span class="nav-text">Monte Carlo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Temporal-Difference-Learning-learning-at-each-time-step"><span class="nav-number">1.4.2.</span> <span class="nav-text">Temporal Difference Learning : learning at each time step</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exploration-Exploitation-trade-off"><span class="nav-number">1.4.3.</span> <span class="nav-text">Exploration/Exploitation trade-off</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Three-approaches-to-RL"><span class="nav-number">1.5.</span> <span class="nav-text">Three approaches to RL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Value-Based"><span class="nav-number">1.5.1.</span> <span class="nav-text">Value Based</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Based"><span class="nav-number">1.5.2.</span> <span class="nav-text">Policy Based</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Based"><span class="nav-number">1.5.3.</span> <span class="nav-text">Model Based</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Q-learning-algorithm"><span class="nav-number">2.</span> <span class="nav-text">The Q-learning algorithm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Example-Design-Self-Driving-Cab"><span class="nav-number">3.</span> <span class="nav-text">Example Design: Self-Driving Cab</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Problem-illustration"><span class="nav-number">3.1.</span> <span class="nav-text">Problem illustration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Rewards"><span class="nav-number">3.1.1.</span> <span class="nav-text">Rewards</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#State-Space"><span class="nav-number">3.1.2.</span> <span class="nav-text">State Space</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-Space"><span class="nav-number">3.1.3.</span> <span class="nav-text">Action Space</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementation-with-Python"><span class="nav-number">3.2.</span> <span class="nav-text">Implementation with Python</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gym’s-interface"><span class="nav-number">3.2.1.</span> <span class="nav-text">Gym’s interface</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reminder-of-the-problem"><span class="nav-number">3.2.2.</span> <span class="nav-text">Reminder of the problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-reward-table"><span class="nav-number">3.2.3.</span> <span class="nav-text">The reward table</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Without-Reinforcement-Learning"><span class="nav-number">3.2.4.</span> <span class="nav-text">Without Reinforcement Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Enter-Reinforcement-Learning"><span class="nav-number">3.2.5.</span> <span class="nav-text">Enter Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Summing-up-the-Q-Learning-Process"><span class="nav-number">3.2.5.1.</span> <span class="nav-text">Summing up the Q-Learning Process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-the-agent"><span class="nav-number">3.2.5.2.</span> <span class="nav-text">Training the agent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating-the-agent"><span class="nav-number">3.2.5.3.</span> <span class="nav-text">Evaluating the agent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Comparison-between-learning-w-o-rl-and-w-rl"><span class="nav-number">3.2.5.4.</span> <span class="nav-text">Comparison between learning w/o rl and w rl</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyperparameters-and-optimizations"><span class="nav-number">3.2.5.5.</span> <span class="nav-text">Hyperparameters and optimizations</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#OpenAI-GYM"><span class="nav-number">4.</span> <span class="nav-text">OpenAI-GYM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Environments"><span class="nav-number">4.1.</span> <span class="nav-text">Environments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spaces"><span class="nav-number">4.2.</span> <span class="nav-text">Spaces</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
