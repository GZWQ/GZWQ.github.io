<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,NLP," />










<meta name="description" content="Recurrent Neural Networks, which are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times se">
<meta name="keywords" content="Deep Learning,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="DP-RNN">
<meta property="og:url" content="http://yoursite.com/2019/03/02/DP-RNN/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Recurrent Neural Networks, which are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times se">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/diags-1578838.jpeg">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-06-19%20at%2012.15.53%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/1042406-20170306142253375-175971779.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/rnn-bptt1.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/rnn-bptt-with-gradients.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-06-18%20at%209.36.36%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-06-18%20at%209.53.34%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-08-04%20at%205.58.02%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-08-04%20at%206.01.14%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-08-04%20at%206.11.11%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-08-04%20at%206.18.31%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-SimpleRNN.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-chain.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM2-notation.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-C-line.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-gate.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-focus-f.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-focus-i.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-focus-C.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/LSTM3-focus-o-2079403.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-07-01%20at%204.30.58%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-07-01%20at%204.34.08%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-06-20%20at%2012.39.27%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-06-20%20at%2012.49.39%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-06-20%20at%2012.51.45%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-08-04%20at%208.51.23%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/Screen%20Shot%202019-08-04%20at%208.51.07%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/03/02/DP-RNN/68747470733a2f2f692e696d6775722e636f6d2f5a32786279534f2e706e67.png">
<meta property="og:updated_time" content="2019-08-05T02:01:57.470Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DP-RNN">
<meta name="twitter:description" content="Recurrent Neural Networks, which are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times se">
<meta name="twitter:image" content="http://yoursite.com/2019/03/02/DP-RNN/diags-1578838.jpeg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/02/DP-RNN/"/>





  <title>DP-RNN | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/02/DP-RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DP-RNN</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-02T20:04:11-06:00">
                2019-03-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Recurrent Neural Networks, which are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies. These algorithms take time and sequence into account, they have a temporal dimension.</p>
<a id="more"></a>
<h1 id="Problems-with-Vanilla-NN-link"><a href="#Problems-with-Vanilla-NN-link" class="headerlink" title="Problems with Vanilla NN link"></a>Problems with Vanilla NN <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">link</a></h1><p>A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes).</p>
<p><img src="/2019/03/02/DP-RNN/diags-1578838.jpeg" alt="iags-157883"></p>
<p>Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN’s state (more on this soon). From left to right: <strong>(1)</strong> Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). <strong>(2)</strong> Sequence output (e.g. image captioning takes an image and outputs a sentence of words). <strong>(3)</strong> Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). <strong>(4)</strong> Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). <strong>(5)</strong> Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.</p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>At a high level, a<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener"> recurrent neural network</a> (RNN) processes sequences — whether daily stock prices, sentences, or sensor measurements — one element at a time while retaining a <em>memory</em> (called a state) of what has come previously in the sequence.</p>
<p><a href="https://arxiv.org/pdf/1610.02583.pdf" target="_blank" rel="noopener">A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation</a> </p>
<p>given an observation sequence $\mathbf{x}=\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{T}\right\}$ and its corresponding label $y=\left\{y_{1}, y_{2}, \ldots, y_{T}\right\}$, we want to learn a map $f : \mathbf{x} \mapsto y$. </p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-19 at 12.15.53 PM.png" alt="creen Shot 2019-06-19 at 12.15.53 P"></p>
<p>Suppose that we have the following RNN model, such that </p>
<script type="math/tex; mode=display">
\begin{array}{l}{\mathbf{h}_{t}=\tanh \left(W_{h h} \mathbf{h}_{t-1}+W_{x h} \mathbf{x}_{t}+\mathbf{b}_{\mathbf{h}}\right)} \\ {z_{t}=\operatorname{softmax}\left(W_{h z} \mathbf{h}_{t}+\mathbf{b}_{z}\right)}\end{array}</script><p>where $z_t$ is the prediction at the time step $t$.</p>
<p>We can minimize the negative log likelihood objective function:</p>
<script type="math/tex; mode=display">
\mathcal{L}(\mathrm{x}, \mathrm{y})=-\sum_{t} y_{t} \log z_{t}</script><p>In the following, we will use notation $L$ as the objective function for simplicity. And further we will use $L(t + 1)$ to indicate the output at the time step t + 1, s.t. $L(t + 1) = -y_{t+1}logz_{t+1}$. </p>
<p>Let’s set $\alpha_{t}=W_{h z} \mathbf{h}_{t}+\mathbf{b}_{z}$ and then we have $z_t=softmax(\alpha_t)$. By taking the derivative with respect to $ \alpha_t$, we get the following:</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial \alpha_{t}}=-\left(y_{t}-z_{t}\right)</script><p>Note the weight $W_{hz}$ is shared across all time sequence, thus we can dierentiate to it at each time</p>
<p>step and sum all together</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial W_{h z}}=\sum_{t} \frac{\partial \mathcal{L}}{\partial z_{t}} \frac{\partial z_{t}}{\partial W_{h z}}</script><p>Similarly, we can get the gradient w.r.t. bias $b_z$</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial b_{z}}=\sum_{t} \frac{\partial \mathcal{L}}{\partial z_{t}} \frac{\partial z_{t}}{\partial b_{z}}</script><p>Now let’s go through the details to derive the gradient w.r.t. $W_{hh}$. Considering at the time step $t \to t+1$ in figure1, </p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(t+1)}{\partial W_{h h}}=\frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{h h}}</script><p>where we only consider one step $t \to t+1$. And because the hidden state $h_{t+1}$ partially dependents</p>
<p>on $h_t$, so we can use backpropagation to compute the above partial derivative. Think further $W_{hh}$ is shared cross the whole time sequence. Thus, at the time step $(t -1) \to t$, we can further get the partial derivative w.r.t. $W_{hh}$ as follows</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(t+1)}{\partial W_{h h}}=\frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{t}} \frac{\partial \mathbf{h}_{t}}{\partial W_{h h}}</script><p>Thus, at the time step $t+1$, we can compute gradient w.r.t. $z_{t+1}$ and further use backpropagation</p>
<p>through time (BPTT) from t to 0 to calculate gradient w.r.t. $W_{hh}$, shown as the red chain in Fig. 1.</p>
<p>Thus, if we only consider the output $z_{t+1}$ at the time step $t+1$, we can yield the following gradient</p>
<p>w.r.t. $W_{hh}$</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(t+1)}{\partial W_{h h}}=\sum_{k=1}^{t} \frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{h h}}</script><p>Aggregate the gradients w.r.t. $W_{hh}$ over the whole time sequence with back propagation, we can</p>
<p>finally yield the following gradient w.r.t. $W_{hh}$</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial W_{h h}}=\sum_{t} \sum_{k=1}^{t+1} \frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{h h}}</script><p>Now we turn to derive the gradient w.r.t. $W_{xh}$. Similarly, we consider the time step $t + 1$ (only</p>
<p>contribution from $x_{t+1}$) and calculate the gradient w.r.t. to $W_{xh}$ as follows</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(t+1)}{\partial W_{x h}}=\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{x h}}</script><p>Because $h_t$ and $x_{t+1}$ both make contribution to $h_{t+1}$, we need to backpropagte to $h_t$ as well. If we</p>
<p>consider the contribution from the time step $t$, we can further get</p>
<script type="math/tex; mode=display">
\begin{aligned} & \frac{\partial \mathcal{L}(t+1)}{\partial W_{x h}}=\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{x h}}+\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t}} \frac{\partial \mathbf{h}_{t}}{\partial W_{x h}} \\=& \frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{x h}}+\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{t}} \frac{\partial \mathbf{h}_{t}}{\partial W_{x h}} \end{aligned}</script><p>Thus, summing up all contributions from $t$ to 0 via backpropagation, we can yield the gradient at</p>
<p>the time step $t + 1$</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}(t+1)}{\partial W_{x h}}=\sum_{k=1}^{t+1} \frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{x h}}</script><p>Further, we can take derivative w.r.t. $W_{xh}$ over the whole sequence as</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial W_{x h}}=\sum_{t} \sum_{k=1}^{t+1} \frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{x h}}</script><p>However, there are gradient vanishing or exploding problems to RNNs. Notice that $\frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}}$ indicates matrix multiplication over the sequence. Because RNNs need to backpropagate gradients over a long sequence (with small values in the matrix multiplication), gradient value will shrink layer over layer, and eventually vanish after a few time steps. Thus, the states that are far away from the current time step does not contribute to the parameters’ gradient computing (or parameters that RNNs is learning). Another direction is the gradient exploding, which attributed to large values in matrix multiplication.</p>
<h2 id="Forward-propagation"><a href="#Forward-propagation" class="headerlink" title="Forward propagation"></a>Forward propagation</h2><p><img src="/2019/03/02/DP-RNN/1042406-20170306142253375-175971779.png" alt="042406-20170306142253375-17597177"></p>
<p>$x$: input, $h$: hidden layer, $o$: output, $y$: target label, $L$: loss function, $t$: time </p>
<p>At time t, we have hidden state:</p>
<script type="math/tex; mode=display">
h^{(t)}=\phi(Ux^{(t)}+Wh^{(t)}+b)</script><p>where $\phi()$ is activation function, typically $tanh()$, and $b$ is the bias.</p>
<p>The output is at time t is:</p>
<script type="math/tex; mode=display">
o^{(t)}=Vh^{(t)}+c</script><p>Then the prediction is:</p>
<script type="math/tex; mode=display">
\hat{y}^{(t)}=\sigma(o^{(t)})</script><p>where $\sigma()$ is the activation function, typically $softmax()$.</p>
<h2 id="Back-Propagation-through-Time"><a href="#Back-Propagation-through-Time" class="headerlink" title="Back Propagation through Time"></a>Back Propagation through Time</h2><p><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="noopener">ref</a> <a href="https://peterroelants.github.io/posts/rnn-implementation-part01/" target="_blank" rel="noopener">ref2</a> <a href="https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d" target="_blank" rel="noopener">ref4</a></p>
<p>Let’s quickly recap the basic equations of our RNN.</p>
<script type="math/tex; mode=display">
\begin{aligned} s_{t} &=\tanh \left(U x_{t}+W s_{t-1}\right) \\ \hat{y}_{t} &=\operatorname{softmax}\left(V s_{t}\right) \end{aligned}</script><p>We also defined our <em>loss</em>, or error, to be the cross entropy loss, given by:</p>
<script type="math/tex; mode=display">
\begin{aligned} E_{t}\left(y_{t}, \hat{y}_{t}\right) &=-y_{t} \log \hat{y}_{t} \\ E(y, \hat{y}) &=\sum_{t} E_{t}\left(y_{t}, \hat{y}_{t}\right) \\ &=-\sum_{t} y_{t} \log \hat{y}_{t} \end{aligned}</script><p>Here, $y_t$ is the correct word at time step $t$, and $\hat y_t$ is our prediction. We typically treat the full sequence (sentence) as one training example, so the total error is just the sum of the errors at each time step (word).</p>
<p><img src="/2019/03/02/DP-RNN/rnn-bptt1.png" alt="nn-bptt"></p>
<p>Remember that our goal is to calculate the gradients of the error with respect to our parameters $U$, $V$ and $W$. </p>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial E_{3}}{\partial V} &=\frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial V} \\ &=\frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial z_{3}} \frac{\partial z_{3}}{\partial V} \\ &=\left(\hat{y}_{3}-y_{3}\right) \otimes s_{3} \end{aligned}</script><p>In the above, $z_{3}=V s_{3}$ and $\oplus$ is the outer product of two vectors. We can find that $\frac{\partial E_{3}}{\partial V}$ only depends on the values at the current time step, $\hat{y}_{3}, y_{3}, s_{3}$. </p>
<p>But the story is different for $\frac{\partial E_{3}}{\partial W}$ (and for $U$). To see why, we write out the chain rule, just as above:</p>
<script type="math/tex; mode=display">
\frac{\partial E_{3}}{\partial W}=\frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial s_{3}} \frac{\partial s_{3}}{\partial W}</script><p>Now, note that $s_{3}=\tanh \left(U x_{t}+W s_{2}\right)$ depends on $s_2$, which depends on $W$ and $s_1$, and so on. So if we take the derivative with respect to $W$ we can’t simply treat $s_2$ as a constant! We need to apply the chain rule again and what we really have is this:</p>
<script type="math/tex; mode=display">
\frac{\partial E_{3}}{\partial W}=\sum_{k=0}^{3} \frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial s_{3}} \frac{\partial s_{3}}{\partial s_{k}} \frac{\partial s_{k}}{\partial W}</script><p>We sum up the contributions of each time step to the gradient. In other words, because $W$ is used in every step up to the output we care about, we need to backpropagate gradients from $t=3$ through the network all the way to $t=0$.</p>
<p><img src="/2019/03/02/DP-RNN/rnn-bptt-with-gradients.png" alt="nn-bptt-with-gradient"></p>
<h2 id="Hand-Written-RNN"><a href="#Hand-Written-RNN" class="headerlink" title="Hand-Written RNN"></a>Hand-Written RNN</h2><p> <a href="https://towardsdatascience.com/only-numpy-vanilla-recurrent-neural-network-back-propagation-practice-math-956fbea32704" target="_blank" rel="noopener">hand-writtrn-deduction-1</a> <a href="https://medium.com/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316" target="_blank" rel="noopener">hand-writtrn-deduction-2</a></p>
<p>Here I am going to give an simple problem about how RNN can be used to solve problems and aim to have a better understanding of how forward and backward propagation in RNN work.</p>
<p>Anyways here we go. The problem is very simple, we are going to use RNN to count how many ones there are in the given data.</p>
<script type="math/tex; mode=display">
\begin{array}{l}{x=[ } \\ {[0,0,1]} \\ {[0,1,1]} \\ {[1,1,1]} \\ {\text { ]}}\end{array}\\
y = [1,2,3]</script><p>As seen above, the training data is $x$  and $y$ is the groundtruth. </p>
<p>The corresponding RNN architecture looks like the following:</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-18 at 9.36.36 PM.png" alt="creen Shot 2019-06-18 at 9.36.36 P"></p>
<p>which is the unrolled version of our network architecture.</p>
<p>For this network, we have two weight matrixs, $W_x$ and $W_{rec}$, and the forward propagation is:</p>
<script type="math/tex; mode=display">
S_{1}=S_0 \cdot W_{r e c}+X_{1} \cdot W_{x} \\
S_{2}=S_1 \cdot W_{r e c}+X_{2} \cdot W_{x}\\
S_{3}=S_2 \cdot W_{r e c}+X_{3} \cdot W_{x}</script><p>We define the MSE loss function:</p>
<script type="math/tex; mode=display">
\cos t=\frac{1}{m}\left(S_{3}-y\right)^{2}</script><p>Now lets perform back propagation through time. We have to get derivative respect to $W_x$ and $W_{rec}$ for each state.</p>
<p><strong>State3:</strong></p>
<script type="math/tex; mode=display">
\frac{d \xi}{d W_{x}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d W_x}=\frac{2}{m}\left(S_{3}-y\right) \cdot x_{3}\\
\frac{d \xi}{d W_{r e c}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d W r e c}=\frac{2}{m}\left(S_{3}-y\right) \cdot S_{2}</script><p><strong>State2:</strong></p>
<script type="math/tex; mode=display">
\frac{d \xi}{d W_{x}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d W_x}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot x_{2}\\

\frac{d \xi}{d W_{rec}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d W_{rec}}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot S_{1}\\</script><p><strong>State1:</strong></p>
<script type="math/tex; mode=display">
\frac{d \xi}{d W_{x}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d S_1}\cdot \frac{d S_{1}}{d W_x}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot W_{rec}\cdot x_{2}\\
\frac{d \xi}{d W_{rec}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d S_1}\cdot \frac{d S_{1}}{d W_{rec}}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot W_{rec}\cdot S_{1}\\</script><p>So that’s it! The simple math behind training RNN.</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-18 at 9.53.34 PM.png" alt="creen Shot 2019-06-18 at 9.53.34 P"></p>
<h2 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></div>
<div class="line"><span class="string">    activation function.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></div>
<div class="line"><span class="string">    a minibatch size of N.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - x: Input data for this timestep, of shape (N, D).</span></div>
<div class="line"><span class="string">    - prev_h: Hidden state from previous timestep, of shape (N, H)</span></div>
<div class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div>
<div class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div>
<div class="line"><span class="string">    - b: Biases of shape (H,)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">    - cache: Tuple of values needed for the backward pass.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    next_h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement a single forward step for the vanilla RNN. Store the next  #</span></div>
<div class="line">    <span class="comment"># hidden state and any values you need for the backward pass in the next_h   #</span></div>
<div class="line">    <span class="comment"># and cache variables respectively.                                          #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">    raw_h = x.dot(Wx) + prev_h.dot(Wh) + b</div>
<div class="line">    next_h = np.tanh(raw_h)</div>
<div class="line">    cache = (x, prev_h, Wx, Wh, raw_h, next_h)</div>
<div class="line"></div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="keyword">return</span> next_h, cache</div>
</pre></td></tr></table></figure>

</div></div>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div>
<div class="line"></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Backward pass for a single timestep of a vanilla RNN.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Inputs:</span></div>
<div class="line"><span class="string">- dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">- cache: Cache object from the forward pass</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Returns a tuple of:</span></div>
<div class="line"><span class="string">- dx: Gradients of input data, of shape (N, D)</span></div>
<div class="line"><span class="string">- dprev_h: Gradients of previous hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">- dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></div>
<div class="line"><span class="string">- dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></div>
<div class="line"><span class="string">- db: Gradients of bias vector, of shape (H,)</span></div>
<div class="line"><span class="string">"""</span></div>
<div class="line">dx, dprev_h, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for a single step of a vanilla RNN.      #</span></div>
<div class="line"><span class="comment">#                                                                            #</span></div>
<div class="line"><span class="comment"># HINT: For the tanh function, you can compute the local derivative in terms #</span></div>
<div class="line"><span class="comment"># of the output value from tanh.                                             #</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">x, prev_h, Wx, Wh, raw_h, next_h = cache</div>
<div class="line"></div>
<div class="line">draw_h = (<span class="number">1</span>-next_h**<span class="number">2</span>)*dnext_h</div>
<div class="line">db = np.sum(draw_h,axis=<span class="number">0</span>)</div>
<div class="line">dx = draw_h.dot(Wx.T)</div>
<div class="line">dWx = x.T.dot(draw_h)</div>
<div class="line">dprev_h = draw_h.dot(Wh.T)</div>
<div class="line">dWh = prev_h.T.dot(draw_h)</div>
<div class="line"></div>
<div class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div>
</pre></td></tr></table></figure>

</div></div>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div>
<div class="line"></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    the RNN forward, we return the hidden states for all timesteps.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Inputs:</span></div>
<div class="line"><span class="string">- x: Input data for the entire timeseries, of shape (N, T, D).</span></div>
<div class="line"><span class="string">- h0: Initial hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">- Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div>
<div class="line"><span class="string">- Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div>
<div class="line"><span class="string">- b: Biases of shape (H,)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Returns a tuple of:</span></div>
<div class="line"><span class="string">- h: Hidden states for the entire timeseries, of shape (N, T, H).</span></div>
<div class="line"><span class="string">- cache: Values needed in the backward pass</span></div>
<div class="line"><span class="string">"""</span></div>
<div class="line">h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement forward pass for a vanilla RNN running on a sequence of    #</span></div>
<div class="line"><span class="comment"># input data. You should use the rnn_step_forward function that you defined  #</span></div>
<div class="line"><span class="comment"># above. You can use a for loop to help compute the forward pass.            #</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">N, T, D = x.shape</div>
<div class="line">_, H = h0.shape</div>
<div class="line">h = np.zeros((N,T,H))</div>
<div class="line">cache = &#123;&#125;</div>
<div class="line">prev_h = h0</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(T):</div>
<div class="line">    x_ = x[:,i,:]</div>
<div class="line">    next_h, cache_ = rnn_step_forward(x_, prev_h, Wx, Wh, b)</div>
<div class="line">    prev_h = next_h</div>
<div class="line">    h[:,i,:] = next_h</div>
<div class="line">    cache[i] = cache_</div>
<div class="line"></div>
<div class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="keyword">return</span> h, cache</div>
</pre></td></tr></table></figure>

</div></div>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - dh: Upstream gradients of all hidden states, of shape (N, T, H). </span></div>
<div class="line"><span class="string">    </span></div>
<div class="line"><span class="string">    NOTE: 'dh' contains the upstream gradients produced by the </span></div>
<div class="line"><span class="string">    individual loss functions at each timestep, *not* the gradients</span></div>
<div class="line"><span class="string">    being passed between timesteps (which you'll have to compute yourself</span></div>
<div class="line"><span class="string">    by calling rnn_step_backward in a loop).</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - dx: Gradient of inputs, of shape (N, T, D)</span></div>
<div class="line"><span class="string">    - dh0: Gradient of initial hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></div>
<div class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)</span></div>
<div class="line"><span class="string">    - db: Gradient of biases, of shape (H,)</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for a vanilla RNN running an entire      #</span></div>
<div class="line">    <span class="comment"># sequence of data. You should use the rnn_step_backward function that you   #</span></div>
<div class="line">    <span class="comment"># defined above. You can use a for loop to help compute the backward pass.   #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    </div>
<div class="line">    N, T, H = dh.shape</div>
<div class="line">    x = cache[<span class="number">0</span>][<span class="number">0</span>]</div>
<div class="line">    _,D  = x.shape</div>
<div class="line">    dx = np.zeros((N, T, D))</div>
<div class="line">    dh0 = np.zeros((N, H))</div>
<div class="line">    dWx = np.zeros((D, H))</div>
<div class="line">    dWh = np.zeros((H, H))</div>
<div class="line">    db = np.zeros(H)</div>
<div class="line">    </div>
<div class="line">    dprev_h = np.zeros((N, H))</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</div>
<div class="line">        dnext_h = dh[:,i,:] + dprev_h</div>
<div class="line">        dx[:, i, :], dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dnext_h, cache[i])</div>
<div class="line">        dWx += dWx_</div>
<div class="line">        dWh += dWh_</div>
<div class="line">        db += db_</div>
<div class="line">    dh0 = dprev_h</div>
<div class="line">        </div>
<div class="line"></div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div>
</pre></td></tr></table></figure>

</div></div>
<h2 id="RNN-Vanishing-Gradients-Problem"><a href="#RNN-Vanishing-Gradients-Problem" class="headerlink" title="RNN Vanishing Gradients Problem"></a>RNN Vanishing Gradients Problem</h2><h3 id="What-is-gradients-vanishing"><a href="#What-is-gradients-vanishing" class="headerlink" title="What is gradients vanishing?"></a>What is gradients vanishing?</h3><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 5.58.02 PM.png" alt="creen Shot 2019-08-04 at 5.58.02 P"></p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 6.01.14 PM.png" alt="creen Shot 2019-08-04 at 6.01.14 P"></p>
<h3 id="Why-it-is-a-problem"><a href="#Why-it-is-a-problem" class="headerlink" title="Why it is a problem?"></a>Why it is a problem?</h3><p>The information from long-term timesteps is gone.</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 6.11.11 PM.png" alt="creen Shot 2019-08-04 at 6.11.11 P"></p>
<p>If the gradient becomes vanishingly small over longer distances (step t to step t+n), then we can’t tell whether:</p>
<ol>
<li>There’s no dependency between step t and t+n in the data</li>
<li>We have wrong parameters to capture the true dependency between t and t+n</li>
</ol>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 6.18.31 PM.png" alt="creen Shot 2019-08-04 at 6.18.31 P"></p>
<h1 id="Long-Short-Term-Memory-networks"><a href="#Long-Short-Term-Memory-networks" class="headerlink" title="Long Short Term Memory networks"></a><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Long Short Term Memory networks</a></h1><p>It is usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.</p>
<p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-SimpleRNN.png" alt="STM3-SimpleRN"></p>
<p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-chain.png" alt="STM3-chai"></p>
<p>For now, let’s just try to get comfortable with the notation we’ll be using.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM2-notation.png" alt="STM2-notatio"></p>
<p>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.</p>
<h2 id="The-core-idea-behind-LSTMs"><a href="#The-core-idea-behind-LSTMs" class="headerlink" title="The core idea behind LSTMs"></a>The core idea behind LSTMs</h2><p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.</p>
<p>The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-C-line.png" alt="STM3-C-lin"></p>
<p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</p>
<p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.<img src="/2019/03/02/DP-RNN/LSTM3-gate.png" alt="STM3-gat"></p>
<p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!” An LSTM has three of these gates, to protect and control the cell state.</p>
<h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”</p>
<p>Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-focus-f.png" alt="STM3-focus-"></p>
<p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, $\tilde C$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.</p>
<p>In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-focus-i.png" alt="STM3-focus-"></p>
<p>It’s now time to update the old cell state, $C_{t-1}$, into the new cell state $C_t$. The previous steps already decided what to do, we just need to actually do it.</p>
<p>We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t* \tilde C_{t}$.</p>
<p> This is the new candidate values, scaled by how much we decided to update each state value.</p>
<p>In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-focus-C.png" alt="STM3-focus-"></p>
<p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between -1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</p>
<p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.</p>
<p><img src="/2019/03/02/DP-RNN/LSTM3-focus-o-2079403.png" alt="STM3-focus-o-207940"></p>
<h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>Recall that the forward pass of LSTM is like:</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-07-01 at 4.30.58 PM.png" alt="creen Shot 2019-07-01 at 4.30.58 P"></p>
<p>The unrolled network during the forward pass is shown above. The cell state at time T, $c^T$ is  responsible for computing h as well as the next cell state $c^{T+1}$. At each time step, the cell output h is shown to be passed to some more layers on which cost function C is computed, as the way an LSTM would be used in a typical application like captioning or language modeling. <a href="http://arunmallya.github.io/writeups/nn/lstm/index.html#/6" target="_blank" rel="noopener">source</a></p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-07-01 at 4.34.08 PM.png" alt="creen Shot 2019-07-01 at 4.34.08 P"></p>
<p>The unrolled network during the backward pass is shown below. All the arrows in the previous slide have now changed their direction. The cell state at time T, $c^{T}$ receive gradients from $h^T$ as well as the next cell state $c^{T+1}$. </p>
<p>Note that for the last node, since it dosen’t have a next time stamp, it receive no gradients from $c^{T+1}$ and $h^T$, which means $d_{next_state} = 0$ and $d_{next_hidden}=0$. </p>
<h2 id="Numpy-Implementation-1"><a href="#Numpy-Implementation-1" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div>
<div class="line"></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Forward pass for a single timestep of an LSTM.</span></div>
<div class="line"><span class="string">The input data has dimension D, the hidden state has dimension H, and we use</span></div>
<div class="line"><span class="string">a minibatch size of N.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Note that a sigmoid() function has already been provided for you in this file.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Inputs:</span></div>
<div class="line"><span class="string">- x: Input data, of shape (N, D)</span></div>
<div class="line"><span class="string">- prev_h: Previous hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">- prev_c: previous cell state, of shape (N, H)</span></div>
<div class="line"><span class="string">- Wx: Input-to-hidden weights, of shape (D, 4H)</span></div>
<div class="line"><span class="string">- Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></div>
<div class="line"><span class="string">- b: Biases, of shape (4H,)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">Returns a tuple of:</span></div>
<div class="line"><span class="string">- next_h: Next hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">- next_c: Next cell state, of shape (N, H)</span></div>
<div class="line"><span class="string">- cache: Tuple of values needed for backward pass.</span></div>
<div class="line"><span class="string">"""</span></div>
<div class="line">next_h, next_c, cache = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line"><span class="comment">#############################################################################</span></div>
<div class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for a single timestep of an LSTM.        #</span></div>
<div class="line"><span class="comment"># You may want to use the numerically stable sigmoid implementation above.  #</span></div>
<div class="line"><span class="comment">#############################################################################</span></div>
<div class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">N,D = x.shape</div>
<div class="line">_,H = prev_h.shape</div>
<div class="line"></div>
<div class="line">gates = x.dot(Wx) + prev_h.dot(Wh) + b <span class="comment"># (N, 4H)</span></div>
<div class="line">gate_i = sigmoid(gates[:,:H]) <span class="comment"># (N,H)</span></div>
<div class="line">gate_f = sigmoid(gates[:,H:<span class="number">2</span>*H])</div>
<div class="line">gate_o = sigmoid(gates[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div>
<div class="line">gate_g = np.tanh(gates[:,<span class="number">3</span>*H:])</div>
<div class="line"></div>
<div class="line">next_c = prev_c*gate_f + gate_g*gate_i</div>
<div class="line">next_h = gate_o*np.tanh(next_c)</div>
<div class="line"></div>
<div class="line">cache = (x, prev_h, prev_c, Wx, Wh, gate_i, gate_f, gate_o, gate_g, next_c)</div>
<div class="line"></div>
<div class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"><span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line"><span class="comment">##############################################################################</span></div>
<div class="line"></div>
<div class="line"><span class="keyword">return</span> next_h, next_c, cache</div>
</pre></td></tr></table></figure>

</div></div>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></div>
<div class="line"><span class="string">    - cache: Values from the forward pass</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></div>
<div class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></div>
<div class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></div>
<div class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></div>
<div class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></div>
<div class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    dx, dprev_h, dprev_c, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="comment">#############################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for a single timestep of an LSTM.       #</span></div>
<div class="line">    <span class="comment">#                                                                           #</span></div>
<div class="line">    <span class="comment"># HINT: For sigmoid and tanh you can compute local derivatives in terms of  #</span></div>
<div class="line">    <span class="comment"># the output value from the nonlinearity.                                   #</span></div>
<div class="line">    <span class="comment">#############################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    </div>
<div class="line">    (x, prev_h, prev_c, Wx, Wh, gate_i, gate_f, gate_o, gate_g, next_c) = cache</div>
<div class="line">    N,H = dnext_h.shape</div>
<div class="line">    _,D = x.shape</div>
<div class="line">    </div>
<div class="line">    dgate_o = dnext_h*np.tanh(next_c)</div>
<div class="line">    dtanh_next_c = dnext_h*gate_o <span class="comment">#(N, H)</span></div>
<div class="line">    </div>
<div class="line">    dnext_c_ = dtanh_next_c*(<span class="number">1</span>-np.tanh(next_c)**<span class="number">2</span>)</div>
<div class="line">    dnext_c = dnext_c_ + dnext_c</div>
<div class="line">    </div>
<div class="line">    dgate_f = dnext_c*prev_c</div>
<div class="line">    dprev_c = dnext_c*gate_f</div>
<div class="line">    </div>
<div class="line">    dgate_g = dnext_c*gate_i</div>
<div class="line">    dgate_i = dnext_c*gate_g</div>
<div class="line">    </div>
<div class="line">    dgate_g = dgate_g*(<span class="number">1</span>-gate_g**<span class="number">2</span>)</div>
<div class="line">    dgate_o = dgate_o*gate_o*(<span class="number">1</span>-gate_o)</div>
<div class="line">    dgate_f = dgate_f*gate_f*(<span class="number">1</span>-gate_f)</div>
<div class="line">    dgate_i = dgate_i*gate_i*(<span class="number">1</span>-gate_i)</div>
<div class="line">    </div>
<div class="line">    dgate = np.concatenate([dgate_i,dgate_f,dgate_o,dgate_g],axis=<span class="number">1</span>)</div>
<div class="line">    db = np.sum(dgate,axis=<span class="number">0</span>)</div>
<div class="line">    dx = dgate.dot(Wx.T)</div>
<div class="line">    dprev_h = dgate.dot(Wh.T)</div>
<div class="line">    dWx = x.T.dot(dgate)</div>
<div class="line">    dWh = prev_h.T.dot(dgate)</div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div>
</pre></td></tr></table></figure>

</div></div>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span></div>
<div class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></div>
<div class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></div>
<div class="line"><span class="string">    the LSTM forward, we return the hidden states for all timesteps.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Note that the initial cell state is passed as input, but the initial cell</span></div>
<div class="line"><span class="string">    state is set to zero. Also note that the cell state is not returned; it is</span></div>
<div class="line"><span class="string">    an internal variable to the LSTM and is not accessed from outside.</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></div>
<div class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></div>
<div class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></div>
<div class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></div>
<div class="line"><span class="string">    - b: Biases of shape (4H,)</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></div>
<div class="line"><span class="string">    - cache: Values needed for the backward pass.</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="comment">#############################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for an LSTM over an entire timeseries.   #</span></div>
<div class="line">    <span class="comment"># You should use the lstm_step_forward function that you just defined.      #</span></div>
<div class="line">    <span class="comment">#############################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    cache = &#123;&#125;</div>
<div class="line">    N, T, D = x.shape</div>
<div class="line">    _, H = h0.shape</div>
<div class="line">    h = np.zeros((N,T,H))</div>
<div class="line">    prev_h = h0</div>
<div class="line">    prev_c = np.zeros_like(h0)</div>
<div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</div>
<div class="line">        prev_h, prev_c, cache[t] = lstm_step_forward(x[:,t,:], prev_h, prev_c, Wx, Wh, b)</div>
<div class="line">        h[:,t,:] = prev_h</div>
<div class="line"></div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> h, cache</div>
</pre></td></tr></table></figure>

</div></div>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Inputs:</span></div>
<div class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></div>
<div class="line"><span class="string">    - cache: Values from the forward pass</span></div>
<div class="line"><span class="string"></span></div>
<div class="line"><span class="string">    Returns a tuple of:</span></div>
<div class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></div>
<div class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></div>
<div class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></div>
<div class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></div>
<div class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div>
<div class="line">    <span class="comment">#############################################################################</span></div>
<div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for an LSTM over an entire timeseries.  #</span></div>
<div class="line">    <span class="comment"># You should use the lstm_step_backward function that you just defined.     #</span></div>
<div class="line">    <span class="comment">#############################################################################</span></div>
<div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line"></div>
<div class="line">    N, T, H = dh.shape</div>
<div class="line">    _, D = cache[<span class="number">0</span>][<span class="number">0</span>].shape</div>
<div class="line">    dx = np.zeros((N,T,D))</div>
<div class="line">    dWx = np.zeros((D,<span class="number">4</span>*H))</div>
<div class="line">    dWh = np.zeros((H,<span class="number">4</span>*H))</div>
<div class="line">    db = np.zeros((<span class="number">4</span>*H))</div>
<div class="line">    dprev_c = np.zeros((N, H)) </div>
<div class="line">    dprev_h = np.zeros((N, H)) </div>
<div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</div>
<div class="line">        dnext_h = dprev_h + dh[:,t,:]</div>
<div class="line">        dnext_c = dprev_c</div>
<div class="line">        dx[:,t,:], dprev_h, dprev_c, dWx_, dWh_, db_ = lstm_step_backward(dnext_h, dnext_c, cache[t])</div>
<div class="line">        dWx += dWx_</div>
<div class="line">        dWh += dWh_</div>
<div class="line">        db += db_</div>
<div class="line">    dh0 = dprev_h</div>
<div class="line"></div>
<div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div>
<div class="line">    <span class="comment">##############################################################################</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div>
</pre></td></tr></table></figure>

</div></div>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a><a href="https://www.d2l.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="noopener">GRU</a></h1><p>So now we know how an LSTM work, let’s briefly look at the GRU. The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-20 at 12.39.27 PM.png" alt="creen Shot 2019-06-20 at 12.39.27 P"></p>
<p><strong>Reset Gate</strong></p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-20 at 12.49.39 PM.png" alt="creen Shot 2019-06-20 at 12.49.39 P"></p>
<script type="math/tex; mode=display">
\mathbf{R}_{t}=\sigma\left(\mathbf{X}_{t} \mathbf{W}_{x r}+\mathbf{H}_{t-1} \mathbf{W}_{h r}+\mathbf{b}_{r}\right)</script><script type="math/tex; mode=display">
\mathbf{Z}_{t}=\sigma\left(\mathbf{X}_{t} \mathbf{W}_{x z}+\mathbf{H}_{t-1} \mathbf{W}_{h z}+\mathbf{b}_{z}\right)</script><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-20 at 12.51.45 PM.png" alt="creen Shot 2019-06-20 at 12.51.45 P"></p>
<script type="math/tex; mode=display">
\tilde{\mathbf{H}}_{t}=\tanh \left(\mathbf{X}_{t} \mathbf{W}_{x h}+\left(\mathbf{R}_{t} \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{h h}+\mathbf{b}_{h}\right)</script><script type="math/tex; mode=display">
\mathbf{H}_{t}=\mathbf{Z}_{t} \odot \mathbf{H}_{t-1}+\left(1-\mathbf{Z}_{t}\right) \odot \tilde{\mathbf{H}}_{t}</script><h1 id="How-LSTM-GRU-Solve-Vanishing-Gradients"><a href="#How-LSTM-GRU-Solve-Vanishing-Gradients" class="headerlink" title="How LSTM/GRU Solve Vanishing Gradients"></a>How LSTM/GRU Solve Vanishing Gradients</h1><p>In order to understand this question, we need to introdcue shortcut firstly. In ResNet, the shortcut is like:</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 8.51.23 PM.png" alt="creen Shot 2019-08-04 at 8.51.23 P"></p>
<p>which is $x^{(t)}=x^{(t-1)}+F(x^{(t-1)})$. The next output is the combination of two parts: shortcut($x^{(t-1)}$) and non-linear transformation of $x^{(t-1)}$. Because of the shortcut, there is always a “1” gradient flowing back in backpropagation. In some extent, it sovles the problem of vanishing gradient.</p>
<p>Now back to our fancy RNN, take GRU as example, the updating rules of GRU is:</p>
<p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 8.51.07 PM.png" alt="creen Shot 2019-08-04 at 8.51.07 P"></p>
<p>Let’s focus on how we get the new hidden state $h^{(t)}$ and let’s roll out the formulation:</p>
<script type="math/tex; mode=display">
\boldsymbol{h}^{(t)}=\boldsymbol{h}^{(t-1)}+ \left(\tilde{\boldsymbol{h}}^{(t)}-\boldsymbol{h}^{(t-1)}\right)  \circ  \boldsymbol{u}^{(t)}</script><p>which is very similar to the ResNet becasue there is a direct flow from previous state to the next state.</p>
<h1 id="Keras-Learn-the-Alphabet"><a href="#Keras-Learn-the-Alphabet" class="headerlink" title="Keras - Learn the Alphabet"></a>Keras - Learn the Alphabet</h1><p>In this implementation, we are going to develop and constrast a number of different LSTM networks. The task we are going to perform is that given a letter of the alphabet, predict the next letter. This is a simple sequence prediction problem that once understood can be generalized to other sequence prediction problems like time series prediction and sequence classification.</p>
<h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line">np.random.seed(<span class="number">7</span>)</div>
<div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div>
<div class="line">char2int = defaultdict(int)</div>
<div class="line">int2char = defaultdict(str)</div>
<div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div>
<div class="line">    char2int[val] = idx</div>
<div class="line">    int2char[idx] = val</div>
</pre></td></tr></table></figure>
<p>Because neural network can only process number, we map the letters to integer value.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line">seq_length = <span class="number">1</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div>
<div class="line">    seq_in = alphabet[i:i+seq_length]</div>
<div class="line">    seq_out = alphabet[i+seq_length]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
</pre></td></tr></table></figure>
<p>Here, we create our input dataset and corresponding output dataset; we use an input length of 1. Running the code will produce the following output:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line">A -&gt; B</div>
<div class="line">B -&gt; C</div>
<div class="line">C -&gt; D</div>
<div class="line">...</div>
<div class="line">X -&gt; Y</div>
<div class="line">Y -&gt; Z</div>
</pre></td></tr></table></figure>
<p>Then, we need to reshape our data into a format expected by the LSTM networks, that is, <code>[data_size,time_steps,feature_num]</code>. Then we can normalize the input integers to the range <code>[0,1]</code>. Finally, we can think of this problem as a sequence classification task, where each of the 26 letters represents a different class. As such, we can convert the output (y) to a one hot encoding, using the Keras built-in function <strong>to_categorical()</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
</pre></td></tr></table></figure>
<h2 id="One-Char-to-One-Char"><a href="#One-Char-to-One-Char" class="headerlink" title="One-Char to One-Char"></a>One-Char to One-Char</h2><p>Let’s start off by designing a simple LSTM to learn how to predict the next character in the alphabet given the context of just one character.</p>
<p>Let’s define an LSTM network with 32 units and an output layer with a softmax activation function for making predictions. Because this is a multi-class classification problem, we can use the log loss function (called “<strong>categorical_crossentropy</strong>” in Keras), and optimize the network using the ADAM optimization function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">model = Sequential()</div>
<div class="line">model.add(LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div>
<div class="line">model.add(Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line">model.fit(X,y,epochs=<span class="number">500</span>,batch_size=<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<p>After we fit the model we can evaluate and summarize the performance on the entire training dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">scores = model.evaluate(X,y)</div>
<div class="line">print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
</pre></td></tr></table></figure>
<p>We can then re-run the training data through the network and generate predictions, converting both the input and output pairs back into their original character format to get a visual idea of how well the network learned the problem.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div>
<div class="line">    x = np.reshape(pattern,(<span class="number">1</span>,len(pattern),<span class="number">1</span>))</div>
<div class="line">    x = x/float(len(alphabet))</div>
<div class="line">    prediction = model.predict(x)</div>
<div class="line">    index = np.argmax(prediction)</div>
<div class="line">    result = int2char[index]</div>
<div class="line">    seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,result)</div>
</pre></td></tr></table></figure>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
</pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">84.00</span>%</div>
<div class="line">['A'] -&gt; B</div>
<div class="line">['B'] -&gt; C</div>
<div class="line">['C'] -&gt; D</div>
<div class="line">['D'] -&gt; E</div>
<div class="line">['E'] -&gt; F</div>
<div class="line">['F'] -&gt; G</div>
<div class="line">['G'] -&gt; H</div>
<div class="line">['H'] -&gt; I</div>
<div class="line">['I'] -&gt; J</div>
<div class="line">['J'] -&gt; K</div>
<div class="line">['K'] -&gt; L</div>
<div class="line">['L'] -&gt; M</div>
<div class="line">['M'] -&gt; N</div>
<div class="line">['N'] -&gt; O</div>
<div class="line">['O'] -&gt; P</div>
<div class="line">['P'] -&gt; Q</div>
<div class="line">['Q'] -&gt; R</div>
<div class="line">['R'] -&gt; S</div>
<div class="line">['S'] -&gt; T</div>
<div class="line">['T'] -&gt; U</div>
<div class="line">['U'] -&gt; W</div>
<div class="line">['V'] -&gt; Y</div>
<div class="line">['W'] -&gt; Z</div>
<div class="line">['X'] -&gt; Z</div>
<div class="line">['Y'] -&gt; Z</div>
</pre></td></tr></table></figure>
<p>We can see that this problem is indeed difficult for the network to learn. The reason is, the poor LSTM units do not have any context to work with. Each input-output pattern is shown to the network in a random order and the state of the network is reset after each pattern (each batch where each batch contains one pattern). This is abuse of the LSTM network architecture, treating it like a standard multilayer Perceptron. Next, let’s try a different framing of the problem in order to provide more sequence to the network from which to learn.</p>
<h2 id="A-Three-Char-Feature-Window-to-One-Char-Mapping"><a href="#A-Three-Char-Feature-Window-to-One-Char-Mapping" class="headerlink" title="A Three-Char Feature Window to One-Char Mapping"></a>A Three-Char Feature Window to One-Char Mapping</h2><p>A popular approach to adding more context to data for multilayer Perceptrons is to use the window method. We can do this by increasing the input sequence length length from 1 to 3, for example:<code>seq_length=3</code>, which creates training patterns like:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">ABC -&gt; D</div>
<div class="line">BCD -&gt; E</div>
<div class="line">CDE -&gt; F</div>
</pre></td></tr></table></figure>
<p>Each element in the sequence is then provided as a new input feature to the network. This requires a modification of how the input sequences reshaped in the data preparation step:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># reshape X to be #[batch_size,time_stamps,features]</span></div>
<div class="line">X = numpy.reshape(dataX, (len(dataX), <span class="number">1</span>, seq_length))</div>
</pre></td></tr></table></figure>
<p>The entire code is provided below for completeness.</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line"></div>
<div class="line">np.random.seed(<span class="number">7</span>)</div>
<div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div>
<div class="line">char2int = defaultdict(int)</div>
<div class="line">int2char = defaultdict(str)</div>
<div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div>
<div class="line">    char2int[val] = idx</div>
<div class="line">    int2char[idx] = val</div>
<div class="line"></div>
<div class="line">seq_length = <span class="number">3</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div>
<div class="line">    seq_in = alphabet[i:i+seq_length]</div>
<div class="line">    seq_out = alphabet[i+seq_length]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
<div class="line"></div>
<div class="line">input_shape = (len(dataX),<span class="number">1</span>,seq_length)</div>
<div class="line"><span class="comment"># input_shape = (len(dataX),1,seq_length)</span></div>
<div class="line"></div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line"></div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
<div class="line"></div>
<div class="line">model = Sequential()</div>
<div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div>
<div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div>
<div class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div>
<div class="line">        x = np.reshape(pattern,(<span class="number">1</span>,<span class="number">1</span>,len(pattern)))</div>
<div class="line">        x = x/float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div>
<div class="line">        print(seq_in,<span class="string">'-&gt;'</span>,result)</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div>
<div class="line">    model.fit(X,y,epochs=<span class="number">500</span>,batch_size=<span class="number">1</span>,verbose=<span class="number">1</span>)</div>
<div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div>
<div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
<div class="line">    predict(model)</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    naive_lstm()</div>
</pre></td></tr></table></figure>

</div></div>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">86.96</span>%</div>
<div class="line">['A', 'B', 'C'] -&gt; D</div>
<div class="line">['B', 'C', 'D'] -&gt; E</div>
<div class="line">...</div>
<div class="line">['T', 'U', 'V'] -&gt; X</div>
<div class="line">['U', 'V', 'W'] -&gt; Z</div>
<div class="line">['V', 'W', 'X'] -&gt; Z</div>
<div class="line">['W', 'X', 'Y'] -&gt; Z</div>
</pre></td></tr></table></figure>
<p>We can see a little improvement in the performance that may or may not be true.</p>
<p>Again, this is a misuse of the LSTM network by a poor framing of the problem. Indeed, the sequences of letters are time steps of one feature rather than one time step of separate features. We have given more context to the network, but not more sequence as it expected.</p>
<p>In the next section, we will give more context to the network in the form of time steps.</p>
<h2 id="A-Three-Char-Time-Step-Window-to-One-Char-Mapping"><a href="#A-Three-Char-Time-Step-Window-to-One-Char-Mapping" class="headerlink" title="A Three-Char Time Step Window to One-Char Mapping"></a>A Three-Char Time Step Window to One-Char Mapping</h2><p>We still take as input a sequence with length being 3, <code>seq_length=3</code>. The difference is that the reshaping of the input data takes the sequence as a time step sequence of one feature, rather than a single time step of multiple features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
</pre></td></tr></table></figure>
<p>This is the correct intended use of providing sequence context to your LSTM in Keras. The full code example is provided below for completeness.</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line"></div>
<div class="line">np.random.seed(<span class="number">7</span>)</div>
<div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div>
<div class="line">char2int = defaultdict(int)</div>
<div class="line">int2char = defaultdict(str)</div>
<div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div>
<div class="line">    char2int[val] = idx</div>
<div class="line">    int2char[idx] = val</div>
<div class="line"></div>
<div class="line">seq_length = <span class="number">3</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div>
<div class="line">    seq_in = alphabet[i:i+seq_length]</div>
<div class="line">    seq_out = alphabet[i+seq_length]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
<div class="line"></div>
<div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line"></div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
<div class="line"></div>
<div class="line">model = Sequential()</div>
<div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div>
<div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div>
<div class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div>
<div class="line">        x = np.reshape(pattern,(<span class="number">1</span>,len(pattern),<span class="number">1</span>))</div>
<div class="line">        x = x/float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div>
<div class="line">        print(seq_in,<span class="string">'-&gt;'</span>,result)</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div>
<div class="line">    model.fit(X,y,epochs=<span class="number">500</span>,batch_size=<span class="number">1</span>,verbose=<span class="number">1</span>)</div>
<div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div>
<div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
<div class="line">    predict(model)</div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    naive_lstm()</div>
</pre></td></tr></table></figure>

</div></div>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">100.00</span>%</div>
<div class="line">['A', 'B', 'C'] -&gt; D</div>
<div class="line">...</div>
<div class="line">['W', 'X', 'Y'] -&gt; Z</div>
</pre></td></tr></table></figure>
<p>We can see that the model learns the problem perfectly as evidenced by the model evaluation and the example predictions.</p>
<h2 id="LSTM-State-Within-A-Batch"><a href="#LSTM-State-Within-A-Batch" class="headerlink" title="LSTM State Within A Batch"></a>LSTM State Within A Batch</h2><p>The Keras implementation of LSTMs resets the state of the network after each batch.</p>
<p>This suggests that if we had a batch size large enough to hold all input patterns and if all the input patterns were ordered sequentially, that the LSTM could use the context of the sequence within the batch to better learn the sequence.</p>
<p>We can demonstrate this easily by modifying the first example for learning a one-to-one mapping and increasing the batch size from 1 to the size of the training dataset.</p>
<p>Additionally, Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.</p>
<p>And the training epoch becomes 5000 from 500.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">model.fit(X,y,epochs=<span class="number">5000</span>,batch_size=len(X),verbose=<span class="number">1</span>)</div>
</pre></td></tr></table></figure>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div>
<div class="line">np.random.seed(<span class="number">7</span>)</div>
<div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div>
<div class="line">char2int = defaultdict(int)</div>
<div class="line">int2char = defaultdict(str)</div>
<div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div>
<div class="line">    char2int[val] = idx</div>
<div class="line">    int2char[idx] = val</div>
<div class="line">seq_length = <span class="number">1</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div>
<div class="line">    seq_in = alphabet[i:i+seq_length]</div>
<div class="line">    seq_out = alphabet[i+seq_length]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
<div class="line">dataX = pad_sequences(dataX,maxlen=seq_length,dtype=<span class="string">'float32'</span>)</div>
<div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
<div class="line">model = Sequential()</div>
<div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div>
<div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div>
<div class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div>
<div class="line">        x = np.reshape(pattern,(<span class="number">1</span>,len(pattern),<span class="number">1</span>))</div>
<div class="line">        x = x/float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div>
<div class="line">        print(seq_in,<span class="string">'-&gt;'</span>,result)</div>
<div class="line">    print(<span class="string">"Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"</span>)</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</div>
<div class="line">        idx = np.random.randint(len(X))</div>
<div class="line">        pattern = dataX[idx] <span class="comment"># X is normalized float but dataX is integer.</span></div>
<div class="line">        x = np.reshape(pattern, (<span class="number">1</span>, len(pattern), <span class="number">1</span>))</div>
<div class="line">        x = x / float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div>
<div class="line">        print(seq_in, <span class="string">'-&gt;'</span>, result)</div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div>
<div class="line">    model.fit(X,y,epochs=<span class="number">5000</span>,batch_size=len(X),verbose=<span class="number">1</span>)</div>
<div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div>
<div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
<div class="line">    predict(model)</div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    naive_lstm()</div>
</pre></td></tr></table></figure>

</div></div>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">100.00</span>%</div>
<div class="line">['A'] -&gt; B</div>
<div class="line">['B'] -&gt; C</div>
<div class="line">...</div>
<div class="line">['X'] -&gt; Y</div>
<div class="line">['Y'] -&gt; Z</div>
<div class="line">Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</div>
<div class="line">['T'] -&gt; U</div>
<div class="line">['N'] -&gt; O</div>
<div class="line">...</div>
<div class="line">['S'] -&gt; T</div>
<div class="line">['T'] -&gt; U</div>
<div class="line">['R'] -&gt; S</div>
</pre></td></tr></table></figure>
<p>As we expected, the network is able to use the within-sequence context to learn the alphabet, achieving 100% accuracy on the training data.</p>
<p>Importantly, the network can make accurate predictions for the next letter in the alphabet for randomly selected characters. </p>
<h2 id="Stateful-LSTM-for-a-One-Char-to-One-Char-Mapping"><a href="#Stateful-LSTM-for-a-One-Char-to-One-Char-Mapping" class="headerlink" title="Stateful LSTM for a One-Char to One-Char Mapping"></a>Stateful LSTM for a One-Char to One-Char Mapping</h2><p>Ideally, we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem.</p>
<p>We can do this in Keras by making the LSTM layers stateful and manually resetting the state of the network at the end of the epoch, which is also the end of the training sequence.</p>
<p>This is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half).</p>
<p>We first need to define our LSTM layer as stateful. In so doing, we must explicitly specify the batch size as a dimension on the input shape. This also means that when we evaluate the network or make predictions, we must also specify and adhere to this same batch size. This is not a problem now as we are using a batch size of 1. This could introduce difficulties when making predictions when the batch size is not one as predictions will need to be made in batch and in sequence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">batch_size = <span class="number">1</span></div>
<div class="line">model.add(LSTM(<span class="number">50</span>, batch_input_shape=(batch_size, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>]), stateful=<span class="keyword">True</span>))</div>
</pre></td></tr></table></figure>
<p>An important difference in training the stateful LSTM is that we train it manually one epoch at a time and reset the state after each epoch. We can do this in a for loop. Again, we do not shuffle the input, preserving the sequence in which the input training data was created.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div>
<div class="line">	model.fit(X, y, epochs=<span class="number">1</span>, batch_size=batch_size, verbose=<span class="number">2</span>, shuffle=<span class="keyword">False</span>)</div>
<div class="line">	model.reset_states()</div>
</pre></td></tr></table></figure>
<p>As mentioned, we specify the batch size when evaluating the performance of the network on the entire training dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># summarize performance of the model</span></div>
<div class="line">scores = model.evaluate(X, y, batch_size=batch_size, verbose=<span class="number">0</span>)</div>
<div class="line">model.reset_states()</div>
<div class="line">print(<span class="string">"Model Accuracy: %.2f%%"</span> % (scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
</pre></td></tr></table></figure>
<p>Finally, we can demonstrate that the network has indeed learned the entire alphabet. We can seed it with the first letter “A”, request a prediction, feed the prediction back in as an input, and repeat the process all the way to “Z”.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># demonstrate some model predictions</span></div>
<div class="line">seed = [char_to_int[alphabet[<span class="number">0</span>]]]</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphabet)<span class="number">-1</span>):</div>
<div class="line">	x = numpy.reshape(seed, (<span class="number">1</span>, len(seed), <span class="number">1</span>))</div>
<div class="line">	x = x / float(len(alphabet))</div>
<div class="line">	prediction = model.predict(x, verbose=<span class="number">0</span>)</div>
<div class="line">	index = numpy.argmax(prediction)</div>
<div class="line">	print(int_to_char[seed[<span class="number">0</span>]], <span class="string">"-&gt;"</span>, int_to_char[index])</div>
<div class="line">	seed = [index]</div>
<div class="line">model.reset_states()</div>
</pre></td></tr></table></figure>
<p>We can also see if the network can make predictions starting from an arbitrary letter.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># demonstrate a random starting point</span></div>
<div class="line">letter = <span class="string">"K"</span></div>
<div class="line">seed = [char_to_int[letter]]</div>
<div class="line">print(<span class="string">"New start: "</span>, letter)</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">5</span>):</div>
<div class="line">	x = numpy.reshape(seed, (<span class="number">1</span>, len(seed), <span class="number">1</span>))</div>
<div class="line">	x = x / float(len(alphabet))</div>
<div class="line">	prediction = model.predict(x, verbose=<span class="number">0</span>)</div>
<div class="line">	index = numpy.argmax(prediction)</div>
<div class="line">	print(int_to_char[seed[<span class="number">0</span>]], <span class="string">"-&gt;"</span>, int_to_char[index])</div>
<div class="line">	seed = [index]</div>
<div class="line">model.reset_states()</div>
</pre></td></tr></table></figure>
<p>The entire code listing is provided below for completeness.</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div>
<div class="line">np.random.seed(<span class="number">7</span>)</div>
<div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div>
<div class="line">char2int = defaultdict(int)</div>
<div class="line">int2char = defaultdict(str)</div>
<div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div>
<div class="line">    char2int[val] = idx</div>
<div class="line">    int2char[idx] = val</div>
<div class="line"></div>
<div class="line">seq_length = <span class="number">1</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div>
<div class="line">    seq_in = alphabet[i:i+seq_length]</div>
<div class="line">    seq_out = alphabet[i+seq_length]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
<div class="line">dataX = pad_sequences(dataX,maxlen=seq_length,dtype=<span class="string">'float32'</span>)</div>
<div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line"></div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
<div class="line"></div>
<div class="line">batch_size = <span class="number">1</span></div>
<div class="line">model = Sequential()</div>
<div class="line">model.add(layers.LSTM(<span class="number">50</span>,batch_input_shape=(batch_size,X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>]),stateful=<span class="keyword">True</span>))</div>
<div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div>
<div class="line">    seed = [char2int[alphabet[<span class="number">0</span>]]]</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(alphabet)<span class="number">-1</span>):</div>
<div class="line">        x = np.reshape(seed,(<span class="number">1</span>,len(seed),<span class="number">1</span>))</div>
<div class="line">        x = x/float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        print(int2char[seed[<span class="number">0</span>]],<span class="string">'-&gt;'</span>,result)</div>
<div class="line">        seed = [index]</div>
<div class="line">    model.reset_states()</div>
<div class="line"></div>
<div class="line">    print(<span class="string">"Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"</span>)</div>
<div class="line">    seed = [char2int[<span class="string">'K'</span>]]</div>
<div class="line">    print(<span class="string">"New start: "</span>, <span class="string">'K'</span>)</div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</div>
<div class="line">        x = np.reshape(seed, (<span class="number">1</span>, len(seed), <span class="number">1</span>))</div>
<div class="line">        x = x / float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        print(int2char[seed[<span class="number">0</span>]], <span class="string">'-&gt;'</span>, result)</div>
<div class="line">        seed = [index]</div>
<div class="line">    model.reset_states()</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div>
<div class="line">        model.fit(X,y,epochs=<span class="number">1</span>,batch_size=batch_size,verbose=<span class="number">1</span>,shuffle=<span class="keyword">False</span>)</div>
<div class="line">        model.reset_states()</div>
<div class="line">    scores = model.evaluate(X,y,batch_size=batch_size,verbose=<span class="number">1</span>)</div>
<div class="line">    model.reset_states()</div>
<div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
<div class="line">    predict(model)</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    naive_lstm()</div>
</pre></td></tr></table></figure>

</div></div>
<p>Running the example provides the following output.</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
</pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">96.00</span>%</div>
<div class="line">A -&gt; B</div>
<div class="line">B -&gt; C</div>
<div class="line">...</div>
<div class="line">W -&gt; X</div>
<div class="line">X -&gt; Y</div>
<div class="line">Y -&gt; Y</div>
<div class="line">Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</div>
<div class="line">New start:  K</div>
<div class="line">K -&gt; B</div>
<div class="line">B -&gt; C</div>
<div class="line">C -&gt; D</div>
<div class="line">D -&gt; E</div>
<div class="line">E -&gt; F</div>
</pre></td></tr></table></figure>
<p>We can see that the network has memorized the entire alphabet perfectly. It used the context of the samples themselves and learned whatever dependency it needed to predict the next character in the sequence.</p>
<p>We can also see that if we seed the network with the first letter, that it can correctly rattle off the rest of the alphabet.</p>
<p>We can also see that it has only learned the full alphabet sequence and that from a cold start. When asked to predict the next letter from “K” that it predicts “B” and falls back into regurgitating the entire alphabet.</p>
<p>To truly predict “K” the state of the network would need to be warmed up iteratively fed the letters from “A” to “J”. This tells us that we could achieve the same effect with a “stateless” LSTM by preparing training data like:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">---a -&gt; b</div>
<div class="line">--ab -&gt; c</div>
<div class="line">-abc -&gt; d</div>
<div class="line">abcd -&gt; e</div>
</pre></td></tr></table></figure>
<p>Where the input sequence is fixed at 25 (a-to-y to predict z) and patterns are prefixed with zero-padding.</p>
<p>Finally, this raises the question of training an LSTM network using variable length input sequences to predict the next character.</p>
<h2 id="LSTM-with-Variable-Length-Input-to-One-Char-Output"><a href="#LSTM-with-Variable-Length-Input-to-One-Char-Output" class="headerlink" title="LSTM with Variable-Length Input to One-Char Output"></a>LSTM with Variable-Length Input to One-Char Output</h2><p>In the previous section, we discovered that the Keras “stateful” LSTM was really only a shortcut to replaying the first n-sequences, but didn’t really help us learn a generic model of the alphabet.</p>
<p>In this section we explore a variation of the “stateless” LSTM that learns random subsequences of the alphabet and an effort to build a model that can be given arbitrary letters or subsequences of letters and predict the next letter in the alphabet.</p>
<p>Firstly, we are changing the framing of the problem. To simplify we will define a maximum input sequence length and set it to a small value like 5 to speed up training. This defines the maximum length of subsequences of the alphabet will be drawn for training. In extensions, this could just as set to the full alphabet (26) or longer if we allow looping back to the start of the sequence.</p>
<p>We also need to define the number of random sequences to create, in this case 1000. This too could be more or less. I expect less patterns are actually required.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
</pre></td><td class="code"><pre><div class="line">num_inputs = <span class="number">1000</span></div>
<div class="line">max_len = <span class="number">5</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_inputs):</div>
<div class="line">    start = np.random.randint(len(alphabet)<span class="number">-2</span>)</div>
<div class="line">    end = np.random.randint(start,min(start+max_len,len(alphabet)<span class="number">-1</span>))</div>
<div class="line">    seq_in = alphabet[start:end+<span class="number">1</span>]</div>
<div class="line">    seq_out = alphabet[end+<span class="number">1</span>]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
<div class="line">dataX = pad_sequences(dataX,maxlen=max_len,dtype=<span class="string">'float32'</span>)</div>
<div class="line">input_shape = (len(dataX),max_len,<span class="number">1</span>)</div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
</pre></td></tr></table></figure>
<p>Running the code, we create input patterns that look like the following:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">STUV -&gt; W</div>
<div class="line">IJKLM -&gt; N</div>
<div class="line">STUV -&gt; W</div>
<div class="line">TUVWX -&gt; Y</div>
<div class="line">RSTU -&gt; V</div>
</pre></td></tr></table></figure>
<p>The input sequences vary in length between 1 and <strong>max_len</strong> and therefore require zero padding. Here, we use left-hand-side (prefix) padding with the Keras built in <strong>pad_sequences()</strong> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">dataX = pad_sequences(dataX,maxlen=max_len,dtype=<span class="string">'float32'</span>)</div>
</pre></td></tr></table></figure>
<p>The trained model is evaluated on randomly selected input patterns. This could just as easily be new randomly generated sequences of characters. I also believe this could also be a linear sequence seeded with “A” with outputs fes back in as single character inputs.</p>
<p>The full code listing is provided below for completeness.</p>
<div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div>
<div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div>
<div class="line">np.random.seed(<span class="number">7</span>)</div>
<div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div>
<div class="line">char2int = defaultdict(int)</div>
<div class="line">int2char = defaultdict(str)</div>
<div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div>
<div class="line">    char2int[val] = idx</div>
<div class="line">    int2char[idx] = val</div>
<div class="line"></div>
<div class="line">num_inputs = <span class="number">1000</span></div>
<div class="line">max_len = <span class="number">5</span></div>
<div class="line">dataX = []</div>
<div class="line">dataY = []</div>
<div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_inputs):</div>
<div class="line">    start = np.random.randint(len(alphabet)<span class="number">-2</span>)</div>
<div class="line">    end = np.random.randint(start,min(start+max_len,len(alphabet)<span class="number">-1</span>))</div>
<div class="line">    seq_in = alphabet[start:end+<span class="number">1</span>]</div>
<div class="line">    seq_out = alphabet[end+<span class="number">1</span>]</div>
<div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div>
<div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div>
<div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div>
<div class="line">dataX = pad_sequences(dataX,maxlen=max_len,dtype=<span class="string">'float32'</span>)</div>
<div class="line">input_shape = (len(dataX),max_len,<span class="number">1</span>)</div>
<div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div>
<div class="line">X = np.reshape(dataX,input_shape)</div>
<div class="line">X = X/float(len(alphabet))</div>
<div class="line">y = np_utils.to_categorical(dataY)</div>
<div class="line"></div>
<div class="line">batch_size = <span class="number">1</span></div>
<div class="line">model = Sequential()</div>
<div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div>
<div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div>
<div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</div>
<div class="line">        pattern_index = np.random.randint(len(dataX))</div>
<div class="line">        pattern = dataX[pattern_index]</div>
<div class="line">        x = pad_sequences([pattern], maxlen=max_len, dtype=<span class="string">'float32'</span>)</div>
<div class="line">        x = np.reshape(x, (<span class="number">1</span>, max_len, <span class="number">1</span>))</div>
<div class="line">        x = x / float(len(alphabet))</div>
<div class="line">        prediction = model.predict(x, verbose=<span class="number">0</span>)</div>
<div class="line">        index = np.argmax(prediction)</div>
<div class="line">        result = int2char[index]</div>
<div class="line">        seq_in = [int2char[value] <span class="keyword">for</span> value <span class="keyword">in</span> pattern]</div>
<div class="line">        print(seq_in, <span class="string">"-&gt;"</span>, result)</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div>
<div class="line">    model.fit(X,y,epochs=<span class="number">500</span>,batch_size=batch_size,verbose=<span class="number">1</span>)</div>
<div class="line">    model.reset_states()</div>
<div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div>
<div class="line">    model.reset_states()</div>
<div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div>
<div class="line">    predict(model)</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div>
<div class="line">    naive_lstm()</div>
</pre></td></tr></table></figure>

</div></div>
<p>Running this code produces the following output:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
</pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">98.90</span>%</div>
<div class="line">['Q', 'R'] -&gt; S</div>
<div class="line">['W', 'X'] -&gt; Y</div>
<div class="line">['W', 'X'] -&gt; Y</div>
<div class="line">['C', 'D'] -&gt; E</div>
<div class="line">['E'] -&gt; F</div>
<div class="line">['S', 'T', 'U'] -&gt; V</div>
<div class="line">['G', 'H', 'I', 'J', 'K'] -&gt; L</div>
<div class="line">['O', 'P', 'Q', 'R', 'S'] -&gt; T</div>
<div class="line">['C', 'D'] -&gt; E</div>
<div class="line">['O'] -&gt; P</div>
<div class="line">['N', 'O', 'P'] -&gt; Q</div>
<div class="line">['D', 'E', 'F', 'G', 'H'] -&gt; I</div>
<div class="line">['X'] -&gt; Y</div>
<div class="line">['K'] -&gt; L</div>
<div class="line">['M'] -&gt; N</div>
<div class="line">['R'] -&gt; T</div>
<div class="line">['K'] -&gt; L</div>
<div class="line">['E', 'F', 'G'] -&gt; H</div>
<div class="line">['Q'] -&gt; R</div>
<div class="line">['Q', 'R', 'S'] -&gt; T</div>
</pre></td></tr></table></figure>
<p>We can see that although the model did not learn the alphabet perfectly from the randomly generated subsequences, it did very well. The model was not tuned and may require more training or a larger network, or both (an exercise for the reader).</p>
<p>This is a good natural extension to the “<em>all sequential input examples in each batch</em>” alphabet model learned above in that it can handle ad hoc queries, but this time of arbitrary sequence length (up to the max length).</p>
<h1 id="PyTorch-Classify-Names-with-a-Character-Level-RNN"><a href="#PyTorch-Classify-Names-with-a-Character-Level-RNN" class="headerlink" title="PyTorch - Classify Names with a Character-Level RNN"></a>PyTorch - Classify Names with a Character-Level RNN</h1><h2 id="Preparing-data"><a href="#Preparing-data" class="headerlink" title="Preparing data"></a>Preparing data</h2><p>Each line contains a name and we need to convert them from Unicode to ASCII.</p>
<p>Once we have read all files, we need to create two dataset: <code>languages=[]</code> containing all target categories ,<code>languages2names={}</code>, mapping each languages to corresponding names.</p>
<p>Then we use one-hot tensor to represent each letter, whose size is <code>&lt;1,n_letters&gt;</code>. Therefore, a name is represented as <code>&lt;name_len, 1, n_letters&gt;</code>.</p>
<p>After that,  we are going to define a RNN structure like the following:</p>
<p><img src="/2019/03/02/DP-RNN/68747470733a2f2f692e696d6775722e636f6d2f5a32786279534f2e706e67.png" alt="8747470733a2f2f692e696d6775722e636f6d2f5a32786279534f2e706e6"></p>
<p>Considering the output of prediction is merely a number, we need to convert this numerical value into a language category.</p>
<p>The last step before we dive into the training, we have to write a training data sampling function.</p>
<h2 id="Train-the-Network"><a href="#Train-the-Network" class="headerlink" title="Train the Network"></a>Train the Network</h2><p>We use <code>nn.NLLLoss()</code> (negative log likelihood loss) as loss function and SGD with lr=0.005 as optimizater.</p>
<h1 id="Referneces"><a href="#Referneces" class="headerlink" title="Referneces"></a>Referneces</h1><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a> </p>
<p><a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="noopener">minimal character-level RNN language model in Python/numpy</a> </p>
<p><a href="http://www.sohu.com/a/118587343_487514" target="_blank" rel="noopener">零基础入门深度学习(四)：循环神经网络 </a> </p>
<p><a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">RNN</a> </p>
<p><a href="https://www.cnblogs.com/zhbzz2007/p/6339346.html" target="_blank" rel="noopener">BPTT Python implementation</a> </p>
<p><a href="https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470" target="_blank" rel="noopener">Recurrent Neural Networks by Example in Python</a> </p>
<p><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras</a>  </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/28/GAN-Metrics/" rel="next" title="GAN Metrics">
                <i class="fa fa-chevron-left"></i> GAN Metrics
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/02/DP-Object-Detection/" rel="prev" title="DP-Object Detection">
                DP-Object Detection <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Problems-with-Vanilla-NN-link"><span class="nav-number">1.</span> <span class="nav-text">Problems with Vanilla NN link</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN"><span class="nav-number">2.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Forward-propagation"><span class="nav-number">2.1.</span> <span class="nav-text">Forward propagation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Back-Propagation-through-Time"><span class="nav-number">2.2.</span> <span class="nav-text">Back Propagation through Time</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hand-Written-RNN"><span class="nav-number">2.3.</span> <span class="nav-text">Hand-Written RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Numpy-Implementation"><span class="nav-number">2.4.</span> <span class="nav-text">Numpy Implementation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-Vanishing-Gradients-Problem"><span class="nav-number">2.5.</span> <span class="nav-text">RNN Vanishing Gradients Problem</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-gradients-vanishing"><span class="nav-number">2.5.1.</span> <span class="nav-text">What is gradients vanishing?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-it-is-a-problem"><span class="nav-number">2.5.2.</span> <span class="nav-text">Why it is a problem?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Long-Short-Term-Memory-networks"><span class="nav-number">3.</span> <span class="nav-text">Long Short Term Memory networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-core-idea-behind-LSTMs"><span class="nav-number">3.1.</span> <span class="nav-text">The core idea behind LSTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-by-Step-LSTM-Walk-Through"><span class="nav-number">3.2.</span> <span class="nav-text">Step-by-Step LSTM Walk Through</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation"><span class="nav-number">3.3.</span> <span class="nav-text">Backpropagation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Numpy-Implementation-1"><span class="nav-number">3.4.</span> <span class="nav-text">Numpy Implementation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GRU"><span class="nav-number">4.</span> <span class="nav-text">GRU</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#How-LSTM-GRU-Solve-Vanishing-Gradients"><span class="nav-number">5.</span> <span class="nav-text">How LSTM/GRU Solve Vanishing Gradients</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Keras-Learn-the-Alphabet"><span class="nav-number">6.</span> <span class="nav-text">Keras - Learn the Alphabet</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Preparation"><span class="nav-number">6.1.</span> <span class="nav-text">Data Preparation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#One-Char-to-One-Char"><span class="nav-number">6.2.</span> <span class="nav-text">One-Char to One-Char</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Three-Char-Feature-Window-to-One-Char-Mapping"><span class="nav-number">6.3.</span> <span class="nav-text">A Three-Char Feature Window to One-Char Mapping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Three-Char-Time-Step-Window-to-One-Char-Mapping"><span class="nav-number">6.4.</span> <span class="nav-text">A Three-Char Time Step Window to One-Char Mapping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-State-Within-A-Batch"><span class="nav-number">6.5.</span> <span class="nav-text">LSTM State Within A Batch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stateful-LSTM-for-a-One-Char-to-One-Char-Mapping"><span class="nav-number">6.6.</span> <span class="nav-text">Stateful LSTM for a One-Char to One-Char Mapping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-with-Variable-Length-Input-to-One-Char-Output"><span class="nav-number">6.7.</span> <span class="nav-text">LSTM with Variable-Length Input to One-Char Output</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch-Classify-Names-with-a-Character-Level-RNN"><span class="nav-number">7.</span> <span class="nav-text">PyTorch - Classify Names with a Character-Level RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Preparing-data"><span class="nav-number">7.1.</span> <span class="nav-text">Preparing data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-the-Network"><span class="nav-number">7.2.</span> <span class="nav-text">Train the Network</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Referneces"><span class="nav-number">8.</span> <span class="nav-text">Referneces</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
