<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Pytorch," />










<meta name="description" content="Tutorials of PyTorch and some useful tips.">
<meta name="keywords" content="Deep Learning,Pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="DP-PyTorch">
<meta property="og:url" content="http://yoursite.com/2019/04/22/DP-PyTorch/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="Tutorials of PyTorch and some useful tips.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/04/22/DP-PyTorch/Screen%20Shot%202019-07-13%20at%2011.21.07%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/DP-PyTorch/Screen%20Shot%202019-07-13%20at%2011.37.00%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/DP-PyTorch/Screen%20Shot%202019-07-13%20at%2011.37.25%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/DP-PyTorch/Screen%20Shot%202019-07-13%20at%2011.40.45%20AM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/DP-PyTorch/Screen%20Shot%202019-05-18%20at%2011.57.01%20AM.png">
<meta property="og:updated_time" content="2019-08-06T17:18:17.306Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DP-PyTorch">
<meta name="twitter:description" content="Tutorials of PyTorch and some useful tips.">
<meta name="twitter:image" content="http://yoursite.com/2019/04/22/DP-PyTorch/Screen%20Shot%202019-07-13%20at%2011.21.07%20AM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/22/DP-PyTorch/"/>





  <title>DP-PyTorch | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/DP-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DP-PyTorch</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-22T10:35:06-05:00">
                2019-04-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Tutorials of PyTorch and some useful tips.</p>
<a id="more"></a>
<h1 id="Pytorch-101"><a href="#Pytorch-101" class="headerlink" title="Pytorch 101"></a>Pytorch 101</h1><h2 id="GPU-vs-CPU"><a href="#GPU-vs-CPU" class="headerlink" title="GPU vs CPU"></a>GPU vs CPU</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> torch.cuda.is_available():</div>
<div class="line">    device = torch.device(<span class="string">'cuda'</span>)</div>
<div class="line"><span class="keyword">else</span>:</div>
<div class="line">    device = torch.device(<span class="string">'cpu'</span>)</div>
<div class="line"><span class="comment"># Move the data to the proper device (GPU or CPU)</span></div>
<div class="line">x = x.to(device=device, dtype=dtype)</div>
<div class="line">y = y.to(device=device, dtype=torch.long)</div>
<div class="line">model = model.to(device=device)  <span class="comment"># move the model parameters to CPU/GPU</span></div>
</pre></td></tr></table></figure>

</div></div>
<h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_weight</span><span class="params">(shape)</span>:</span></div>
<div class="line">    <span class="string">"""</span></div>
<div class="line"><span class="string">    Create random Tensors for weights; setting requires_grad=True means that we</span></div>
<div class="line"><span class="string">    want to compute gradients for these Tensors during the backward pass.</span></div>
<div class="line"><span class="string">    We use Kaiming normalization: sqrt(2 / fan_in)</span></div>
<div class="line"><span class="string">    """</span></div>
<div class="line">    <span class="keyword">if</span> len(shape) == <span class="number">2</span>:  <span class="comment"># FC weight</span></div>
<div class="line">        fan_in = shape[<span class="number">0</span>]</div>
<div class="line">    <span class="keyword">else</span>:</div>
<div class="line">        fan_in = np.prod(shape[<span class="number">1</span>:]) <span class="comment"># conv weight [out_channel, in_channel, kH, kW]</span></div>
<div class="line">    <span class="comment"># randn is standard normal distribution generator. </span></div>
<div class="line">    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(<span class="number">2.</span> / fan_in)</div>
<div class="line">    w.requires_grad = <span class="keyword">True</span></div>
<div class="line">    <span class="keyword">return</span> w</div>
<div class="line"><span class="comment"># nn.init.kaiming_normal_(self.fc.weight)</span></div>
<div class="line">----------------------------------------------------------</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_weight</span><span class="params">(shape)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> torch.zeros(shape, device=device, dtype=dtype, requires_grad=<span class="keyword">True</span>)</div>
<div class="line"><span class="comment"># nn.init.constant_(self.conv1.bias, 0)</span></div>
</pre></td></tr></table></figure>

</div></div>
<h1 id="The-basics"><a href="#The-basics" class="headerlink" title="The basics"></a>The basics</h1><p>In this section, we’ll go through the basic ideas of PyTorch starting at tensors and computational graphs and finishing at the Variable class and the PyTorch autograd functionality.</p>
<h2 id="Computational-graphs"><a href="#Computational-graphs" class="headerlink" title="Computational graphs"></a>Computational graphs</h2><p><a href="https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">link</a></p>
<p>The first thing to understand about any deep learning library is the idea of a computational graph. A computational graph is a set of calculations, which are called <em>nodes</em>, and these nodes are connected in a directional ordering of computation. In other words, some nodes are dependent on other nodes for their input, and these nodes in turn output the results of their calculations to other nodes. A simple example of a computational graph for the calculation $a=(b+c)*(c+2)$ can be seen below – we can break this calculation up into the following steps/nodes:</p>
<p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.21.07 AM.png" alt="creen Shot 2019-07-13 at 11.21.07 A"></p>
<p>The benefits of using a computational graph is that each node is like its own independently functioning piece of code (once it receives all its required inputs). This allows various performance optimizations to be performed in running the calculations such as threading and multiple processing / parallelism. All the major deep learning frameworks (TensorFlow, Theano, PyTorch etc.) involve constructing such computational graphs, through which neural network operations can be built and through which gradients can be back-propagated.</p>
<h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p>Tensors are matrix-like data structures which are essential components in deep learning libraries and efficient computation. Graphical Processing Units (GPUs) are especially effective at calculating operations between tensors, and this has spurred the surge in deep learning capability in recent times. In PyTorch, tensors can be declared simply in a number of ways:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div>
<div class="line">x = torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</div>
</pre></td></tr></table></figure>
<p>This code creates a tensor of size (2, 3) – i.e. 2 rows and 3 columns, filled with zero float values.</p>
<p>We can also create tensors filled random float values:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">x = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</div>
</pre></td></tr></table></figure>
<p>Multiplying tensors, adding them and so forth is straight-forward:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">x = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</div>
<div class="line">y = torch.ones(<span class="number">2</span>,<span class="number">3</span>) * <span class="number">2</span></div>
<div class="line">print(x + y)</div>
<div class="line"><span class="comment"># 3 3 3</span></div>
<div class="line"><span class="comment"># 3 3 3</span></div>
</pre></td></tr></table></figure>
<p>Another great thing is the numpy slice functionality that is available – for instance y[:, 1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">y[:,<span class="number">1</span>] = y[:,<span class="number">1</span>] + <span class="number">1</span></div>
<div class="line"><span class="comment"># 2 3 2</span></div>
<div class="line"><span class="comment"># 2 3 2</span></div>
</pre></td></tr></table></figure>
<p><strong>Numpy Bridge</strong></p>
<p>Converting a Torch Tensor to a NumPy array and vice versa is a breeze. <strong>The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other.</strong></p>
<p>Converting a Torch Tensor to a NumPy Array</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line">a = torch.ones(<span class="number">5</span>)</div>
<div class="line">print(a) <span class="comment">#tensor([1., 1., 1., 1., 1.])</span></div>
<div class="line">b = a.numpy()</div>
<div class="line">print(b) <span class="comment">#[1. 1. 1. 1. 1.]</span></div>
<div class="line">a.add_(<span class="number">1</span>)</div>
<div class="line">print(a) <span class="comment">#tensor([2., 2., 2., 2., 2.])</span></div>
<div class="line">print(b) <span class="comment">#[2. 2. 2. 2. 2.]</span></div>
</pre></td></tr></table></figure>
<p>Converting NumPy Array to Torch Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line">a = np.ones(<span class="number">5</span>)</div>
<div class="line">b = torch.from_numpy(a)</div>
<div class="line">np.add(a, <span class="number">1</span>, out=a)</div>
<div class="line">print(a) <span class="comment">#[2. 2. 2. 2. 2.]</span></div>
<div class="line">print(b) <span class="comment">#tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></div>
</pre></td></tr></table></figure>
<p><code>torch.Tensor</code> is the central class of the package. If you set its attribute <code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. When you finish your computation you can call <code>.backward()</code> and have all the gradients computed automatically. The gradient for this tensor will be accumulated into <code>.grad</code> attribute.</p>
<p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.37.00 AM.png" alt="creen Shot 2019-07-13 at 11.37.00 A"></p>
<p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.37.25 AM.png" alt="creen Shot 2019-07-13 at 11.37.25 A"></p>
<p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.40.45 AM.png" alt="creen Shot 2019-07-13 at 11.40.45 A"></p>
<h2 id="Autograd-in-Pytorch"><a href="#Autograd-in-Pytorch" class="headerlink" title="Autograd in Pytorch"></a>Autograd in Pytorch</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank" rel="noopener">link</a></p>
<p>n any deep learning library, there needs to be a mechanism where error gradients are calculated and back-propagated through the computational graph. This mechanism, called autograd in PyTorch. Pytorch allows automatic gradient computation on the tensor when the .backward() function is called. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div>
<div class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>)</div>
<div class="line">print(x) </div>
<div class="line"><span class="comment"># tensor([[1., 1.],</span></div>
<div class="line"><span class="comment">#         [1., 1.]],requires_grad=True)</span></div>
</pre></td></tr></table></figure>
<p>do a tensor operation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">y = x+<span class="number">2</span></div>
<div class="line">print(y)</div>
<div class="line"><span class="comment">#tensor([[3., 3.],</span></div>
<div class="line"><span class="comment">#        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span></div>
</pre></td></tr></table></figure>
<p><code>y</code> was created as a result of an operation, so it has a <code>grad_fn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">print(y.grad_fn) </div>
<div class="line"><span class="comment"># &lt;AddBackward0 object at 0x7f27857f7c88&gt;</span></div>
</pre></td></tr></table></figure>
<p>do more operations on y:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">z = y * y * <span class="number">3</span></div>
<div class="line">out = z.mean()</div>
<div class="line">print(z, out)</div>
<div class="line"><span class="comment">#tensor([[27., 27.],</span></div>
<div class="line"><span class="comment">#       [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span></div>
</pre></td></tr></table></figure>
<p><strong>Gradients</strong></p>
<p>Let’s backprop now. <strong>Because <code>out</code> contains a single scalar, <code>out.backward()</code> is equivalent to <code>out.backward(torch.tensor(1.))</code>.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">out.backward()</div>
</pre></td></tr></table></figure>
<p>Print gradients d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
</pre></td><td class="code"><pre><div class="line">print(x.grad)</div>
<div class="line"><span class="comment">#tensor([[4.5000, 4.5000],</span></div>
<div class="line"><span class="comment">#        [4.5000, 4.5000]])</span></div>
</pre></td></tr></table></figure>
<h1 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a><a href="https://pytorch.org/docs/stable/torch.html#" target="_blank" rel="noopener">Torch</a></h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. </p>
<p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-05-18 at 11.57.01 AM.png" alt="creen Shot 2019-05-18 at 11.57.01 A"></p>
<h3 id="Creation-Ops"><a href="#Creation-Ops" class="headerlink" title="Creation Ops"></a>Creation Ops</h3><h4 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.tensor(data, dtype=<span class="keyword">None</span>, device=<span class="keyword">None</span>, requires_grad=<span class="keyword">False</span>, pin_memory=<span class="keyword">False</span>) → Tensor</div>
</pre></td></tr></table></figure>
<p>Constructs a tensor with <code>data</code>.</p>
<h4 id="topk"><a href="#topk" class="headerlink" title="topk"></a>topk</h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)</div>
</pre></td></tr></table></figure>
<p>Returns the <code>k</code> largest elements of the given <code>input</code> tensor along a given dimension.</p>
<p>If <code>dim</code> is not given, the last dimension of the input is chosen.</p>
<p>If <code>largest</code> is <code>False</code> then the k smallest elements are returned.</p>
<p>A namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.</p>
<h4 id="view"><a href="#view" class="headerlink" title="view"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" target="_blank" rel="noopener">view</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">view(*shape) → Tensor</div>
</pre></td></tr></table></figure>
<p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</p>
<h4 id="size"><a href="#size" class="headerlink" title="size"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size" target="_blank" rel="noopener">size</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">size() → torch.Size</div>
</pre></td></tr></table></figure>
<p>Returns the size of the <code>self</code> tensor. The returned value is a subclass of <a href="https://docs.python.org/3/library/stdtypes.html#tuple" target="_blank" rel="noopener"><code>tuple</code></a>.</p>
<h4 id="div"><a href="#div" class="headerlink" title="div"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.div" target="_blank" rel="noopener">div</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">div_(value) → Tensor</div>
</pre></td></tr></table></figure>
<p>Divides each element of the input <code>input</code> with the scalar <code>value</code> and returns a new resulting tensor.</p>
<h4 id="mm"><a href="#mm" class="headerlink" title="mm"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mm" target="_blank" rel="noopener">mm</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">mm(mat2) → Tensor</div>
</pre></td></tr></table></figure>
<p>matrxi multiply. see <a href="https://pytorch.org/docs/stable/torch.html#torch.mm" target="_blank" rel="noopener">torch.mm()</a>. </p>
<h4 id="torch-from-numpy"><a href="#torch-from-numpy" class="headerlink" title="torch.from_numpy"></a><a href="https://pytorch.org/docs/stable/torch.html#torch.from_numpy" target="_blank" rel="noopener">torch.from_numpy</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.from_numpy(ndarray) → Tensor</div>
</pre></td></tr></table></figure>
<p>Creates a <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><code>Tensor</code></a> from a <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" target="_blank" rel="noopener"><code>numpy.ndarray</code></a>.</p>
<p>The returned tensor and <code>ndarray</code> share the same memory. Modifications to the tensor will be reflected in the <code>ndarray</code> and vice versa. The returned tensor is not resizable.</p>
<h2 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h2><p><code>torch.autograd</code> provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. </p>
<h3 id="Variable-deprecated"><a href="#Variable-deprecated" class="headerlink" title="Variable (deprecated)"></a>Variable (deprecated)</h3><p>The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with <code>requires_grad</code> set to <code>True</code>. Below please find a quick guide on what has changed:</p>
<ul>
<li><code>Variable(tensor)</code> and <code>Variable(tensor, requires_grad)</code> still work as expected, but they return Tensors instead of Variables.</li>
<li><code>var.data</code> is the same thing as <code>tensor.data</code>.</li>
<li>Methods such as <code>var.backward(), var.detach(), var.register_hook()</code> now work on tensors with the same method names.</li>
</ul>
<h2 id="max"><a href="#max" class="headerlink" title="max"></a>max</h2><ol>
<li><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.max(input) -&gt; Tensor</div>
</pre></td></tr></table></figure>
<p>Returns the maximum value of all elements in the <code>input</code> tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>a</div>
<div class="line">tensor([[ <span class="number">0.6763</span>,  <span class="number">0.7445</span>, <span class="number">-2.2369</span>]])</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a)</div>
<div class="line">tensor(<span class="number">0.7445</span>)</div>
</pre></td></tr></table></figure>
</li>
<li><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)</div>
</pre></td></tr></table></figure>
<p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the maximum value of each row of the <code>input</code> tensor in the given dimension <code>dim</code>. And <code>indices</code> is the index location of each maximum value found (argmax).</p>
<p>If <code>keepdim</code> is <code>True</code>, the output tensors are of the same size as <code>input</code> except in the dimension <code>dim</code> where they are of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="https://pytorch.org/docs/stable/torch.html#torch.squeeze" target="_blank" rel="noopener"><code>torch.squeeze()</code></a>), resulting in the output tensors having 1 fewer dimension than <code>input</code>.</p>
<ul>
<li><strong>input</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the input tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) – the dimension to reduce</li>
<li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – whether the output tensors have <code>dim</code> retained or not. Default: <code>False</code>.</li>
<li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <em>optional</em>) – the result tuple of two output tensors (max, max_indices)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>a</div>
<div class="line">tensor([[<span class="number">-1.2360</span>, <span class="number">-0.2942</span>, <span class="number">-0.1222</span>,  <span class="number">0.8475</span>],</div>
<div class="line">        [ <span class="number">1.1949</span>, <span class="number">-1.1127</span>, <span class="number">-2.2379</span>, <span class="number">-0.6702</span>],</div>
<div class="line">        [ <span class="number">1.5717</span>, <span class="number">-0.9207</span>,  <span class="number">0.1297</span>, <span class="number">-1.8768</span>],</div>
<div class="line">        [<span class="number">-0.6172</span>,  <span class="number">1.0036</span>, <span class="number">-0.6060</span>, <span class="number">-0.2432</span>]])</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a, <span class="number">1</span>)</div>
<div class="line">torch.return_types.max(values=tensor([<span class="number">0.8475</span>, <span class="number">1.1949</span>, <span class="number">1.5717</span>, <span class="number">1.0036</span>]), indices=tensor([<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]))</div>
</pre></td></tr></table></figure>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.max(input, other, out=<span class="keyword">None</span>) → Tensor</div>
</pre></td></tr></table></figure>
<p>Each element of the tensor <code>input</code> is compared with the corresponding element of the tensor <code>other</code> and an element-wise maximum is taken.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>)</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>a</div>
<div class="line">tensor([ <span class="number">0.2942</span>, <span class="number">-0.7416</span>,  <span class="number">0.2653</span>, <span class="number">-0.1584</span>])</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">4</span>)</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div>
<div class="line">tensor([ <span class="number">0.8722</span>, <span class="number">-1.7421</span>, <span class="number">-0.4141</span>, <span class="number">-0.5055</span>])</div>
<div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a, b)</div>
<div class="line">tensor([ <span class="number">0.8722</span>, <span class="number">-0.7416</span>,  <span class="number">0.2653</span>, <span class="number">-0.1584</span>])</div>
</pre></td></tr></table></figure>
</li>
</ol>
<h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a><a href="https://pytorch.org/docs/stable/torch.html#torch.cat" target="_blank" rel="noopener">cat</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.cat(tensors, dim=<span class="number">0</span>, out=<span class="keyword">None</span>) → Tensor</div>
</pre></td></tr></table></figure>
<h2 id="multinomial"><a href="#multinomial" class="headerlink" title="multinomial"></a><a href="https://pytorch.org/docs/stable/torch.html#torch.multinomial" target="_blank" rel="noopener">multinomial</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.multinomial(input, num_samples, replacement=<span class="keyword">False</span>, out=<span class="keyword">None</span>) → LongTensor</div>
</pre></td></tr></table></figure>
<blockquote>
<p><a href="https://blog.csdn.net/monchin/article/details/79787621" target="_blank" rel="noopener">understanding</a> </p>
</blockquote>
<h2 id="nn"><a href="#nn" class="headerlink" title="nn"></a>nn</h2><h3 id="functional"><a href="#functional" class="headerlink" title="functional"></a><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">functional</a></h3><h3 id="Linear-layers"><a href="#Linear-layers" class="headerlink" title="Linear layers"></a><a href="https://pytorch.org/docs/stable/nn.html#linear-layers" target="_blank" rel="noopener">Linear layers</a></h3><h4 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.Linear(in_features, out_features, bias=<span class="keyword">True</span>)</div>
</pre></td></tr></table></figure>
<p>Applies a linear transformation to the incoming data: $y=x A^{T}+b$.</p>
<ul>
<li><strong>in_features</strong> – size of each input sample</li>
<li><strong>out_features</strong> – size of each output sample</li>
<li><strong>bias</strong> – If set to <code>False</code>, the layer will not learn an additive bias. Default: <code>True</code></li>
</ul>
<p><strong>Variables</strong> - Linear<strong>.weight</strong> and <strong>Linear.bias</strong> </p>
<h3 id="Convolution-layers"><a href="#Convolution-layers" class="headerlink" title="Convolution layers"></a><a href="https://pytorch.org/docs/stable/nn.html#convolution-layers" target="_blank" rel="noopener">Convolution layers</a></h3><h4 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a><a href="https://pytorch.org/docs/stable/nn.html#conv2d" target="_blank" rel="noopener">Conv2d</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="keyword">True</span>, padding_mode=<span class="string">'zeros'</span>)</div>
</pre></td></tr></table></figure>
<ul>
<li><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">link</a> has a nice visualization of what <code>dilation</code> does.</li>
</ul>
<p><strong>Variables</strong> - <strong>Conv2d.weight</strong> and <strong>Conv2d.bias</strong> </p>
<h3 id="Non-linear-activations"><a href="#Non-linear-activations" class="headerlink" title="Non-linear activations"></a><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" target="_blank" rel="noopener">Non-linear activations</a></h3><h4 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.ReLU(inplace=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.Softmax(dim=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\operatorname{Softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}\</script><h3 id="LogSoftmax"><a href="#LogSoftmax" class="headerlink" title="LogSoftmax"></a>LogSoftmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.LogSoftmax(dim=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor. </p>
<h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.CrossEntropyLoss(weight=<span class="keyword">None</span>, size_average=<span class="keyword">None</span>, ignore_index=<span class="number">-100</span>, reduce=<span class="keyword">None</span>, reduction=<span class="string">'mean'</span>)</div>
</pre></td></tr></table></figure>
<p>This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.</p>
<blockquote>
<p>Therefore, in network architecture, we should not define a softmax layer.</p>
</blockquote>
<h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss()"></a>NLLLoss()</h3><blockquote>
<p>go with LogSoftmax.</p>
</blockquote>
<h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding" target="_blank" rel="noopener">Embedding</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="keyword">None</span>, max_norm=<span class="keyword">None</span>, norm_type=<span class="number">2.0</span>, scale_grad_by_freq=<span class="keyword">False</span>, sparse=<span class="keyword">False</span>, _weight=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<p><strong>Shape</strong></p>
<ul>
<li>Input: (*)(∗), LongTensor of arbitrary shape containing the indices to extract</li>
<li>Output: (<em>, H)(∗,H), where </em> is the input shape and $H=\text{embedding_dim}$</li>
</ul>
<p><strong>Parameters</strong></p>
<ul>
<li>num_embeddings(int) - </li>
<li>embedding_dim(int) - size of each embedding vector</li>
<li>padding_idx(int,optional) - If given, pads the output with the embedding vector at <code>padding_idx</code>(initialized to zeros) whenever it encounters the index. <a href="http://www.linzehui.me/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/" target="_blank" rel="noopener">understanding</a> </li>
</ul>
<blockquote>
<p><a href="https://stackoverflow.com/questions/50747947/embedding-in-pytorch" target="_blank" rel="noopener">understanding</a>  </p>
</blockquote>
<h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.GRU" target="_blank" rel="noopener">GRU</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.nn.GRU(*args, **kwargs)</div>
</pre></td></tr></table></figure>
<p><strong>Parameters</strong></p>
<ul>
<li>input_size - The number of expected features in the input x</li>
<li>hidden_szie - The number of features in the hidden state h</li>
<li>num_layers - Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li>
</ul>
<h2 id="optim"><a href="#optim" class="headerlink" title="optim"></a><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">optim</a></h2><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" target="_blank" rel="noopener">Adam</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>, amsgrad=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<h2 id="Utils"><a href="#Utils" class="headerlink" title="[Utils]"></a>[Utils]</h2><h3 id="DATA"><a href="#DATA" class="headerlink" title="DATA"></a><a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener">DATA</a></h3><h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" target="_blank" rel="noopener">torch.utils.data.DataLoader</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.utils.data.DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="keyword">False</span>, sampler=<span class="keyword">None</span>, batch_sampler=<span class="keyword">None</span>, num_workers=<span class="number">0</span>, collate_fn=&lt;function default_collate&gt;, pin_memory=<span class="keyword">False</span>, drop_last=<span class="keyword">False</span>, timeout=<span class="number">0</span>, worker_init_fn=<span class="keyword">None</span>)</div>
</pre></td></tr></table></figure>
<ul>
<li><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" target="_blank" rel="noopener"><em>Dataset</em></a>) – dataset from which to load the data.</li>
<li><strong>batch_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – how many samples per batch to load (default: <code>1</code>).</li>
<li><strong>shuffle</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to have the data reshuffled at every epoch (default: <code>False</code>).</li>
<li><strong>sampler</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler" target="_blank" rel="noopener"><em>Sampler</em></a><em>,</em> <em>optional</em>) – defines the strategy to draw samples from the dataset. If specified, <code>shuffle</code>must be False.</li>
<li><strong>batch_sampler</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler" target="_blank" rel="noopener"><em>Sampler</em></a><em>,</em> <em>optional</em>) – like sampler, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code>.</li>
<li><strong>num_workers</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: <code>0</code>)</li>
</ul>
<h4 id="torch-utils-data-Sampler"><a href="#torch-utils-data-Sampler" class="headerlink" title="torch.utils.data.Sampler"></a><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler" target="_blank" rel="noopener">torch.utils.data.Sampler</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.utils.data.Sampler(data_source)</div>
</pre></td></tr></table></figure>
<p>Base class for all Samplers.</p>
<p>Every Sampler subclass has to provide an <strong>iter</strong> method, providing a way to iterate over indices of dataset elements, and a <strong>len</strong> method that returns the length of the returned iterators.</p>
<h4 id="torch-utils-data-SubsetRandomSampler"><a href="#torch-utils-data-SubsetRandomSampler" class="headerlink" title="torch.utils.data.SubsetRandomSampler"></a><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler" target="_blank" rel="noopener">torch.utils.data.SubsetRandomSampler</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torch.utils.data.SubsetRandomSampler(indices)</div>
</pre></td></tr></table></figure>
<p>Samples elements randomly from a given list of indices, without replacement.</p>
<ul>
<li><strong>indices</strong> (<em>sequence</em>) – a sequence of indices</li>
</ul>
<h1 id="Torchvision"><a href="#Torchvision" class="headerlink" title="Torchvision"></a>Torchvision</h1><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a><a href="https://pytorch.org/docs/stable/torchvision/models.html" target="_blank" rel="noopener">Models</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</div>
<div class="line">resnet18 = models.resnet18(pretrained=<span class="keyword">True</span>)</div>
<div class="line">alexnet = models.alexnet(pretrained=<span class="keyword">True</span>)</div>
<div class="line">squeezenet = models.squeezenet1_0(pretrained=<span class="keyword">True</span>)</div>
<div class="line">vgg16 = models.vgg16(pretrained=<span class="keyword">True</span>)</div>
<div class="line">densenet = models.densenet161(pretrained=<span class="keyword">True</span>)</div>
<div class="line">inception = models.inception_v3(pretrained=<span class="keyword">True</span>)</div>
<div class="line">googlenet = models.googlenet(pretrained=<span class="keyword">True</span>)</div>
<div class="line">shufflenet = models.shufflenetv2(pretrained=<span class="keyword">True</span>)</div>
</pre></td></tr></table></figure>
<p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>. You can use the following transform to normalize:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
</pre></td><td class="code"><pre><div class="line">normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</div>
<div class="line">                                 std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div>
</pre></td></tr></table></figure>
<h3 id="Stop-undating"><a href="#Stop-undating" class="headerlink" title="Stop-undating"></a>Stop-undating</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
</pre></td><td class="code"><pre><div class="line"><span class="comment"># Download and load the pretrained SqueezeNet model.</span></div>
<div class="line">model = torchvision.models.squeezenet1_1(pretrained=<span class="keyword">True</span>)</div>
<div class="line"></div>
<div class="line"><span class="comment"># We don't want to train the model, so tell PyTorch not to compute gradients</span></div>
<div class="line"><span class="comment"># with respect to model parameters.</span></div>
<div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div>
<div class="line">    param.requires_grad = <span class="keyword">False</span></div>
</pre></td></tr></table></figure>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a><a href="https://pytorch.org/docs/stable/torchvision/datasets.html" target="_blank" rel="noopener">Datasets</a></h2><p>All datasets are subclasses of <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" target="_blank" rel="noopener"><code>torch.utils.data.Dataset</code></a> i.e, they have <code>__getitem__</code> and <code>__len__</code> methods implemented. Hence, they can all be passed to a <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" target="_blank" rel="noopener"><code>torch.utils.data.DataLoader</code></a> which can load multiple samples parallelly using <code>torch.multiprocessing</code> workers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
</pre></td><td class="code"><pre><div class="line">imagenet_data = torchvision.datasets.ImageNet(<span class="string">'path/to/imagenet_root/'</span>)</div>
<div class="line">data_loader = torch.utils.data.DataLoader(imagenet_data,</div>
<div class="line">                                          batch_size=<span class="number">4</span>,</div>
<div class="line">                                          shuffle=<span class="keyword">True</span>,</div>
<div class="line">                                          num_workers=args.nThreads)</div>
</pre></td></tr></table></figure>
<p>The following datasets are available: MNIST, COCO (Captions and Detection), LSUN, ImageNet, CIFAR etc.</p>
<p>All the datasets have almost similar API. They all have two common arguments: <code>transform</code> and <code>target_transform</code> to transform the input and target respectively.</p>
<h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a><a href="https://pytorch.org/docs/stable/torchvision/datasets.html#mnist" target="_blank" rel="noopener">MNIST</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torchvision.datasets.MNIST(root, train=<span class="keyword">True</span>, transform=<span class="keyword">None</span>, target_transform=<span class="keyword">None</span>, download=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<ul>
<li><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code>MNIST/processed/training.pt</code> and<code>MNIST/processed/test.pt</code> exist.</li>
<li><strong>train</strong> (bool, <em>optional</em>) – If True, creates dataset from <code>training.pt</code>, otherwise from <code>test.pt</code>.</li>
<li><strong>download</strong> (bool, <em>optional</em>) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li>
<li><strong>transform</strong> (callable, <em>optional</em>) – A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li>
<li><strong>target_transform</strong> (callable, <em>optional</em>) – A function/transform that takes in the target and transforms it.</li>
</ul>
<h1 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h1><h3 id="Compose"><a href="#Compose" class="headerlink" title="Compose"></a>Compose</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torchvision.transforms.Compose(transforms)</div>
</pre></td></tr></table></figure>
<p>Composes several transforms together.</p>
<ul>
<li><strong>transforms</strong> (list of <code>Transform</code> objects) – list of transforms to compose.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line">transforms.Compose([</div>
<div class="line">   transforms.CenterCrop(<span class="number">10</span>),</div>
<div class="line">   transforms.ToTensor(),</div>
<div class="line">])</div>
</pre></td></tr></table></figure>
<h3 id="Resize"><a href="#Resize" class="headerlink" title="Resize"></a>Resize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torchvision.transforms.Resize(size, interpolation=<span class="number">2</span>)</div>
</pre></td></tr></table></figure>
<p>Resize the input PIL Image to the given size.</p>
<ul>
<li><strong>size</strong> (<em>sequence</em> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) – Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size)</li>
<li><strong>interpolation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – Desired interpolation. Default is <code>PIL.Image.BILINEAR</code></li>
</ul>
<blockquote>
<p><code>torchvision.transforms.``Scale</code>(<strong>args*, </strong>*kwargs*) is deprecated in favor of Resize.</p>
</blockquote>
<h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torchvision.transforms.ToTensor()</div>
</pre></td></tr></table></figure>
<p>Convert a <code>PIL Image</code> or <code>numpy.ndarray</code> to tensor.</p>
<h3 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize"></a>Normalize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torchvision.transforms.Normalize(mean, std, inplace=<span class="keyword">False</span>)</div>
</pre></td></tr></table></figure>
<p>Normalize a tensor image with mean and standard deviation. Given mean: <code>(M1,...,Mn)</code> and std: <code>(S1,..,Sn)</code> for <code>n</code>channels, this transform will normalize each channel of the input <code>torch.*Tensor</code> i.e. <code>input[channel] =(input[channel] - mean[channel]) / std[channel]</code></p>
<blockquote>
<p>This transform acts out of place, i.e., it does not mutates the input tensor.</p>
</blockquote>
<h3 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
</pre></td><td class="code"><pre><div class="line">torchvision.transforms.Lambda(lambd)</div>
</pre></td></tr></table></figure>
<p>Apply a user-defined lambda as a transform.</p>
<ul>
<li><strong>lambd</strong> (<em>function</em>) – Lambda/function to be used for transform.</li>
</ul>
<h1 id="HyperParameters-Search"><a href="#HyperParameters-Search" class="headerlink" title="HyperParameters Search"></a>HyperParameters Search</h1><p><a href="https://github.com/ray-project/ray/tree/master/python/ray/tune/examples" target="_blank" rel="noopener">source</a> </p>
<h1 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h1><p><a href="http://visualdl.paddlepaddle.org/documentation/visualdl/en/develop/getting_started/demo/pytorch/TUTORIAL_EN.html" target="_blank" rel="noopener">source</a></p>
<h1 id="Pytorch-in-Practice"><a href="#Pytorch-in-Practice" class="headerlink" title="Pytorch in Practice"></a>Pytorch in Practice</h1><h2 id="Char-Level-Names-Classification"><a href="#Char-Level-Names-Classification" class="headerlink" title="Char_Level Names Classification"></a>Char_Level Names Classification</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
<div class="line">71</div>
<div class="line">72</div>
<div class="line">73</div>
<div class="line">74</div>
<div class="line">75</div>
<div class="line">76</div>
<div class="line">77</div>
<div class="line">78</div>
<div class="line">79</div>
<div class="line">80</div>
<div class="line">81</div>
<div class="line">82</div>
<div class="line">83</div>
<div class="line">84</div>
<div class="line">85</div>
<div class="line">86</div>
<div class="line">87</div>
<div class="line">88</div>
<div class="line">89</div>
<div class="line">90</div>
<div class="line">91</div>
<div class="line">92</div>
<div class="line">93</div>
<div class="line">94</div>
<div class="line">95</div>
<div class="line">96</div>
<div class="line">97</div>
<div class="line">98</div>
<div class="line">99</div>
<div class="line">100</div>
<div class="line">101</div>
<div class="line">102</div>
<div class="line">103</div>
<div class="line">104</div>
<div class="line">105</div>
<div class="line">106</div>
<div class="line">107</div>
<div class="line">108</div>
<div class="line">109</div>
<div class="line">110</div>
<div class="line">111</div>
<div class="line">112</div>
<div class="line">113</div>
<div class="line">114</div>
<div class="line">115</div>
<div class="line">116</div>
<div class="line">117</div>
<div class="line">118</div>
<div class="line">119</div>
<div class="line">120</div>
<div class="line">121</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> string,unicodedata</div>
<div class="line"><span class="keyword">import</span> glob</div>
<div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div>
<div class="line"><span class="keyword">import</span> torch</div>
<div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div>
<div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div>
<div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div>
<div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"></div>
<div class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></div>
<div class="line">n_letters = len(all_letters)</div>
<div class="line">print(n_letters, all_letters)</div>
<div class="line"></div>
<div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427</span></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicode_to_ascii</span><span class="params">(s)</span>:</span></div>
<div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div>
<div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div>
<div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div>
<div class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</div>
<div class="line">    )</div>
<div class="line"></div>
<div class="line">print(unicode_to_ascii(<span class="string">'Ślusàrski'</span>))</div>
<div class="line">print(os.getcwd())</div>
<div class="line"></div>
<div class="line">all_files = glob.glob(<span class="string">'/content/colabdataset/ColabDataset/names/*.txt'</span>)</div>
<div class="line">print(all_files)</div>
<div class="line">Country2Name = defaultdict(list)</div>
<div class="line">Countries = []</div>
<div class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</div>
<div class="line">  country = file.split(<span class="string">'/'</span>)[<span class="number">-1</span>][:<span class="number">-4</span>]</div>
<div class="line">  Countries.append(country)</div>
<div class="line">  print(country)</div>
<div class="line">  f = open(file)</div>
<div class="line">  lines = f.readlines()</div>
<div class="line">  names = []</div>
<div class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> lines:</div>
<div class="line">    names.append(unicode_to_ascii(line.replace(<span class="string">'\n'</span>,<span class="string">''</span>)))</div>
<div class="line">  Country2Name[country] = names</div>
<div class="line">print(Country2Name[<span class="string">'Chinese'</span>][:<span class="number">10</span>])</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2tensor</span><span class="params">(name)</span>:</span></div>
<div class="line">  time_step = len(name)</div>
<div class="line">  feature_dim = n_letters</div>
<div class="line">  batch_size = <span class="number">1</span></div>
<div class="line">  tensor = torch.zeros(time_step,batch_size,feature_dim)</div>
<div class="line">  <span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(name):</div>
<div class="line">    tensor[idx,<span class="number">0</span>,all_letters.index(val)] = <span class="number">1</span></div>
<div class="line">  tensor = tensor.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</div>
<div class="line">  <span class="keyword">return</span> tensor</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_training_pair</span><span class="params">()</span>:</span></div>
<div class="line">  country_label = np.random.choice(Countries)</div>
<div class="line">  names = np.random.choice(Country2Name[country_label])</div>
<div class="line">  tensor_names = str2tensor(names)</div>
<div class="line">  tensor_label = torch.tensor([Countries.index(country_label)])</div>
<div class="line">  <span class="keyword">return</span> tensor_names,tensor_label</div>
<div class="line"></div>
<div class="line"><span class="class"><span class="keyword">class</span> <span class="title">rnn_classifier</span><span class="params">(nn.Module)</span>:</span></div>
<div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,hidden_size,output_size)</span>:</span></div>
<div class="line">    super(rnn_classifier,self).__init__()</div>
<div class="line">    self.input_size = input_size</div>
<div class="line">    self.hidden_size = hidden_size</div>
<div class="line">    self.output_size = output_size</div>
<div class="line">    </div>
<div class="line">    self.embed = nn.Embedding(input_size,hidden_size)</div>
<div class="line">    <span class="comment"># input of shape (seq_len, batch, input_size)</span></div>
<div class="line">    <span class="comment"># output of shape (seq_len, batch, num_directions * hidden_size)</span></div>
<div class="line">    self.lstm = nn.LSTM(self.input_size,self.hidden_size,batch_first=<span class="keyword">True</span>) </div>
<div class="line">    self.fc = nn.Linear(self.hidden_size,self.output_size)</div>
<div class="line">  </div>
<div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></div>
<div class="line">    hidden,(_,_) = self.lstm(input) <span class="comment">#AttributeError: 'tuple' object has no attribute 'dim' if hidden = self.lstm(input) </span></div>
<div class="line">    output = self.fc(hidden)</div>
<div class="line">    <span class="keyword">return</span> output</div>
<div class="line"></div>
<div class="line">n_hidden = <span class="number">128</span></div>
<div class="line">rnn = rnn_classifier(n_letters,n_hidden,len(Countries))</div>
<div class="line">criterion = nn.CrossEntropyLoss()</div>
<div class="line">opt = torch.optim.SGD(rnn.parameters(),lr=<span class="number">0.005</span>)</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input,output)</span>:</span></div>
<div class="line">  opt.zero_grad()</div>
<div class="line">  pred = rnn(input)</div>
<div class="line">  pred = pred[:,<span class="number">-1</span>,:].squeeze(dim=<span class="number">1</span>)</div>
<div class="line">  loss = criterion(pred,output)</div>
<div class="line">  loss.backward()</div>
<div class="line">  opt.step()</div>
<div class="line">  <span class="keyword">return</span> pred,loss.item()</div>
<div class="line"></div>
<div class="line">n_epochs = <span class="number">100000</span></div>
<div class="line">print_every = <span class="number">5000</span></div>
<div class="line">current_loss = <span class="number">0</span></div>
<div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</div>
<div class="line">  input,output = get_training_pair()</div>
<div class="line">  pred,loss = train(input,output)</div>
<div class="line">  current_loss += loss</div>
<div class="line">  <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div>
<div class="line">    print(epoch,<span class="string">'/'</span>,n_epochs,loss)</div>
<div class="line">    </div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_testing_pair</span><span class="params">()</span>:</span></div>
<div class="line">  country_label = np.random.choice(Countries)</div>
<div class="line">  names = np.random.choice(Country2Name[country_label])</div>
<div class="line">  tensor_names = str2tensor(names)</div>
<div class="line">  tensor_label = torch.tensor([Countries.index(country_label)])</div>
<div class="line">  <span class="keyword">return</span> names,country_label,tensor_names,tensor_label</div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prediction</span><span class="params">(name_tensor,names,gt)</span>:</span></div>
<div class="line">  pred = rnn(name_tensor)</div>
<div class="line">  pred = pred[:,<span class="number">-1</span>,:].squeeze(<span class="number">1</span>)</div>
<div class="line">  _,idx = torch.max(pred.data,<span class="number">1</span>)</div>
<div class="line">  <span class="keyword">return</span> Countries[idx]</div>
<div class="line"></div>
<div class="line">n = <span class="number">100</span></div>
<div class="line">c = <span class="number">0</span></div>
<div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</div>
<div class="line">  names,country_label,tensor_names,tensor_label = get_testing_pair() </div>
<div class="line">  pred = prediction(tensor_names,names,country_label)</div>
<div class="line">  print(<span class="string">'name is &#123;&#125;, the prediction is &#123;&#125;, while the gound_t is &#123;&#125;'</span>.format(names,pred,country_label))</div>
<div class="line">  <span class="keyword">if</span> pred==country_label:</div>
<div class="line">    c += <span class="number">1</span></div>
<div class="line">print(<span class="string">"Acurracy is &#123;&#125;"</span>.format(c/n))</div>
</pre></td></tr></table></figure>
<p>There are several points needed the attention:</p>
<ol>
<li><p><strong>Input data and groundtruth</strong></p>
<p>The input data should be in the size <code>(time_steps, batch_size, feature_dim)</code> for the original <code>LSTM</code> function. But if you specify the parameter <code>batch_first</code> in <code>LSTM</code>, then you need to switch the dimentaion to <code>(batch_size, time_steps, feature_dim)</code>. </p>
<p>As for the groundtruth, it should be a scalar instead of a one-hot label if we are dealing with the classification problem, whose size should be <code>(batch_size,1)</code>. For example, if the batch_size is 1, so one possilbility could be <code>[1]</code> instead of <code>1</code>.</p>
<p>In this case, we usually use <code>CrossEntropyLoss</code> or <code>NLLLoss</code>.</p>
</li>
<li><p><strong>Loss function</strong></p>
<p>If in the model definition we have a layer called <code>LogSoftmax</code>, then we should use <code>CrossEntropyLoss</code>.</p>
</li>
<li><p><strong>Training function</strong></p>
<p>For <code>LSTM</code>, the input of shape should be <code>[seq_len, batch, input_size]</code>, while the output shape would be <code>[seq_len, batch, hidden_size]</code>. And then going through a Linear layer, the output size should be <code>[seq_len, batch, output_size]</code>. Here the <code>output_size</code> is class number. But we only need the last element of output, which means output<code>[-1,:,:]</code>. This is because for each time step, there is always a output.</p>
</li>
<li><p><strong>Prediction</strong></p>
<p>When testing, we should reshape the prediction to <code>[batch_size,output_size]</code>, then we can use <code>val,idx = torch.max(prediction,1)</code> for printing.</p>
</li>
<li><p>xxx</p>
</li>
<li><p>​</p>
</li>
</ol>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://medium.com/@josh_2774/deep-learning-with-pytorch-9574e74d17ad?" target="_blank" rel="noopener">Deep Learning With PyTorch</a></p>
<p><a href="https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">A PyTorch tutorial – deep learning in Python</a></p>
<p><a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="noopener">jcjohnson’s PyTorch examples</a> </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Pytorch/" rel="tag"># Pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/19/Python-Seaborn/" rel="next" title="Python-Seaborn">
                <i class="fa fa-chevron-left"></i> Python-Seaborn
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/05/28/PlanOne/" rel="prev" title="PlanOne">
                PlanOne <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">66</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch-101"><span class="nav-number">1.</span> <span class="nav-text">Pytorch 101</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU-vs-CPU"><span class="nav-number">1.1.</span> <span class="nav-text">GPU vs CPU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">1.2.</span> <span class="nav-text">Weight Initialization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-basics"><span class="nav-number">2.</span> <span class="nav-text">The basics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Computational-graphs"><span class="nav-number">2.1.</span> <span class="nav-text">Computational graphs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensors"><span class="nav-number">2.2.</span> <span class="nav-text">Tensors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Autograd-in-Pytorch"><span class="nav-number">2.3.</span> <span class="nav-text">Autograd in Pytorch</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Torch"><span class="nav-number">3.</span> <span class="nav-text">Torch</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor"><span class="nav-number">3.1.</span> <span class="nav-text">Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Creation-Ops"><span class="nav-number">3.1.1.</span> <span class="nav-text">Creation Ops</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">tensor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#topk"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">topk</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#view"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">view</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#size"><span class="nav-number">3.1.1.4.</span> <span class="nav-text">size</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#div"><span class="nav-number">3.1.1.5.</span> <span class="nav-text">div</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mm"><span class="nav-number">3.1.1.6.</span> <span class="nav-text">mm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-from-numpy"><span class="nav-number">3.1.1.7.</span> <span class="nav-text">torch.from_numpy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Autograd"><span class="nav-number">3.2.</span> <span class="nav-text">Autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Variable-deprecated"><span class="nav-number">3.2.1.</span> <span class="nav-text">Variable (deprecated)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#max"><span class="nav-number">3.3.</span> <span class="nav-text">max</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cat"><span class="nav-number">3.4.</span> <span class="nav-text">cat</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multinomial"><span class="nav-number">3.5.</span> <span class="nav-text">multinomial</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn"><span class="nav-number">3.6.</span> <span class="nav-text">nn</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#functional"><span class="nav-number">3.6.1.</span> <span class="nav-text">functional</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-layers"><span class="nav-number">3.6.2.</span> <span class="nav-text">Linear layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear"><span class="nav-number">3.6.2.1.</span> <span class="nav-text">Linear</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolution-layers"><span class="nav-number">3.6.3.</span> <span class="nav-text">Convolution layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Conv2d"><span class="nav-number">3.6.3.1.</span> <span class="nav-text">Conv2d</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-linear-activations"><span class="nav-number">3.6.4.</span> <span class="nav-text">Non-linear activations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Relu"><span class="nav-number">3.6.4.1.</span> <span class="nav-text">Relu</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax"><span class="nav-number">3.6.5.</span> <span class="nav-text">Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LogSoftmax"><span class="nav-number">3.6.6.</span> <span class="nav-text">LogSoftmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CrossEntropyLoss"><span class="nav-number">3.6.7.</span> <span class="nav-text">CrossEntropyLoss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NLLLoss"><span class="nav-number">3.6.8.</span> <span class="nav-text">NLLLoss()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layers"><span class="nav-number">3.6.9.</span> <span class="nav-text">Layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Embedding"><span class="nav-number">3.6.9.1.</span> <span class="nav-text">Embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GRU"><span class="nav-number">3.6.9.2.</span> <span class="nav-text">GRU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optim"><span class="nav-number">3.7.</span> <span class="nav-text">optim</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">3.7.1.</span> <span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Utils"><span class="nav-number">3.8.</span> <span class="nav-text">[Utils]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DATA"><span class="nav-number">3.8.1.</span> <span class="nav-text">DATA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-utils-data-DataLoader"><span class="nav-number">3.8.1.1.</span> <span class="nav-text">torch.utils.data.DataLoader</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-utils-data-Sampler"><span class="nav-number">3.8.1.2.</span> <span class="nav-text">torch.utils.data.Sampler</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-utils-data-SubsetRandomSampler"><span class="nav-number">3.8.1.3.</span> <span class="nav-text">torch.utils.data.SubsetRandomSampler</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Torchvision"><span class="nav-number">4.</span> <span class="nav-text">Torchvision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Models"><span class="nav-number">4.1.</span> <span class="nav-text">Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Stop-undating"><span class="nav-number">4.1.1.</span> <span class="nav-text">Stop-undating</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Datasets"><span class="nav-number">4.2.</span> <span class="nav-text">Datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST"><span class="nav-number">4.2.1.</span> <span class="nav-text">MNIST</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transforms"><span class="nav-number">5.</span> <span class="nav-text">Transforms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Compose"><span class="nav-number">5.0.1.</span> <span class="nav-text">Compose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Resize"><span class="nav-number">5.0.2.</span> <span class="nav-text">Resize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ToTensor"><span class="nav-number">5.0.3.</span> <span class="nav-text">ToTensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalize"><span class="nav-number">5.0.4.</span> <span class="nav-text">Normalize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lambda"><span class="nav-number">5.0.5.</span> <span class="nav-text">Lambda</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HyperParameters-Search"><span class="nav-number">6.</span> <span class="nav-text">HyperParameters Search</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Visualization"><span class="nav-number">7.</span> <span class="nav-text">Visualization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch-in-Practice"><span class="nav-number">8.</span> <span class="nav-text">Pytorch in Practice</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Char-Level-Names-Classification"><span class="nav-number">8.1.</span> <span class="nav-text">Char_Level Names Classification</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">9.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
