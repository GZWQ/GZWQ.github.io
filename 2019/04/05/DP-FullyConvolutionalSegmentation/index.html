<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Deep Learning,Object Segmentation,Computer Vision," />










<meta name="description" content="This post is based on the paper Fully Convolutional Networks for Semantic Segmentation, which aims to perform image segmentation.">
<meta name="keywords" content="Deep Learning,Object Segmentation,Computer Vision">
<meta property="og:type" content="article">
<meta property="og:title" content="DP-FullyConvolutionalSegmentation">
<meta property="og:url" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/index.html">
<meta property="og:site_name" content="Blog of Qing">
<meta property="og:description" content="This post is based on the paper Fully Convolutional Networks for Semantic Segmentation, which aims to perform image segmentation.">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-7.42.16-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/deeplabcityscape.gif">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-9.02.15-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-16-at-9.36.00-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-19-at-12.54.50-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/1_wstv4l1dXB8vl8pclcEmTQ-4479112.gif">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-05%20at%2012.34.21%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-05%20at%2012.48.33%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-05%20at%2012.53.29%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-05%20at%2012.57.06%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-09%20at%2012.44.43%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-09%20at%2012.46.55%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-9.53.20-AM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-16-at-10.34.02-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-10.15.09-AM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-12.26.53-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-12.10.25-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-1.46.43-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-24-at-10.46.16-PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen%20Shot%202019-04-13%20at%202.23.41%20PM.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131430474.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131434053.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131451012.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/1_U8uoGoZDs8nwzQE3tOhfkw@2x.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/iou_stop_sign.jpg">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/iou_equation.png">
<meta property="og:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/iou_examples.png">
<meta property="og:updated_time" content="2019-04-14T21:04:56.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DP-FullyConvolutionalSegmentation">
<meta name="twitter:description" content="This post is based on the paper Fully Convolutional Networks for Semantic Segmentation, which aims to perform image segmentation.">
<meta name="twitter:image" content="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-7.42.16-PM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/"/>





  <title>DP-FullyConvolutionalSegmentation | Blog of Qing</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog of Qing</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/05/DP-FullyConvolutionalSegmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qing Wong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog of Qing">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DP-FullyConvolutionalSegmentation</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-05T09:30:14-05:00">
                2019-04-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This post is based on the paper <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a>, which aims to perform image segmentation.</p>
<a id="more"></a>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><a href="https://www.jeremyjordan.me/semantic-segmentation/" target="_blank" rel="noopener">ref</a></p>
<p>More specifically, the goal of semantic image segmentation is to label <em>each pixel</em> of an image with a corresponding <strong>class</strong> of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as <strong>dense prediction</strong>.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-7.42.16-PM.png" alt="creen-Shot-2018-05-17-at-7.42.16-P"></p>
<p>One important thing to note is that we’re not separating <em>instances</em> of the same class; we only care about the category of each pixel. In other words, if you have two objects of the same category in your input image, the segmentation map does not inherently distinguish these as separate objects. There exists a different class of models, known as <em>instance segmentation</em> models, which <em>do</em> distinguish between separate objects of the same class.</p>
<p>Segmentation models are useful for a variety of tasks, including:</p>
<ul>
<li><p><strong>Autonomous vehicles</strong><br>We need to equip cars with the necessary perception to understand their environment so that self-driving cars can safely integrate into our existing roads.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/deeplabcityscape.gif" alt="eeplabcityscap"></p>
</li>
<li><p><strong>Medical image diagnostics</strong></p>
</li>
</ul>
<h1 id="Representing-the-task"><a href="#Representing-the-task" class="headerlink" title="Representing the task"></a>Representing the task</h1><p>Simply, our goal is to take either a RGB color image (height×width×3height×width×3) or a grayscale image (height×width×1height×width×1) and output a segmentation map where each pixel contains a class label represented as an integer (height×width×1height×width×1).</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-9.02.15-PM.png" alt="creen-Shot-2018-05-17-at-9.02.15-P"></p>
<p>Similar to how we treat standard categorical values, we’ll create our <strong>target</strong> by one-hot encoding the class labels - essentially creating an output channel for each of the possible classes.<img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-16-at-9.36.00-PM.png" alt="creen-Shot-2018-05-16-at-9.36.00-P"></p>
<p>A prediction can be collapsed into a segmentation map (as shown in the first image) by taking the <code>argmax</code> of each depth-wise pixel vector.</p>
<h1 id="Fully-convolutional-networks"><a href="#Fully-convolutional-networks" class="headerlink" title="Fully convolutional networks"></a>Fully convolutional networks</h1><p>In the traditional image classification deep learning architecture, we take a image and pass this image through a number of convolution layers so that we get the high-level features. And the features are connected with several dense layers and output the classification prediction. </p>
<p>In FCN, image segmentation models is to follow an <strong>encoder/decoder structure</strong> where we <em>downsample</em> the spatial resolution of the input, developing lower-resolution feature mappings which are learned to be highly efficient at discriminating between classes, and the <em>upsample</em> the feature representations into a full-resolution segmentation map.</p>
<h2 id="Upsampling-ref"><a href="#Upsampling-ref" class="headerlink" title="Upsampling ref"></a>Upsampling <a href="https://towardsdatascience.com/transpose-convolution-77818e55a123" target="_blank" rel="noopener">ref</a></h2><p>There are a few different approaches that we can use to <em>upsample</em> the resolution of a feature map. </p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-19-at-12.54.50-PM.png" alt="creen-Shot-2018-05-19-at-12.54.50-P"></p>
<p>However, <strong>transpose convolutions</strong> are by far the most popular approach as they allow for us to develop a <em>learned upsampling</em>.</p>
<p>Before diving into transpose convolutions, we review what convolutions do. Suppose We are applying the convolution to an image of 5x5x1 with a kernel of 3x3, stride 2x2 and padding VALID. </p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/1_wstv4l1dXB8vl8pclcEmTQ-4479112.gif" alt="_wstv4l1dXB8vl8pclcEmTQ-447911"></p>
<p>As you can see in the left image the output will be a 2x2 image. You can calculate the output size of a convolution operation by using below formula as well:</p>
<blockquote>
<p>Convolution Output Size = 1 + (Input Size - Filter size + 2 * Padding) / Stride</p>
</blockquote>
<p>Now suppose you want to up-sample this to same dimension as input image. You will use same parameters as for convolution and will first calculate what was the size of Image before down-sampling.</p>
<blockquote>
<p><strong>SAME PADDING:</strong><br>Transpose Convolution Size = Input Size <em> Stride<br><strong>VALID PADDING: 0</strong>Transpose Convolution Size = Input Size </em> Stride + max(Filter Size - Stride, 0)</p>
</blockquote>
<p>Suppose we have the following image:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.34.21 PM.png" alt="creen Shot 2019-04-05 at 12.34.21 P"></p>
<p>For transpose convolution, we use $3 \times 3$ filter and stride 2, then we go over each pixel and each single pixel is multiplied by a $3\times 3$ filter and formed a $3 \times 3$ block which is then put in output matrix. Say the initial filter is all-one matrix, so after processing the first pixel, we have:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.48.33 PM.png" alt="creen Shot 2019-04-05 at 12.48.33 P"></p>
<p>For the second pixel, we have:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.53.29 PM.png" alt="creen Shot 2019-04-05 at 12.53.29 P"></p>
<p>Since the stride step is 2, so we are going to combine the above two matrix:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.57.06 PM.png" alt="creen Shot 2019-04-05 at 12.57.06 P"></p>
<p>we get $3 \times 5$ matrix.</p>
<p>For the third and fouth matrix, we have </p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-09 at 12.44.43 PM.png" alt="creen Shot 2019-04-09 at 12.44.43 P"></p>
<p>So after the deconvolution, we have $5\times 5$ matrix:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-09 at 12.46.55 PM.png" alt="creen Shot 2019-04-09 at 12.46.55 P"></p>
<p>To sum up, the convolution transpose is the opposite of convolution. Since for convolution, the new size is  <code>(old_row - k + 2*p ) / s + 1</code>, so for convolution transpose, its size is <code>(old_row - 1) * s - 2*p + k</code>. If we use no padding and kernel size equals stride step, then new size is <code>old_row * s</code>.</p>
<h2 id="Network-Structures"><a href="#Network-Structures" class="headerlink" title="Network Structures"></a>Network Structures</h2><p>The original paper use well-studied <em>image classification</em> networks (eg. AlexNet) to serve as the encoder module of the network, appending a decoder module with transpose convolutional layers to upsample the coarse feature maps into a full-resolution segmentation map. The encoder structure looks like the following:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-9.53.20-AM.png" alt="creen-Shot-2018-05-20-at-9.53.20-A"></p>
<p>The full network, as shown below, is trained according to a pixel-wise cross entropy loss.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-16-at-10.34.02-PM.png" alt="creen-Shot-2018-05-16-at-10.34.02-P"></p>
<p>However, because the encoder module reduces the resolution of the input by a factor of 32, the decoder module <strong>struggles to produce fine-grained segmentations</strong> (as shown below).</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-10.15.09-AM.png" alt="creen-Shot-2018-05-20-at-10.15.09-A"></p>
<p>The paper’s authors comment eloquently on this struggle:</p>
<blockquote>
<p>Semantic segmentation faces an inherent tension between semantics and location: global information resolves <strong>what</strong> while local information resolves <strong>where</strong>… Combining fine layers and coarse layers lets the model make local predictions that respect global structure. ― <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Long et al.</a></p>
</blockquote>
<h2 id="Skip-Connections"><a href="#Skip-Connections" class="headerlink" title="Skip Connections"></a>Skip Connections</h2><p>The authors address this tension by slowly upsampling (in stages) the encoded representation, adding “skip connections” from earlier layers, and summing these two feature maps.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-12.26.53-PM.png" alt="creen-Shot-2018-05-20-at-12.26.53-P"></p>
<p>These skip connections from earlier layers in the network (prior to a downsampling operation) should provide the necessary detail in order to reconstruct accurate shapes for segmentation boundaries. Indeed, we can recover more fine-grain detail with the addition of these skip connections.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-12.10.25-PM.png" alt="creen-Shot-2018-05-20-at-12.10.25-P"></p>
<p><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">Ronneberger et al.</a> improve upon the “fully convolutional” architecture primarily through <strong>expanding the capacity of the decoder</strong> module of the network. More concretely, they propose the <strong>U-Net architecture</strong> which “consists of a contracting path to capture context and a <strong>symmetric</strong> expanding path that enables precise localization.” This simpler architecture has grown to be very popular and has been adapted for a variety of segmentation problems.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-1.46.43-PM.png" alt="creen-Shot-2018-05-20-at-1.46.43-P"></p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>The most commonly used loss function for the task of image segmentation is a <strong>pixel-wise cross entropy loss</strong>. This loss examines <em>each pixel individually</em>, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-24-at-10.46.16-PM.png" alt="creen-Shot-2018-05-24-at-10.46.16-P"></p>
<p>Because the cross entropy loss evaluates the class predictions for each pixel vector individually and then averages over all pixels, we’re essentially asserting equal learning to each pixel in the image. This can be a problem if your various classes have unbalanced representation in the image, as training can be dominated by the most prevalent class. <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Long et al.</a> (FCN paper) discuss weighting this loss for each <strong>output channel</strong> in order to counteract a class imbalance present in the dataset.</p>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>In order to get a full understanding of FCN, we will get our hands on streetview images segmentation. The state of the art dataset is pascal VOC2012, however it requires preprocessing and is kind of large, we choose a <a href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view" target="_blank" rel="noopener">streetview dataset</a> with 12 object classes which includes 367 training dataset and 101 test images. It is perfect for us to feel how FCN works in image segmentation.</p>
<h2 id="Data-exploration"><a href="#Data-exploration" class="headerlink" title="Data exploration"></a>Data exploration</h2><p>The data directory looks like the following:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-13 at 2.23.41 PM.png" alt="creen Shot 2019-04-13 at 2.23.41 P"></p>
<p>where in the <code>images_prepped_train</code> there are original images for training, while in <code>annotations_prepped_train</code> is our groundtruth for each image. For a groundtruth image, it is the same size with the original image where each pixel is labeled with a class. It is same with the test images directory.</p>
<p>The first thing to do is to visualize a single segmentation image. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cv2,os</div>
<div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div>
<div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div>
<div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div>
<div class="line"></div>
<div class="line">dir_seg = <span class="string">"/Users/daniel/Desktop/streets/annotations_prepped_train/"</span></div>
<div class="line">dir_img = <span class="string">"/Users/daniel/Desktop/streets/images_prepped_train/"</span></div>
<div class="line"></div>
<div class="line">n_classes = <span class="number">12</span> <span class="comment">#total classes for segmentation</span></div>
<div class="line"></div>
<div class="line">ldseg = os.listdir(dir_seg)</div>
<div class="line">firstimage = ldseg[<span class="number">0</span>] <span class="comment"># choose the first image for visualization</span></div>
<div class="line"></div>
<div class="line">seg = cv2.imread(dir_seg+firstimage) <span class="comment"># read segmentation groundtruth</span></div>
<div class="line">img = cv2.imread(dir_img+firstimage) <span class="comment"># read original image</span></div>
<div class="line"></div>
<div class="line">mi,ma = np.min(seg),np.max(seg) <span class="comment">#get the range for classes</span></div>
<div class="line">total_classes = ma-mi+<span class="number">1</span> </div>
<div class="line"><span class="comment"># print("The total segmentation classes are : &#123;&#125;".format(ma-mi+1))</span></div>
<div class="line"></div>
<div class="line">fig = plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</div>
<div class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div>
<div class="line">ax.imshow(img)</div>
<div class="line">ax.set_title(<span class="string">"Original Image"</span>)</div>
<div class="line"></div>
<div class="line">fig = plt.figure(figsize=(<span class="number">15</span>,<span class="number">10</span>))</div>
<div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(mi,ma+<span class="number">1</span>):</div>
<div class="line">    ax = fig.add_subplot(<span class="number">3</span>,total_classes/<span class="number">3</span>,k+<span class="number">1</span>) <span class="comment">#the first digit is the number of rows, the second the number of columns, and the third the index of the subplot.</span></div>
<div class="line">    ax.imshow((seg==k)*<span class="number">1.0</span>) <span class="comment">#(seg==k) return a True/False list</span></div>
<div class="line">    ax.set_title(<span class="string">"label=&#123;&#125;"</span>.format(k))</div>
<div class="line">plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131430474.png" alt="mage-20190413143047">  </p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131434053.png" alt="mage-20190413143405"></p>
<p>We can roughly observe from the segmentaion image that the first class is buildings, while the <code>class label=8</code> is cars.</p>
<p>Next, I will give the different objects with different colors to visualize them in one image.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_colorized</span><span class="params">(seg,n_classes)</span>:</span></div>
<div class="line">    <span class="keyword">if</span> len(seg.shape) == <span class="number">3</span>:</div>
<div class="line">        seg = seg[:, :, <span class="number">0</span>]</div>
<div class="line">    seg_img = np.zeros((seg.shape[<span class="number">0</span>], seg.shape[<span class="number">1</span>], <span class="number">3</span>)).astype(<span class="string">'float'</span>)</div>
<div class="line">    colors = sns.color_palette(<span class="string">"hls"</span>, n_classes) <span class="comment"># return n_classes colors</span></div>
<div class="line"></div>
<div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(n_classes):</div>
<div class="line">        segc = (seg == c)</div>
<div class="line">        seg_img[:, :, <span class="number">0</span>] += (segc * (colors[c][<span class="number">0</span>]))</div>
<div class="line">        seg_img[:, :, <span class="number">1</span>] += (segc * (colors[c][<span class="number">1</span>]))</div>
<div class="line">        seg_img[:, :, <span class="number">2</span>] += (segc * (colors[c][<span class="number">2</span>]))</div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> (seg_img)</div>
</pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
</pre></td><td class="code"><pre><div class="line">color_img = img_colorized(seg,n_classes)</div>
<div class="line">fig = plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</div>
<div class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div>
<div class="line">ax.imshow(color_img)</div>
<div class="line">ax.set_title(<span class="string">"Colored Image"</span>)</div>
<div class="line">plt.show()</div>
</pre></td></tr></table></figure>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131451012.png" alt="mage-20190413145101"></p>
<h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><p>We will resize the image to <code>(224,224)</code>. In the following code, we resize the image and normalize the image to range <code>(-1,1)</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataProcessing</span><span class="params">(path,width,height)</span>:</span></div>
<div class="line">    img = cv2.imread(path,<span class="number">1</span>)</div>
<div class="line">    img = np.float32(cv2.resize(img,(width,height)))/<span class="number">127.5</span><span class="number">-1</span></div>
<div class="line">    <span class="keyword">return</span> img</div>
</pre></td></tr></table></figure>
<p>Then considering we are going to predict class label for each pixel, so for each pixel, there are going to be <code>total_classes</code> output. So for a input image with size <code>(row,col)</code>, the output should be <code>(row, col, total_classes)</code>. Therefore, for each groundtruth image, we are going to generate <code>total_classes</code> groundtruth lable images, where in the first image, the pixel belonging to class 0 has value 1 while the rest has value 0. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSegmentationArr</span><span class="params">(path,nClasses,width,height)</span>:</span></div>
<div class="line">    seg_labels = np.zeros((height,width,nClasses))</div>
<div class="line">    img = cv2.imread(path,<span class="number">1</span>)</div>
<div class="line">    img = cv2.resize(img,(width,height))</div>
<div class="line">    img = img[:,:,<span class="number">0</span>]</div>
<div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(nClasses):</div>
<div class="line">        seg_labels[:, :, c] = (img == c).astype(int)</div>
<div class="line">    <span class="keyword">return</span> seg_labels</div>
</pre></td></tr></table></figure>
<p>Then we create our training dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
</pre></td><td class="code"><pre><div class="line">images = os.listdir(dir_img)</div>
<div class="line">images.sort()</div>
<div class="line">segmentations = os.listdir(dir_seg)</div>
<div class="line">segmentations.sort()</div>
<div class="line"></div>
<div class="line">X = []</div>
<div class="line">Y = []</div>
<div class="line"><span class="keyword">for</span> im, seg <span class="keyword">in</span> zip(images, segmentations):</div>
<div class="line">    X.append(dataProcessing(dir_img + im, <span class="number">224</span>, <span class="number">224</span>))</div>
<div class="line">    Y.append(getSegmentationArr(dir_seg + seg, total_classes, <span class="number">224</span>, <span class="number">224</span>))</div>
<div class="line"></div>
<div class="line">X, Y = np.array(X), np.array(Y)</div>
</pre></td></tr></table></figure>
<h2 id="Network-architecture"><a href="#Network-architecture" class="headerlink" title="Network architecture"></a>Network architecture</h2><p>Then we can implement the network, here we use the <code>vgg16</code> architecture, where the architecture looks like:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/1_U8uoGoZDs8nwzQE3tOhfkw@2x.png" alt="_U8uoGoZDs8nwzQE3tOhfkw@2"></p>
<p>Since the <code>vgg16</code> downsample the input image to the original size of $1/2^5=1/32$ due to <code>maxpooling</code>. Therefore we need to ensure the input size can be divided by 32. In the beginning, we implement the structure of <code>vgg16</code>: <code>block1 -&gt; block2 -&gt; block3 -&gt; block4 -&gt; block5</code> .</p>
<p>In order to keep more information, we directly use output from <code>block3</code> and <code>block4</code> to combine them with the output of <code>block5</code>. To be specific, for the output of <code>block5</code>, we use <code>conv2d_k4096_s7 + conv2d_k4096_s1 + conv2dT_k12_s4_p4</code>; for the output of <code>block4</code>, we need to upsample it by 2, <code>conv2d_k12_s1 + conv2dT_k12_s2_p2</code>; for output of <code>block3</code>, <code>conv2d_k12_s1</code>. And finally, combine them by adding them.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
<div class="line">22</div>
<div class="line">23</div>
<div class="line">24</div>
<div class="line">25</div>
<div class="line">26</div>
<div class="line">27</div>
<div class="line">28</div>
<div class="line">29</div>
<div class="line">30</div>
<div class="line">31</div>
<div class="line">32</div>
<div class="line">33</div>
<div class="line">34</div>
<div class="line">35</div>
<div class="line">36</div>
<div class="line">37</div>
<div class="line">38</div>
<div class="line">39</div>
<div class="line">40</div>
<div class="line">41</div>
<div class="line">42</div>
<div class="line">43</div>
<div class="line">44</div>
<div class="line">45</div>
<div class="line">46</div>
<div class="line">47</div>
<div class="line">48</div>
<div class="line">49</div>
<div class="line">50</div>
<div class="line">51</div>
<div class="line">52</div>
<div class="line">53</div>
<div class="line">54</div>
<div class="line">55</div>
<div class="line">56</div>
<div class="line">57</div>
<div class="line">58</div>
<div class="line">59</div>
<div class="line">60</div>
<div class="line">61</div>
<div class="line">62</div>
<div class="line">63</div>
<div class="line">64</div>
<div class="line">65</div>
<div class="line">66</div>
<div class="line">67</div>
<div class="line">68</div>
<div class="line">69</div>
<div class="line">70</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</div>
<div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</div>
<div class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</div>
<div class="line"></div>
<div class="line">vgg_weights = <span class="string">'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'</span></div>
<div class="line"></div>
<div class="line"><span class="function"><span class="keyword">def</span> <span class="title">FCN8</span><span class="params">(nClasses,input_height=<span class="number">224</span>,input_width=<span class="number">224</span>)</span>:</span></div>
<div class="line">    <span class="keyword">assert</span>  input_height%<span class="number">32</span> == <span class="number">0</span></div>
<div class="line">    <span class="keyword">assert</span> input_width%<span class="number">32</span> == <span class="number">0</span></div>
<div class="line"></div>
<div class="line">    img_input = Input(shape=(input_height, input_width, <span class="number">3</span>))  <span class="comment">## Assume 224,224,3</span></div>
<div class="line"></div>
<div class="line">    <span class="comment">## Block 1</span></div>
<div class="line">    x = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block1_conv1'</span>)(</div>
<div class="line">        img_input)</div>
<div class="line">    x = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block1_conv2'</span>)(x)</div>
<div class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block1_pool'</span>)(x)</div>
<div class="line">    f1 = x</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Block 2</span></div>
<div class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv1'</span>)(x)</div>
<div class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv2'</span>)(x)</div>
<div class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block2_pool'</span>)(x)</div>
<div class="line">    f2 = x</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Block 3</span></div>
<div class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block3_conv1'</span>)(x)</div>
<div class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block3_conv2'</span>)(x)</div>
<div class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block3_conv3'</span>)(x)</div>
<div class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block3_pool'</span>)(x)</div>
<div class="line">    pool3 = x</div>
<div class="line"></div>
<div class="line">    <span class="comment"># Block 4</span></div>
<div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block4_conv1'</span>)(x)</div>
<div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block4_conv2'</span>)(x)</div>
<div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block4_conv3'</span>)(x)</div>
<div class="line">    pool4 = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block4_pool'</span>)(x)  <span class="comment">## (None, 14, 14, 512)</span></div>
<div class="line"></div>
<div class="line">    <span class="comment"># Block 5</span></div>
<div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block5_conv1'</span>)(pool4)</div>
<div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block5_conv2'</span>)(x)</div>
<div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block5_conv3'</span>)(x)</div>
<div class="line">    pool5 = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block5_pool'</span>)(x)  <span class="comment">## (None, 7, 7, 512)</span></div>
<div class="line"></div>
<div class="line">    vgg = Model(img_input, pool5)</div>
<div class="line">    vgg.load_weights(vgg_weights)  <span class="comment">## loading VGG weights for the encoder parts of FCN8</span></div>
<div class="line"><span class="comment">#########################################################</span></div>
<div class="line">    n = <span class="number">4096</span></div>
<div class="line">    o = Conv2D(n, (<span class="number">7</span>, <span class="number">7</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"conv6"</span>)(pool5)</div>
<div class="line">    conv7 = Conv2D(n, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"conv7"</span>)(o)</div>
<div class="line"></div>
<div class="line">    <span class="comment">## 4 times upsamping for pool4 layer</span></div>
<div class="line">    conv7_4 = Conv2DTranspose(nClasses, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), strides=(<span class="number">4</span>, <span class="number">4</span>), use_bias=<span class="keyword">False</span>)(conv7)</div>
<div class="line">    <span class="comment">## (None, 224, 224, 10)</span></div>
<div class="line">    <span class="comment">## 2 times upsampling for pool411</span></div>
<div class="line">    pool411 = Conv2D(nClasses, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"pool4_11"</span>)(pool4)</div>
<div class="line">    pool411_2 =Conv2DTranspose(nClasses, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), use_bias=<span class="keyword">False</span>)(pool411)</div>
<div class="line"></div>
<div class="line">    pool311 = Conv2D(nClasses, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"pool3_11"</span>)(pool3)</div>
<div class="line"></div>
<div class="line">    o = Add(name=<span class="string">"add"</span>)([pool411_2, pool311, conv7_4])</div>
<div class="line">    o = Conv2DTranspose(nClasses, kernel_size=(<span class="number">8</span>, <span class="number">8</span>), strides=(<span class="number">8</span>, <span class="number">8</span>), use_bias=<span class="keyword">False</span>)(o)</div>
<div class="line">    o = Activation(<span class="string">'softmax'</span>)(o)</div>
<div class="line"></div>
<div class="line">    model = Model(img_input, o)</div>
<div class="line"></div>
<div class="line">    <span class="keyword">return</span> model</div>
<div class="line"></div>
<div class="line">model = FCN8(nClasses=n_classes,input_height=<span class="number">224</span>,input_width=<span class="number">224</span>)</div>
<div class="line">model.summary()</div>
</pre></td></tr></table></figure>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</div>
<div class="line">train_rate = <span class="number">0.85</span></div>
<div class="line">index_train = np.random.choice(X.shape[<span class="number">0</span>],int(X.shape[<span class="number">0</span>]*train_rate),replace=<span class="keyword">False</span>)</div>
<div class="line">index_test  = list(set(range(X.shape[<span class="number">0</span>])) - set(index_train))</div>
<div class="line">X, Y = shuffle(X,Y)</div>
<div class="line">X_train, y_train = X[index_train],Y[index_train]</div>
<div class="line">X_test, y_test = X[index_test],Y[index_test]</div>
<div class="line"></div>
<div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</div>
<div class="line">sgd = optimizers.SGD(lr=<span class="number">1e-2</span>,decay=<span class="number">5</span>**(<span class="number">-4</span>),momentum=<span class="number">0.9</span>,nesterov=<span class="keyword">True</span>)</div>
<div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class="string">'accuracy'</span>])</div>
<div class="line"></div>
<div class="line">hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),</div>
<div class="line">                 batch_size=<span class="number">32</span>,epochs=<span class="number">200</span>,verbose=<span class="number">2</span>)</div>
<div class="line">model.save(<span class="string">"seg_model.h5"</span>)</div>
</pre></td></tr></table></figure>
<h2 id="mean-Intersection-over-Union"><a href="#mean-Intersection-over-Union" class="headerlink" title="mean Intersection over Union"></a>mean Intersection over Union</h2><p>Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset. However, Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU. </p>
<p>More formally, in order to apply Intersection over Union to evaluate an (arbitrary) object detector we need:</p>
<ol>
<li>The <em>ground-truth bounding boxes</em> (i.e., the hand labeled bounding boxes from the testing set that specify <em>where</em> in the image our object is).</li>
<li>The <em>predicted bounding boxes</em> from our model.</li>
</ol>
<p>As long as we have these two sets of bounding boxes we can apply Intersection over Union.</p>
<p>Below I have included a visual example of a ground-truth bounding box versus a predicted bounding box:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/iou_stop_sign.jpg" alt="ou_stop_sig"></p>
<p>Computing Intersection over Union can therefore be determined via:</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/iou_equation.png" alt="ou_equatio"></p>
<p>In the numerator we compute the <strong>area of overlap</strong> between the <em>predicted</em> bounding box and the <em>ground-truth</em> bounding box.</p>
<p>The denominator is the <strong>area of union</strong>, or more simply, the area encompassed by <em>both</em> the predicted bounding box and the ground-truth bounding box.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
<div class="line">19</div>
<div class="line">20</div>
<div class="line">21</div>
</pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">IoU</span><span class="params">(Yi, y_predi)</span>:</span></div>
<div class="line">    <span class="comment">## mean Intersection over Union</span></div>
<div class="line">    <span class="comment">## Mean IoU = TP/(FN + TP + FP)</span></div>
<div class="line"></div>
<div class="line">    IoUs = []</div>
<div class="line">    Nclass = int(np.max(Yi)) + <span class="number">1</span></div>
<div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(Nclass):</div>
<div class="line">        TP = np.sum((Yi == c) &amp; (y_predi == c))</div>
<div class="line">        FP = np.sum((Yi != c) &amp; (y_predi == c))</div>
<div class="line">        FN = np.sum((Yi == c) &amp; (y_predi != c))</div>
<div class="line">        IoU = TP / float(TP + FP + FN)</div>
<div class="line">        print(<span class="string">"class &#123;:02.0f&#125;: #TP=&#123;:6.0f&#125;, #FP=&#123;:6.0f&#125;, #FN=&#123;:5.0f&#125;, IoU=&#123;:4.3f&#125;"</span>.format(c, TP, FP, FN, IoU))</div>
<div class="line">        IoUs.append(IoU)</div>
<div class="line">    mIoU = np.mean(IoUs)</div>
<div class="line">    print(<span class="string">"_________________"</span>)</div>
<div class="line">    print(<span class="string">"Mean IoU: &#123;:4.3f&#125;"</span>.format(mIoU))</div>
<div class="line"></div>
<div class="line">y_pred = model.predict(X_test)</div>
<div class="line">y_predi = np.argmax(y_pred, axis=<span class="number">3</span>)</div>
<div class="line">y_testi = np.argmax(y_test, axis=<span class="number">3</span>)</div>
<div class="line">IoU(y_testi, y_predi)</div>
</pre></td></tr></table></figure>
<p><strong>WHY WE USE IoU?</strong></p>
<p>In all reality, it’s <em>extremely unlikely</em> that the <em>(x, y)</em>-coordinates of our predicted bounding box are going to <strong>exactly match</strong> the <em>(x, y)</em>-coordinates of the ground-truth bounding box.</p>
<p>Due to varying parameters of our model (image pyramid scale, sliding window size, feature extraction method, etc.), a complete and total match between predicted and ground-truth bounding boxes is simply unrealistic.</p>
<p>Because of this, we need to define an evaluation metric that <strong><em>rewards</em> predicted bounding boxes for heavily overlapping with the ground-truth</strong>.</p>
<p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/iou_examples.png" alt="ou_example"></p>
<p>As you can see, predicted bounding boxes that heavily overlap with the ground-truth bounding boxes have higher scores than those with less overlap. This makes Intersection over Union an excellent metric for evaluating custom object detectors.</p>
<p>We aren’t concerned with an <em>exact</em> match of <em>(x, y)</em>-coordinates, but we do want to ensure that our predicted bounding boxes match as closely as possible — Intersection over Union is able to take this into account.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div>
<div class="line">2</div>
<div class="line">3</div>
<div class="line">4</div>
<div class="line">5</div>
<div class="line">6</div>
<div class="line">7</div>
<div class="line">8</div>
<div class="line">9</div>
<div class="line">10</div>
<div class="line">11</div>
<div class="line">12</div>
<div class="line">13</div>
<div class="line">14</div>
<div class="line">15</div>
<div class="line">16</div>
<div class="line">17</div>
<div class="line">18</div>
</pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div>
<div class="line">    img_is = (X_test[i] + <span class="number">1</span>) * (<span class="number">255.0</span> / <span class="number">2</span>)</div>
<div class="line">    seg = y_predi[i]</div>
<div class="line">    segtest = y_testi[i]</div>
<div class="line"></div>
<div class="line">    fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">30</span>))</div>
<div class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</div>
<div class="line">    ax.imshow(img_is / <span class="number">255.0</span>)</div>
<div class="line">    ax.set_title(<span class="string">"original"</span>)</div>
<div class="line"></div>
<div class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</div>
<div class="line">    ax.imshow(img_colorized(seg, n_classes))</div>
<div class="line">    ax.set_title(<span class="string">"predicted class"</span>)</div>
<div class="line"></div>
<div class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</div>
<div class="line">    ax.imshow(img_colorized(segtest, n_classes))</div>
<div class="line">    ax.set_title(<span class="string">"true class"</span>)</div>
<div class="line">    plt.savefig(<span class="string">'&#123;&#125;.png'</span>.format(i))</div>
</pre></td></tr></table></figure>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Overall, in this paper, the most important thing is how use convolution transpose to upsample the feature to the original image size and utilize <code>u-net</code> structure to preserve features for acurate pixel classification. And in the convolution network, we only use convolution and its transpose, that is why it is called <strong>Fully Convolution Network</strong>. </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Object-Segmentation/" rel="tag"># Object Segmentation</a>
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/18/NLP-Attention/" rel="next" title="NLP-Attention">
                <i class="fa fa-chevron-left"></i> NLP-Attention
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/13/Spanish/" rel="prev" title="Spanish">
                Spanish <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qing Wong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">88</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Representing-the-task"><span class="nav-number">2.</span> <span class="nav-text">Representing the task</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fully-convolutional-networks"><span class="nav-number">3.</span> <span class="nav-text">Fully convolutional networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Upsampling-ref"><span class="nav-number">3.1.</span> <span class="nav-text">Upsampling ref</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Network-Structures"><span class="nav-number">3.2.</span> <span class="nav-text">Network Structures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Skip-Connections"><span class="nav-number">3.3.</span> <span class="nav-text">Skip Connections</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-function"><span class="nav-number">3.4.</span> <span class="nav-text">Loss function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Implementation"><span class="nav-number">4.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-exploration"><span class="nav-number">4.1.</span> <span class="nav-text">Data exploration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-preprocessing"><span class="nav-number">4.2.</span> <span class="nav-text">Data preprocessing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Network-architecture"><span class="nav-number">4.3.</span> <span class="nav-text">Network architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training"><span class="nav-number">4.4.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mean-Intersection-over-Union"><span class="nav-number">4.5.</span> <span class="nav-text">mean Intersection over Union</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">5.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qing Wong</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
