<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>ML-Knowledge</title>
      <link href="/2019/12/13/ML-Knowledge/"/>
      <url>/2019/12/13/ML-Knowledge/</url>
      <content type="html"><![CDATA[<p>All kinds of Machine learning methods.</p><a id="more"></a><h1 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h1><p>Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the data produced by the model were actually observed.</p><h2 id="Intuitive-explanation"><a href="#Intuitive-explanation" class="headerlink" title="Intuitive explanation"></a>Intuitive explanation</h2><p>Let’s suppose we have observed 10 data points from some process. For example, each data point could represent the length of time in seconds that it takes a student to answer a specific exam question. These 10 data points are shown in the figure below</p><p><img src="/2019/12/13/ML-Knowledge/1_Z3JJGvEtOjmpLFvmWiUR3Q.png" alt="_Z3JJGvEtOjmpLFvmWiUR3"></p><p>We first have to decide which model we think best describes the process of generating the data. This part is very important. At the very least, we should have a good idea about which model to use. For these data we’ll assume that the data generation process can be adequately described by a Gaussian (normal) distribution. Visual inspection of the figure above suggests that a Gaussian distribution is plausible because most of the 10 points are clustered in the middle with few points scattered to the left and the right.</p><p>Recall that the Gaussian distribution has 2 parameters. The mean, $\mu$, and the standard deviation, $\sigma$. Different values of these parameters result in different curves (just like with the straight lines above). We want to know <em>which curve was most likely responsible for creating the data points that we observed?</em> (See figure below). Maximum likelihood estimation is a method that will find the values of μ and σ that result in the curve that best fits the data.</p><p><img src="/2019/12/13/ML-Knowledge/1_uLKl0Nz1vFg6bmfiqpCKZQ.png" alt="_uLKl0Nz1vFg6bmfiqpCKZ"></p><h2 id="Calculating-the-Maximum-Likelihood-Estimates"><a href="#Calculating-the-Maximum-Likelihood-Estimates" class="headerlink" title="Calculating the Maximum Likelihood Estimates"></a>Calculating the Maximum Likelihood Estimates</h2><p>Now that we have an intuitive understanding of what maximum likelihood estimation is we can move on to learning how to calculate the parameter values. The values that we find are called the maximum likelihood estimates (MLE).</p><p>Again we’ll demonstrate this with an example. Suppose we have three data points this time and we assume that they have been generated from a process that is adequately described by a Gaussian distribution. These points are 9, 9.5 and 11. How do we calculate the maximum likelihood estimates of the parameter values of the Gaussian distribution $\mu$ and $\sigma$?</p><p>What we want to calculate is the total probability of observing all of the data, i.e. the joint probability distribution of all observed data points. To do this we would need to calculate some conditional probabilities, which can get very difficult. So it is here that we’ll make our first assumption. <em>The assumption is that each data point is generated independently of the others</em>. This assumption makes the maths much easier. If the events (i.e. the process that generates the data) are independent, then the total probability of observing all of data is the product of observing each data point individually (i.e. the product of the marginal probabilities).</p><p>The probability density of observing a single data point $x$, that is generated from a Gaussian distribution is given by:</p><p><img src="/2019/12/13/ML-Knowledge/1_t4zrihvhtlZJZsvcX3jRjg-6256352.png" alt="_t4zrihvhtlZJZsvcX3jRjg-625635"></p><p>In our example the total (joint) probability density of observing the three data points is given by:</p><p><img src="/2019/12/13/ML-Knowledge/1_rFzbQ614IR4zEwBM3k1V0Q.png" alt="_rFzbQ614IR4zEwBM3k1V0"></p><p>We just have to figure out the values of $\mu$ and $\sigma$ that results in giving the maximum value of the above expression.</p><p>If you’ve covered calculus in your maths classes then you’ll probably be aware that there is a technique that can help us find maxima (and minima) of functions. It’s called <em>differentiation.</em> All we have to do is find the derivative of the function, set the derivative function to zero and then rearrange the equation to make the parameter of interest the subject of the equation.</p><h3 id="The-log-likelihood"><a href="#The-log-likelihood" class="headerlink" title="The log likelihood"></a>The log likelihood</h3><p>The above expression for the total probability is actually quite a pain to differentiate, so it is almost always simplified by taking the natural logarithm of the expression. Taking logs of the original expression gives us:</p><p><img src="/2019/12/13/ML-Knowledge/1_iEdEaqWWiruaw_Fr2ophxw.png" alt="_iEdEaqWWiruaw_Fr2ophx"></p><p>This expression can be simplified again using the laws of logarithms to obtain:</p><p><img src="/2019/12/13/ML-Knowledge/1_xjDrGJ_JHLMa7619jFkjLA.png" alt="_xjDrGJ_JHLMa7619jFkjL"></p><p>This expression can be differentiated to find the maximum. In this example we’ll find the MLE of the mean, $\mu$. To do this we take the partial derivative of the function with respect to $\mu$, giving</p><p><img src="/2019/12/13/ML-Knowledge/1_kdjQQo5jUX9a2Z0kblJ4Hg.png" alt="_kdjQQo5jUX9a2Z0kblJ4H"></p><p>Finally, setting the left hand side of the equation to zero and then rearranging for $\mu$ gives:</p><p><img src="/2019/12/13/ML-Knowledge/1_rHtqdjFXRw4sdnLU9n_WsQ.png" alt="_rHtqdjFXRw4sdnLU9n_Ws"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><strong>Can maximum likelihood estimation always be solved in an exact manner?</strong></p><p><em>No</em> is the short answer. It’s more likely that in a real world scenario the derivative of the log-likelihood function is still analytically intractable (i.e. it’s way too hard/impossible to differentiate the function by hand). Therefore, iterative methods like <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" target="_blank" rel="noopener">Expectation-Maximization algorithms</a> are used to find numerical solutions for the parameter estimates. The overall idea is still the same though.</p><p><strong>So why maximum likelihood and not maximum probability?</strong></p><p>Well this is just statisticians being pedantic (but for good reason). Most people tend to use probability and likelihood interchangeably but statisticians and probability theorists distinguish between the two. The reason for the confusion is best highlighted by looking at the equation.</p><p><img src="/2019/12/13/ML-Knowledge/1_XUHA8X_WauSB8anrb6lllA.png" alt="_XUHA8X_WauSB8anrb6lll"></p><p>These expressions are equal! So what does this mean? Let’s first define P(data; $mu$, $\sigma$)? It means “the probability density of observing the data with model parameters $ mu$ and $\sigma$”. It’s worth noting that we can generalise this to any number of parameters and any distribution.</p><p>On the other hand L($mu$, $\sigma$; data) means <em>“the likelihood of the parameters $mu$ and $\sigma$ taking certain values given that we’ve observed a bunch of data.”</em></p><p>The equation above says that the probability density of the data given the parameters is equal to the likelihood of the parameters given the data. But despite these two things being equal, the likelihood and the probability density are fundamentally asking different questions — one is asking about the data and the other is asking about the parameter values. This is why the method is called maximum likelihood and not maximum probability.</p><p><strong>When is least squares minimisation the same as maximum likelihood estimation?</strong></p><p>Least squares minimisation is another common method for estimating parameter values for a model in machine learning. It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the least squares method. For a more in-depth mathematical derivation check out <a href="https://web.archive.org/web/20111202153913/http://www.cs.cmu.edu/~epxing/Class/10701/recitation/recitation3.pdf" target="_blank" rel="noopener">these slides</a>.</p><p>Intuitively we can interpret the connection between the two methods by understanding their objectives. For least squares parameter estimation we want to find the line that minimises the total squared distance between the data points and the regression line (see the figure below). In maximum likelihood estimation we want to maximise the total probability of the data. When a Gaussian distribution is assumed, the maximum probability is found when the data points get closer to the mean value. Since the Gaussian distribution is symmetric, this is equivalent to minimising the distance between the data points and the mean value.</p><p><img src="/2019/12/13/ML-Knowledge/1_dgdJ47sNQHxZ5t1uWutflA.png" alt="_dgdJ47sNQHxZ5t1uWutfl"></p><h1 id="Expectation-Maximization"><a href="#Expectation-Maximization" class="headerlink" title="Expectation Maximization"></a>Expectation Maximization</h1><p><a href="http://bjlkeng.github.io/posts/the-expectation-maximization-algorithm/" target="_blank" rel="noopener">ref1</a> <a href="https://medium.com/analytics-vidhya/expectation-maximization-algorithm-step-by-step-30157192de9f" target="_blank" rel="noopener">ref2</a> </p><p>Although Maximum Likelihood Estimation (MLE) and EM can both find “best-fit” parameters, <em>how</em> they find the models are very different. MLE accumulates all of the data first and then uses that data to construct the most likely model. EM takes a guess at the parameters first — accounting for the missing data — then tweaks the model to fit the guesses and the observed data. The basic steps for the algorithm are:</p><ol><li>An initial guess is made for the model’s parameters and a probability distribution is created. This is sometimes called the “E-Step” for the “<strong>E</strong>xpected” distribution.</li><li>Newly observed data is fed into the model.</li><li>The probability distribution from the E-step is tweaked to include the new data. This is sometimes called the “M-step.”</li><li>Steps 2 through 4 are repeated until stability (i.e. a distribution that doesn’t change from the E-step to the M-step) is reached.</li></ol><p><strong>Latent Variables</strong></p><p>A latent variable model is a type of statistical model that contains two types of variables: <em>observed variables</em> and <em>latent variables</em>. Observed variables are ones that we can measure or record, while latent (sometimes called <em>hidden</em>) variables are ones that we cannot directly observe but rather inferred from the observed variables. One reason why we add latent variables is to model “higher level concepts” in the data, usually these “concepts” are unobserved but easily understood by the modeller. Adding these variables can also simplify our model by reducing the number of parameters we have to estimate.</p><p>Consider the problem of modelling medical symptoms such as blood pressure, heart rate and glucose levels (observed outcomes) and mediating factors such as smoking, diet and exercise (observed “inputs”). We could model all the possible relationships between the mediating factors and observed outcomes but the number of connections grows very quickly. Instead, we can model this problem as having mediating factors causing a non-observable hidden variable such as heart disease, which in turn causes our medical symptoms. This is shown in the next figure (example taken from <em>Machine Learning: A Probabilistic Perspective</em>).</p><p><img src="/2019/12/13/ML-Knowledge/latent_vars.png" alt="atent_var"></p><p><strong>Gaussian Mixture Models</strong></p><p>As an example, suppose we’re trying to understand the prices of houses across the city. The housing price will be heavily dependent on the neighborhood, that is, houses clustered around a neighborhood will be close to the average price of the neighborhood. In this context, it is straight forward to observe the prices at which houses are sold (observed variables) but what is not so clear is how is to observe or estimate the price of a “neighborhood” (the latent variables). A simple model for modelling the neighborhood price is using a Gaussian (or normal) distribution, but which house prices should be used to estimate the average neighborhood price? Should all house prices be used in equal proportion, even those on the edge? What if a house is on the border between two neighborhoods? Can we even define clearly if a house is in one neighborhood or the other? These are all great questions that lead us to a particular type of latent variable model called a <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model" target="_blank" rel="noopener">Gaussian mixture model</a>.</p><p>Following along with this housing price example, let’s represent the price of each house as real-valued random variable $x_i$ and the unobserved neighborhood it belongs to as a discrete valued random variable $z_i$. Further, let’s suppose we have $K$ neighborhoods, therefore $z_i$ can be modelled as a categorical distribution with parameter $\pi=[\pi_1,…,\pi_k]$, and the price distribution of the $k^{th}$ neighborhood as a Gaussian $N(\mu_k,\sigma^{2}_{k})$. The density of $x_i$ is given by:</p><script type="math/tex; mode=display">\begin{aligned} p\left(x_{i} | \theta\right) &=\sum_{k=1}^{K} p\left(z_{i}=k\right) p\left(x_{i} | z_{i}=k, \mu_{k}, \sigma_{k}^{2}\right) \\ x_{i} | z_{i} & \sim \mathcal{N}\left(\mu_{k}, \sigma_{k}^{2}\right) \\ z_{i} & \sim \text { Categorical }(\pi) \end{aligned}</script><p>where $\theta $ represents the parameters of the Gaussians (all the $\mu_k$, $\sigma^2_k$) and the categorical variables $\pi$. $\pi$ represents the prior mixture weights of the neighborhoods. </p><p>In the Expectation Step, we assume that the values of all the parameters $(\theta=(\mu_k,\sigma^2_k,\pi))$ are fixed and are set to the ones from the previous iteration of the algorithm. We then just need to compute the responsiblity of each cluster to each point. Re-phasing this problem: Assuming you know the locations of each of the $K$ Gaussians $(\mu_k,\sigma_k)$ , and the prior mixture weights of the Gaussians $(\pi_k)$, what is the probability that a given point $x_i$ is drawn from cluster $k$?</p><p>We can write this in terms of probability and use Bayes theorem to find the answer:</p><script type="math/tex; mode=display">\begin{aligned} p\left(z_{i}=k | x_{i}, \theta\right) &=\frac{p\left(x_{i} | z_{i}=k, \theta\right) \cdot p\left(z_{i}=k\right)}{\sum_{j=1}^{K} p\left(x_{i} | z_{i}=j, \theta\right) \cdot p\left(z_{i}=j\right)} \\ &=\frac{\mathcal{N}\left(x_{i} | \mu_{k}, \sigma_{k}\right) \cdot \pi_{k}}{\sum_{j=1}^{K} \mathcal{N}\left(x_{i} | \mu_{j}, \sigma_{j}\right) \cdot \pi_{j}} \end{aligned}</script><p>The Maximization Step turns things around and assumes the responsibilities (proxies for the latent variables) are fixed, and now the problem is we want to maximize our (expected complete data log) likelihood function across all the $(\theta=(\mu_k,\sigma^2_k,\pi))$ variables. </p><p>First up, the distribution of the prior mixture weights $\pi$. Assuming you know all the values of the latent variables; then intuitively, we just need to sum up the contribution to each cluster and normalize:</p><script type="math/tex; mode=display">\begin{equation*}\pi_k = \frac{1}{N} \sum_i r_{ik} \tag{3}\end{equation*}</script><p>Next, we need to estimate the Gaussians. Again, since we know the responsibilities of each point to each cluster, we can just use our standard methods for estimating the mean and standard deviation of Gaussians but weighted according to the responsibilities:</p><script type="math/tex; mode=display">\begin{align*}\mu_k &= \frac{\sum_i r_{ik}x_i}{\sum_i r_{ik}} \\\sigma_k &= \frac{\sum_i r_{ik}(x_i - \mu_k)(x_i - \mu_k)}{\sum_i r_{ik}} \tag{4}\end{align*}</script><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p><a href="https://liam.page/2018/11/08/Expectation-Maximization-Algorithm/" target="_blank" rel="noopener">link</a> </p>]]></content>
      
      <categories>
          
          <category> Macheine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Math </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Math-Linear Algebra</title>
      <link href="/2019/12/11/Math-Linear-Algebra/"/>
      <url>/2019/12/11/Math-Linear-Algebra/</url>
      <content type="html"><![CDATA[<p>This is my study notes for <a href="https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/" target="_blank" rel="noopener">MIT Linear Algebra</a>.</p><a id="more"></a><h1 id="The-geometry-of-linear-equations"><a href="#The-geometry-of-linear-equations" class="headerlink" title="The geometry of linear equations"></a>The geometry of linear equations</h1><p>The fundamental problem of linear algebra is to solve $n$ linear equations in $n$ unknowns, for example</p><script type="math/tex; mode=display">\begin{aligned} 2 x-y &=0 \\-x+2 y &=3 \end{aligned}</script><p>Here, we are going to view this problem in three ways.</p><h2 id="Row-Picture"><a href="#Row-Picture" class="headerlink" title="Row Picture"></a>Row Picture</h2><p>Plot the points that satisfy each equation. The intersection of the plots (if they do intersect) represents the solution to the system of equations. Looking at the following figure we see that the solution to this system of equations is $x=1,y=2$</p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-16 at 6.53.24 PM.png" alt="creen Shot 2019-12-16 at 6.53.24 P"></p><h2 id="Column-Picture"><a href="#Column-Picture" class="headerlink" title="Column Picture"></a>Column Picture</h2><p>In the column picture we rewrite the system of linear equations as a single<br>equation by turning the coefficients in the columns of the system into vectors:</p><script type="math/tex; mode=display">x\left[\begin{array}{r}{2} \\ {-1}\end{array}\right]+y\left[\begin{array}{r}{-1} \\ {2}\end{array}\right]=\left[\begin{array}{l}{0} \\ {3}\end{array}\right]</script><p>Given two vectors $c$ and $d$ and scalars $x$ and $y$, then sum $xc+yd$ is called a linear combination of $c$ and $d$. </p><h2 id="Matrix-Picture"><a href="#Matrix-Picture" class="headerlink" title="Matrix Picture"></a>Matrix Picture</h2><p>We can write the system of equations as a single equation by using matrices and vectors:</p><script type="math/tex; mode=display">\left[\begin{array}{rr}{2} & {-1} \\ {-1} & {2}\end{array}\right]\left[\begin{array}{l}{x} \\ {y}\end{array}\right]=\left[\begin{array}{l}{0} \\ {3}\end{array}\right]</script><p>The matrix $A=\left[\begin{array}{rr}{2} &amp; {-1} \\ {-1} &amp; {2}\end{array}\right]$ is called the coefficient matrix. The vector $x=\left[\begin{array}{l}{x} \\ {y}\end{array}\right]$ is the vector of unknowns. The values on the right hand side of the equations form the vector $b$</p><script type="math/tex; mode=display">Ax=b</script><p><strong>Matrix Multiplication</strong></p><p>One method to multiply a matrix $A$ by a vector $x$ is to think of $x$ as the coefficients of a linear combination of the column vectors of the matrix</p><script type="math/tex; mode=display">\left[\begin{array}{ll}{2} & {5} \\ {1} & {3}\end{array}\right]\left[\begin{array}{l}{1} \\ {2}\end{array}\right]=1\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+2\left[\begin{array}{l}{5} \\ {3}\end{array}\right]=\left[\begin{array}{c}{12} \\ {7}\end{array}\right]</script><p>or if</p><script type="math/tex; mode=display">\begin{bmatrix}1&2\end{bmatrix}\left[\begin{array}{ll}{2} & {5} \\ {1} & {3}\end{array}\right]=1\begin{bmatrix}2&5\end{bmatrix}+2\begin{bmatrix}1&3\end{bmatrix}</script><h2 id="Linear-Independence"><a href="#Linear-Independence" class="headerlink" title="Linear Independence"></a>Linear Independence</h2><p>Given a matrix $A$, can we solve</p><script type="math/tex; mode=display">Ax=b</script><p>for every possible vector $b$? In other words, do the linear combinations of the column vectors fill the $xy$-plane (or space, in the three dimensional case)?</p><p>If the answer is “no”, we say that A is a singular matrix. In this singular<br>case its column vectors are linearly dependent; all linear combinations of those vectors lie on a point or line (in two dimensions) or on a point, line or plane (in three dimensions). The combinations don’t fill the whole space. </p><h1 id="Elimination-with-Matrices"><a href="#Elimination-with-Matrices" class="headerlink" title="Elimination with Matrices"></a>Elimination with Matrices</h1><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-17 at 6.28.03 PM.png" alt="creen Shot 2019-12-17 at 6.28.03 P"></p><p>Let’s look at the result matrix, i.e., the third one. For the first row,  $\begin{bmatrix}1&amp;2&amp;1\end{bmatrix}$, it is computed by only using the first row of the second matrix, which means the elimination matrix, i.e., the first matrix, has $\begin{bmatrix}1&amp;0&amp;0\end{bmatrix}$ as its first row. It is the same situation with the third row. So we have elimination matrix $\begin{bmatrix}1 &amp; 0 &amp; 0\\ &amp;  &amp; \\ 0&amp;0&amp;1\end{bmatrix}$. As for the second row, we can see that in the second matrix, its first row contributes -3 and its second row contributes 1. But there is no contribution from third row. Therefore, the elimination matrix is $\begin{bmatrix}1 &amp; 0 &amp; 0\\ -3&amp;  1&amp; 0\\ 0&amp;0&amp;1\end{bmatrix}$. </p><h2 id="Inverses"><a href="#Inverses" class="headerlink" title="Inverses"></a>Inverses</h2><p>If $A$ is a square matrix, the most important question you can ask about it is whether it has an inverse $A^{-1}$. If it does, then $ A^{-1}A=I=AA^{-1}$ (This is just a special form of the equation Ax = b). And we say that $A$ is invertible or nonsingular.</p><p>If A is singular – i.e. A does not have an inverse – its determinant is zero and we can find some non-zero vector x for which Ax = 0. For example:</p><script type="math/tex; mode=display">\left[\begin{array}{ll}{1} & {3} \\ {2} & {6}\end{array}\right]\left[\begin{array}{r}{3} \\ {-1}\end{array}\right]=\left[\begin{array}{l}{0} \\ {0}\end{array}\right]</script><p>We can use the method of elimination to solve two or more linear equations at<br>the same time. Just augment the matrix with the whole identity matrix I:</p><script type="math/tex; mode=display">\left[\begin{array}{ll|ll}{1} & {3} & {1} & {0} \\ {2} & {7} & {0} & {1}\end{array}\right] \rightarrow\left[\begin{array}{rr|rr}{1} & {3} & {1} & {0} \\ {0} & {1} & {-2} & {1}\end{array}\right] \rightarrow\left[\begin{array}{rr|rr}{1} & {0} & {7} & {-3} \\ {0} & {1} & {-2} & {1}\end{array}\right]</script><script type="math/tex; mode=display">A^{-1}=\left[\begin{array}{rr}{7} & {-3} \\ {-2} & {1}\end{array}\right]</script><blockquote><p>Use GaussJordan elimination on $\begin{bmatrix}U&amp;I\end{bmatrix}$to find the upper triangular $U^{-1}$.</p><script type="math/tex; mode=display">U U^{-1}=I\left[\begin{array}{ccc}{1} & {a} & {b} \\ {0} & {1} & {c} \\ {0} & {0} & {1}\end{array}\right]\left[\begin{array}{lll}{x_{1}} & {x_{2}} & {x_{3}} \\ {} & {} & {}\end{array}\right]=\left[\begin{array}{lll}{1} & {0} & {0} \\ {0} & {1} & {0} \\ {0} & {0} & {1}\end{array}\right]</script><p>Solution: Row reduce $\begin{bmatrix}U&amp;I\end{bmatrix}$ to get $\begin{bmatrix}I&amp;U^{-1}\end{bmatrix}$ as follows:</p><script type="math/tex; mode=display"></script><script type="math/tex; mode=display">\left.\left[\begin{array}{cccccc}{1} & {a} & {b} & {1} & {0} & {0} \\ {0} & {1} & {c} & {0} & {1} & {0} \\ {0} & {0} & {1} & {0} & {0} & {1}\end{array}\right] \longrightarrow \begin{array}{l}{\left(R_{1}=R_{1}-a R_{2}\right)} \\ {R_{2}=R_{2}-c R_{2}}\end{array}\right)\left[\begin{array}{cccccc}{1} & {0} & {b-a c} & {1} & {-a} & {0} \\ {0} & {1} & {} & {0} & {0} & {1} & {-c} \\ {0} & {0} & {1} & {0} & {0} & {1}\end{array}\right]\\\left(R_{1}=R_{1}-(b-a c) R_{3}\right)\left[\begin{array}{ccccc}{1} & {0} & {0} & {1} & {-a} & {a c-b} \\ {0} & {1} & {0} & {0} & {1} & {-c} \\ {0} & {0} & {1} & {0} & {0} & {1}\end{array}\right]=\left[\begin{array}{cc}{I} & {L^{-1}}\end{array}\right]</script></blockquote><h2 id="Permutations"><a href="#Permutations" class="headerlink" title="Permutations"></a>Permutations</h2><p>Multiplication by a permutation matrix $P$ swaps the rows of a matrix; when applying the method of elimination we use permutation matrices to move zeros out of pivot positions. Our factorization $A=LU$ then becomes $PA=LU$, where $P$ is a permutation matrix which recorders any number of rows of $A$. Recall that $P^{-1}=P^{T}$, i.e., that $P^{T}P=I$.</p><h1 id="Vector-Space"><a href="#Vector-Space" class="headerlink" title="Vector Space"></a>Vector Space</h1><p>We can add vectors and multiply them by numbers, which means we can discuss linear combinations of vectors. These combinations follow the rules of a vector space.</p><p><strong>Closure</strong></p><p>The collection of vectors with exactly two positive real valued components is not a vector space. The sum of any two vectors in that collection is again in the collection, but multiplying any vector by, say, −5, gives a vector that’s not in the collection. We say that this collection of positive vectors is closed under addition but not under multiplication.<br>If a collection of vectors is closed under linear combinations (i.e. under addition and multiplication by any real numbers), and if multiplication and addition behave in a reasonable way, then we call that collection a vector space.</p><p>A vector space must contain origin, i.e., it has zero vector. Because it should allow multiply by 0, or addition of negative and positive identity vectors.</p><p><strong>A vector space is a collection of vectors which is closed under linear combinations. In other words, for any two vectors v and w in the space and any two real numbers c and d, the vector cv + dw is also in the vector space.</strong></p><p>A plane $P$ containing $\begin{bmatrix}0\\0\\0\end{bmatrix}$ and a line $L$ containing $\begin{bmatrix}0\\0\\0\end{bmatrix}$ are both subspace of $R^3$. The union $P \cup L$ of these two subspaces is generally not a subspace, because the sum of a vector in $P$ and a vector in $L$ is probably not contained in $P \cup L$. The intersection $S \cap T$ of two subspaces $S$ and $T$ is a subspace. To prove this, use the fact that both $S$ and $T$ are closed under linear combinations to show that their intersection is closed under linear combinations.  </p><h2 id="Subspace"><a href="#Subspace" class="headerlink" title="Subspace"></a>Subspace</h2><p>A vector space that is contained inside of another vector space is called a subspace of that space. For example, take any non-zero vector $v$ in $R^2$. Then the set of all vectors $cv$, where $c$ is a real number, forms a subspace of $R^2    $. This collection of vectors describes a line through $\begin{bmatrix}0\\0\end{bmatrix}$ in $R^2$ and is closed under addition.</p><p>A line in $R^2$ that does not pass through the origin is not a subspace of $R^2$. Multiplying any vector on that line by 0 gives the zero vector, whcih does not line on the line. Every subspace must contain the zero vector because vector spaces are closed under multiplication.</p><p>The subspace of $R^2$ are:</p><ol><li>all of $R^2$</li><li>any line through $\begin{bmatrix}0\\0\end{bmatrix}$ and</li><li>the zero vector alone</li></ol><p>The subspace of $R^3$ are:</p><ol><li>all of $R^3$</li><li>any plane through the origin</li><li>any line through the origin, and</li><li>the zero vector alone</li></ol><blockquote><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-18 at 6.44.46 PM.png" alt="creen Shot 2019-12-18 at 6.44.46 P"></p><p>First of all, here $M$ is a set containing all symmetric matrices. According to the definition of subspace, as long as the vectors in that set is closed, then it is a space. So, we use two symmetric matrix $A$ and $B$ and prove whether $(A+B)$ and $cA$ are symmetric.</p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-18 at 6.48.06 PM.png" alt="creen Shot 2019-12-18 at 6.48.06 P"></p></blockquote><h2 id="Column-space"><a href="#Column-space" class="headerlink" title="Column space"></a>Column space</h2><p>Given a matrix $A$ with columns in $R^3$, these columns and all their linear combinations form a subspace of $R^3$. This is the column space $C(A)$. If $A=\begin{bmatrix}1&amp;2\\2&amp;3\\4&amp;1\end{bmatrix}$, the column space of $A$ is the plane through the origin in $R^3$ containing $A=\begin{bmatrix}1\\2\\4\end{bmatrix}$ and $A=\begin{bmatrix}2\\3\\1\end{bmatrix}$</p><p>In a nutshell, the column space of a matrix $A$ is the vector space made up of all linear combinations of the columns of $A$. </p><p><strong>Solving $Ax=b$</strong></p><p>Given a matrix $A$, for what vectors $b$ does $Ax=b$ have a solution?</p><script type="math/tex; mode=display">\text { Let } A=\left[\begin{array}{lll}{1} & {1} & {2} \\ {2} & {1} & {3} \\ {3} & {1} & {4} \\ {4} & {1} & {5}\end{array}\right]</script><p>Then $Ax = b$ does not have a solution for every choice of b because solving $Ax = b$ is equivalent to solving four linear equations in three unknowns. If there is a solution to $Ax = b$, then $b$ must be a linear combination of the columns of A. <strong>Only three columns cannot fill the entire four dimensional vector space</strong> – some vectors b cannot be expressed as linear combinations of columns of A. </p><p>Big question: what $b$’s allow $Ax=b$ to be solved?</p><p>The system of linear equations $Ax=b$ is solvable exactly when b is a vector in the column space of $A$.</p><p>For our example matrix $A$, what can we say about the column space of $A$? Are the columns of $A$ independent? In other words, does each column contribute something new to the subspace? The third column of $A$ is the sum of the first two columns, so does not add anything to the subspace. The column space of our matrix $A$ is a two dimensional subspace of $R^4$.</p><h2 id="Nullspace-of-Matrix"><a href="#Nullspace-of-Matrix" class="headerlink" title="Nullspace of Matrix"></a>Nullspace of Matrix</h2><p>The nullspace of a matrix $A$ is the collection of all solutions $x=\begin{bmatrix}x_1 \\x_2 \\x_3 \end{bmatrix}$ to the equation $Ax=0$. The column space of the matrix in our example was a subpace of $R^4$. The nullspace of $A$ is a subspace of $R^3$. To see that it’s a vector space, check that any sum or multiple of solutions to $Ax=0$ is a also solution: $A(x_1+x_2)=Ax_1+Ax_2=0+0$ and $A(cx)=cAx=c(0)$.</p><p>In the example,</p><script type="math/tex; mode=display">\left[\begin{array}{lll}{1} & {1} & {2} \\ {2} & {1} & {3} \\ {3} & {1} & {4} \\ {4} & {1} & {5}\end{array}\right]\left[\begin{array}{l}{x_{1}} \\ {x_{2}} \\ {x_{3}}\end{array}\right]=\left[\begin{array}{l}{0} \\ {0} \\ {0} \\ {0}\end{array}\right]</script><p>the nullspace $N(A)$ consists of all multiples of $\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}$. column 1 plus column 2 minus column 3 equals the zero vector. This nullspace is a line in $R^3$.</p><p><strong>Other values of b</strong></p><p>The solutions to the equations:</p><script type="math/tex; mode=display">\left[\begin{array}{lll}{1} & {1} & {2} \\ {2} & {1} & {3} \\ {3} & {1} & {4} \\ {4} & {1} & {5}\end{array}\right]\left[\begin{array}{l}{x_{1}} \\ {x_{2}} \\ {x_{3}}\end{array}\right]=\left[\begin{array}{l}{1} \\ {2} \\ {3} \\ {4}\end{array}\right]</script><p>do not form a subspace. The zero vector is not a solution to this equation. The set of solutions forms a line in $R^3$ that passes through the points $\begin{bmatrix}1 \\0 \\0 \end{bmatrix}$ and $\begin{bmatrix}0 \\ -1 \\1 \end{bmatrix}$ but not $\begin{bmatrix}0 \\0 \\0 \end{bmatrix}$</p><blockquote><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-19 at 9.55.56 AM.png" alt="creen Shot 2019-12-19 at 9.55.56 A"></p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-19 at 9.56.12 AM.png" alt="creen Shot 2019-12-19 at 9.56.12 A"></p></blockquote><h2 id="computing-the-nullspace"><a href="#computing-the-nullspace" class="headerlink" title="computing the nullspace"></a>computing the nullspace</h2><p>Suppose </p><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {2} & {4} & {6} & {8} \\ {3} & {6} & {8} & {10}\end{array}\right]</script><p>Our algorithm for computing the nullspace of this matrix uses the method of elimination, despite the fact that A is not invertible. We don’t need to use an augmented matrix because the right side (the vector b) is 0 in this computation. </p><p>The first step of elimination gives us: </p><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {2} & {4} & {6} & {8} \\ {3} & {6} & {8} & {10}\end{array}\right] \quad \longrightarrow \quad\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {2} & {4}\end{array}\right]</script><p>We don’t find a pivot in the second column, so our next pivot is the 2 in the third column of the second row: </p><script type="math/tex; mode=display">\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {2} & {4}\end{array}\right] \quad \longrightarrow \quad\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {0} & {0}\end{array}\right]=U</script><p>The matrix U is in echelon (staircase) form. The third row is zero because row 3 was a linear combination of rows 1 and 2; it was eliminated. <strong>The rank of a matrix A equals the number of pivots it has</strong>. In this example, the rank of A (and of U) is 2. </p><p><strong>special solutions</strong></p><p>Once we’ve found U we can use back-substitution to find the solutions x to the equation Ux = 0. In our example, columns 1 and 3 are pivot columns containing pivots, and columns 2 and 4 are free columns. We can assign any value to x2 and x4; we call these free variables. Suppose x2 = 1 and x4 = 0. Then: </p><script type="math/tex; mode=display">2x_3+4x_4=0 => x_3=0</script><p>and </p><script type="math/tex; mode=display">x_1+2x_2+2x_3+2x_4=0  =>x_1=-2</script><p>so one solution is $x=\begin{bmatrix}-2\\1\\0\\0\end{bmatrix}$. Any multiple of this vector is in the nullspace. </p><p>Letting a different free variable equal 1 and setting the other free variables equal to 0 gives us other vectors in the nullspace. For example:</p><script type="math/tex; mode=display">x=\begin{bmatrix}2\\0\\-2\\1\end{bmatrix}</script><p><strong>The rank $r$ of A equals the number of pivot columns, so the number of free columns is $n − r$: the number of columns (variables) minus the number of pivot columns. This equals the number of special solution vectors and the dimension of the nullspace. </strong></p><h2 id="Reduced-row-echelon-form"><a href="#Reduced-row-echelon-form" class="headerlink" title="Reduced row echelon form"></a>Reduced row echelon form</h2><p>By continuing to use the method of elimination we can convert U to a matrix R in reduced row echelon form (rref form), with pivots equal to 1 and zeros above and below the pivots. </p><script type="math/tex; mode=display">U=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {0} & {0}\end{array}\right] \rightarrow\left[\begin{array}{rrrr}{1} & {2} & {0} & {-2} \\ {0} & {0} & {2} & {4} \\ {0} & {0} & {0} & {0}\end{array}\right] \rightarrow\left[\begin{array}{rrrr}{1} & {2} & {0} & {-2} \\ {0} & {0} & {1} & {2} \\ {0} & {0} & {0} & {0}\end{array}\right]=R</script><p>By exchanging some columns, R can be rewritten with a copy of the identity matrix in the upper left corner, possibly followed by some free columns on the right. If some rows of A are linearly dependent, the lower rows of the matrix R will be filled with zeros: </p><script type="math/tex; mode=display">R=\left[\begin{array}{cc}{I} & {F} \\ {0} & {0}\end{array}\right]</script><p>(Here I is an r by r square matrix.) </p><p>If N is a nullspace matrix $N=\begin{bmatrix}-F\\I\end{bmatrix}$ then $RN=0$.  (Here I is an n − r by n − r square matrix and 0 is an m by n − r matrix.) The columns of N are the special solutions.</p><p>In this example, we have $-F=\begin{bmatrix}0&amp;-2\-1&amp;-2\\1&amp;0\\0&amp;1\end{bmatrix}$. So the special solutions to $Ax=0$ are:</p><script type="math/tex; mode=display">\left[\begin{array}{r}{0} \\ {-1 } \\ {1} \\ {0}\end{array}\right] \text { and }\left[\begin{array}{r}{-2 } \\ {-2} \\ {0} \\ {1}\end{array}\right]</script><h2 id="Solving-Ax-b"><a href="#Solving-Ax-b" class="headerlink" title="Solving Ax=b"></a>Solving Ax=b</h2><h3 id="Solvability-conditions-on-b"><a href="#Solvability-conditions-on-b" class="headerlink" title="Solvability conditions on b"></a>Solvability conditions on b</h3><p>when does $Ax=b$ have solutions and how can we describe those solutions?</p><p>We again use the example:</p><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{1} & {2} & {2} & {2} \\ {2} & {4} & {6} & {8} \\ {3} & {6} & {8} & {10}\end{array}\right]</script><p>The third row of A is the sum of its first and second rows, so we know that if Ax = b the third component of b equals the sum of its first and second components. If b does not satisfy b3 = b1 + b2 the system has no solution. If a combination of the rows of A gives the zero row, then the same combination of the entries of b must equal zero. </p><p>One way to find out whether Ax = b is solvable is to use elimination on the augmented matrix. If a row of A is completely eliminated, so is the corresponding entry in b. In our example, row 3 of A is completely eliminated: </p><script type="math/tex; mode=display">\left[\begin{array}{ccccc}{1} & {2} & {2} & {2} & {b_{1}} \\ {2} & {4} & {6} & {8} & {b_{2}} \\ {3} & {6} & {8} & {10} & {b_{3}}\end{array}\right] \rightarrow \cdots \rightarrow\left[\begin{array}{ccccc}{1} & {2} & {2} & {2} & {b_{1}} \\ {0} & {0} & {2} & {4} & {b_{2}-2 b_{1}} \\ {0} & {0} & {0} & {0} & {b_{3}-b_{2}-b_{1}}\end{array}\right]</script><p>If $Ax=b$ has a solution, then $b_3-b_2-b_1=0$. For example, we could choose $b=\begin{bmatrix}1\\5\\6\end{bmatrix}$. From an earlier lecture, we know that Ax = b is solvable exactly when b is in the column space C(A). We have these two conditions on b; in fact they are equivalent. </p><h3 id="Complete-solution"><a href="#Complete-solution" class="headerlink" title="Complete solution"></a>Complete solution</h3><p>In order to find all solutions to Ax = b we first check that the equation is solvable, then find a particular solution. We get the complete solution of the equation by adding the particular solution to all the vectors in the nullspace. </p><p><strong>A particular solution</strong></p><p>One way to find a particular solution to the equation Ax = b is to set all free variables to zero, then solve for the pivot variables.</p><p>For our example matrix $A$, we let $x_2=x_4=0$ to get the system of equations:</p><script type="math/tex; mode=display">\begin{aligned} x_{1}+2 x_{3} &=1 \\ 2 x_{3} &=3 \end{aligned}</script><p>which has the solution $x_3=\frac{3}{2}, x_1=-2$. Our particular solution is:</p><script type="math/tex; mode=display">\mathbf{x}_{p}=\left[\begin{array}{r}{-2} \\ {0} \\ {3 / 2} \\ {0}\end{array}\right]</script><p><strong>Combined with the nullspace</strong></p><p>The general solution to $Ax=b$ is given by $X_{complete}=x_p+x_n$, where $x_n$ is a generic vector in the nullspace. To see this, we add $Ax_p=b$ to $Ax_n=0$ and get $A(x_p+x_n)=b$ for every vector in the nullspace.</p><p>We know that the nullspace of $A$ is the collection of all combinations of the special solutions  $\begin{bmatrix}-2\\1\\0\\0\end{bmatrix}$ and $\begin{bmatrix}2\\0\-2\\1\end{bmatrix}$. So the complete solution to the equation $Ax=\begin{bmatrix}1\\5\\6\end{bmatrix}$ is:</p><script type="math/tex; mode=display">\mathbf{x}_{\text {complete }}=\left[\begin{array}{r}{-2} \\ {0} \\ {3 / 2} \\ {0}\end{array}\right]+c_1\left[\begin{array}{r}{-2} \\ {1} \\ {0} \\ {0}\end{array}\right]+c_{2}\left[\begin{array}{r}{2} \\ {0} \\ {-2} \\ {1}\end{array}\right]</script><p>where $c_1$ and $c_2$ are real numbers.</p><p>The nullspace of $A$ is a two dimensional subspace of $R^4$, and the solutions to the equation $Ax=b$ form a plane parallel to that through $x_p=\left[\begin{array}{r}{-2} \\ {0} \\ {3 / 2} \\ {0}\end{array}\right]$.</p><h2 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h2><p>The rank of a matrix equals the number of pivots of that matrix. If $A$ is an $m$ by $n$ matrix of rank $r$, we know that $r\le m$ and $r\le n$.</p><h3 id="Full-column-rank"><a href="#Full-column-rank" class="headerlink" title="Full column rank"></a>Full column rank</h3><p>If $r=n$, then the nullspace has dimension $n-r=0$ and contains only zero vector. There are no free variables or special solutions.</p><p>Since there are going to have zero rows after row reduction, so the combinations of $b$ should be zero too. In this case, $Ax=b$ has a solution and it is unique. Otherwise, there is no solution. </p><p>The row reduced echelon form of the matrix will look like $R=\begin{bmatrix}I\\0\end{bmatrix}$. </p><h3 id="Full-row-rank"><a href="#Full-row-rank" class="headerlink" title="Full row rank"></a>Full row rank</h3><p>If $r=m$, then the reduced matrix $R=\begin{bmatrix}I&amp;F\end{bmatrix}$ has no rows of zeros and so there are no requirements for the entries of $b$ to satisfy. The equation $Ax=b$ is solvable for every $b$. There are $n-m$ free variables, so there are $n-m$ special solutions to $Ax=0$</p><h3 id="Full-row-and-column-rank"><a href="#Full-row-and-column-rank" class="headerlink" title="Full row and column rank"></a>Full row and column rank</h3><p>If $r=m=n$ is the number of pivots of $A$, then $A$ is an invertible square matrix and $R$ is the identity matrix. The nullspace has dimension zero and $Ax=b$ has a unique solution for every $b$.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2019-12-20 at 11.28.04 AM.png" alt="creen Shot 2019-12-20 at 11.28.04 A"></p><h1 id="Linear-independence"><a href="#Linear-independence" class="headerlink" title="Linear independence"></a>Linear independence</h1><p>Suppose $A$ is $m$ by $n$ with $m &lt; n$. Then there are non-zero solutions to $Ax=0$. This is a equation with more unknowns than equations. So there will be free variables. A combination of the columns is zero, so the columns of this $A$ are dependent.</p><p>We say vectors $x_1,x_2,…x_n$ are linearly indepent if $c_1x_1+c_2x_2+…+c_nx_n=0$ only when $c_1,c_2,…,c_n$ are all 0. When those vectors are the columns of $A$, the only solution to $Ax=0$ is $x=0$. </p><p>Two vectors are independent if they do not lie on the same line. Three vectors are independent if they do npt lie in the same plane. Thinking of $Ax$ as a linear combination of the column vctors of $A$, we see that the column vectors of $A$ are independent exactly when the nullspace of $A$ contains only the zero vector.</p><blockquote><p>Say we have three vectors in the $x-y$ plane. So each vector is a $\begin{bmatrix}a\\b \end{bmatrix}$. But we have three unknowns, meaning there must be free variables according to the first paragraph.</p></blockquote><p>If the columns of $A$ are independent then all columns are pivot columns, the rank of $A$ is $n$, and there are no free variables. If the columns of $A$ are dependent then the rank of $A$ is less than $n$ and there are free variables.</p><h2 id="Spanning-a-space"><a href="#Spanning-a-space" class="headerlink" title="Spanning a space"></a>Spanning a space</h2><p>Vectors $v_1,v_2,…,v_k$ span a space when the space consists of all combinations of those vectors. For example, the column vectors of $A$ span the column space of $A$. If vectors $v_1,v_2,…,v_k$ span a space $S$, then $S$ is the smalles space containing those vectors.</p><h2 id="Basis-and-dimension"><a href="#Basis-and-dimension" class="headerlink" title="Basis and dimension"></a>Basis and dimension</h2><p>A basis for a vector space is a sequence of vectors $v_1,v_2,…,v_d$ with two properties:</p><ul><li>$v_1,v_2,…,v_d$ are independent</li><li>$v_1,v_2,…,v_d$ span the vector space</li></ul><p><strong>Example $R^3$:</strong></p><p>One basis for $R^3$ is $\left\{\left[\begin{array}{l}{1} \\ {0} \\ {0}\end{array}\right],\left[\begin{array}{l}{0} \\ {1} \\ {0}\end{array}\right],\left[\begin{array}{l}{0} \\ {0} \\ {1}\end{array}\right]\right\}$.However, the vectors $\begin{bmatrix}1\\1\\2\end{bmatrix}$, $\begin{bmatrix}2\\2\\5\end{bmatrix}$ and $\begin{bmatrix}3\\3\\8\end{bmatrix}$ do not form a basis for $R^3$ because the third vector is in the space of the first two vectors. In general, for a square matrix, i.e., $n$ vectors in $R^n$, form a basis if they are the column vectors of an invertible matrix.</p><p>The vectors $\begin{bmatrix}1\\1\\2\end{bmatrix}$ and $\begin{bmatrix}2\\2\\5\end{bmatrix}$ span a plane in $R^3$ but they cannot form a basis for $R^3$. However, they form a basis of column space for matrix $\begin{bmatrix}1 ,2,3\\1,2,3\\2,5,8\end{bmatrix}$.</p><p>Given a space, every basis for that space has the same number of vectors; that number is the dimension of the space, So there are exactly $n$ vectors in every basis for $R^n$.</p><h2 id="Bases-of-a-column-space-and-nullspace"><a href="#Bases-of-a-column-space-and-nullspace" class="headerlink" title="Bases of a column space and nullspace"></a>Bases of a column space and nullspace</h2><p>Suppose:</p><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{1} & {2} & {3} & {1} \\ {1} & {1} & {2} & {1} \\ {1} & {2} & {3} & {1}\end{array}\right]</script><p>By definition, the four column vectors of $A$ span the column space of $A$. The third and fourth column vectors are dependent on the first and second, and the first two columns are independent. Therefore, the first two column vectors are the pivot columns. They form a basis for the column space $C(A)$. The matrix has rank 2. </p><script type="math/tex; mode=display">\text{rank}(A)=\text{number of pivot columns of }A=\text{dimension of } C(A)</script><p>The column vectors of this $A$ are not independent, so the nullspace $N(A)$ contains more than just zero vectors. Because the third column is the sum of the first two, we know that the vector $\begin{bmatrix}-1\\ -1\\1\\0\end{bmatrix}$ is in the nullspace. Similarly, $\begin{bmatrix}-1\\0\\0\\ -1\end{bmatrix}$ is also in $N(A)$. These are the two special solutions to $Ax=0$.</p><script type="math/tex; mode=display">\text{dimension of }N(A)=\text{number of free variables}=n-r</script><p>so we know that the dimension of $N(A)$ is 4-2=2. These two special solutions form a basis for the nullspace.</p><h2 id="The-Four-Fundamental-Subspaces"><a href="#The-Four-Fundamental-Subspaces" class="headerlink" title="The Four Fundamental Subspaces"></a>The Four Fundamental Subspaces</h2><p>Any  $m$ by $n$ matrix $A$ determines four subspaces</p><p><strong>column space</strong> $C(A)$: consists of all combinations of the columns of $A$ and is a vector space in $R^m$.</p><p>The $r$ pivot columns form a basis for $C(A)$: $\text{dim} C(A)=r$.</p><p><strong>nullspace</strong> $N(A)$ consists of all solutions $x$ of the equation $Ax=0$ and lies in $R^n$.</p><p>The special solutions to $Ax=0$ correspond to free variables and form a basis for $N(A)$. An $m$ by $n$ matrix has $n-r$ free variables: $\text{dim} N(A)=n-r$. </p><p><strong>row space</strong> $C(A^T)$, the combinations of the row vectors of $A$ form a subspace of $R^n$. We equate this with $C(A^T)$, the column space of the transpose of $A$.</p><p>We could perform row reduction on $A^T$, but insted we make use of $R$, the row reduced echelon form of $A$.</p><script type="math/tex; mode=display">A=\left[\begin{array}{llll}{1} & {2} & {3} & {1} \\ {1} & {1} & {2} & {1} \\ {1} & {2} & {3} & {1}\end{array}\right] \rightarrow \cdots \rightarrow\left[\begin{array}{llll}{1} & {0} & {1} & {1} \\ {0} & {1} & {1} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right]=\left[\begin{array}{ll}{I} & {F} \\ {0} & {0}\end{array}\right]=R</script><p>Although the column spaces of $A$ and $R$ are different, the row space of $R$ is the same as the row space of $A$. The rows of $R$ are combinations of the rows of $A$, and because reduction is reversible the rows of $A$ are combinations of the rows of $R$. The first $r$ rows of $R$ are the echelon basis for the row space of $A$: $\text{dim}C(A^T)=r$. In this exampe, there is a basis consisting of two vectors: $\begin{bmatrix}1\ 0\ 1\ 1\end{bmatrix}$ and $\begin{bmatrix}0\ 1\ 1\ 0\end{bmatrix}$.</p><p><strong>left nullspace</strong>, $N(A^T)$, is a subspace of $R^m$.</p><p>The matrix $A^T$ has $m$ columns. We just saw that $r$ is the rank of $A^T$, so the number of free columns of $A^T$ must be $m-r$: $\text{dim}N(A^T)=m-r$. The left nullspace is the collection of vectors $y$ for which $A^Ty=0$. Equivalently, $y^T A=0$; here $y$ and 0 are row vectors. We say left nullspace because $y^T$ is on the left of $A$ in this equation. To find a basis for the left nullspace we reduce an augmented version of $A$:</p><script type="math/tex; mode=display">\left[\begin{array}{ll}{A_{m \times n}} & {I_{m \times n}}\end{array}\right] \longrightarrow\left[\begin{array}{ll}{R_{m \times n}} & {E_{m \times n}}\end{array}\right]</script><p>From this we get the matrix $E$ for which $EA=R$. (If $A$ is a square, invertible matrix then $E=A^{-1}$). In our example,</p><script type="math/tex; mode=display">E A=\left[\begin{array}{rrr}{-1} & {2} & {0} \\ {1} & {-1} & {0} \\ {-1} & {0} & {1}\end{array}\right]\left[\begin{array}{llll}{1} & {2} & {3} & {1} \\ {1} & {1} & {2} & {1} \\ {1} & {2} & {3} & {1}\end{array}\right]=\left[\begin{array}{llll}{1} & {0} & {1} & {1} \\ {0} & {1} & {1} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right]=R</script><p>the bottom $m-r$ rows of $E$ describe linear dependencies of rows of $A$, because the bottome $m-r$ rows of $R$ are zero. Here $m-r=1$, the bottom $m-r$ rows of $E$ satisfy the equation $y^TA=0$ and form a basis for the left nullspace of $A$.</p><h2 id="Rank-one-matrices"><a href="#Rank-one-matrices" class="headerlink" title="Rank one matrices"></a>Rank one matrices</h2><p>The rank of a matrix is the dimension of its column (or row) space. The matrix </p><script type="math/tex; mode=display">A=\left[\begin{array}{ccc}{1} & {4} & {5} \\ {2} & {8} & {10}\end{array}\right]</script><p>has rank 1 because each of its columns is a multiple of the first column. </p><script type="math/tex; mode=display">A=\left[\begin{array}{l}{1} \\ {2}\end{array}\right]\left[\begin{array}{lll}{1} & {4} & {5}\end{array}\right]</script><p>Every rank 1 matrix $A$ can be written $A=UV^T$, where $U$ and $V$ are column vectors. We’ll use rank 1 matrices as building blocks for more complex matrices.</p><h1 id="Orthogonal-amp-Projection"><a href="#Orthogonal-amp-Projection" class="headerlink" title="Orthogonal&amp;Projection"></a>Orthogonal&amp;Projection</h1><p>If two vectors are orthogonal, they form a right triangle whose hypotenuse is the sum of the vectors. Thus, we can use the Pythagorean theorem to prove that the dot product $x^Ty=y^Tx$ is zero exactly when $x$ and $y$ are orthogonal. The length squared $||x||^2$ equals $x^Tx$. Note that all vectors are orthogonal to the zero vector. </p><p>Subspace S is orthogonal to subspace T means: every vector in S is orthogonal to every vector in T.</p><h2 id="Nullspace-is-perpendicular-to-row-space"><a href="#Nullspace-is-perpendicular-to-row-space" class="headerlink" title="Nullspace is perpendicular to row space"></a>Nullspace is perpendicular to row space</h2><p>The row space of a matrix is orthogonal to the nullspace, because Ax = 0 means the dot product of x with each row of A is 0. Then the product of x with any combination of rows of A must be 0. The column space is orthogonal to the left nullspace of A because the row<br>space of $A^T$ is perpendicular to the nullspace of $A^T$.  </p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-06 at 10.06.53 AM.png" alt="creen Shot 2020-01-06 at 10.06.53 A"></p><p>$A^TA$ is square and symmetric. In fact:</p><script type="math/tex; mode=display">\begin{aligned} N\left(A^{T} A\right) &=N(A) \\ \text { rank of } A^{T} A &=\operatorname{rank} \text { of } A \end{aligned}</script><p>We conclude that $A^TA$ is invertible exactly when A has independent columns. </p><h2 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h2><p>If we have a vector $b$ and a line determined by a vector $a$, how do we find the point on the line that is closest to $b$?</p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-07 at 11.36.52 AM.png" alt="creen Shot 2020-01-07 at 11.36.52 A"></p><p>We can see from Figure 1 that this closest point $p$ is at the intersection formed by a line through $b$ that is orthogonal to $a$. If we think of $p$ as an approximation of $b$, then the length of $e=b-p$ is the error in that approximation. Since $p$ lies on the line through $a$, we know $p=xa$ for some number $x$. We also know that $a$ is perpendicular to $e=b-xa$:</p><script type="math/tex; mode=display">a^T(b-ax)=0\\a^Txa=a^Tb\\x=\frac{a^Tb}{a^Ta}</script><p>Remember that $a^Ta$ is a number. $p=ax=a\frac{a^Tb}{a^Ta}$. Doubling $b$ doubles $p$. Doubling $a$ does not affect $p$.</p><h3 id="Projection-matrix"><a href="#Projection-matrix" class="headerlink" title="Projection matrix"></a>Projection matrix</h3><p>We’d like to write this projection in terms of a projection matrix $P:p=Pb$.</p><script type="math/tex; mode=display">p=ax=a\frac{a^Tb}{a^Ta}</script><p>so the matrix is:</p><script type="math/tex; mode=display">P=\frac{aa^T}{a^Ta}</script><p>Note that $aa^T$ is a matrix, not a number. Since for any $b$, $Pb$ lines on the line $a$, thus, the column space of $P$ is spanned by $a$. The rank of $P$ is 1. $P$ is symmetric. $P^2b=Pb$ because the projection of a vector already on the line through $a$ is just that vector. In general, projection matrices have the properties:</p><script type="math/tex; mode=display">P^T=P \ \text{and} \ P^2=P</script><h3 id="why-projection"><a href="#why-projection" class="headerlink" title="why projection"></a>why projection</h3><p>As we know, the equation $Ax=b$ may have no solution. The vector $Ax$ is always in the column space of $A$, and $b$ is unlikely to be in the column space. So we project $b$ onto a vector $p$ in the column space of $A$ and solve $A \hat x=p$. </p><h3 id="Projection-in-higher-dimensions"><a href="#Projection-in-higher-dimensions" class="headerlink" title="Projection in higher dimensions"></a>Projection in higher dimensions</h3><p> In $R^3$,how do we project a vector $b$ onto the closest point $p$ in a plane? If $a_1$ and $a_2$ form a basis for the plane, then that plane is the column space of the matrix $A=\begin{bmatrix}a_1 \ a_2 \end{bmatrix}$. We know that $p=\hat x_1 a_1+\hat x_2a_2=A \hat x$. We want to find $\hat x$. There are many ways to show that $e=b-p=b-A \hat x$ is orthogonal to the planewe are projecting onto, after which we can use the fact that $e$ is perpendicular to $a_1$ and $a_2$:</p><script type="math/tex; mode=display">\mathbf{a}_{1}^{T}(\mathbf{b}-A \hat{\mathbf{x}})=0 \quad \text { and } \quad \mathbf{a}_{2}^{T}(\mathbf{b}-A \hat{\mathbf{x}})=0</script><p>In matrix form, $A^T(b-A \hat x)=0$. When we were projecting onto a line, $A$ only had one column and so this equation looked like: $a^T (b-xa)=0$. Note that $e=b-A \hat x$ is in the nullspace of $A^T$ and so is in the left nullspace of $A$. We know that everything in the left nullspace of $A$ is perpendicular to the column space of $A$, so this is another confirmation that our calculation are correct.</p><p>We can rewrite the equation $A^T(b-A \hat x)=0$ as:</p><script type="math/tex; mode=display">A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b}</script><p>When projecting onto a line, $A^TA$ was just a number; now it is a square matrix. So insted of dividing by $a^Ta$ we now have to multiply by $(A^TA)^{-1}$, in $n$ dimensions:</p><script type="math/tex; mode=display">\begin{aligned} \hat{\mathbf{x}} &=\left(A^{T} A\right)^{-1} A^{T} \mathbf{b} \\ \mathbf{p}=A \hat{\mathbf{x}} &=A\left(A^{T} A\right)^{-1} A^{T} \mathbf{b} \\ P &=A\left(A^{T} A\right)^{-1} A^{T} \end{aligned}</script><p>It is tempting to try to simplify these expressions, but if $A$ isn’t a square matrix we can’t say that $(A^TA)^{-1}=A^{-1}(A^T)^{-1}$. If $A$ does happen to be a square and invertible matrix, then its column space is the whole space and contains $b$. In this case $P$ is the identity, as we find when we simplify. It is still true that:</p><script type="math/tex; mode=display">P^T=P \ \text{and} \ P^2=P</script><p>If $b$ is perpendicular to the column space, then it’s in the left nullspace $N(A^T)$ of $A$ and $Pb=0$. </p><blockquote><p>According to b perpendicular to the column space of $A$, we have $A^Tb=0$. And we have $Pb$ as follows:</p><script type="math/tex; mode=display">Pb = A(A^TA)^{-1}A^T b= A(A^TA)^{-1}\cdot 0 = 0</script></blockquote><p>If $b$ is in the column space, then $b=Ax$ for some $x$, and $Pb=b$.</p><blockquote><p>Similarly, $Pb = A(A^TA)^{-1}A^T b$. Substitute $b$ with $Ax$, we have $Pb=A(A^TA)^{-1}A^T Ax$. $A^TA$ are cancelled by its inverse matrix $(A^TA)^{-1}$ and results in $I$. So we have $Pb=Ax=b$.</p></blockquote><p>For a typical vector $b$, it will have a component $p$ in the column space and a component $e$ perpendicular to the column space, which menas $e$ is in the left nullspace. The matrix projecting $b$ onto $N(A^T)$ is $I-P$:</p><script type="math/tex; mode=display">\begin{array}{l}{\mathbf{e}=\mathbf{b}-\mathbf{p}} \\ {\mathbf{e}=(I-P) \mathbf{b}}\end{array}</script><p>Naturally, $I-P$ has all the properties of a projection matrix.</p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-08 at 12.57.32 PM.png" alt="creen Shot 2020-01-08 at 12.57.32 P"></p><h3 id="Least-Squares"><a href="#Least-Squares" class="headerlink" title="Least Squares"></a>Least Squares</h3><p>Suppose we’re given a collection of data points $(t,b)$:</p><script type="math/tex; mode=display">\{(1,1),(2,2),(3,2)\}</script><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-07 at 2.34.28 PM.png" alt="creen Shot 2020-01-07 at 2.34.28 P"></p><p>and we want to find the closest line $b=C+Dt$ to that collection. By “closest” line we mean one that minimizes the error represented by the distance from the points to the line. We measure that error by adding up the squares of these distances. In other words, we want to minimize $||Ax-b||^2=||e||^2$. </p><p>If the line went through all three points, we’d have:</p><script type="math/tex; mode=display">C+D=1\\C+2D=2\\C+3D=2</script><p>which is equivalent to:</p><script type="math/tex; mode=display">\left[\begin{array}{ll}{1} & {1} \\ {1} & {2} \\ {1} & {3}\end{array}\right] \quad\left[\begin{array}{l}{C} \\ {D}\end{array}\right]=\left[\begin{array}{l}{1} \\ {2} \\ {2} \\ {b}\end{array}\right]</script><p>There are two ways of viewing this. In the space of the line we’re trying to find, $e_1,e_2$ and $e_3$ are the vertical distances from the data points to the line. The components $p_1, p_2$ and $p_3$ are the values of $C+Dt$ near each data point; $p \approx b$.</p><p>In the other view we have a vector $b$ in $R^3$, its projection $p$ onto the column space of $A$, and its projection $e$ onto $N(A^T)$. We will now find $\hat x=\begin{bmatrix}\hat C \ \hat D \end{bmatrix}$ and $p$. We know:</p><script type="math/tex; mode=display">A^TA \hat x=A^T b\\\left[\begin{array}{cc}{3} & {6} \\ {6} & {14}\end{array}\right]\left[\begin{array}{c}{\hat{\mathrm{C}}} \\ {\hat{D}}\end{array}\right]=\left[\begin{array}{c}{5} \\ {11}\end{array}\right]</script><p>From this equation we get the normal equations:</p><script type="math/tex; mode=display">\begin{aligned} 3 \hat C+6 \hat D &=5 \\ 6 \hat{C}+14 \hat D &=11 \end{aligned}</script><p>We solve these to find $\hat D=\frac{1}{2}$ and $\hat C=\frac{2}{3}$.</p><p>By using these closest line $b=\frac{2}{3}+\frac{1}{2}t$, we can find the $p$ by plugging three points into the line:</p><script type="math/tex; mode=display">p=\begin{bmatrix}\frac{2}{3}+\frac{1}{2}*1 \\ \frac{2}{3}+\frac{1}{2}*2 \\ \frac{2}{3}+\frac{1}{2}*3 \end{bmatrix} =\begin{bmatrix}\frac{7}{6} \\ \frac{5}{3} \\ \frac{13}{6} \end{bmatrix}</script><p>So the error is:</p><script type="math/tex; mode=display">p=\begin{bmatrix}1-\frac{7}{6} \\ 2-\frac{5}{3} \\ 2-\frac{13}{6} \end{bmatrix}=\begin{bmatrix}-\frac{1}{6} \\ \frac{1}{3} \\ -\frac{1}{6} \end{bmatrix}</script><p>Note that $p$ and $e$ are orthogonal, i.e., $p*e=0$ and $p+e=b$.</p><h3 id="The-matrix-A-TA"><a href="#The-matrix-A-TA" class="headerlink" title="The matrix $A^TA$"></a>The matrix $A^TA$</h3><p>In this example, the line does not go through all three points, so this equation is not solvable. Instead we’ll solve:</p><script type="math/tex; mode=display">A^TA \hat x=A^Tb</script><p>But we assumed that the matrix $A^TA$ is invertible. Is this justified?</p><p>If $A$ has independent columns, then $A^TA$ is invertible. To prove this, we assume that $A^TAx=0$, then show that it must be true that $x=0$:</p><script type="math/tex; mode=display">\begin{aligned} A^{T} A \mathbf{x} &=\mathbf{0} \\ \mathbf{x}^{T} A^{T} A \mathbf{x} &=\mathbf{x}^{T} \mathbf{0} \\(A \mathbf{x})^{T}(A \mathbf{x}) &=0 \\ A \mathbf{x} &=\mathbf{0} \end{aligned}</script><p>Since $A$ has independent columns, $Ax=0$ only when $x=0$.</p><p>As long as the columns of A are independent, we can use linear regression to find approximate solutions to unsolvable systems of linear equations. The columns of A are guaranteed to be independent if they are orthonormal, i.e., if they are perpendicular unit vectors like $\begin{bmatrix}1 \\ 0\\ 0\end{bmatrix}, \begin{bmatrix}0 \\ 1\\ 0\end{bmatrix}$ and $\begin{bmatrix}0 \\ 0\\ 1\end{bmatrix}$, or like $\begin{bmatrix} cos\theta \\ sin\theta \end{bmatrix}$ and $\begin{bmatrix} -sin \theta \\ cos \theta \end{bmatrix}$.</p><h2 id="Orthogonal"><a href="#Orthogonal" class="headerlink" title="Orthogonal"></a>Orthogonal</h2><h3 id="Orthonormal-vectors"><a href="#Orthonormal-vectors" class="headerlink" title="Orthonormal vectors"></a>Orthonormal vectors</h3><p>The vectors $q_1,q_2,…,q_n$ are orthonormal if:</p><script type="math/tex; mode=display">\mathbf{q}_{i}^{T} \mathbf{q}_{j}=\left\{\begin{array}{ll}{0} & {\text { if } i \neq j} \\ {1} & {\text { if } i=j}\end{array}\right.</script><p>In other words, they all have (normal) length 1 and are perpendicular (ortho) to each other. Orthonormal vectors are always independent. </p><h3 id="Orthonormal-matrix"><a href="#Orthonormal-matrix" class="headerlink" title="Orthonormal matrix"></a>Orthonormal matrix</h3><p>If the columns of $Q=\begin{bmatrix}q_1 \ … \ q_n \end{bmatrix}$ are orthonormal, then $Q^TQ=I$ is the identity. Matrices with orthonormal columns are a new class of important matrices to add to those on our list: triangular, diagonal, permutation, symmetric, reduced row echelon, and projection matrices. We’ll call them “orthonormal matrices”. </p><p>A square orthonormal matrix $Q$ is called an orthogonal matrix. If $Q$ is square, then $Q^TQ$ tells us that $Q^T=Q^{-1}$. The matrix $\left[\begin{array}{rr}{1} &amp; {1} \\ {1} &amp; {-1}\end{array}\right]$ is not orthogonal, but we can adjust that matrix to get the orthogonal matrix $Q=\frac{1}{\sqrt 2}\left[\begin{array}{rr}{1} &amp; {1} \\ {1} &amp; {-1}\end{array}\right]$. </p><h3 id="Why-orthonormal"><a href="#Why-orthonormal" class="headerlink" title="Why orthonormal"></a>Why orthonormal</h3><p>Suppose Q has orthonormal columns. The matrix that projects onto the column space of Q is: </p><script type="math/tex; mode=display">P=Q(Q^TQ)^{-1}Q^T</script><p>If the columns of $Q$ are orthonormal, then $Q^TQ=I$ and $P=QQ^T$.If $Q$ is square, then $P=I$ because the columns of $Q$ span the entire space. Many equations become trivial when using a matrix with orthonormal columns. If our basis is orthonormal, the projection component $\hat x_i$ is just $q_i^Tb$ because $A^TA \hat x=A^Tb$ becomes $\hat x=Q^T b$. </p><h3 id="Gram-Schmidt"><a href="#Gram-Schmidt" class="headerlink" title="Gram-Schmidt"></a>Gram-Schmidt</h3><p>With elimination, our goal was “make the matrix triangular”. Now our goal is “make the matrix orthonormal”. We start with two independent vectors a and b and want to find orthonormal vectors q1 and q2 that span the same plane. We start by finding orthogonal<br>vectors A and B that span the same space as a and b. Then the unit vectors $q_1 = \frac{A}{||A||}$ and $q_2=\frac{B}{||B||}$ from the desired orthonormal basis.</p><p>Let A = a. We get a vector orthogonal to A in the space spanned by a and b by projecting b onto a and letting B = b − p. (B is what we previously called e.) </p><script type="math/tex; mode=display">\mathbf{B}=\mathbf{b}-\frac{\mathbf{A}^{T} \mathbf{b}}{\mathbf{A}^{T} \mathbf{A}} \mathbf{A}</script><p>If we multiply both sides of this equation by $A^T$, we see that $A^TB=0$.</p><p>What if we had started with three independent vectors, a, b and c? Then we’d find a vector C orthogonal to both A and B by subtracting from c its components in the A and B directions: </p><script type="math/tex; mode=display">\mathbf{C}=\mathbf{c}-\frac{\mathbf{A}^{T} \mathbf{c}}{\mathbf{A}^{T} \mathbf{A}} \mathbf{A}-\frac{\mathbf{B}^{T} \mathbf{c}}{\mathbf{B}^{T} \mathbf{B}} \mathbf{B}</script><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-09 at 11.08.57 AM.png" alt="creen Shot 2020-01-09 at 11.08.57 A"></p><p>When we studied elimination, we wrote the process in terms of matrices and found A = LU. A similar equation A = QR relates our starting matrix A to the result Q of the Gram-Schmidt process. Where L was lower triangular, R is upper triangular. </p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-09 at 11.12.20 AM.png" alt="creen Shot 2020-01-09 at 11.12.20 A"></p><p>If R is upper triangular, then it should be true that $a_1^Tq_2 = 0$. This must be true<br>because we chose $q_1$ to be a unit vector in the direction of $a_1$. All the later $q_i$<br>were chosen to be perpendicular to the earlier ones. Notice that $R = Q^TA$. This makes sense; $Q^TQ = I$.  </p><h1 id="Determinants-and-Eigenvalues"><a href="#Determinants-and-Eigenvalues" class="headerlink" title="Determinants and Eigenvalues"></a>Determinants and Eigenvalues</h1><h2 id="Properties-of-Determinants"><a href="#Properties-of-Determinants" class="headerlink" title="Properties of Determinants"></a>Properties of Determinants</h2><p>The determinant is a number associated with any square matrix; we’ll write it as det A or |A|. The determinant encodes a lot of information about the matrix; the matrix is invertible exactly when the determinant is non-zero. We know that $\left|\begin{array}{ll}{a} &amp; {b} \\ {c} &amp; {d}\end{array}\right|=a d-b c$; but the following properties will give us a formula for the determinant of square matrices of all size.</p><ol><li><p>$\text{det} I=1$</p></li><li><p>If you exchange two rows of a matrix, you reverse the sign of its determinant from positive to negative or from negative to positive. </p></li><li><p><strong>(A)</strong> If we multiply one row of a matrix by $t$, the determinant is multiplied by $t$: $\left|\begin{array}{ll}{ta} &amp; {tb} \\ {c} &amp; {d}\end{array}\right|=t\left|\begin{array}{ll}{a} &amp; {b} \\ {c} &amp; {d}\end{array}\right|$.</p><p><strong>(B)</strong> The determinant behaves like a linear function on the rows of the matrix:</p><script type="math/tex; mode=display">\left|\begin{array}{ll}{a+a'} & {b+b'} \\ {c} & {d}\end{array}\right|=\left|\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right|+\left|\begin{array}{ll}{a'} & {b'} \\ {c} & {d}\end{array}\right|</script></li><li><p>If two rows of a matrix are equal, its determinant is zero. </p><p>This is because of property 2, the exchange rule. On the one hand, exchanging the two identical rows does not change the determinant. On the other hand, exchanging the two rows changes the sign of the determinant. Therefore the determinant must be 0. </p></li><li><p>If $i \ne j$, subtracting $t$ times row $i$ from row $j$ doesn’t change the determinant.</p><script type="math/tex; mode=display">\begin{aligned}\left|\begin{array}{cc}{a} & {b} \\ {c-t a} & {d-t b}\end{array}\right| &=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right|-\left|\begin{array}{cc}{a} & {b} \\ {t a} & {t b}\end{array}\right| & \text { property } 3(b) \\ &=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right|-t\left|\begin{array}{cc}{a} & {b} \\ {a} & {b}\end{array}\right| \quad \text { property } 3(a) \\ &=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right| & \text { property } 4 \end{aligned}</script></li><li><p>If $A$ has a row that is all zeros, then $\text{det} A=0$. We get this from property 3(A) by letting $t=0$.</p></li><li><p>The determinant of a triangular matrix is the product of the diagonal<br>entries (pivots) $d_1,d_2,…,d_n$.</p><p>Property 5 tells us that the determinant of the triangular matrix won’t change if we use elimination to convert it to a diagonal matrix with the entries di on its diagonal. Then property 3 (a) tells us that the determinant of this diagonal matrix is the product $d_1d_2 · · · d_n$ times the determinant of the identity matrix. Property 1 completes the argument. </p><p>Note that we cannot use elimination to get a diagonal matrix if one of the $d_i$ is zero. In that case elimination will give us a row of zeros and property 6 gives us the conclusion we want. </p></li><li><p>$\text{det} A=0$ exactly when $A$ is singular.</p><p>If A is singular, then we can use elimination to get a row of zeros, and property 6 tells us that the determinant is zero. If A is not singular, then elimination produces a full set of pivots $d_1, d_2, …, d_n$ and the determinant is $d_1d_2 · · · d_n \ne 0$ (with minus signs from row exchanges). </p></li><li><p>$\text{det}AB=(\text{det}A)(\text{det}B)$</p><p>This is very useful. Although the determinant of a sum does not equal the sum of the determinants, it is true that the determinant of a product equals the product of the determinants. </p><p>For example:</p><script type="math/tex; mode=display">\text{det}A^{-1}=\frac{1}{\text{det}A}</script><p>because $A^{-1}A=1.$ (Note that if $A$ is singular then $A^{-1}$ does not exist and $\text{det}A^{-1}$ is undefined.) Also,  $\text{det}A^2=(\text{det}A)^2$ and $\text{det}2A=2^n\text{det}A$ (applying property 3 to each row of the matrix). This reminds us of volume – if we double the length, width and height of a three dimensional box, we increase its volume by a multiple of $2^3=8$.</p></li><li><p>$\text{det}A^T=\text{det}A$.</p><script type="math/tex; mode=display">\left|\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right|=\left|\begin{array}{cc}{a} & {c} \\ {b} & {d}\end{array}\right|=a d-b c</script><p>To see why $|A^T|=|A|$, use elimination to write $A=LU$. The statement becomes $|U^TL^T|=|LU|$. Rule 9 then tells us $|U^T||L^T|=|L||U|$. Matrix $L$ is a lower triangular matrix with 1’s on the diagonal, so rule 5 tells us that $|L|=|L^t|=1$. Because $U$ is upper triangular, rule 5 tells us that $|U|=|U^T|$. Therefore $|U^T||L^T|=|L||U|$ and $|A^T|=|A|$.</p></li></ol><h2 id="Determinant-Formulas"><a href="#Determinant-Formulas" class="headerlink" title="Determinant Formulas"></a>Determinant Formulas</h2><p>Firstly, we try to find a formula for the $2 \times 2$ matrix:</p><script type="math/tex; mode=display">\begin{aligned}\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right| &=\left|\begin{array}{cc}{a} & {0} \\ {c} & {d}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {c} & {d}\end{array}\right| \\ &=\left|\begin{array}{cc}{a} & {0} \\ {c} & {0}\end{array}\right|+\left|\begin{array}{cc}{a} & {0} \\ {0} & {0}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {c} & {0}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {c} & {0}\end{array}\right|+\left|\begin{array}{cc}{0} & {b} \\ {0} & {d}\end{array}\right| \\ &=0+a d+(-c b)+0 \\ &=a d-b c \end{aligned}</script><p>By applying property 3 (The determinant is linear in each row separately) to separate the individual entries of each row we could get a formula for any other square matrix. However, for a 3 by 3 matrix we’ll have to add the determinants of twenty seven different matrices! Many of those determinants are zero. The non-zero pieces are: </p><script type="math/tex; mode=display">\left|\begin{array}{ccc}{a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ {a_{31}} & {a_{32}} & {a_{33}}\end{array}\right|=\left|\begin{array}{ccc}{a_{11}} & {0} & {0} \\ {0} & {a_{22}} & {0} \\ {0} & {0} & {a_{33}}\end{array}\right|+\left|\begin{array}{ccc}{a_{11}} & {0} & {0} \\ {0} & {0} & {a_{23}} \\ {0} & {a_{32}} & {0}\end{array}\right|+\left|\begin{array}{ccc}{0} & {a_{12}} & {0} \\ {a_{21}} & {0} & {0} \\ {0} & {0} & {a_{33}}\end{array}\right| \\+\left|\begin{array}{ccc}{0} & {a_{12}} & {0} \\ {0} & {0} & {a_{23}} \\ {a_{31}} & {0} & {0}\end{array}\right|+\left|\begin{array}{ccc}{0} & {0} & {a_{13}} \\ {a_{21}} & {0} & {0} \\ {0} & {a_{32}} & {0}\end{array}\right|+\left|\begin{array}{ccc}{0} & {0} & {a_{13}} \\ {0} & {a_{22}} & {0} \\ {a_{31}} & {0} & {0}\end{array}\right|\\=a_{11}a_{22}a_{33}-a_{11}a_{23}a_{33}-a_{12}a_{21}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}</script><p>Each of the non-zero pieces has one entry from each row in each column, as in a permutation matrix. Since the determinant of a permutation matrix is either 1 or -1, we can again use property 3 to find the determinants of each of these summands and obtain our formula. </p><p>The number of parts with non-zero determinants was 2 in the 2 by 2 case, 6 in the 3 by 3 case, and will be $24=4!$ in the 4 by 4 case. This is because there are n ways to choose an element from the first row, after which there are only n − 1 ways to choose an element from the second row that avoids a zero determinant. Then there are n − 2 choices from the third<br>row, n − 3 from the fourth, and so on. The big formula for computing the determinant of any square matrix is: </p><script type="math/tex; mode=display">\operatorname{det} A=\sum_{n ! \text { terms }} \pm a_{1 \alpha} a_{2 \beta} a_{3 \gamma} \ldots a_{n \omega}</script><p>where $(\alpha, \beta,\gamma, …, \varpi )$ is some permutation of $(1,2,3,..,n)$. </p><h2 id="Cofactor-formula"><a href="#Cofactor-formula" class="headerlink" title="Cofactor formula"></a>Cofactor formula</h2><p>The cofactor formula rewrites the big formula for the determinant of an n by n matrix in terms of the determinants of smaller matrices. In the $3 \times 3$ case, the formula looks like:</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{det} A &=a_{11}\left(a_{22} a_{33}-a_{23} a_{32}\right)+a_{12}\left(-a_{21} a_{33}+a_{23} a_{31}\right)+a_{13}\left(a_{21} a_{32}-a_{22} a_{31}\right) \\ &=\left|\begin{array}{ccc}{a_{11}} & {0} & {0} \\ {0} & {a_{22}} & {a_{23}} \\ {0} & {a_{32}} & {a_{33}}\end{array}\right|+\left|\begin{array}{ccc}{0} & {a_{12}} & {0} \\ {a_{21}} & {0} & {a_{23}} \\ {a_{31}} & {0} & {a_{33}}\end{array}\right|+\left|\begin{array}{ccc}{0} & {0} & {a_{13}} \\ {a_{21}} & {a_{22}} & {0} \\ {a_{31}} & {a_{32}} & {0}\end{array}\right| \end{aligned}</script><p>Each element is multiplied by the cofactors in the parentheses following it. Note that each cofactor is (plus or minus) the determinant of a two by two matrix. Thatdeterminant is made up of products of elements in the rows and columns NOT containing $a_{1j}$. </p><p>In general, the cofactor $C_{ij}$ of $a_{ij}$ can be found by looking at all the terms in the big formula that contain $a_{ij}$. $C_{ij}$ equals $(-1)^{i+j}$ times the determinant of the $n-1$ by $n-1$ square matrix obtained by removing row $i$ and column $j$. ($C_{ij}$ is positive if $i+j$ is even.)</p><p>For $n \times n$ matrices, the cofactor formula is:</p><script type="math/tex; mode=display">\text{det}A=a_{11}C_{11}+a_{12}C_{12}+...+a_{1n}C_{1n}</script><blockquote><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-10 at 4.05.19 PM.png" alt="creen Shot 2020-01-10 at 4.05.19 P"></p></blockquote><h2 id="Inverse-matrix-formula"><a href="#Inverse-matrix-formula" class="headerlink" title="Inverse matrix formula"></a>Inverse matrix formula</h2><p>We know:</p><script type="math/tex; mode=display">\left[\begin{array}{ll}{a} & {b} \\ {c} & {d}\end{array}\right]^{-1}=\frac{1}{a d-b c}\left[\begin{array}{rr}{d} & {-b} \\ {-c} & {a}\end{array}\right]</script><p>In fact: $A^{-1}=\frac{1}{\text{det}A}C^T$, where $C$ is the matrix of cofactors.</p><p>To more formally verify the formula, we’ll check that $AC^T=\text{(detA)}I$.</p><script type="math/tex; mode=display">A C^{T}=\left[\begin{array}{ccc}{a_{11}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {\ddots} & {\vdots} \\ {a_{n 1}} & {\cdots} & {a_{n n}}\end{array}\right]\left[\begin{array}{ccc}{C_{11}} & {\cdots} & {C_{n 1}} \\ {\vdots} & {\ddots} & {\vdots} \\ {C_{1 n}} & {\cdots} & {C_{n n}}\end{array}\right]</script><p>The entry in the first row and first column of the product matrix is: </p><script type="math/tex; mode=display">\sum_{j=1}^na_{1j}C_{j1}=\text{det}A</script><p>(This is just the cofactor formula for the determinant.) This happens for every entry on the diagonal of $AC^T$. To finish proving that $AC^T=(\text{det}A)I$, we just need to check that the offdiagonal entries of $AC^T$ are zero. In the two by two case, multiplying the entries in row 1 of $A$ by the entries in column 2 of $C^T$ gives $a(-b)+b(a)=0$. This is the determinant of $A_{S}=\left[\begin{array}{ll}{a} &amp; {b} \\ {a} &amp; {b}\end{array}\right]$. In higher dimensions, the product of the first row of $A$ and the last column of $C^T$ equals the the determinant of a matrix whose first and last rows are identical. This happens with all the off diagonal matrices, which confirms that $A^{-1}=\frac{1}{\text{det}A}C^T$. </p><h3 id="Cramer’s-rule-for-x-A-1-b"><a href="#Cramer’s-rule-for-x-A-1-b" class="headerlink" title="Cramer’s rule for $x=A^{-1}b$"></a>Cramer’s rule for $x=A^{-1}b$</h3><p>We know that if $Ax=b$ and $A$ is nonsingular, then $x=A^{-1}b$. Applying the formula $A^{-1}=\frac{C^T}{\text{det}A}$ gives us:</p><script type="math/tex; mode=display">x=\frac{C^Tb}{\text{det}A}</script><p>Cramer’s rule gives us another way of looking at this equation. To derive this rule we break x down into its components. Because the i’th component of $C^Tb$ is a sum of cofactors times some number, it is the determinant of some matrix $B_j$.</p><script type="math/tex; mode=display">x=\frac{\text{det}B_J}{\text{det}A}</script><p>where $B_j$ is the matrix created by starting with $A$ and then replacing column $j$ with $b$, so</p><script type="math/tex; mode=display">\begin{aligned} B_{1} &=\left[\begin{array}{c}{\text { last } \mathrm{n}-1} \\ {\mathbf{b}} & {\text { columns }} \\ {\text { of } A}\end{array}\right] \\ B_{n} &=\left[\begin{array}{cc}{\text { first } \mathrm{n}-1} \\ {\text { columns }} & {\mathbf{b}} \\ {\text { of } A} & {}\end{array}\right] \end{aligned}</script><p>Computing inverses using Cramer’s rule is usually less efficient than using elimination. </p><h3 id="text-det-A-volume-of-box"><a href="#text-det-A-volume-of-box" class="headerlink" title="$|\text{det}A|=$ volume of box"></a>$|\text{det}A|=$ volume of box</h3><p>Claim: | det A| is the volume of the box (parallelepiped) whose edges are the column vectors of A. (We could equally well use the row vectors, forming a different box with the same volume.) </p><p>If A = I, then the box is a unit cube and its volume is 1. Because this agrees with our claim, we can conclude that the volume obeys determinant property 1.</p><p>If A = Q is an orthogonal matrix then the box is a unit cube in a different orientation with volume 1 = det Q . (Because Q is an orthogonal matrix, $Q^TQ=I$ and so det Q = ±1.) </p><p>Swapping two columns of A does not change the volume of the box  (remembering that det A = det AT) or the absolute value of the determinant (property 2).  If we show that the volume of the box also obeys property 3 we’ll have proven | det A| equals the volume of the box.</p><p>If we double the length of one column of A, we double the volume of the box formed by its columns. Volume satisfies property 3(a). Property 3(b) says that the determinant is linear in the rows of the matrix: </p><script type="math/tex; mode=display">\left|\begin{array}{cc}{a+a^{\prime}} & {b+b^{\prime}} \\ {c} & {d}\end{array}\right|=\left|\begin{array}{cc}{a} & {b} \\ {c} & {d}\end{array}\right|+\left|\begin{array}{cc}{a^{\prime}} & {b^{\prime}} \\ {c} & {d}\end{array}\right|</script><p>Although it’s not needed for our proof, we can also see that determinants obey property 4. If two edges of a box are equal, the box flattens out and has no volume. </p><p>Important note: If you know the coordinates for the corners of a box, then computing the volume of the box is as easy as calculating a determinant. In particular, the area of a parallelogram with edges $\begin{bmatrix}a\\b\end{bmatrix}$ and $\begin{bmatrix}c\\d\end{bmatrix}$is $ad-bc$. The area of a triangle with edges $\begin{bmatrix}a\\b\end{bmatrix}$ and $\begin{bmatrix}c\\d\end{bmatrix}$is $\frac{1}{2}(ad-bc)$.</p><h1 id="Eigenvalues-and-eigenvectors"><a href="#Eigenvalues-and-eigenvectors" class="headerlink" title="Eigenvalues and eigenvectors"></a>Eigenvalues and eigenvectors</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>A matrix $A$ acts on vectors $x$ like a function does, with input $x$ and output $Ax$. Eigenvectors are vectors for which $Ax$ is parallel to $x$. In other words:</p><script type="math/tex; mode=display">Ax=\lambda x</script><p>In this equation, $x$ is an eigenvector of $A$ and $\lambda $ is an eigenvalue of $A$.</p><p><strong>Eignevalue 0</strong> </p><p>If the eigenvalue $\lambda $ equals 0 then $Ax = 0x = 0$. Vectors with eigenvalue 0 make<br>up the nullspace of $A$; if A is singular, then $\lambda = 0$ is an eigenvalue of A. </p><p><strong>Example</strong></p><p>Suppose P is the matrix of a projection onto a plane. For any x in the plane Px = x, so x is an eigenvector with eigenvalue 1. A vector x perpendicular to the plane has Px = 0, so this is an eigenvector with eigenvalue $\lambda = 0$. The eigenvectors of P span the whole space (but this is not true for every matrix). </p><p>The matrix $B=\begin{bmatrix}0 \ 1 \\ 1 \ 0 \end{bmatrix}$ has an eigenvector $x=\begin{bmatrix} 1 \\ 1  \end{bmatrix}$ with eigenvalue 1 and another eigenvector $x=\begin{bmatrix} 1 \\ -1  \end{bmatrix}$ with eigenvalue -1. These eigenvectors span the space. They are perpendicular because $B=B^T$ (as we will prove).</p><p>$\text{det}(A-\lambda I)=0$</p><p>An n by n matrix will have n eigenvalues, and their sum will be the sum of the diagonal entries of the matrix: $a_{11}+a_{22}+…+a_{nn}$. This sum is the trace of the matrix. For a two by two matrix, if we know one eigenvalue we can use this fact to find the second. </p><p>In order to solve $Ax=\lambda x$ for the eigenvalues and eigenvectors of $A$, we need to be clever to solve this problem:</p><script type="math/tex; mode=display">Ax=\lambda x\\(A-\lambda I)x=0</script><p>In order for $\lambda $ to be an eigenvector, $A-\lambda I$ must be singular. In other words, $\text{det}(A-\lambda I)=0$. Let $A=\begin{bmatrix}3 \ 1 \\ 1 \ 3 \end{bmatrix}$, then:</p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 11.52.58 AM.png" alt="creen Shot 2020-01-12 at 11.52.58 A"></p><p>Note that the coefficient 6 is the trace (sum of diagonal entries) and 8 is the determinant of A. In general, the eigenvalues of a two by two matrix are the solutions to: </p><script type="math/tex; mode=display">\lambda ^2-\text{trace}(A) \cdot \lambda + \text{det} A=0</script><p>Just as the trace is the sum of the eigenvalues of a matrix, the product of the eigenvalues of any matrix equals its determinant. </p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 11.55.36 AM.png" alt="creen Shot 2020-01-12 at 11.55.36 A"><br><strong>A caution</strong></p><p>Similarly, if $Ax=\lambda x$ and $Bx=\alpha x$, $(A+B)x=(\lambda + \alpha)x$. It would be nice if the eigenvalues of a matrix sum were always the sums of the eigenvelues, but this is only true if A and B have the same eigenvectors. The eigenvalues of the product $AB$ are not usually equal to the products $\lambda (A) \lambda (B)$, either.</p><p><strong>Complex eigenvalues</strong></p><p>The matrix $Q=\begin{bmatrix}0 \ -1 \\ 1 \ 0 \end{bmatrix}$ rotates every vector in the plane by $90^o$. It has trace $0=\lambda_1 +\lambda_2$ and determinant $0=\lambda_1 \cdot \lambda_2$. Its only real eigenvector is the zero vector; any other vector’s direction changes when it is multiplied by $Q$. How will this affect our eigenvalue calculation? </p><script type="math/tex; mode=display">\begin{aligned} \operatorname{det}(A-\lambda I) &=\left|\begin{array}{cc}{-\lambda} & {-1} \\ {1} & {-\lambda}\end{array}\right| \\ &=\lambda^{2}+1 \end{aligned}</script><p>$\text{det}(A-\lambda I)=0$ has solutions $\lambda_1=i$ and $\lambda_2=-i$. If a matrix has a complex eigenvalue $a+bi$ then the complex conjugate $a-bi$ is also en eigenvalue of that matrix. </p><p>Symmetric matrices have real eigenvalues. For antisymmetric matrices like $Q$, for which $A^T=-A$, all eigenvalues are imaginary $(\lambda =bi)$.</p><p><strong>Triangular matrices and repeated eigenvalues</strong></p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 12.06.56 PM.png" alt="creen Shot 2020-01-12 at 12.06.56 P"></p><blockquote><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-12 at 12.09.35 PM.png" alt="creen Shot 2020-01-12 at 12.09.35 P"></p></blockquote><h2 id="Diagonalization-and-powers-of-A"><a href="#Diagonalization-and-powers-of-A" class="headerlink" title="Diagonalization and powers of $A$"></a>Diagonalization and powers of $A$</h2><p><strong>Diagonalizing a matrix</strong></p><p>If $A$ has $n$ linearly independent eigenvectors, we can put those vectors in the columns of a matrix $S$. Then,</p><script type="math/tex; mode=display">\begin{aligned} A S &=A\left[\begin{array}{llll}{\mathbf{x}_{1}} & {\mathbf{x}_{2}} & {\cdots} & {\mathbf{x}_{n}}\end{array}\right] \\ &=\left[\begin{array}{llll}{\mathbf{\lambda}_{1} \mathbf{x}_{1}} & {\lambda_{2} \mathbf{x}_{2}} & {\cdots} & {\lambda_{n} \mathbf{x}_{n}}\end{array}\right] \\ &=\left[\begin{array}{cccc}{\lambda_{1} \mathbf{x}_{1}} & {0} & {\cdots} & {0} \\ {0} & {\lambda_{2}} & {} & {0} \\ {\vdots} & {} & {\ddots} & {\vdots} \\ {0} & {\cdots} & {0} & {\lambda_{n}}\end{array}\right]=S \Lambda \end{aligned}</script><p>Note that $\Lambda$ is a diagonal matrix whose non-zero entries are the eigenvalues of $A$. Because the columns of $S$ are indepedent, $S^{-1}$ exists and we can multiply both sides of $AS=S\Lambda$ by $S^{-1}$:</p><script type="math/tex; mode=display">S^{-1}AS=\Lambda \\A=S\Lambda S^{-1}</script><p><strong>Power of</strong> $A$</p><p>If $Ax=\lambda x$, then $A^2 x = A\lambda x = \lambda Ax = \lambda ^2 x$.</p><p>The eigenvalues of $A^2$ are the squares of the eigenvalues of $A$. The eigenvectors of $A^2$ are the same as the eigenvectors of $A$. If we write $A=S\Lambda S^{-1}$ then:</p><script type="math/tex; mode=display">A^2=S\Lambda S^{-1}S\Lambda S^{-1}=S\Lambda ^2 S^{-1}</script><p>Similarly, $A^k=S\Lambda^k S^{-1}$ tells us that raising the eigenvalues of $A$ to the $k$th power gives us the eigenvalues of $A^k$, and that the eigenvectors of $A^k$ are the same as those of $A$.</p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-13 at 10.53.07 AM.png" alt="creen Shot 2020-01-13 at 10.53.07 A"></p><p>A is guaranteed to have n independent eigenvectors (and be diagonalizable) if all its eigenvalues are different. Most matrices do have distinct eigenvalues.</p><p><strong>Repeated eigenvalues</strong></p><p>If A has repeated eigenvalues, it may or may not have n independent eigenvectors. For example, the eigenvalues of the identity matrix are all 1, but that matrix still has n independent eigenvectors. </p><p>If $A$ is the triangular matrix $\begin{bmatrix}2 &amp;1\\ 0&amp;2\end{bmatrix}$, its eigenvalues are 2 and 2. Its eigenvectors are in the nullspace of $A-\lambda I=\begin{bmatrix}0 &amp;1\\ 0&amp;0\end{bmatrix}$ which is spanned by $x=\begin{bmatrix}1\\0 \end{bmatrix}$. This particular A does not have two independent eigenvectors. </p><h2 id="Difference-equations"><a href="#Difference-equations" class="headerlink" title="Difference equations"></a>Difference equations</h2><p>Start with a given vector $u_0$. We can create a sequence of vectors in which each new vector is A times the previous vector: $u_{k+1}=Au_k$. $u_{k+1}=Au_k$ is a first order difference equation, and $u_k=A^ku_0$ is a solution to this system. We can get a more satisfying solution if we write $u_0$ as a combination of eigenvectors of $A$:</p><script type="math/tex; mode=display">u_0=c_1x_1+c_2x_2+\cdot \cdot \cdot +c_nx_n=Sc</script><p>Then:</p><script type="math/tex; mode=display">Au_0=c_1\lambda_1x_1+c_2\lambda_2x_2+\cdot \cdot \cdot +c_n\lambda_nx_n</script><p>and:</p><script type="math/tex; mode=display">u_k=A^ku_0=c_1\lambda_1^kx_1+c_2\lambda_2^kx_2+\cdot \cdot \cdot +c_n\lambda_n^kx_n=\Lambda^kSc</script><h3 id="Fibonacci-sequence"><a href="#Fibonacci-sequence" class="headerlink" title="Fibonacci sequence"></a>Fibonacci sequence</h3><p>The Fibonacci sequence is $0,1,1,2,3,5,8,13,\cdot\cdot \cdot$ In general, $F_{k+2}=F_{k+1}+F_{k}$. If we could understand this in terms of matrices, the eigenvalues of the matrices would tell us how fast the numbers in the sequence are increasing.</p><p>$u_{k+1}=Au_k$ was a first order system. $F_{k+2}=F_{k+1}+F_k$ is a second order scalar equation, but we can convert it to first order linear system by using a clever trick. If $u_k=\begin{bmatrix}F_{k+1} \\ F_k\end{bmatrix}$, then:</p><script type="math/tex; mode=display">F_{k+2}=F_{k+1}+F_k\\F_{k+1}=F_{k+1}</script><p>is equivalent to the first order system $u_{k+1}=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}u_k$. </p><p>Because $A=\begin{bmatrix}1&amp;1\\1&amp;0\end{bmatrix}$ is symmetric, its eigenvalues will be real and its eigenvectors will be orthogonal.Because A is a two by two matrix we know its eigenvalues sum to 1 (the trace)<br>and their product is −1 (the determinant). </p><script type="math/tex; mode=display">\begin{equation}|A-\lambda I|=\left|\begin{array}{cc}{1-\lambda} & {1} \\ {1} & {-\lambda}\end{array}\right|=\lambda^{2}-\lambda-1\end{equation}</script><p>Setting this to zero we find $\lambda = \frac{1\pm \sqrt{1+4}}{2}$; i.e. $\lambda_1 = \frac{1}{2}(1+\sqrt5)\approx 1.618$ and $\lambda_2 = \frac{1}{2}(1-\sqrt5)\approx -0.618$. The growth rate of the $F_k$ is controlled by $\lambda_1$, the only eigenvalue with absolute value greater than 1. This tells us that for large $k$, $F_k \approx c_1 \frac{1+\sqrt{5}}{2}$ for some constant $c_1$. To find the eigenvectors of $A$:</p><script type="math/tex; mode=display">\begin{equation}(A-\lambda I) \mathbf{x}=\left[\begin{array}{cc}{1-\lambda} & {1} \\ {1} & {-\lambda}\end{array}\right] \mathbf{x}\end{equation}</script><p>equals $0$ when $x=\begin{bmatrix}\lambda \\ 1\end{bmatrix}$, so $x_1=\begin{bmatrix}\lambda_1 \\ 1\end{bmatrix}$ and $x_2=\begin{bmatrix}\lambda_2 \\ 1\end{bmatrix}$. </p><p>Finally, $u_0=\begin{bmatrix}F_{1} \\ F_0 \end{bmatrix}=\begin{bmatrix}1 \\ 0\end{bmatrix}=c_1x_1+c_2x_x$ tells us that $c_1=-c_2=\frac{1}{\sqrt{5}}$. Because $\begin{bmatrix}F_{k+1} \\ F_k\end{bmatrix}=u_k=c_1\lambda^k_1x_1+c_2\lambda_2^kx_2$, we get:</p><script type="math/tex; mode=display">\begin{equation}F_{k}=\frac{1}{\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{k}-\frac{1}{\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^{k}\end{equation}</script><p>Using eigenvalues and eigenvectors, we have found a closed form expression for the Fibonacci numbers.    </p><p><strong>Summary</strong>: When a sequence evolves over time according to the rules of a<br>first order system, the eigenvalues of the matrix of that system determine the<br>long term behavior of the series. To get an exact formula for the series we find<br>the eigenvectors of the matrix and then solve for the coefficients $c_1, c_2,…$</p><h3 id="e-At-in-differential-equations"><a href="#e-At-in-differential-equations" class="headerlink" title="$e^{At}$ in differential equations"></a>$e^{At}$ in differential equations</h3><p>The system of equations below describes how the values of variables $u_1$ and $u_2$ affect each other over time:</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned} \frac{d u_{1}}{d t} &=-u_{1}+2 u_{2} \\ \frac{d u_{2}}{d t} &=u_{1}-2 u_{2} \end{aligned}\end{equation}</script><p>Just as we applied linear algebra to solve a difference equation, we can use it to solve this differential equation. For example, the initial condition $u_1=1,u_2=0$ can written $u(0)=\begin{bmatrix}1\\0\end{bmatrix}$.</p><p>By looking at the eigenvalues of the matrix $A=\begin{bmatrix}-1&amp;2\\1&amp;-2\end{bmatrix}$, we can see that $A$ is singular and its trace is $-3$ we know that its eigenvalues are $\lambda_1=0$ and $\lambda_2=-3$. The solution will turn out to include $e^{-3t}$ and $e^{0t}$. As $t$ increases, $e^{-3t}$ vanishes and $e^{0t}=1$ remains constant. Eigenvalues equal to zero have eigenvectors that are steady state solutions.</p><p>$x_1=\begin{bmatrix}2\\1\end{bmatrix}$ is an eigenvector for which $Ax_1=0x_1$. To find an eigenvector corresponding to $\lambda_2=-3$ we solve $(A-\lambda_2I)x_2=0$:</p><script type="math/tex; mode=display">\begin{bmatrix}2&2\\1&1\end{bmatrix}x_2=0 \ \text{so} \ x_2=\begin{bmatrix}1\\-1\end{bmatrix}</script><p>The general solution to this system of differential equations will be:</p><script type="math/tex; mode=display">u(t)=c_1e^{\lambda_1t}x_1+c_2e^{\lambda_2t}x_2</script><p>To find out whether $e^{\lambda_1t}x_1$ really a solution to $\frac{du}{dt}=Au$, plug in $u=e^{\lambda_1t}x_1$:</p><script type="math/tex; mode=display">\frac{du}{dt}=\lambda_1e^{\lambda_1t}x_1</script><p>whcih agrees with:</p><script type="math/tex; mode=display">Au=e^{\lambda_1t}Ax_1=\lambda_1e^{\lambda_1t}x_1</script><p>The two pure terms $e^{\lambda_1t}x_1$ and $e^{\lambda_2t}x_2$ are analogous to the terms ${\lambda_i^k}x_i$ we saw in the solution $c_1\lambda_1^kx_1+c_2\lambda_2^kx_2+\cdot \cdot \cdot +c_n\lambda_n^kx_n$ to the difference equation $u_{k+1}=Au_k$.</p><p>Plugging in the values of the eigenvectors, we get:</p><script type="math/tex; mode=display">\begin{equation}\mathbf{u}(t)=c_{1} e^{\lambda_{1} t} \mathbf{x}_{1}+c_{2} e^{\lambda_{2} t} \mathbf{x}_{2}=c_{1}\left[\begin{array}{c}{2} \\ {1}\end{array}\right]+c_{2} e^{-3 t}\left[\begin{array}{r}{1} \\ {-1}\end{array}\right]\end{equation}</script><p>we know  $u(0)=\begin{bmatrix}1\\0\end{bmatrix}$, so at $t=0$:</p><script type="math/tex; mode=display">\begin{bmatrix}1\\0\end{bmatrix} = c_1\begin{bmatrix}2\\1\end{bmatrix}+c_2\begin{bmatrix}1\\-1\end{bmatrix}</script><p>$c_1=c_2=\frac{1}{3}$ and $u(t)=\frac{1}{3}\begin{bmatrix}2\\1\end{bmatrix}+\frac{1}{3}e^{-3t}\begin{bmatrix}1\-1\end{bmatrix}$ . This tells us that the system starts with $u_1 = 1$ and $u_2 = 0$ but that as t approaches infinity, $u_1$ decays to 2/3 and $u_2$ increases to 1/3. This might describe stuff moving from $u_1$ to $u_2$. The steady state of this system is $u(\infin)=\begin{bmatrix}2/3\\1/3\end{bmatrix}$ </p><h3 id="Stability"><a href="#Stability" class="headerlink" title="Stability"></a>Stability</h3><p>Not all systems have a steady state. The eigenvalues of A will tell us what sort of solutions to expect: </p><ol><li>Stability: $u(t) \to 0$ when $R(\lambda) &lt; 0$</li><li>Steady state: One eigenvalue is 0 and all other eigenvalues have negative real part.</li><li>Blow up: if $R(\lambda )&gt;0$ for any eigenvalue $\lambda$</li></ol><p>If a two by two matrix $A=\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}$ has two eigenvalues with negative real part, its trace $a+d$ is negative. If $A$ has a positive determinant and negative trace then the corresponding solutions must be stable. </p><p><strong>Applying $S$</strong></p><p>The final step of our solution to the system $\frac{du}{dt}=Au$ was to solve:</p><script type="math/tex; mode=display">\begin{equation}\begin{array}{l}{c_{1}\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+c_{2}\left[\begin{array}{r}{1} \\ {-1}\end{array}\right]=\left[\begin{array}{l}{1} \\ {0}\end{array}\right]} \\ {\left[\begin{array}{rr}{2} & {1} \\ {1} & {-1}\end{array}\right]\left[\begin{array}{l}{c_{1}} \\ {c_{2}}\end{array}\right]=\left[\begin{array}{l}{1} \\ {0}\end{array}\right]}\end{array}\end{equation}</script><p>or $Sc=u(0)$, where $S$ is the eigenvector matrix. The components of c determine the contribution from each pure exponential solution, based on the initial conditions of the system. In the equation $\frac{du}{dt}=Au$, the matrix $A$ couples the pure solutions. We set $u=Sv$, where S is the matrix of eigenvectors of A, to get: </p><script type="math/tex; mode=display">S\frac{dv}{dt}=ASv</script><p>or:</p><script type="math/tex; mode=display">\frac{dv}{dt}=S^{-1}ASv=\Lambda v</script><p>This diagonalizes the system: $\frac{dv_i}{dt}=\lambda_i v_i$. The general solution is then:</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned} \mathbf{v}(t) &=e^{\Lambda t} \mathbf{v}(0), \quad \text { and } \\ \mathbf{u}(t) &=S e^{\Lambda t} S^{-1} \mathbf{v}(0)=e^{A t} \mathbf{u}(0) \end{aligned}\end{equation}</script><p><strong>Matrix exponential $e^{At}$</strong></p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-14 at 5.42.50 PM.png" alt="creen Shot 2020-01-14 at 5.42.50 P"></p><p><img src="/2019/12/11/Math-Linear-Algebra/Screen Shot 2020-01-14 at 5.41.25 PM.png" alt="creen Shot 2020-01-14 at 5.41.25 P"></p><h2 id="Markov-matrices"><a href="#Markov-matrices" class="headerlink" title="Markov matrices"></a>Markov matrices</h2><p>The eigenvalues of $A$ and the eigenvalues of $A^T$ are the same:</p><script type="math/tex; mode=display">(A-\lambda I)^T=A^T-\lambda I</script><p>so property 10 of determinates tells us that $\text{det}(A-\lambda I)=\text{det}(A^T-\lambda I)$. If $\lambda $ is an eigenvalue of $A$ then:</p><script type="math/tex; mode=display">\text{det}(A^T-\lambda I)=0</script><p>which means $\lambda $ is an eigenvalue of $A^T$.</p><p>For a matrix like:</p><script type="math/tex; mode=display">A=\begin{bmatrix}0.1&0.01&0.3\\ 0.2&0.99&0.3\\ 0.7&0&0.4\end{bmatrix}</script><p>in which all entries are non-negative and each column adds to 1 is called a Markov matrix. These requirements come from Markov matrices’ use in probability. Squaring or raising a Markov matrix to a power gives us another Markov matrix. </p><p>When dealing with systems of differential equations, eigenvectors with the eigenvalue 0 represented steady states. Here we’re dealing with powers of matrices and get a steady state when $\lambda = 1$ is an eigenvalue. </p><p>The constraint that the columns add to 1 guarantees that 1 is an eigenvalue. All other eigenvalues will be less than 1. Remember that (if $A$ has $n$ independent eigenvectors) the solution to $u_k=A^k u_0$ is $u_k = c_1 \lambda_1^k x_1+c_2 \lambda_2^k x_2+\cdot \cdot \cdot +c_n \lambda_n^k x_n$. If $\lambda_1=1$ and all other eigenvalues are less than one, then the system approaches the steady state $c_1x-1$. This is the $x_1$ component of $u_0$. </p><p>Why does the fact that the columns sum to 1 guarantee that 1 is an eigenvalue? If 1 is an eigenvalue of A, then: </p><script type="math/tex; mode=display">A-1I=\begin{bmatrix}-0.9&0.01&0.3\\ 0.2&-0.01&0.3\\ 0.7&0&-0.6\end{bmatrix}</script><p>should be singular. Since we’ve subtracted 1 from each diagonal entry, the sum of the entries in each column of A − I is zero. But then the sum of the rows of A-I must be the zero row, and so A-I is singular. The eigenvector $x_1$ is in the nullspace of $A-I$ and has eigenvalue 1. It is not hard to find $x_1=\begin{bmatrix}0.6\\ 0.33\\ 0.7\end{bmatrix}$. </p><p>We’re studying the equation $u_{k+1} = Au_k$ where A is a Markov matrix. For example $u_1$ might be the population of (number of people in) Massachusetts and $u_2$ might be the population of California. A might describe what fraction of the population moves from state to state, or the probability of a single personmoving. We can’t have negative numbers of people, so the entries of A will always be positive. We want to account for all the people in our model, so the columns of A add to 1 = 100%. For example:</p><script type="math/tex; mode=display">\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=k+1}=\begin{bmatrix}0.9&0.2  \\ 0.1&0.8 \end{bmatrix}\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=k}</script><p>assumes that there’s a 90% chance that a person in California will stay in California and only a 10% chance that she or he will move, while there’s a 20% percent chance that a Massachusetts resident will move to California. If our initial conditions are $\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=1}=\begin{bmatrix}0 \\ 1000 \end{bmatrix}$, then after one move $u_1=Au_0$ is:</p><script type="math/tex; mode=display">\begin{bmatrix}u_{Cal} \\ u_{Mass} \end{bmatrix}_{t=1}=\begin{bmatrix}0.9&0.2  \\ 0.1&0.8 \end{bmatrix}\begin{bmatrix}0\\1000 \end{bmatrix}=\begin{bmatrix}200\\800 \end{bmatrix}</script><p>For the next few values of $k$,  the Massachusetts population will decrease and the California population will increase while the total population remains constant at 1000. </p><p>To understand the long term behavior of this system we’ll need the eigen­vectors and eigenvalues of $\begin{bmatrix}0.9&amp;0.2  \\ 0.1&amp;0.8 \end{bmatrix}$. We know that one eigenvalue is $\lambda_1=1$. Becasue the trace 0.9+0.8=1.7 is the sum of the eigenvalues, we see that $\lambda_2=0.7$. Next we calculate the eigenvectors: $x_1=\begin{bmatrix}2  \\1 \end{bmatrix}$ . The eigenvalue 1 corresponds to the steady state solution, and $\lambda_2=0.7 &lt;1$, so the system approaches a limit in which $\frac{2}{3}$ of 1000 people live in Californina and $\frac{1}{3}$ of 1000 people are in Massachusetts. This will be the limit frim any starting vector $u_0$.</p><p>To know how the population is distributed after a finite number of steps we look for an eigenvector corresponding to $\lambda_2=0.7$ is $x_2=\begin{bmatrix}1  \-1 \end{bmatrix}$. From what we learned about difference equation we know that:</p><script type="math/tex; mode=display">\begin{equation}\mathbf{u}_{k}=c_{1} 1^{k}\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+c_{2}(.7)^{k}\left[\begin{array}{r}{-1} \\ {1}\end{array}\right]\end{equation}</script><p>when $k=0$ we have:</p><script type="math/tex; mode=display">\begin{equation}\mathbf{u}_{0}=\left[\begin{array}{r}{0} \\ {1000}\end{array}\right]=c_{1}\left[\begin{array}{l}{2} \\ {1}\end{array}\right]+c_{2}\left[\begin{array}{r}{-1} \\ {1}\end{array}\right]\end{equation}</script><p>so $c_1=\frac{1000}{3}$ and $c_2=\frac{2000}{3}$</p><h2 id="Fourier-series"><a href="#Fourier-series" class="headerlink" title="Fourier series"></a>Fourier series</h2><p><strong>Expansion with an orthonormal basis</strong></p><p>If we have an orthonormal basis $q_1, q_2,…,q_n$ then we can write any vector $v$ as $v+x_1q_1+x_2q_x+\cdot \cdot \cdot +x_nq_n$, where:</p><script type="math/tex; mode=display">\begin{equation}\mathbf{q}_{i}^{T} \mathbf{v}=x_{1} \mathbf{q}_{i}^{T} \mathbf{q}_{1}+x_{2} \mathbf{q}_{i}^{T} \mathbf{q}_{2}+\cdots+x_{n} \mathbf{q}_{i}^{T} \mathbf{q}_{n}=x_{i}\end{equation}</script><p>since $q_i^Tq_j=0$ unless $i=j$, this equation gives $x_i=q_i^Tv$.</p><p>In terms of matrices, $\begin{bmatrix}q_1 &amp;\cdot \cdot \cdot &amp; q_n\end{bmatrix}\begin{bmatrix}x_1 \\ \cdot \\ \cdot \\ \cdot \\ q_n\end{bmatrix}=v$, or $Qx=v$. So $x=Q^{-1}v$. Because the $q_i$ form an orthonormal basis, $Q^{-1}=Q^T$ and $x=Q^Tv$. This another way to see that $x_i=q_i^Tv$.</p><p><strong>Fourier series</strong></p><p>The key idea above was that the basis of vectors $q_i$ was orthonormal. Fourier series are built on this idea. We can describe a function $f(x)$ in terms of trigonometric functions: </p><script type="math/tex; mode=display">f(x)=a_0+a_1cosx+b_1sinx+a_2cos2x+b_2sin2x+\cdot \cdot \cdot</script><p>This Fourier series is an infinite sum and the previous example was finite, but the two are related by the fact that the cosines and sines in the Fourier series are orthogonal. We’re now working in an infinite dimensional vector space. The vectors in this space are functions and the (orthogonal) basis vectors are $1, cosx, sinx, cos2x, sin2x, \cdot \cdot \cdot$</p><p>What does “orthogonal” mean in this context? How do we compute a dot product or inner product in this vector space? For vectors in $R^n$ the inner product is $v^Tw=v_1w_1+v_2w_2+ \cdot \cdot \cdot+v_nw_n$. Functions are described by a continuing of values $f(x)$ rather than by a discrete collection of components $v_i$. The best parallel to the vector dot product is:</p><script type="math/tex; mode=display">f^Tg=\int_{0}^{2\pi} f(x)g(x)dx</script><p>We integrate from 0 to 2π because Fourier series are periodic: </p><script type="math/tex; mode=display">f(x)=f(x+2\pi)</script><p>The inner product of two basis vectors is zero, as desired. For example, </p><script type="math/tex; mode=display">\begin{equation}\int_{0}^{2 \pi} \sin x \cos x d x=\left.\frac{1}{2}(\sin x)^{2}\right|_{0} ^{2 \pi}=0\end{equation}</script><p>How do we find $a_0, a_1$, etc. to find the coordinates or Fourier coefficients of a function in this space? The constant term $a_0$ is the average value of the function. Because we’re working with an orthonormal basis, we can use the inner product to find the coefficients ai. </p><script type="math/tex; mode=display">\begin{equation}\begin{aligned} \int_{0}^{2 \pi} f(x) \cos x d x &=\int_{0}^{2 \pi}\left(a_{0}+a_{1} \cos x+b_{1} \sin x+a_{2} \cos 2 x+\cdots\right) \cos x d x \\ &=0+\int_{0}^{2 \pi} a_{1} \cos ^{2} x d x+0+0+\cdots \\ &=a_{1} \pi \end{aligned}\end{equation}</script><p>we conclude that $a_1=\frac{1}{\pi} \int_0^{2\pi}f(x)cosxdx$. We can use the same technique to find any of the values $a_i$.</p>]]></content>
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Math </tag>
            
            <tag> Linear Algebra </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP-TorchText</title>
      <link href="/2019/07/16/NLP-TorchText/"/>
      <url>/2019/07/16/NLP-TorchText/</url>
      <content type="html"><![CDATA[<p>If you’ve ever worked on a project for deep learning for NLP, you’ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to:</p><ol><li>Read the data from disk</li><li>Tokenize the text</li><li>Create a mapping from word to a unique integer</li><li>Convert the text into lists of integers</li><li>Load the data in whatever format your deep learning framework requires</li><li>Pad the text so that all the sequences are the same length, so you can process them in batch</li></ol><p><a href="https://github.com/pytorch/text" target="_blank" rel="noopener">Torchtext</a> is a library that makes all the above processing much easier. </p><a id="more"></a><p>In this post, I’ll demonstrate how torchtext can be used to build and train a text classifier from scratch.To make this tutorial realistic, I’m going to use a small sample of data from <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" target="_blank" rel="noopener">this Kaggle competition</a>. The data and code are available in <a href="https://github.com/keitakurita/practical-torchtext" target="_blank" rel="noopener">GitHub repo</a>, so feel free to clone it and follow along. Or, if you just want to see the minimal working example, feel free to skip the rest of this tutorial and just read the <a href="https://github.com/keitakurita/practical-torchtext/blob/master/Lesson%201:%20intro%20to%20torchtext%20with%20text%20classification.ipynb" target="_blank" rel="noopener">notebook</a>.</p><h1 id="The-Overview"><a href="#The-Overview" class="headerlink" title="The Overview"></a>The Overview</h1><p>Torchtext follows the following basic formula for transforming data into working input for your neural network:</p><p><img src="/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-10.32.59.png" alt="クリーンショット-2018-02-07-10.32.5"></p><p>Torchtext takes in <strong>raw data</strong> in the form of text files, csv/tsv files, json files, and directories (as of now) and <strong>converts them to Datasets</strong>. Datasets are simply preprocessed blocks of data read into memory with various fields.</p><p>Torchtext then passes the <strong>Dataset to an Iterator</strong>. Iterators handle <strong>numericalizing, batching, packaging, and moving the data to the GPU</strong>. Basically, it does all the heavy lifting necessary to pass the data to a neural network.</p><p>In the following sections, we’ll see how each of these processes plays out in an actual working example.</p><h1 id="Declaring-the-Fields"><a href="#Declaring-the-Fields" class="headerlink" title="Declaring the Fields"></a>Declaring the Fields</h1><p>Torchtext takes a declarative approach to loading its data: you tell torchtext how you want the data to look like, and torchtext handles it for you.</p><p>The way you do this is by declaring a Field. The Field specifies how you want a certain (you guessed it) field to be processed. Let’s look at an example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field</div><div class="line">tokenize = <span class="keyword">lambda</span> x: x.split()</div><div class="line">TEXT = Field(sequential=<span class="keyword">True</span>, tokenize=tokenize, lower=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">LABEL = Field(sequential=<span class="keyword">False</span>, use_vocab=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>In the toxic comment classification dataset, there are two kinds of fields: the comment text and the labels (toxic, severe toxic, obscene, threat, insult, and identity hate).</p><p><img src="/2019/07/16/NLP-TorchText/スクリーンショット-2018-02-07-12.19.35.png" alt="クリーンショット-2018-02-07-12.19.3"></p><p>Let’s look at the LABEL field first, since it’s simpler. All fields, by default, expect a sequence of words to come in, and they expect to build a mapping from the words to integers later on (this mapping is called the vocab, and we will see how it is created later). If you are passing a field that is already numericalized by default and is not sequential, you should pass use_vocab=False and sequential=False.</p><p>For the comment text, we pass in the preprocessing we want the field to do as keyword arguments. We give it the tokenizer we want the field to use, tell it to convert the input to lowercase, and also tell it the input is sequential.</p><p>In addition to the keyword arguments mentioned above, the Field class also allows the user to specify special tokens (the <code>unk_token</code> for out-of-vocabulary words, the <code>pad_token</code> for padding, the <code>eos_token</code> for the end of a sentence, and an optional <code>init_token</code> for the start of the sentence), choose whether to make the first dimension the batch or the sequence (<strong>the first dimension is the sequence by default</strong>), and choose whether to allow the sequence lengths to be decided at runtime or decided in advance. Fortunately, <a href="https://github.com/pytorch/text/blob/c839a7934930819be7e240ea972e4d600966afdc/torchtext/data/field.py#L61" target="_blank" rel="noopener">the docstrings</a> for the Field class are relatively well written, so if you need some advanced preprocessing you should refer to them for more information.</p><p>The field class is at the center of torchtext and is what makes preprocessing such an ease. Aside from the standard field class, here’s a list of the fields that are currently available (along with their use cases):</p><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th><th>Use Case</th></tr></thead><tbody><tr><td>Field</td><td>A regular field that defines preprocessing and postprocessing</td><td>Non-text fields and text fields where you don’t need to map integers back to words</td></tr><tr><td>ReversibleField</td><td>An extension of the field that allows reverse mapping of word ids to words</td><td>Text fields if you want to map the integers back to natural language (such as in the case of language modeling)</td></tr><tr><td>NestedField</td><td>A field that takes processes non-tokenized text into a set of smaller fields</td><td>Char-based models</td></tr><tr><td>LabelField (New!)</td><td>A regular field with sequential=False and no <unk> token. Newly added on the master branch of the torchtext github repo, not yet available for release.</unk></td><td>Label fields in text classification.</td></tr></tbody></table></div><h1 id="Constructing-the-Dataset"><a href="#Constructing-the-Dataset" class="headerlink" title="Constructing the Dataset"></a>Constructing the Dataset</h1><p>The fields know what to do when given raw data. Now, we need to tell the fields what data it should work on. This is where we use Datasets.</p><p>There are various built-in Datasets in torchtext that handle common data formats. For csv/tsv files, the TabularDataset class is convenient. Here’s how we would read data from a csv file using the TabularDataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</div><div class="line"></div><div class="line">tv_datafields = [(<span class="string">"id"</span>, <span class="keyword">None</span>), <span class="comment"># we won't be needing the id, so we pass in None as the field</span></div><div class="line">                 (<span class="string">"comment_text"</span>, TEXT), (<span class="string">"toxic"</span>, LABEL),</div><div class="line">                 (<span class="string">"severe_toxic"</span>, LABEL), (<span class="string">"threat"</span>, LABEL),</div><div class="line">                 (<span class="string">"obscene"</span>, LABEL), (<span class="string">"insult"</span>, LABEL),</div><div class="line">                 (<span class="string">"identity_hate"</span>, LABEL)]</div><div class="line">trn, vld = TabularDataset.splits(</div><div class="line">               path=<span class="string">"data"</span>, <span class="comment"># the root directory where the data lies</span></div><div class="line">               train=<span class="string">'train.csv'</span>, validation=<span class="string">"valid.csv"</span>,</div><div class="line">               format=<span class="string">'csv'</span>,</div><div class="line">               skip_header=<span class="keyword">True</span>, <span class="comment"># if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!</span></div><div class="line">               fields=tv_datafields)</div><div class="line"></div><div class="line">tst_datafields = [(<span class="string">"id"</span>, <span class="keyword">None</span>), <span class="comment"># we won't be needing the id, so we pass in None as the field</span></div><div class="line">                  (<span class="string">"comment_text"</span>, TEXT)]</div><div class="line">tst = TabularDataset(</div><div class="line">           path=<span class="string">"data/test.csv"</span>, <span class="comment"># the file path</span></div><div class="line">           format=<span class="string">'csv'</span>,</div><div class="line">           skip_header=<span class="keyword">True</span>, <span class="comment"># if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!</span></div><div class="line">           fields=tst_datafields)</div></pre></td></tr></table></figure><p>For the TabularDataset, we pass in a list of (name, field) pairs as the fields argument. The fields we pass in must be in the same order as the columns. For the columns we don’t use, we pass in a tuple where the field element is None</p><p>The splits method creates a dataset for the train and validation data by applying the same processing. It can also handle the test data, but since out test data has a different format from the train and validation data, we create a different dataset.</p><p>Datasets can mostly be treated in the same way as lists. To understand this, it’s instructive to take a look inside our Dataset. Datasets can be indexed and iterated over like normal lists, so let’s see what the first element looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>trn[<span class="number">0</span>]</div><div class="line">torchtext.data.example.Example at <span class="number">0x10d3ed3c8</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>trn[<span class="number">0</span>].__dict__.keys()</div><div class="line">dict_keys([<span class="string">'comment_text'</span>, <span class="string">'toxic'</span>, <span class="string">'severe_toxic'</span>, <span class="string">'threat'</span>, <span class="string">'obscene'</span>, <span class="string">'insult'</span>, <span class="string">'identity_hate'</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>trn[<span class="number">0</span>].comment_text[:<span class="number">3</span>]</div><div class="line">[<span class="string">'explanation'</span>, <span class="string">'why'</span>, <span class="string">'the'</span>]</div></pre></td></tr></table></figure><p>we get an Example object. The Example object bundles the attributes of a single data point together. We also see that the text has already been tokenized for us, but has not yet been converted to integers. This makes sense since we have not yet constructed the mapping from words to ids. Constructing this mapping is our next step.</p><p>Torchtext handles mapping words to integers, but it has to be told the full range of words it should handle. In our case, we probably want to build the vocabulary on the training set only, so we run the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">TEXT.build_vocab(trn)</div></pre></td></tr></table></figure><p>This makes torchtext go through all the elements in the training set, check the contents corresponding to the <code>TEXT</code> field, and register the words in its vocabulary. Torchtext has its own class called Vocab for handling the vocabulary. The Vocab class holds a mapping from word to id in its <code>stoi</code>attribute and a reverse mapping in its <code>itos</code> attribute. In addition to this, it can automatically build an embedding matrix for you using various pretrained embeddings like word2vec. The Vocab class can also take options like <code>max_size</code> and <code>min_freq</code> that dictate how many words are in the vocabulary or how many times a word has to appear to be registered in the vocabulary. Words that are not included in the vocabulary will be converted into <unk>, a token standing for “unknown”.</unk></p><p>Here is a list of the currently available set of datasets and the format of data they take in:</p><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th><th>Use Case</th></tr></thead><tbody><tr><td>TabularDataset</td><td>Takes paths to csv/tsv files and json files or Python dictionaries as inputs.</td><td>Any problem that involves a label (or labels) for each piece of text</td></tr><tr><td>LanguageModelingDataset</td><td>Takes the path to a text file as input.</td><td>Language modeling</td></tr><tr><td>TranslationDataset</td><td>Takes a path and extensions to a file for each language. e.g. If the files are English: “hoge.en”, French: “hoge.fr”, path=”hoge”, exts=(“en”,”fr”)</td><td>Translation</td></tr><tr><td>SequenceTaggingDataset</td><td>Takes a path to a file with the input sequence and output sequence separated by tabs. <a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/#easy-footnote-bottom-2-310" target="_blank" rel="noopener">2</a></td><td>Sequence taggingNow that we have our data formatted and read into memory, we turn to the next step: creating an Iterator to pass the data to our model.</td></tr></tbody></table></div><p>Now that we have our data formatted and read into memory, we turn to the next step: creating an Iterator to pass the data to our model.</p><h1 id="Constructing-the-Iterator"><a href="#Constructing-the-Iterator" class="headerlink" title="Constructing the Iterator"></a>Constructing the Iterator</h1><p>In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.</p><p>Below is code for how you would initialize the Iterators for the train, validation, and test data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator, BucketIterator</div><div class="line"></div><div class="line">train_iter, val_iter = BucketIterator.splits(</div><div class="line"> (trn, vld), <span class="comment"># we pass in the datasets we want the iterator to draw data from</span></div><div class="line"> batch_sizes=(<span class="number">64</span>, <span class="number">64</span>),</div><div class="line"> device=<span class="number">-1</span>, <span class="comment"># if you want to use the GPU, specify the GPU number here</span></div><div class="line"> sort_key=<span class="keyword">lambda</span> x: len(x.comment_text), <span class="comment"># the BucketIterator needs to be told what function it should use to group the data.</span></div><div class="line"> sort_within_batch=<span class="keyword">False</span>,</div><div class="line"> repeat=<span class="keyword">False</span> <span class="comment"># we pass repeat=False because we want to wrap this Iterator layer.</span></div><div class="line">)</div><div class="line">test_iter = Iterator(tst, batch_size=<span class="number">64</span>, device=<span class="number">-1</span>, sort=<span class="keyword">False</span>, sort_within_batch=<span class="keyword">False</span>, repeat=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>Update: The <code>sort_within_batch</code> argument, when set to True, sorts the data within each minibatch in decreasing order according to the <code>sort_key</code>. This is necessary when you want to use <code>pack_padded_sequence</code> with the padded sequence data and convert the padded sequence tensor to a <code>PackedSequence</code>object.</p><p>The BucketIterator is one of the most powerful features of torchtext. It automatically shuffles and buckets the input sequences into sequences of similar length.</p><p>The reason this is powerful is that – as I mentioned earlier – we need to pad the input sequences to be of the same length to enable batch processing. For instance, the sequences</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[ [<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>],</div><div class="line">  [<span class="number">4</span>, <span class="number">1</span>],</div><div class="line">  [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>] ]</div></pre></td></tr></table></figure><p>would need to be padded to become</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[ [<span class="number">3</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">0</span>],</div><div class="line">  [<span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</div><div class="line">  [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>] ]</div></pre></td></tr></table></figure><p>As you can see, the amount of padding necessary is determined by the longest sequence in the batch. Therefore, padding is most efficient when the sequences are of similar lengths. The BucketIterator does all this behind the scenes. As a word of caution, you need to tell the BucketIterator what attribute you want to bucket the data on. In our case, we want to bucket based on the lengths of the comment_text field, so we pass that in as a keyword argument. See the code above for details on the other arguments.</p><p>For the test data, we don’t want to shuffle the data since we’ll be outputting the predictions at the end of training. This is why we use a standard iterator.</p><p>Here’s a list of the Iterators that torchtext currently implements:</p><div class="table-container"><table><thead><tr><th>Name</th><th>Description</th><th>Use Case</th></tr></thead><tbody><tr><td>Iterator</td><td>Iterates over the data in the order of the dataset.</td><td>Test data, or any other data where the order is important.</td></tr><tr><td>BucketIterator</td><td>Buckets sequences of similar lengths together.</td><td>Text classification, sequence tagging, etc. (use cases where the input is of variable length)</td></tr><tr><td>BPTTIterator</td><td>An iterator built especially for language modeling that also generates the input sequence delayed by one timestep. It also varies the BPTT (backpropagation through time) length. This iterator <a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">deserves its own post</a>, so I’ll omit the details here.</td><td>Language modeling</td></tr></tbody></table></div><h1 id="Wrapping-the-Iterator"><a href="#Wrapping-the-Iterator" class="headerlink" title="Wrapping the Iterator"></a>Wrapping the Iterator</h1><p>Currently, the iterator returns a custom datatype called torchtext.data.Batch. The Batch class has a similar API to the Example type, with a batch of data from each field as attributes. Unfortunately, this custom datatype makes code reuse difficult (since each time the column names change, we need to modify the code), and makes torchtext hard to use with other libraries for some use cases (like torchsample and fastai).</p><p>Concretely, we’ll convert the batch to a tuple in the form (x, y) where x is the independent variable (the input to the model) and y is the dependent variable (the supervision data). Here’s the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchWrapper</span>:</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dl, x_var, y_vars)</span>:</span></div><div class="line">            self.dl, self.x_var, self.y_vars = dl, x_var, y_vars <span class="comment"># we pass in the list of attributes for x </span></div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></div><div class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> self.dl:</div><div class="line">                  x = getattr(batch, self.x_var) <span class="comment"># we assume only one input in this wrapper</span></div><div class="line"></div><div class="line">                  <span class="keyword">if</span> self.y_vars <span class="keyword">is</span> <span class="keyword">None</span>: <span class="comment"># we will concatenate y into a single tensor</span></div><div class="line">                        y = torch.cat([getattr(batch, feat).unsqueeze(<span class="number">1</span>) <span class="keyword">for</span> feat <span class="keyword">in</span> self.y_vars], dim=<span class="number">1</span>).float()</div><div class="line">                  <span class="keyword">else</span>:</div><div class="line">                        y = torch.zeros((<span class="number">1</span>))</div><div class="line"></div><div class="line">                  <span class="keyword">yield</span> (x, y)</div><div class="line"></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></div><div class="line">            <span class="keyword">return</span> len(self.dl)</div><div class="line"></div><div class="line">train_dl = BatchWrapper(train_iter, <span class="string">"comment_text"</span>, [<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>])</div><div class="line">valid_dl = BatchWrapper(val_iter, <span class="string">"comment_text"</span>, [<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>])</div><div class="line">test_dl = BatchWrapper(test_iter, <span class="string">"comment_text"</span>, <span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>All we’re doing here is converting the batch object to a tuple of inputs and outputs.</p><h1 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h1><p>We’ll use a simple LSTM to demonstrate how to train the text classifier on the data we’ve built:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLSTMBaseline</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dim, emb_dim=<span class="number">300</span>, num_linear=<span class="number">1</span>)</span>:</span></div><div class="line">        super().__init__() <span class="comment"># don't forget to call this!</span></div><div class="line">        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)</div><div class="line">        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=<span class="number">1</span>)</div><div class="line">        self.linear_layers = []</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_linear - <span class="number">1</span>):</div><div class="line">            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))</div><div class="line">            self.linear_layers = nn.ModuleList(self.linear_layers)</div><div class="line">        self.predictor = nn.Linear(hidden_dim, <span class="number">6</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, seq)</span>:</span></div><div class="line">        hdn, _ = self.encoder(self.embedding(seq))</div><div class="line">        feature = hdn[<span class="number">-1</span>, :, :]</div><div class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linear_layers:</div><div class="line">          feature = layer(feature)</div><div class="line">          preds = self.predictor(feature)</div><div class="line">        <span class="keyword">return</span> preds</div><div class="line"></div><div class="line">em_sz = <span class="number">100</span></div><div class="line">nh = <span class="number">500</span></div><div class="line">nl = <span class="number">3</span></div><div class="line">model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)</div></pre></td></tr></table></figure><p>Now, we’ll write the training loop. Thanks to all our preprocessing, this is very simple. We can iterate using our wrapped Iterator, and the data will automatically be passed to us after being moved to the GPU and numericalized appropriately.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tqdm</div><div class="line"></div><div class="line">opt = optim.Adam(model.parameters(), lr=<span class="number">1e-2</span>)</div><div class="line">loss_func = nn.BCEWithLogitsLoss()</div><div class="line"></div><div class="line">epochs = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epochs + <span class="number">1</span>):</div><div class="line">    running_loss = <span class="number">0.0</span></div><div class="line">    running_corrects = <span class="number">0</span></div><div class="line">    model.train() <span class="comment"># turn on training mode</span></div><div class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(train_dl): <span class="comment"># thanks to our wrapper, we can intuitively iterate over our data!</span></div><div class="line">        opt.zero_grad()</div><div class="line"></div><div class="line">        preds = model(x)</div><div class="line">        loss = loss_func(y, preds)</div><div class="line">        loss.backward()</div><div class="line">        opt.step()</div><div class="line"></div><div class="line">        running_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</div><div class="line"></div><div class="line">    epoch_loss = running_loss / len(trn)</div><div class="line"></div><div class="line">    <span class="comment"># calculate the validation loss for this epoch</span></div><div class="line">    val_loss = <span class="number">0.0</span></div><div class="line">    model.eval() <span class="comment"># turn on evaluation mode</span></div><div class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> valid_dl:</div><div class="line">        preds = model(x)</div><div class="line">        loss = loss_func(y, preds)</div><div class="line">        val_loss += loss.data[<span class="number">0</span>] * x.size(<span class="number">0</span>)</div><div class="line"></div><div class="line">    val_loss /= len(vld)</div><div class="line">    print(<span class="string">'Epoch: &#123;&#125;, Training Loss: &#123;:.4f&#125;, Validation Loss: &#123;:.4f&#125;'</span>.format(epoch, epoch_loss, val_loss))</div></pre></td></tr></table></figure><p>There’s not much to explain here: this is just a standard training loop. Now, let’s generate our predictions</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">test_preds = []</div><div class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> tqdm.tqdm(test_dl):</div><div class="line">    preds = model(x)</div><div class="line">    preds = preds.data.numpy()</div><div class="line">    <span class="comment"># the actual outputs of the model are logits, so we need to pass these values to the sigmoid function</span></div><div class="line">    preds = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-preds))</div><div class="line">    test_preds.append(preds)</div><div class="line">    test_preds = np.hstack(test_preds)</div></pre></td></tr></table></figure><p>Finally, we can write our predictions to a csv file.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">df = pd.read_csv(<span class="string">"data/test.csv"</span>)</div><div class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate([<span class="string">"toxic"</span>, <span class="string">"severe_toxic"</span>, <span class="string">"obscene"</span>, <span class="string">"threat"</span>, <span class="string">"insult"</span>, <span class="string">"identity_hate"</span>]):</div><div class="line">    df[col] = test_preds[:, i]</div><div class="line"></div><div class="line">df.drop(<span class="string">"comment_text"</span>, axis=<span class="number">1</span>).to_csv(<span class="string">"submission.csv"</span>, index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>And we’re done! We can submit this file to Kaggle, try refining our model, changing the tokenizer, or whatever we feel like, and it will only take a few changes in the code above.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/" target="_blank" rel="noopener">A Comprehensive Introduction to Torchtext (Practical Torchtext part 1)</a> </p><p><a href="http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/" target="_blank" rel="noopener">Language modeling tutorial in torchtext (Practical Torchtext part 2)</a> </p><p><a href="http://anie.me/On-Torchtext/" target="_blank" rel="noopener">A Tutorial on Torchtext</a> </p>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Clasical CNNs</title>
      <link href="/2019/07/07/DP-Clasical-CNNs/"/>
      <url>/2019/07/07/DP-Clasical-CNNs/</url>
      <content type="html"><![CDATA[<p>In this post, we’ll go into summarizing a lot of the new and important developments in the field of computer vision and convolutional neural networks. </p><p><a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html" target="_blank" rel="noopener">The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)</a> </p><a id="more"></a><h1 id="Alex"><a href="#Alex" class="headerlink" title="Alex"></a>Alex</h1><p>AlexNet is introduced in the seminal paper <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a>, which is a deep convolutional neural network and kicked ass on the ImageNet LSVRC-2010 contest. AlexNet does a pretty good job classifying the 1.2 million high-resolution images into 1000 different classes. Actually, top-1 and top-5 error rate of 37.5% and 17.5% are achieved on the test data, which is way better than the previous state-of-the-art. In this article, I will introduce what AlexNet is, namely the network architecture, how they deal with overfitting, and evaluation results.</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2019/07/07/DP-Clasical-CNNs/AlexNet-1.png" alt="lexNet-"></p><p>The <a href="https://www.learnopencv.com/understanding-alexnet/" target="_blank" rel="noopener">picture</a> above shows the architecture of AlexNet.We can eyeball the graph and find that AlexNet contains eight layers  —  five convolutional and three fully-connected layers. <a href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" target="_blank" rel="noopener">ref</a> </p><p><strong>Input</strong>: $227 \times 227 \times 3$ images</p><p><strong>1st Convolutional Layers</strong>: 96 kernels with kernel size $11 \times 11$, strides=4, pad=0, activation=”relu”</p><p>$55 \times 55 \times 96$ feature maps</p><blockquote><p> total number of parameters = $(11<em>11</em>3)*96=35K$. </p></blockquote><p>Then $3 \times 3$ <strong>Max Pooling</strong> (strides=2) </p><blockquote><p>bO PARAMETERS.</p></blockquote><p>$27 \times 27 \times 96$ feature maps</p><blockquote><ol><li>Max Pooling layers are usually used to downsample the width and height of the tensors, keeping the depth same. </li><li>ReLU is chosen because of its non-saturating-nonlinearity peoperty, which is $f(x)=max(x,0)$, resulting in the benefits that deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.</li><li>Compared with the previous state-of-the-art, the overlapping pooling instead of nonoverlapping pooling is utilized, which helps improve the classification accuray.</li></ol></blockquote><p><strong>2nd Convolutional Layers</strong>: 256 kernels with kernel size $5 \times 5$, strides=1, pad=2, activation=”relu”</p><blockquote><p>pad=2 are used to keep output maps having smae size with input maps.</p></blockquote><p>$27 \times 27 \times 256$ feature maps</p><p>Then $3 \times 3$ <strong>Max Pooling</strong> (strides=2)</p><p>$13 \times 13 \times 256$ feature maps</p><p><strong>3rd Convolutional Layers</strong>: 384 kernels with kernel size $3 \times 3$, strides=1, pad=, activation=”relu”1</p><p>$13 \times 13 \times 384$ feature maps</p><p><strong>4th Convolutional Layers</strong>: 384 kernels with kernel size $3 \times 3$, strides=1, pad=1, activation=”relu”</p><p>$13 \times 13 \times 384$ feature maps</p><p><strong>5th Convolutional Layers</strong>: 256 kernels with kernel size $3 \times 3$, strides=1, pad=1, activation=”relu”</p><p>$13 \times 13 \times 384$ feature maps</p><p>Then $3 \times 3$ <strong>Max Pooling</strong> (strides=2)</p><p>$6 \times 6 \times 256$ feature maps</p><p><strong>6th Fully-Connected Layers</strong>: 4096 neurons, activation=’tanh’</p><p><strong>7th Fully-Connected Layers</strong>: 4096 neurons, activation=’tanh’</p><p><strong>8th Fully-Connected Layers</strong>: 1000 neurons (since there 1000 classes), activation=’softmax’</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Alright, let’s write a program that learns how to recognize images using cifar10 dataset. We’ll do this with a short keras program. </p><p>Let me explain the dataset cifar10 firstly before going to the architecture implementation. Instead of all images, we just use two categories, i.e. cat and dog. <a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">Dog vs Cat</a> consists of 12500 training cat images and 12500 training dog images. We are going to use 1000 cat images and 1000 dog images as training dataset while 400 cat images and 400 dog images as testing datasets. Here’s the code we use to organize the training and test datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> cv2</div><div class="line"></div><div class="line">inpath = <span class="string">"/Users/daniel/Documents/all/train/"</span></div><div class="line"></div><div class="line">outcatTra = <span class="string">"/Users/daniel/Documents/all/Training/cats/"</span></div><div class="line"></div><div class="line">outdogTra = <span class="string">"/Users/daniel/Documents/all/Training/dogs/"</span></div><div class="line"></div><div class="line">outcatTest = <span class="string">"/Users/daniel/Documents/all/Testing/cats/"</span></div><div class="line"></div><div class="line">outdogTest = <span class="string">"/Users/daniel/Documents/all/Testing/dogs/"</span></div><div class="line"></div><div class="line">os.makedirs(outcatTra,exist_ok=<span class="keyword">True</span>)</div><div class="line">os.makedirs(outdogTra,exist_ok=<span class="keyword">True</span>)</div><div class="line">os.makedirs(outcatTest,exist_ok=<span class="keyword">True</span>)</div><div class="line">os.makedirs(outdogTest,exist_ok=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    cat = cv2.imread(inpath+<span class="string">"cat.%d.jpg"</span>%i)</div><div class="line">    dog = cv2.imread(inpath+<span class="string">"dog.%d.jpg"</span>%i)</div><div class="line"></div><div class="line">    cv2.imwrite(outcatTra+<span class="string">"cat.%d.jpg"</span>%i,cat)</div><div class="line">    cv2.imwrite(outdogTra+<span class="string">"dog.%d.jpg"</span>%i,dog)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">400</span>):</div><div class="line">    j = <span class="number">4000</span>+i</div><div class="line">    cat = cv2.imread(inpath + <span class="string">"cat.%d.jpg"</span> % j)</div><div class="line">    dog = cv2.imread(inpath + <span class="string">"dog.%d.jpg"</span> % j)</div><div class="line"></div><div class="line">    cv2.imwrite(outcatTest + <span class="string">"cat.%d.jpg"</span> % i, cat)</div><div class="line">    cv2.imwrite(outdogTest + <span class="string">"dog.%d.jpg"</span> % i, dog)</div></pre></td></tr></table></figure><p>After running the codes, we will have the dataset organization like this:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">Data/</div><div class="line">    Training/</div><div class="line">            cats/</div><div class="line">                cat.0.jpg</div><div class="line">                cat.1.jpg</div><div class="line">                .</div><div class="line">                .</div><div class="line">                .</div><div class="line">                cat.999.jpg</div><div class="line">            dogs/</div><div class="line">                dog.0.jpg</div><div class="line">                dog.1.jpg</div><div class="line">                .</div><div class="line">                .</div><div class="line">                .</div><div class="line">                dog.999.jpg</div><div class="line">    Testing/</div><div class="line">           cats/</div><div class="line">               cat.0.jpg</div><div class="line">               cat.1.jpg</div><div class="line">               .</div><div class="line">               .</div><div class="line">               .</div><div class="line">               cat.399.jpg</div><div class="line">           dogs/</div><div class="line">               dog.0.jpg</div><div class="line">               dog.1.jpg</div><div class="line">               .</div><div class="line">               .</div><div class="line">               .</div><div class="line">               dog.399.jpg</div></pre></td></tr></table></figure><p>Then, I am going to introduce <code>ImageDataGenerator</code>, which is employed to argument dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</div><div class="line"></div><div class="line"><span class="comment"># https://blog.csdn.net/jacke121/article/details/79245732</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,img_res=<span class="params">(<span class="number">227</span>, <span class="number">227</span>)</span>)</span>:</span></div><div class="line">        self.img_res = img_res</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_batch</span><span class="params">(self,batch_size)</span>:</span></div><div class="line">        train_datagen = ImageDataGenerator(</div><div class="line">            shear_range= <span class="number">0.2</span>,</div><div class="line">            zoom_range= <span class="number">0.2</span>,</div><div class="line">            horizontal_flip=<span class="keyword">True</span></div><div class="line">        )</div><div class="line">        test_datagen = ImageDataGenerator()</div><div class="line">        train_generator = train_datagen.flow_from_directory(</div><div class="line">            <span class="string">'./dataset_path/'</span>,</div><div class="line">            batch_size=batch_size,</div><div class="line">            shuffle=<span class="keyword">True</span>,</div><div class="line">            target_size=self.img_res,</div><div class="line">            class_mode=<span class="string">'categorical'</span></div><div class="line">        )</div><div class="line">        validation_generator = test_datagen.flow_from_directory(</div><div class="line">            <span class="string">'./dataset_path/'</span>,</div><div class="line">            batch_size=batch_size,</div><div class="line">            target_size=self.img_res,</div><div class="line">            shuffle=<span class="keyword">True</span>,</div><div class="line">            class_mode=<span class="string">'categorical'</span></div><div class="line">        )</div><div class="line"></div><div class="line">        <span class="keyword">return</span> train_generator,validation_generator</div></pre></td></tr></table></figure><p><code>img_size</code> is used to resize all the input image so that they have the same width and height. Then we create an <code>ImageDataGenerator</code> instance with specified parameters, where:</p><ul><li>shear_range is used to sheer images and its value is shear angle in counter-clockwise direction in degrees. </li><li>zoom_range is is used for image zoom and a value greater than 1 corresponds to image amplification otherwise image reduction.</li><li>horizontal_flip is used to flip inputs horizontally.</li></ul><p>Then we create a data generator by using the instance above. Note that we should explicitly point out the dataset directory, which contain one subdirectory per class. Here we generate <code>train_generator</code> and <code>validation_generator</code>. </p><p>Let me explain the core features of the AlexNet code, before giving a full listing. The centerpiece is a <code>AlexNet</code> class, which we use to represent a <code>AlexNet</code>. Here is the code we use to initialize a <code>AlexNet</code> object.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div><div class="line"><span class="keyword">from</span> data_load <span class="keyword">import</span> DataLoader</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.height = <span class="number">227</span></div><div class="line">        self.width = <span class="number">227</span></div><div class="line">        self.channels = <span class="number">3</span></div><div class="line">        self.img_shape = (self.height,self.width,self.channels)</div><div class="line">        self.nb_classes = <span class="number">2</span></div><div class="line">        self.mean_flag = <span class="keyword">False</span></div><div class="line">        self.data_loader = DataLoader(img_res=(self.height, self.width))</div><div class="line">        optimizer = Adam()</div><div class="line"></div><div class="line">        self.alexnet = self.make_model()</div><div class="line">        self.alexnet.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">                             optimizer=optimizer,</div><div class="line">                             metrics=[<span class="string">'accuracy'</span>])</div></pre></td></tr></table></figure><p>In this code, we make all images share the same height and weight, i.e., 227, and the input images are three channels. Since we only have two categories, so <code>nb_classes=2</code>. Then we create a data loader class. By <code>self.alexnet = self.make_model()</code>, we build a AlexNet and in order to train the network, we use <code>categorical_crossentropy</code> as loss and use Adam to optimize the network. Here is the code for <code>make_model</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(self)</span>:</span></div><div class="line">        input = layers.Input(shape=self.img_shape)</div><div class="line">        <span class="keyword">if</span> self.mean_flag:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        x = layers.Conv2D(<span class="number">96</span>,kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>,activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(input)</div><div class="line">        x = layers.MaxPooling2D(pool_size=(<span class="number">3</span>,<span class="number">3</span>),strides=<span class="number">2</span>)(x)</div><div class="line">        x = layers.BatchNormalization()(x)</div><div class="line"></div><div class="line">        x = layers.Conv2D(<span class="number">256</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line">        x = layers.MaxPooling2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)(x)</div><div class="line">        x = layers.BatchNormalization()(x)</div><div class="line"></div><div class="line">        x = layers.Conv2D(<span class="number">384</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line">        x = layers.Conv2D(<span class="number">384</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line"></div><div class="line">        x = layers.Conv2D(<span class="number">256</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line">        x = layers.MaxPooling2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)(x)</div><div class="line">        x = layers.BatchNormalization()(x)</div><div class="line"></div><div class="line">        x = layers.Flatten()(x)</div><div class="line">        x = layers.Dense(<span class="number">4096</span>,activation=<span class="string">'tanh'</span>)(x)</div><div class="line">        x = layers.Dropout(<span class="number">0.5</span>)(x)</div><div class="line">        x = layers.Dense(<span class="number">4096</span>,activation=<span class="string">'tanh'</span>)(x)</div><div class="line">        x = layers.Dropout(<span class="number">0.5</span>)(x)</div><div class="line">        output = layers.Dense(self.nb_classes,activation=<span class="string">'softmax'</span>)(x)</div><div class="line"></div><div class="line">        model = Model(input,output)</div><div class="line">        model.summary()</div><div class="line">        <span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>Basically, the codes correspond to the network architecture we talk about above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,epochs,batch_size=<span class="number">16</span>)</span>:</span></div><div class="line">    train_generator, validation_generator = self.data_loader.load_batch(batch_size)</div><div class="line"></div><div class="line">    self.alexnet.fit_generator(train_generator,</div><div class="line">                               steps_per_epoch=<span class="number">100</span>,</div><div class="line">                               epochs=epochs,</div><div class="line">                               validation_data=validation_generator,</div><div class="line">                               validation_steps=<span class="number">20</span>,</div><div class="line">                               verbose=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>Here, we train the model <code>epochs</code> times and <code>sample_per_epoch</code> is batches of training samples;  <code>validation_steps</code> is batches of validation samples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># https://rahulduggal2608.wordpress.com/2017/04/02/alexnet-in-keras/</span></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div><div class="line"><span class="keyword">from</span> data_load <span class="keyword">import</span> DataLoader</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.height = <span class="number">227</span></div><div class="line">        self.width = <span class="number">227</span></div><div class="line">        self.channels = <span class="number">3</span></div><div class="line">        self.img_shape = (self.height,self.width,self.channels)</div><div class="line">        self.nb_classes = <span class="number">2</span></div><div class="line">        self.mean_flag = <span class="keyword">False</span></div><div class="line">        self.data_loader = DataLoader(img_res=(self.height, self.width))</div><div class="line">        optimizer = Adam()</div><div class="line"></div><div class="line">        self.alexnet = self.make_model()</div><div class="line">        self.alexnet.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">                             optimizer=optimizer,</div><div class="line">                             metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(self)</span>:</span></div><div class="line">        input = layers.Input(shape=self.img_shape)</div><div class="line">        <span class="keyword">if</span> self.mean_flag:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        x = layers.Conv2D(<span class="number">96</span>,kernel_size=<span class="number">11</span>,strides=<span class="number">4</span>,activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(input)</div><div class="line">        x = layers.MaxPooling2D(pool_size=(<span class="number">3</span>,<span class="number">3</span>),strides=<span class="number">2</span>)(x)</div><div class="line">        x = layers.BatchNormalization()(x)</div><div class="line"></div><div class="line">        x = layers.Conv2D(<span class="number">256</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line">        x = layers.MaxPooling2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)(x)</div><div class="line">        x = layers.BatchNormalization()(x)</div><div class="line"></div><div class="line">        x = layers.Conv2D(<span class="number">384</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line">        x = layers.Conv2D(<span class="number">384</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line"></div><div class="line">        x = layers.Conv2D(<span class="number">256</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,padding=<span class="string">'same'</span>)(x)</div><div class="line">        x = layers.MaxPooling2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)(x)</div><div class="line">        x = layers.BatchNormalization()(x)</div><div class="line"></div><div class="line">        x = layers.Flatten()(x)</div><div class="line">        x = layers.Dense(<span class="number">4096</span>,activation=<span class="string">'tanh'</span>)(x)</div><div class="line">        x = layers.Dropout(<span class="number">0.5</span>)(x)</div><div class="line">        x = layers.Dense(<span class="number">4096</span>,activation=<span class="string">'tanh'</span>)(x)</div><div class="line">        x = layers.Dropout(<span class="number">0.5</span>)(x)</div><div class="line">        output = layers.Dense(self.nb_classes,activation=<span class="string">'softmax'</span>)(x)</div><div class="line"></div><div class="line">        model = Model(input,output)</div><div class="line">        model.summary()</div><div class="line">        <span class="keyword">return</span> model</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,epochs,batch_size=<span class="number">16</span>)</span>:</span></div><div class="line">        train_generator, validation_generator = self.data_loader.load_batch(batch_size)</div><div class="line"></div><div class="line">        self.alexnet.fit_generator(train_generator,</div><div class="line">                                   steps_per_epoch=<span class="number">100</span>,</div><div class="line">                                   epochs=epochs,</div><div class="line">                                   validation_data=validation_generator,</div><div class="line">                                   validation_steps=<span class="number">20</span>,</div><div class="line">                                   verbose=<span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    alexnet = AlexNet()</div><div class="line">    alexnet.train(<span class="number">500</span>)</div></pre></td></tr></table></figure></div></div><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><p>The author also proposed several methods to attack the overfitting problem, i.e. data argmentation.</p><p> The authors employ two distinct forms of data augmentation.</p><p>The first form is to generated cropped images and horizontal reflections. For each training image of size $256 \times 256$, multiple patches of size $227 \times 227$ are extracted. Then, we can get 841$227 \times 227$ patches from a single image ($(256-227) \times (256-227)=841$). And for each patch we take a horizontal reflection, resulting in increasing the size of training set by a factor of 1682. <a href="https://stats.stackexchange.com/questions/178184/data-augmentation-step-in-krizhevsky-et-al-paper" target="_blank" rel="noopener">ref</a> </p><h1 id="ZFNET"><a href="#ZFNET" class="headerlink" title="ZFNET"></a>ZFNET</h1><h2 id="Architecture-1"><a href="#Architecture-1" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 5.56.55 PM.png" alt="creen Shot 2019-07-08 at 5.56.55 P"></p><h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><h2 id="Architecture-2"><a href="#Architecture-2" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 6.28.27 PM-2628528.png" alt="creen Shot 2019-07-08 at 6.28.27 PM-262852"></p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 6.44.05 PM.png" alt="creen Shot 2019-07-08 at 6.44.05 P"></p><p>Most memory is in early CONV while most params are in late FC.</p><p>FC7 features generalize well to other tasks</p><h2 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments"></a>Comments</h2><ol><li><p>Why use samller filters? ($3\times 3$ conv)</p><p><strong>Stack of three</strong> 3x3 conv (stride 1) layers has same effective receptive field as <strong>one</strong> 7x7 conv layer.</p><p>For one $7\times 7$ layers, the filters range it can see is from 1 to 7.</p><p>But for three layers $3\times 3$, at the first layer: </p><p>first neuron: 1-3, second neuron: 2-4, …, firth neuron: 5-7.</p><p>At the second layer:</p><p>first neuron see first, second and third neurons from the first layer, resulting in seeing neurons: 1-5.</p><p>second neuron see: 2-6; third neuron: 3-7.</p><p>Then in the third layer:</p><p>it sees 1-5, + 2-6, + 3-7 = 1-7. </p></li><li><p>What is the effective receptive field of three 3x3 conv (stride 1) layers?</p><p>It is deeper and has more non-linearities. What is more, fewer parameters: $3 \times (3^2C^2)$ vs $7^2\times C^2$.</p><p>​</p></li></ol><h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h2 id="Architecture-3"><a href="#Architecture-3" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.22.57 PM.png" alt="creen Shot 2019-07-08 at 7.22.57 P"></p><p><strong>Naive Inception module</strong></p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.23.55 PM.png" alt="creen Shot 2019-07-08 at 7.23.55 P"></p><p>The problem:</p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.24.54 PM.png" alt="creen Shot 2019-07-08 at 7.24.54 P"></p><p>This is very expensive computation. Furthermore, Pooling layer also preserves feature depth, which means total depth after concatenation can only grow at every layer!</p><p>$1\times 1 \text{convolutions}$</p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.27.01 PM.png" alt="creen Shot 2019-07-08 at 7.27.01 P"></p><p><strong>Inception module with dimension reduction</strong></p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.27.52 PM.png" alt="creen Shot 2019-07-08 at 7.27.52 P"></p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.28.06 PM.png" alt="creen Shot 2019-07-08 at 7.28.06 P"></p><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h2 id="Architecture-4"><a href="#Architecture-4" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.32.44 PM.png" alt="creen Shot 2019-07-08 at 7.32.44 P"></p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.53.49 PM.png" alt="creen Shot 2019-07-08 at 7.53.49 P"></p><h2 id="Comments-2"><a href="#Comments-2" class="headerlink" title="Comments"></a>Comments</h2><ol><li><p>What happens when we continue stacking deeper layers on a “plain” convolutional neural network?</p><p><img src="/2019/07/07/DP-Clasical-CNNs/Screen Shot 2019-07-08 at 7.55.07 PM.png" alt="creen Shot 2019-07-08 at 7.55.07 P"></p><p>56-layer model performs worse on both training and test error, according to the worse training loss, we can say that it is not caused by overfitting.</p></li></ol><h1 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h1><p>In <strong>DenseNet</strong>, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. <strong>Concatenation</strong> is used. <strong>Each layer is receiving a “collective knowledge” from all preceding layers</strong>.</p><h1 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h1>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP-Seq2Seq</title>
      <link href="/2019/07/01/NLP-Seq2Seq/"/>
      <url>/2019/07/01/NLP-Seq2Seq/</url>
      <content type="html"><![CDATA[<p>There’s a whole class of NLP tasks that rely on sequential output, or outputs that are sequences of potentially varying length. For example,</p><ul><li>Translation: taking a sentence in one language as input and outputting the same sentence in another language.</li><li>Conversation: taking a statement or question as input and responding to it.</li><li>Summarization: taking a large body of text as input and outputting a summary of it.</li></ul><p>Therefore, in this post, sequence-to-sequence models are introduced, a deep learning-based framework for handling these types of problems.</p><a id="more"></a><h1 id="The-Seq2Seq-Model"><a href="#The-Seq2Seq-Model" class="headerlink" title="The Seq2Seq Model"></a>The Seq2Seq Model</h1><p>The sequence-to-sequence model is an example of a Conditional Language Model.</p><ul><li><strong>Language Model</strong> because the decoder is predicting the next word of the target sentence y</li><li><strong>Conditional</strong> because its predictions are also conditioned on the source sentence x</li></ul><p>NMT directly calculates $P(y | x)$, </p><script type="math/tex; mode=display">P(y | x)=P\left(y_{1} | x\right) P\left(y_{2} | y_{1}, x\right) P\left(y_{3} | y_{1}, y_{2}, x\right) \ldots P\left(y_{T} | y_{1}, \ldots, y_{T-1}, x\right)</script><p>Here $P\left(y_{T} | y_{1}, \ldots, y_{T-1}, x\right)$ is the probability of next target word given target words so far and source sentence $x$. </p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 9.59.19 PM.png" alt="creen Shot 2019-08-05 at 9.59.19 P"></p><h2 id="Output-decoding"><a href="#Output-decoding" class="headerlink" title="Output decoding"></a>Output decoding</h2><h3 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h3><p>When generating next word, we always take the most probable word on next step  and we stop untl the model produces a <code>&lt;END&gt;</code> token.</p><p>But this method has no way to undo decisions.</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.04.18 PM.png" alt="creen Shot 2019-08-05 at 10.04.18 P"></p><h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><h4 id="Core-idea"><a href="#Core-idea" class="headerlink" title="Core idea"></a>Core idea</h4><p>On each step of decoder, keep track of the <code>k</code> most probable partial translations (which we call hypotheses). Here <code>k</code>  is the beam size.</p><p>A hypothesis has a <strong>score</strong> which is its log probability:</p><script type="math/tex; mode=display">\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\log P_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} | x\right)=\sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><ul><li>Scores are all negative, and higher score is better</li><li>We search for high-scoring hypotheses, tracking top k on each step</li></ul><h4 id="Stop-criterion"><a href="#Stop-criterion" class="headerlink" title="Stop criterion"></a>Stop criterion</h4><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.24.32 PM.png" alt="creen Shot 2019-08-05 at 10.24.32 P"></p><h4 id="Selection-criterion"><a href="#Selection-criterion" class="headerlink" title="Selection criterion"></a>Selection criterion</h4><p>We have our list of completed hypotheses, but How to select top one with highest score?</p><p>Say if we choose the hypothese with highest score, which is computed by:</p><script type="math/tex; mode=display">\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\log P_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} | x\right)=\sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><p>Then it will tend to choose the shorter sentence, which usually has a higher score.</p><p>In this case, we fix it by normalizing by length:</p><script type="math/tex; mode=display">\frac{1}{t} \sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><p>and use this to select top one.</p><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>Say we set <code>k=2</code>. We start generation with <code>&lt;START&gt;</code> token. Suppose the top two candidates are <code>he</code> and <code>I</code>:</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.15.39 PM.png" alt="creen Shot 2019-08-05 at 10.15.39 P"></p><p>Then we keep track of the two lines to generate next words:</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.16.44 PM.png" alt="creen Shot 2019-08-05 at 10.16.44 P"></p><p>The new score is parent’s score plus current probablity. Here we have four candidates, but since our <code>k</code> is 2, we choose top 2 among these 4 candidates.</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.18.38 PM.png" alt="creen Shot 2019-08-05 at 10.18.38 P"></p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.18.54 PM.png" alt="creen Shot 2019-08-05 at 10.18.54 P"></p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.19.21 PM.png" alt="creen Shot 2019-08-05 at 10.19.21 P"></p><h2 id="Machine-Translation-Evaluation"><a href="#Machine-Translation-Evaluation" class="headerlink" title="Machine Translation Evaluation"></a>Machine Translation Evaluation</h2><p>BLEU (Bilingual Evaluation Understudy) is the one we use for evaluation. BLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on:</p><ul><li>n-gram precision (usually for 1, 2, 3 and 4-grams)</li><li>Plus a penalty for too-short system translations</li></ul><p>BLEU is useful but imperfect:</p><ul><li>There are many valid ways to translate a sentence</li><li>So a good translation can get a poor BLEU score because it<br>has low n-gram overlap with the human translation </li></ul><h1 id="The-Attention-Mechanism"><a href="#The-Attention-Mechanism" class="headerlink" title="The Attention Mechanism"></a>The Attention Mechanism</h1><h2 id="Problem-in-Seq2Seq"><a href="#Problem-in-Seq2Seq" class="headerlink" title="Problem in Seq2Seq"></a>Problem in Seq2Seq</h2><p>The fixed-length vector carries the burden of encoding the the entire “meaning” of the input sequence, no matter how long that may be. With all the variance in language, this is a very hard problem. </p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.35.49 PM-5062577.png" alt="creen Shot 2019-08-05 at 10.35.49 PM-506257"></p><h2 id="Core-ideas"><a href="#Core-ideas" class="headerlink" title="Core ideas"></a>Core ideas</h2><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.39.50 PM.png" alt="creen Shot 2019-08-05 at 10.39.50 P"></p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.43.43 PM.png" alt="creen Shot 2019-08-05 at 10.43.43 P"></p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.44.10 PM.png" alt="creen Shot 2019-08-05 at 10.44.10 P"></p><p>In decoder, insted of just use the hidden state at the first step of the decoder, we operate the dot product between the decoder hidden state with the encoder hidden state of each step in encoder. So that we can get several scalar scores. </p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.44.43 PM.png" alt="creen Shot 2019-08-05 at 10.44.43 P"></p><p>Once we have these scores, we use a softmax function to turn the scores into a distribution.</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.46.38 PM.png" alt="creen Shot 2019-08-05 at 10.46.38 P"></p><p>Once we have the attention output, we use the attention distribution to take a weighted sum of the encoder hidden states.<img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.48.26 PM.png" alt="creen Shot 2019-08-05 at 10.48.26 P"></p><p>Finally Concatenate weighted sum of the encoder hidden states with decoder hidden state, then use to compute $\widehat{y}_{1}$ as before.</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.50.52 PM.png" alt="creen Shot 2019-08-05 at 10.50.52 P"></p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.51.04 PM.png" alt="creen Shot 2019-08-05 at 10.51.04 P"></p><h2 id="Attention-in-Equations"><a href="#Attention-in-Equations" class="headerlink" title="Attention in Equations"></a>Attention in Equations</h2><p>Remember that our seqzseq model is made of two parts, an encoder that encodes the input sentence, and a decoder that leverages the information extracted by the decoder to produce the translated sentence. Basically, our input is a sequence of words $x_1,…, x_n$ that we want to translate, and our target sentence is a sequence of words $y_1,…,y_m$.</p><ol><li><p>Encoder</p><p>Let $(h_1,., h_n)$ be the hidden vectors representing the input sentence. These vectors are the output of a BI-LSTM for instance, ane capture contextual representation of each word in the sentence</p></li><li><p>Decoder</p><p>We want to compute the hidden states $s_i$ of the decoder using a recursive formula of the form </p></li></ol><script type="math/tex; mode=display">   s_i =f (s_{i-1}, y_{i-1}, c_i)</script><p>   where $s_{i-1}$ is the previous hidden vector, $y_{i-1}$ is the generated word at the previous step, and $c_i$ is a context vector that capture the context from the original sentence that is relevant to the time step i of the decoder. The context vector $c_i$ captures relevant information for the i-th decoding time step (unlike the standard Seg2 seq in which theres only one context vector). For each hidden vector from the original sentence hi, compute a score</p><script type="math/tex; mode=display">    e_{i,i}= a (s_{i-1}, h_i)</script><p>   where a is any function, for instance a single layer fully-connected neural network. Then we end up with a sequence of scalar value $e_{i,1},…, e_{i,n}$. Normalize these scores into a vector $\alpha_{i}=\left(\alpha_{i, 1}, \ldots, \alpha_{i, n}\right)$, using a softmax layer.</p><script type="math/tex; mode=display">   \alpha_{i, j}=\frac{\exp \left(e_{i, j}\right)}{\sum_{k=1}^{n} \exp \left(e_{i, k}\right)}</script><p>   Then, compute the context vector $c_i$ as the weighted average of the hidden vectors from the original sentence</p><script type="math/tex; mode=display">   c_{i}=\sum_{j=1}^{n} \alpha_{i, j} h_{j}</script><p>   Intuitively, this vector captures the relevant contextual information from the original sentence for the i-th step of the decoder.</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.52.05 PM.png" alt="creen Shot 2019-08-05 at 10.52.05 P"></p><h2 id="Attention-Variants"><a href="#Attention-Variants" class="headerlink" title="Attention Variants"></a>Attention Variants</h2><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.52.34 PM.png" alt="creen Shot 2019-08-05 at 10.52.34 P"></p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-08-05 at 10.52.43 PM.png" alt="creen Shot 2019-08-05 at 10.52.43 P"></p><h2 id="Conncetion-with-translation-alignment"><a href="#Conncetion-with-translation-alignment" class="headerlink" title="Conncetion with translation alignment"></a>Conncetion with translation alignment</h2><p>The attention-based model learns to assign significance to different parts of the input for each step of the output. In the context of translation, attention can be thought of as “alignment.” Bahdanau et al. argue that the attention scores aij at decoding step i signify the words in the source sentence that align with word i in the target. Noting this, we can use attention scores to build an alignment table – a table mapping words in the source to corresponding words in the target sentence – based on the learned encoder and decoder from our Seq2Seq NMT system.</p><p><img src="/2019/07/01/NLP-Seq2Seq/Screen Shot 2019-07-13 at 5.50.09 PM.png" alt="creen Shot 2019-07-13 at 5.50.09 P"></p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a><a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank" rel="noopener">Keras</a></h2>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP-Dependency Parser</title>
      <link href="/2019/06/12/NLP-Dependency-Parser/"/>
      <url>/2019/06/12/NLP-Dependency-Parser/</url>
      <content type="html"><![CDATA[<p>Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate).</p><a id="more"></a><h1 id="What-is-Dependency-Parsing"><a href="#What-is-Dependency-Parsing" class="headerlink" title="What is Dependency Parsing"></a>What is Dependency Parsing</h1><p>Dependency parsing is the task of analyzing the syntactic dependency structure of a given input sentence S. The output of a dependency parser is a dependency tree where the words of the input sentence are connected by typed dependency relations. Formally, the dependency parsing problem asks to create a mapping from the input sentence with words $S = w_0w_1…w_n$ (where $w_0$ is the ROOT) to its dependency tree graph G. </p><p>To be precise, there are two subproblems in dependency parsing:</p><ol><li>Learning: Given a training set D of sentences annotated with dependency graphs, induce a parsing model M that can be used to<br>parse new sentences.</li><li>Parsing: Given a parsing model M and a sentence S, derive the<br>optimal dependency graph D for S according to M.</li></ol><h1 id="How-to-train-a-Dependency-Parsing"><a href="#How-to-train-a-Dependency-Parsing" class="headerlink" title="How to train a Dependency Parsing"></a>How to train a Dependency Parsing</h1><h2 id="Gready-Deterministic-Transition-Based-Parsing"><a href="#Gready-Deterministic-Transition-Based-Parsing" class="headerlink" title="Gready Deterministic Transition-Based Parsing"></a>Gready Deterministic Transition-Based Parsing</h2><p>This transition system is a state machine, which consists of states and transitions between those states. The model induces a sequence of transitions from some initial state to one of several terminal states.</p><p><strong>States:</strong></p><p>For any sentence $S = w_0w_1…w_n$, a state can be described with a triple $c = (\alpha, \beta, A)$:</p><ol><li>a stack $\alpha$ of words $w_i$ from $S$</li><li>a buffer $\beta$ of words  $w_i$ from $S$</li><li>a set of dependency arcs $A$ of the form $(w_i,r,w_j)$, where $r$ describes a dependency relation</li></ol><p>It follows that for any sentence $S = w_0w_1…w_n$,</p><ol><li>an initial state $c_0$ is of the form $([w_0]_{\alpha},[w_1,…,w_n]_{beta},\empty)$ (only the ROOT is on the stack, all other words are in the buffer and no actions have been chosen yet)</li><li>a terminate state has the form $(\alpha, []_{\beta}, A)$</li></ol><p><strong>Transitions:</strong></p><p>There are three types of transitions between states:</p><ol><li><p>Shift: Remove the first word in the buffer and push it on top of the stack. (Pre-condition: buffer has to be non-empty.)</p></li><li><p>LEFT-ARC: Add a dependency arc $(w_j,r,w_i)$ to the arc set $A$, where $w_i$ is the word second to the top of the stack and $w_j$ is the word at the top of the stack. Remove $w_i$ from the stack.</p><blockquote><p>$[w_0, w_1,…,w_i \gets w_j]$ </p></blockquote></li><li><p>RIGHT-ARC: Add a dependency arc $(w_i,r,w_j)$ to the arc set $A$, where $w_i$ is the word second to the top of the stack and $w_j$ is the word at the top of the stack. Remove $w_j$ from the stack.</p><blockquote><p>$[w_0, w_1,…,w_i \to w_j]$ </p></blockquote></li></ol><h2 id="Neural-Dependency-Parsing"><a href="#Neural-Dependency-Parsing" class="headerlink" title="Neural Dependency Parsing"></a>Neural Dependency Parsing</h2><p>The network is to predict the transitions between words, i.e., ${Shift, Left-Arc, Right-Arc}$. </p><p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.32.04 PM.png" alt="creen Shot 2019-06-12 at 5.32.04 P"></p><p>For each feature type, we will have a corresponding embedding matrix, mapping from the feature’s one hot encoding, to a d-dimensional<br>dense vector representation. </p><p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.33.19 PM.png" alt="creen Shot 2019-06-12 at 5.33.19 P"></p><p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.34.07 PM.png" alt="creen Shot 2019-06-12 at 5.34.07 P"></p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.35.18 PM.png" alt="creen Shot 2019-06-12 at 5.35.18 P"></p><p><img src="/2019/06/12/NLP-Dependency-Parser/Screen Shot 2019-06-12 at 5.36.31 PM.png" alt="creen Shot 2019-06-12 at 5.36.31 P"></p><blockquote><p>A sentence containing $n$ words will be parsed in $2n$ steps. Because for each word, it will be pushed from buffer to stack, which result in n steps. Then each word in the stack has to be assigned a transition, i.e., LEFT-ARC or RIGHT-ARC, which results in another n steps. Therefore, the total steps are $2n$.</p></blockquote><h1 id="Pytorch-Implementation"><a href="#Pytorch-Implementation" class="headerlink" title="Pytorch Implementation"></a>Pytorch Implementation</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartialParse</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence)</span>:</span></div><div class="line">        <span class="string">"""Initializes this partial parse.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @param sentence (list of str): The sentence to be parsed as a list of words.</span></div><div class="line"><span class="string">                                        Your code should not modify the sentence.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment"># The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.</span></div><div class="line">        self.sentence = sentence</div><div class="line"></div><div class="line">        <span class="comment">### YOUR CODE HERE (3 Lines)</span></div><div class="line">        <span class="comment">### Your code should initialize the following fields:</span></div><div class="line">        <span class="comment">###     self.stack: The current stack represented as a list with the top of the stack as the</span></div><div class="line">        <span class="comment">###                 last element of the list.</span></div><div class="line">        <span class="comment">###     self.buffer: The current buffer represented as a list with the first item on the</span></div><div class="line">        <span class="comment">###                  buffer as the first item of the list</span></div><div class="line">        <span class="comment">###     self.dependencies: The list of dependencies produced so far. Represented as a list of</span></div><div class="line">        <span class="comment">###             tuples where each tuple is of the form (head, dependent).</span></div><div class="line">        <span class="comment">###             Order for this list doesn't matter.</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">### Note: The root token should be represented with the string "ROOT"</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        self.stack = [<span class="string">'ROOT'</span>]</div><div class="line">        self.buffer = sentence.copy()</div><div class="line">        self.dependencies = []</div><div class="line">        <span class="comment">### END YOUR CODE</span></div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_step</span><span class="params">(self, transition)</span>:</span></div><div class="line">        <span class="string">"""Performs a single parse step by applying the given transition to this partial parse</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @param transition (str): A string that equals "S", "LA", or "RA" representing the shift,</span></div><div class="line"><span class="string">                                left-arc, and right-arc transitions. You can assume the provided</span></div><div class="line"><span class="string">                                transition is a legal transition.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment">### YOUR CODE HERE (~7-10 Lines)</span></div><div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">        <span class="comment">###     Implement a single parsing step, i.e. the logic for the following as</span></div><div class="line">        <span class="comment">###     described in the pdf handout:</span></div><div class="line">        <span class="comment">###         1. Shift</span></div><div class="line">        <span class="comment">###         2. Left Arc</span></div><div class="line">        <span class="comment">###         3. Right Arc</span></div><div class="line">        <span class="keyword">if</span> transition==<span class="string">'S'</span>:</div><div class="line">            self.stack.append(self.buffer.pop(<span class="number">0</span>))</div><div class="line">        <span class="keyword">elif</span> transition==<span class="string">'LA'</span>:</div><div class="line">            stack_second = self.stack.pop(<span class="number">-2</span>)</div><div class="line">            self.dependencies.append((self.stack[<span class="number">-1</span>],stack_second))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            stack_top = self.stack.pop()</div><div class="line">            self.dependencies.append((self.stack[<span class="number">-1</span>],stack_top))</div><div class="line"></div><div class="line">        <span class="comment">### END YOUR CODE</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, transitions)</span>:</span></div><div class="line">        <span class="string">"""Applies the provided transitions to this PartialParse</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @param transitions (list of str): The list of transitions in the order they should be applied</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @return dsependencies (list of string tuples): The list of dependencies produced when</span></div><div class="line"><span class="string">                                                        parsing the sentence. Represented as a list of</span></div><div class="line"><span class="string">                                                        tuples where each tuple is of the form (head, dependent).</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> transitions:</div><div class="line">            self.parse_step(transition)</div><div class="line">        <span class="keyword">return</span> self.dependencies</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">minibatch_parse</span><span class="params">(sentences, model, batch_size)</span>:</span></div><div class="line">    <span class="string">"""Parses a list of sentences in minibatches using a model.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    @param sentences (list of list of str): A list of sentences to be parsed</span></div><div class="line"><span class="string">                                            (each sentence is a list of words and each word is of type string)</span></div><div class="line"><span class="string">    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function</span></div><div class="line"><span class="string">                                model.predict(partial_parses) that takes in a list of PartialParses as input and</span></div><div class="line"><span class="string">                                returns a list of transitions predicted for each parse. That is, after calling</span></div><div class="line"><span class="string">                                    transitions = model.predict(partial_parses)</span></div><div class="line"><span class="string">                                transitions[i] will be the next transition to apply to partial_parses[i].</span></div><div class="line"><span class="string">    @param batch_size (int): The number of PartialParses to include in each minibatch</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    @return dependencies (list of dependency lists): A list where each element is the dependencies</span></div><div class="line"><span class="string">                                                    list for a parsed sentence. Ordering should be the</span></div><div class="line"><span class="string">                                                    same as in sentences (i.e., dependencies[i] should</span></div><div class="line"><span class="string">                                                    contain the parse for sentences[i]).</span></div><div class="line"><span class="string">    """</span></div><div class="line">    dependencies = []</div><div class="line"></div><div class="line">    <span class="comment">### YOUR CODE HERE (~8-10 Lines)</span></div><div class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">    <span class="comment">###     Implement the minibatch parse algorithm as described in the pdf handout</span></div><div class="line">    <span class="comment">###</span></div><div class="line">    <span class="comment">###     Note: A shallow copy (as denoted in the PDF) can be made with the "=" sign in python, e.g.</span></div><div class="line">    <span class="comment">###                 unfinished_parses = partial_parses[:].</span></div><div class="line">    <span class="comment">###             Here `unfinished_parses` is a shallow copy of `partial_parses`.</span></div><div class="line">    <span class="comment">###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances</span></div><div class="line">    <span class="comment">###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.</span></div><div class="line">    <span class="comment">###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`</span></div><div class="line">    <span class="comment">###             contains references to the same objects. Thus, you should NOT use the `del` operator</span></div><div class="line">    <span class="comment">###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that</span></div><div class="line">    <span class="comment">###             is being accessed by `partial_parses` and may cause your code to crash.</span></div><div class="line"></div><div class="line">    <span class="comment">#initialization</span></div><div class="line">    partial_parses = []</div><div class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</div><div class="line">        partial_parses.append(PartialParse(sentence))</div><div class="line">    unfinished_parses = partial_parses[:]</div><div class="line"></div><div class="line">    <span class="keyword">while</span> unfinished_parses:</div><div class="line">        minibatch = unfinished_parses[:batch_size]</div><div class="line">        transitions = model.predict(minibatch)</div><div class="line">        <span class="keyword">for</span> i,parses <span class="keyword">in</span> enumerate(minibatch):</div><div class="line">            parses.parse([transitions[i]])</div><div class="line">            <span class="keyword">if</span> len(parses.buffer)==<span class="number">0</span> <span class="keyword">and</span> len(parses.stack)==<span class="number">1</span>:</div><div class="line">                dependencies.append(parses.dependencies)</div><div class="line">                unfinished_parses.remove(parses)</div><div class="line">    <span class="comment">### END YOUR CODE</span></div><div class="line">    <span class="keyword">return</span> dependencies</div></pre></td></tr></table></figure><h2 id="Train-Parser-Network"><a href="#Train-Parser-Network" class="headerlink" title="Train Parser Network"></a>Train Parser Network</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParserModel</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">""" Feedforward neural network with an embedding layer and single hidden layer.</span></div><div class="line"><span class="string">    The ParserModel will predict which transition should be applied to a</span></div><div class="line"><span class="string">    given partial parse configuration.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    PyTorch Notes:</span></div><div class="line"><span class="string">        - Note that "ParserModel" is a subclass of the "nn.Module" class. In PyTorch all neural networks</span></div><div class="line"><span class="string">            are a subclass of this "nn.Module".</span></div><div class="line"><span class="string">        - The "__init__" method is where you define all the layers and their respective parameters</span></div><div class="line"><span class="string">            (embedding layers, linear layers, dropout layers, etc.).</span></div><div class="line"><span class="string">        - "__init__" gets automatically called when you create a new instance of your class, e.g.</span></div><div class="line"><span class="string">            when you write "m = ParserModel()".</span></div><div class="line"><span class="string">        - Other methods of ParserModel can access variables that have "self." prefix. Thus,</span></div><div class="line"><span class="string">            you should add the "self." prefix layers, values, etc. that you want to utilize</span></div><div class="line"><span class="string">            in other ParserModel methods.</span></div><div class="line"><span class="string">        - For further documentation on "nn.Module" please see https://pytorch.org/docs/stable/nn.html.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embeddings, n_features=<span class="number">36</span>,</span></span></div><div class="line"><span class="function"><span class="params">        hidden_size=<span class="number">200</span>, n_classes=<span class="number">3</span>, dropout_prob=<span class="number">0.5</span>)</span>:</span></div><div class="line">        <span class="string">""" Initialize the parser model.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @param embeddings (Tensor): word embeddings (num_words, embedding_size)</span></div><div class="line"><span class="string">        @param n_features (int): number of input features</span></div><div class="line"><span class="string">        @param hidden_size (int): number of hidden units</span></div><div class="line"><span class="string">        @param n_classes (int): number of output classes</span></div><div class="line"><span class="string">        @param dropout_prob (float): dropout probability</span></div><div class="line"><span class="string">        """</span></div><div class="line">        super(ParserModel, self).__init__()</div><div class="line">        self.n_features = n_features</div><div class="line">        self.n_classes = n_classes</div><div class="line">        self.dropout_prob = dropout_prob</div><div class="line">        self.embed_size = embeddings.shape[<span class="number">1</span>]</div><div class="line">        self.hidden_size = hidden_size</div><div class="line">        self.pretrained_embeddings = nn.Embedding(embeddings.shape[<span class="number">0</span>], self.embed_size)</div><div class="line">        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))</div><div class="line"></div><div class="line">        <span class="comment">### YOUR CODE HERE (~5 Lines)</span></div><div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">        <span class="comment">###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix</span></div><div class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></div><div class="line">        <span class="comment">###     2) Construct `self.dropout` layer.</span></div><div class="line">        <span class="comment">###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix</span></div><div class="line">        <span class="comment">###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.</span></div><div class="line">        <span class="comment">###         It has been shown empirically, that this provides better initial weights</span></div><div class="line">        <span class="comment">###         for training networks than random uniform initialization.</span></div><div class="line">        <span class="comment">###         For more details checkout this great blogpost:</span></div><div class="line">        <span class="comment">###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization </span></div><div class="line">        <span class="comment">### Hints:</span></div><div class="line">        <span class="comment">###     - After you create a linear layer you can access the weight</span></div><div class="line">        <span class="comment">###       matrix via:</span></div><div class="line">        <span class="comment">###         linear_layer.weight</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">### Please see the following docs for support:</span></div><div class="line">        <span class="comment">###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></div><div class="line">        <span class="comment">###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_</span></div><div class="line">        <span class="comment">###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></div><div class="line"></div><div class="line">        self.embed_to_hidden = nn.Linear(self.embed_size*self.n_features,self.hidden_size) <span class="comment"># input_size = n_features * embedding_size</span></div><div class="line">        nn.init.xavier_uniform_(self.embed_to_hidden.weight)</div><div class="line">        self.dropout = nn.Dropout(self.dropout_prob)</div><div class="line">        self.hidden_to_logits = nn.Linear(hidden_size,self.n_classes)</div><div class="line">        nn.init.xavier_uniform_(self.hidden_to_logits.weight)</div><div class="line"></div><div class="line">        <span class="comment">### END YOUR CODE</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(self, t)</span>:</span></div><div class="line">        <span class="string">""" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)</span></div><div class="line"><span class="string">            to embedding vectors.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">            PyTorch Notes:</span></div><div class="line"><span class="string">                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__</span></div><div class="line"><span class="string">                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).</span></div><div class="line"><span class="string">                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to</span></div><div class="line"><span class="string">                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)</span></div><div class="line"><span class="string">                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">            @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">            @return x (Tensor): tensor of embeddings for words represented in t</span></div><div class="line"><span class="string">                                (batch_size, n_features * embed_size)</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment">### YOUR CODE HERE (~1-3 Lines)</span></div><div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">        <span class="comment">###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.</span></div><div class="line">        <span class="comment">###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).</span></div><div class="line">        <span class="comment">###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">### Note: In order to get batch_size, you may need use the tensor .size() function:</span></div><div class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">###  Please see the following docs for support:</span></div><div class="line">        <span class="comment">###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></div><div class="line">        <span class="comment">###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></div><div class="line">        batch_size = t.shape[<span class="number">0</span>]</div><div class="line">        x = self.pretrained_embeddings(t).view(batch_size,<span class="number">-1</span>)</div><div class="line">        <span class="comment">### END YOUR CODE</span></div><div class="line">        <span class="keyword">return</span> x</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></div><div class="line">        <span class="string">""" Run the model forward.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">            PyTorch Notes:</span></div><div class="line"><span class="string">                - Every nn.Module object (PyTorch model) has a `forward` function.</span></div><div class="line"><span class="string">                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.</span></div><div class="line"><span class="string">                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,</span></div><div class="line"><span class="string">                    the `forward` function would called on `t` and the result would be stored in the `output` variable:</span></div><div class="line"><span class="string">                        model = ParserModel()</span></div><div class="line"><span class="string">                        output = model(t) # this calls the forward function</span></div><div class="line"><span class="string">                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @param t (Tensor): input tensor of tokens (batch_size, n_features)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)</span></div><div class="line"><span class="string">                                 without applying softmax (batch_size, n_classes)</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment">###  YOUR CODE HERE (~3-5 lines)</span></div><div class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">        <span class="comment">###     1) Apply `self.embedding_lookup` to `t` to get the embeddings</span></div><div class="line">        <span class="comment">###     2) Apply `embed_to_hidden` linear layer to the embeddings</span></div><div class="line">        <span class="comment">###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.</span></div><div class="line">        <span class="comment">###     4) Apply dropout layer to the output of step 3.</span></div><div class="line">        <span class="comment">###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">### Note: We do not apply the softmax to the logits here, because</span></div><div class="line">        <span class="comment">### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.</span></div><div class="line">        <span class="comment">###</span></div><div class="line">        <span class="comment">### Please see the following docs for support:</span></div><div class="line">        <span class="comment">###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu</span></div><div class="line">        features_embeddings = self.embedding_lookup(t)</div><div class="line">        o = F.relu(self.embed_to_hidden(features_embeddings))</div><div class="line">        o = self.dropout(o)</div><div class="line">        logits = self.hidden_to_logits(o)</div><div class="line">        <span class="comment">### END YOUR CODE</span></div><div class="line">        <span class="keyword">return</span> logits</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(parser, train_data, dev_data, output_path, batch_size=<span class="number">1024</span>, n_epochs=<span class="number">10</span>, lr=<span class="number">0.0005</span>)</span>:</span></div><div class="line">    <span class="string">""" Train the neural dependency parser.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></div><div class="line"><span class="string">    @param train_data ():</span></div><div class="line"><span class="string">    @param dev_data ():</span></div><div class="line"><span class="string">    @param output_path (str): Path to which model weights and results are written.</span></div><div class="line"><span class="string">    @param batch_size (int): Number of examples in a single batch</span></div><div class="line"><span class="string">    @param n_epochs (int): Number of training epochs</span></div><div class="line"><span class="string">    @param lr (float): Learning rate</span></div><div class="line"><span class="string">    """</span></div><div class="line">    best_dev_UAS = <span class="number">0</span></div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment">### YOUR CODE HERE (~2-7 lines)</span></div><div class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">    <span class="comment">###      1) Construct Adam Optimizer in variable `optimizer`</span></div><div class="line">    <span class="comment">###      2) Construct the Cross Entropy Loss Function in variable `loss_func`</span></div><div class="line">    <span class="comment">###</span></div><div class="line">    <span class="comment">### Hint: Use `parser.model.parameters()` to pass optimizer</span></div><div class="line">    <span class="comment">###       necessary parameters to tune.</span></div><div class="line">    <span class="comment">### Please see the following docs for support:</span></div><div class="line">    <span class="comment">###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html</span></div><div class="line">    <span class="comment">###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss</span></div><div class="line">    optimizer = optim.Adam(parser.model.parameters(),lr=lr)</div><div class="line">    loss_func = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line">    <span class="comment">### END YOUR CODE</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</div><div class="line">        print(<span class="string">"Epoch &#123;:&#125; out of &#123;:&#125;"</span>.format(epoch + <span class="number">1</span>, n_epochs))</div><div class="line">        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)</div><div class="line">        <span class="keyword">if</span> dev_UAS &gt; best_dev_UAS:</div><div class="line">            best_dev_UAS = dev_UAS</div><div class="line">            print(<span class="string">"New best dev UAS! Saving model."</span>)</div><div class="line">            torch.save(parser.model.state_dict(), output_path)</div><div class="line">        print(<span class="string">""</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_for_epoch</span><span class="params">(parser, train_data, dev_data, optimizer, loss_func, batch_size)</span>:</span></div><div class="line">    <span class="string">""" Train the neural dependency parser for single epoch.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Note: In PyTorch we can signify train versus test and automatically have</span></div><div class="line"><span class="string">    the Dropout Layer applied and removed, accordingly, by specifying</span></div><div class="line"><span class="string">    whether we are training, `model.train()`, or evaluating, `model.eval()`</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    @param parser (Parser): Neural Dependency Parser</span></div><div class="line"><span class="string">    @param train_data ():</span></div><div class="line"><span class="string">    @param dev_data ():</span></div><div class="line"><span class="string">    @param optimizer (nn.Optimizer): Adam Optimizer</span></div><div class="line"><span class="string">    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function</span></div><div class="line"><span class="string">    @param batch_size (int): batch size</span></div><div class="line"><span class="string">    @param lr (float): learning rate</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data</span></div><div class="line"><span class="string">    """</span></div><div class="line">    parser.model.train() <span class="comment"># Places model in "train" mode, i.e. apply dropout layer</span></div><div class="line">    n_minibatches = math.ceil(len(train_data) / batch_size)</div><div class="line">    loss_meter = AverageMeter()</div><div class="line">    dev_UAS, _ = parser.parse(dev_data)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tqdm(total=(n_minibatches)) <span class="keyword">as</span> prog:</div><div class="line">        <span class="keyword">for</span> i, (train_x, train_y) <span class="keyword">in</span> enumerate(minibatches(train_data, batch_size)):</div><div class="line">            optimizer.zero_grad()   <span class="comment"># remove any baggage in the optimizer</span></div><div class="line">            loss = <span class="number">0.</span> <span class="comment"># store loss for this batch here</span></div><div class="line">            train_x = torch.from_numpy(train_x).long()</div><div class="line">            train_y = torch.from_numpy(train_y.nonzero()[<span class="number">1</span>]).long()</div><div class="line"></div><div class="line">            <span class="comment">### YOUR CODE HERE (~5-10 lines)</span></div><div class="line">            <span class="comment">### <span class="doctag">TODO:</span></span></div><div class="line">            <span class="comment">###      1) Run train_x forward through model to produce `logits`</span></div><div class="line">            <span class="comment">###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.</span></div><div class="line">            <span class="comment">###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss</span></div><div class="line">            <span class="comment">###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)</span></div><div class="line">            <span class="comment">###         are the predictions (y^ from the PDF).</span></div><div class="line">            <span class="comment">###      3) Backprop losses</span></div><div class="line">            <span class="comment">###      4) Take step with the optimizer</span></div><div class="line">            <span class="comment">### Please see the following docs for support:</span></div><div class="line">            <span class="comment">###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step</span></div><div class="line">            logits = parser.model(train_x)</div><div class="line">            loss = loss_func(logits,train_y)</div><div class="line">            loss.backward()</div><div class="line">            optimizer.step()</div><div class="line">            <span class="comment">### END YOUR CODE</span></div><div class="line">            prog.update(<span class="number">1</span>)</div><div class="line">            loss_meter.update(loss.item())</div><div class="line"></div><div class="line">    <span class="keyword">print</span> (<span class="string">"Average Train Loss: &#123;&#125;"</span>.format(loss_meter.avg))</div><div class="line"></div><div class="line">    print(<span class="string">"Evaluating on dev set"</span>,)</div><div class="line">    parser.model.eval() <span class="comment"># Places model in "eval" mode, i.e. don't apply dropout layer</span></div><div class="line">    dev_UAS, _ = parser.parse(dev_data)</div><div class="line">    print(<span class="string">"- dev UAS: &#123;:.2f&#125;"</span>.format(dev_UAS * <span class="number">100.0</span>))</div><div class="line">    <span class="keyword">return</span> dev_UAS</div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Neural Network</title>
      <link href="/2019/06/11/DP-Neural-Network/"/>
      <url>/2019/06/11/DP-Neural-Network/</url>
      <content type="html"><![CDATA[<p>本节介绍最基本的神经网络。</p><a id="more"></a><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><p><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap1/c1s1.html" target="_blank" rel="noopener">ref1</a> <a href="https://www.leiphone.com/news/201706/QFydbeV7FXQtRIOl.html" target="_blank" rel="noopener">ref2</a> </p><h3 id="二分类线性模型"><a href="#二分类线性模型" class="headerlink" title="二分类线性模型"></a>二分类线性模型</h3><p>感知机作为一种二元线性分类模型，能（且一定能）将线性可分的数据集分开。什么叫线性可分？在二维平面上、线性可分意味着能用一条线将正负样本分开，在三维空间中、线性可分意味着能用一个平面将正负样本分开。</p><p>虽然简单，但它既可以发展成支持向量机（通过简单地修改损失函数），又可以发展为神经网络（通过简单地叠加），它的模型如下：</p><p><img src="/2019/06/11/DP-Neural-Network/tikz0.png" alt="ikz"></p><p>感知机接受几个输入$x_i$，对每一个权重赋予一个权重衡量该输入对输入的重要性，其输出为0或者1，由加权和$\sum_{j}{w_jx_j}$是否小于或者大于某一个阈值决定。和权重一样，阈值也是一个实数，同时它是神经元的一个参数。使用更严密的代数形式来表示：</p><script type="math/tex; mode=display">\begin{eqnarray} \mbox{output} & = & \left\{ \begin{array}{ll} 0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\ 1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}</script><p>以上为感知机的工作方式。鉴于上述表示方法过于繁琐，我们通过使用两个新记法来简化它。</p><p>第一个是使用点乘代替$\sum_{j}w_jx_j$：$w\cdot x \equiv \sum_{j}w_jx_j$</p><p>第二个是将阈值移到不等式的另一侧，并使用偏置(bias)来代替阈值：$bias \equiv -threshold$ </p><script type="math/tex; mode=display">\begin{eqnarray} \mbox{output} = \left\{ \begin{array}{ll} 0 & \mbox{if } w\cdot x + b \leq 0 \\ 1 & \mbox{if } w\cdot x + b > 0 \end{array} \right. \tag{2}\end{eqnarray}</script><p>如下图所示。</p><p><img src="/2019/06/11/DP-Neural-Network/perceptron-0271060.png" alt="erceptron-027106"></p><p>可以将偏置理解为感知机为了得到输出为1的容易度的度量，如果一个感知机的偏置非常大，那么这个感知机的输出很容易为1，相反如果偏置非常小，那么输出1就很困难。</p><h3 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><blockquote><p>为确定感知机模型参数$w$和$b$,需要定义一个损失函数并将损失函数最小化。样本点到超平面的距离:</p></blockquote><script type="math/tex; mode=display">\frac{1}{||w||}{|w\times{x}+b|}</script><p>$||w||$是$w$的$L_2$范数，为保计算结果为正，对$w\times x+b$进行绝对值运算。</p><blockquote><p>对于误分类的数据$(x_i,y_i)$来说，$-y_i(w\times{x_i}+b)&gt;0$，假设超平面$S$的误分类点集合为$M$，那么损失函数为所有误分类点到平面距离:</p></blockquote> $$L(w,b)=-\frac{1}{||w||}\sum_{x_i\in{M}}{{y_i(w\times{x_i}+b)}}$$$\frac{1}{||w||}$是常量，不影响，$L(w,b)=-\sum_{x_i\in{M}}{{y_i(w\times{x_i}+b)}}$<blockquote><p>采用梯度下降法，对于一个误分类样本点$(x_i,y_i)$，$\frac{\partial L}{\partial w}=-\eta{y_ix_i}$，$\frac{\partial{L}}{\partial{b}}=-\eta{y_i}$。</p><p>则参数更新公式为：</p><p>$w=w+\eta y_ix_i, b = b+\eta y_i$ </p></blockquote><h3 id="逻辑计算"><a href="#逻辑计算" class="headerlink" title="逻辑计算"></a>逻辑计算</h3><p>感知机可以用于计算初等逻辑函数：与、或、非。</p><div class="table-container"><table><thead><tr><th style="text-align:center">$x_1$</th><th style="text-align:center">$x_2$</th><th style="text-align:center">AND</th><th style="text-align:center">OR</th><th style="text-align:center">NOT</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr></tbody></table></div><p>感知机的输入是两个取值0、1的变量，</p><p>对于$AND$，它们对输出的贡献是相同的，则权重相同，我们取权重为-2和-2，但是发现4个里面才一个1，得到1比较困难，故有较高的阈值，设置阈值为3。</p><p><img src="/2019/06/11/DP-Neural-Network/tikz2.png" alt="ikz"></p><p>对于$OR$，权重相同，但是得到1比较容易，所以取较小的阈值，设置阈值为0.5。</p><p>对于$NOT$，发现只有一个输入，则只有一个输入对输出有影响，所以权重不同，$w_1=-1, w_2=0$，阈值为$-0.5$。</p><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。</p><p>（1）M-P神经元模型：</p><p>神经元接收来自$n$个其他神经元传递过来的输入信号，这些输入信号通过待权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过”激活函数”处理以产生神经元的输出。</p><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-08-23 at 8.45.15 PM.png" alt="creen Shot 2018-08-23 at 8.45.15 P"></p><p>（2）激活函数</p><p>采用（a）阶跃函数作为激活函数，它将输入值映射为输出值“1”（对应于神经元兴奋）或“0”（神经元抑制），可是阶跃函数不连续、不光滑，故采用sigmoid函数作为激活函数。</p><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-08-23 at 8.46.24 PM.png" alt="creen Shot 2018-08-23 at 8.46.24 P"></p><p>故我们称上述神经元为sigmoid神经元。</p><blockquote><p>感知机到sigmoid神经元的motivation：</p><p>We want a samll change in a weight (or bias) to cause a small change in output.</p><p>Compared to perceptron, both inputs and outputs of sigmoid neural can take on any values between 0 and 1 instead of just 0 or 1.</p></blockquote><h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><p>多个sigmoid神经元构成复杂的神经网络。网络的最左边的一层被称为输入层，其中的神经元被称为输入神经元(input neurons)；最右边的一层是输出层(output layer)，包含的神经元被称为输出神经元(output neurons)。下图中我们的输出层只有一个神经元，网络的中间一层被称为隐层(hidden layer)，因为它既不是输入层，也不是输出层。</p><p><img src="/2019/06/11/DP-Neural-Network/tikz11.png" alt="ikz1"></p><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p><a href="https://ilewseu.github.io/2017/12/17/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/" target="_blank" rel="noopener">ref1</a> <a href="https://zhuanlan.zhihu.com/p/34378516" target="_blank" rel="noopener">ref2</a> <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">gotta</a> </p><p>神经网络的前向传播实质就是一个输入向量$\vec x$到输出向量$\vec y$的函数。下面我们使用一个例子来说明这个过程</p><p><img src="/2019/06/11/DP-Neural-Network/2256672-bfbb364740f898d1.png" alt="256672-bfbb364740f898d"></p><p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$w_{41}, w_{42},w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？</p><script type="math/tex; mode=display">\begin{align}a_4&=sigmoid(\vec{w}^T\centerdot\vec{x})\\&=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\end{align}</script><p>其中$w_{4b}$是节点4的偏置项，图中没有画出来。而$w_{41}, w_{42},w_{43}$分别为节点1、2、3到节点4连接的权重，在给权重编号时，我们把目标节点的编号$j$放在前面，把源节点的编号$i$放在后面。</p><p>同样，我们可以继续计算出节点5、6、7的输出值$a_5,a_6,a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$：</p><script type="math/tex; mode=display">\begin{align}y_1&=sigmoid(\vec{w}^T\centerdot\vec{a})\\&=sigmoid(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b})\end{align}</script><p>同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$时，神经网络的输出向量$\vec{y}=\begin{bmatrix}y_1\\y_2\end{bmatrix}$。这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong>。 </p><h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便，我们先来看看隐藏层的矩阵表示。</p><p>首先我们把隐藏层4个节点的计算依次排列出来：</p><script type="math/tex; mode=display">a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\</script><p>接着，定义网络的输入向量和隐藏层每个节点的权重向量$\vec w_j$，</p><script type="math/tex; mode=display">\begin{align}\vec{x}&=\begin{bmatrix}x_1\\x_2\\x_3\\1\end{bmatrix}\\\vec{w}_4&=[w_{41},w_{42},w_{43},w_{4b}]\\\vec{w}_5&=[w_{51},w_{52},w_{53},w_{5b}]\\\vec{w}_6&=[w_{61},w_{62},w_{63},w_{6b}]\\\vec{w}_7&=[w_{71},w_{72},w_{73},w_{7b}]\\f&=sigmoid\\\end{align}</script><p>代入得：</p><script type="math/tex; mode=display">\begin{align}a_4&=f(\vec{w_4}\centerdot\vec{x})\\a_5&=f(\vec{w_5}\centerdot\vec{x})\\a_6&=f(\vec{w_6}\centerdot\vec{x})\\a_7&=f(\vec{w_7}\centerdot\vec{x})\end{align}</script><p>如果把$a_4,a_5,a_6,a_7$表示成一个矩阵，</p><script type="math/tex; mode=display">\vec{a}=\begin{bmatrix}a_4 \\a_5 \\a_6 \\a_7 \\\end{bmatrix},\qquad W=\begin{bmatrix}\vec{w}_4 \\\vec{w}_5 \\\vec{w}_6 \\\vec{w}_7 \\\end{bmatrix}=\begin{bmatrix}w_{41},w_{42},w_{43},w_{4b} \\w_{51},w_{52},w_{53},w_{5b} \\w_{61},w_{62},w_{63},w_{6b} \\w_{71},w_{72},w_{73},w_{7b} \\\end{bmatrix},\qquad f(\begin{bmatrix}x_1\\x_2\\x_3\\.\\.\\.\\\end{bmatrix})=\begin{bmatrix}f(x_1)\\f(x_2)\\f(x_3)\\.\\.\\.\\\end{bmatrix}</script><p>代入得：</p><script type="math/tex; mode=display">\vec{a}=f(W\centerdot\vec{x})\qquad</script><p>上式中，$f$是激活函数；$W$是某一层的权重矩阵；$\vec x$是某层的输入向量；$\vec a$是某层的输出向量。上式说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p><p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$W_1, W_2,W_3,W_4$，每个隐藏层的输出分别是$\vec a_1, \vec a_2, \vec a_3$，神经网络的输入为$\vec x$，神经网络的输出为$\vec y$，如下图所示：</p><p><img src="/2019/06/11/DP-Neural-Network/2256672-c1388dc8fdcce427.png" alt="256672-c1388dc8fdcce42"></p><p>则每一层的输出向量的计算可以表示为：</p><script type="math/tex; mode=display">\begin{align}&\vec{a}_1=f(W_1\centerdot\vec{x})\\&\vec{a}_2=f(W_2\centerdot\vec{a}_1)\\&\vec{a}_3=f(W_3\centerdot\vec{a}_2)\\&\vec{y}=f(W_4\centerdot\vec{a}_3)\\\end{align}</script><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p><p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p><p>我们假设每个训练样本为$(\vec{x},\vec{t})$，其中向量$\vec x$是训练样本的特征，而$\vec t$是样本的目标值。</p><p><img src="/2019/06/11/DP-Neural-Network/2256672-6f27ced45cf5c0d8.png" alt="256672-6f27ced45cf5c0d"></p><p>首先，我们根据前向传播算法，用样本的特征$\vec x$，计算出神经网络中每个隐藏层节点的输出$a_i$，以及输出层每个节点的输出$y_i$。</p><p>然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$：</p><h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>对于输出层的节点$i$ </p><script type="math/tex; mode=display">\delta_i=y_i(1-y_i)(y_i-t_i)\qquad</script><p>其中$\delta_i$是节点$i$的误差项，$y_i$是节点$i$的输出值，$t_i$是样本对应于节点$i$的目标值。比如：节点8的误差是$\delta_8=y_1(1-y_1)(t_1-y_1)$。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用<strong>随机梯度下降</strong>优化算法去求目标函数最小值时的参数值。</p><p>我们取网络所有输出层节点的误差平方和作为目标函数：</p><script type="math/tex; mode=display">E_d\equiv\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2</script><p>其中，$E_d$表示是样本$d$的误差，展开得到：</p><script type="math/tex; mode=display">E_d = E_{y_1}+E_{y_2}=\frac{1}{2}(t{_1}-{y_1})^2+\frac{1}{2}(t{_2}-{y_2})^2</script><p>然后我们用<strong>随机梯度下降</strong>算法对目标函数进行优化：</p><script type="math/tex; mode=display">w_{ji}\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}</script><p>随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？</p><p><img src="/2019/06/11/DP-Neural-Network/2256672-6f27ced45cf5c0d8-5514755.png" alt="256672-6f27ced45cf5c0d8-551475"></p><p>对于隐藏层与输出层间的权值，我们需要计算$\frac{\partial E_d}{\partial w_{ji} }$，由链式法则：</p><script type="math/tex; mode=display">\frac{\partial E_d}{\partial w_{ji} }=\frac{\partial E_d}{y_j}\frac{y_i}{net_j}\frac{net_j}{w_{ji}}</script><p>其中，$net_j$是是节点$j$的<strong>加权输入</strong>，即$ net_j=\vec{w_j}\centerdot\vec{x_j}=\sum_{i}{w_{ji}}x_{ji} $ </p><p>$y_i$是激活函数处理之后的输出值。</p><p>考虑第一项：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial{E_d}}{\partial{y_j}}&=\frac{\partial}{\partial{y_j}}\frac{1}{2}\sum_{i\in outputs}(t_i-y_i)^2\\&=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2\\&=y_j-t_j\end{align}</script><p>考虑第二项：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial{y_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\&=y_j(1-y_j)\\\end{align}</script><blockquote><p><a href="https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/" target="_blank" rel="noopener">source: Derivation: Derivatives for Common Neural Network Activation Functions</a>  </p><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2019-02-13 at 10.26.59 AM.png" alt="creen Shot 2019-02-13 at 10.26.59 A"></p></blockquote><p>将第一项和第二项带入，得到：</p><script type="math/tex; mode=display">\frac{\partial{E_d}}{\partial{net_j}}=(y_j-t_j)y_j(1-y_j)</script><p>如果令$\sigma_j=\frac{\partial{E_d}}{\partial{net_j}}$来表示该节点的误差，则</p><script type="math/tex; mode=display">\sigma_j =(y_j-t_j)y_j(1-y_j)</script><p>将上述推导带入随机梯度下降公式，得到：</p><script type="math/tex; mode=display">\begin{align}w_{ji}&\gets w_{ji}-\eta\frac{\partial{E_d}}{\partial{w_{ji}}}\\&=w_{ji}+\eta(t_j-y_j)y_j(1-y_j)x_{ji}\\&=w_{ji}+\eta\delta_jx_{ji}\end{align}</script><p>这样我们就得到了隐藏层权重的更新公式。</p></div></div> <h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><script type="math/tex; mode=display">\delta_i=a_i(1-a_i)\sum_{k\in{outputs}}w_{ki}\delta_k\qquad</script><p>其中，$a_i$是节点的输出值，$w_{ki}$是节点$i$到它的下一层节点$k$的连接的权重，$\sigma_k$是节点$i$的下一层节点$k$的误差项。例如，对于隐藏层节点4来说，计算方法为：$\delta_4=a_4(1-a_4)(w_{84}\delta_8+w_{94}\delta_9)$ </p><p>最后更新每个连接上的权值：</p><script type="math/tex; mode=display">w_{ji}\gets w_{ji}+\eta\delta_jx_{ji}\qquad</script><p>其中，$w_ji$是节点$i$到节点$j$的权重，$\eta$是学习速率，$\sigma_j$是节点$j$的误差项，$x_{ji}$是节点$i$传递给节点$j$的输入。例如，权重的更新方法如下：$w_{84}\gets w_{84}+\eta\delta_8 a_4$；类似的，权重$w_{41}$的更新方法如下：$w_{41}\gets w_{41}+\eta\delta_4 x_1$；</p><p>偏置项的输入值永远为1。例如，节点4的偏置项$w_{4b}$应该按照下面的方法计算： $w_{4b}\gets w_{4b}+\eta\delta_4$ </p><p>显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>同样地，我们需要计算$\frac{\partial E_d } {\partial w_{ji} }$,由链式法则：</p><script type="math/tex; mode=display">\frac{\partial{E_d}}{\partial{w_{ji}}}={\frac{\partial{E_d}}{\partial{a_j}}}{\frac{\partial{a_j}}{\partial{net_j}}}{\frac{\partial{net_j}}{\partial{w_{ji}}}}</script><p>考虑第一项：</p><p>首先，我们需要定义节点$j$的所有直接下游节点的集合$Downstream(j)$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$a_j$只能通过影响$Downstream(j)$再影响$E_d$。设$net_j$是节点$j$的下游节点的输入，则</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}{\frac{\partial{E_d}}{\partial{a_j}}}&=\sum_{k\in Downstream(j)}{\frac{\partial{E_d}}{\partial{net_k}}}{\frac{\partial{net_k}}{\partial{a_j}}}\\&=\sum_{k\in Downstream(j)}\sigma_k{\frac{\partial{net_k}}{\partial{a_j}}}\\&=\sum_{k\in Downstream(j)}\sigma_k w_{kj}\\\end{aligned}\end{equation}</script><p>考虑第二项：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial{a_j}}{\partial{net_j}}&=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\\&=a_j(1-a_j)\\\end{align}</script><p>综合第一项和第二项：</p><script type="math/tex; mode=display">{\frac{\partial{E_d}}{\partial{net_j}}}=a_j(1-a_j)\sum_{k\in Downstream(j)}\sigma_k w_{kj}</script><p>令$\delta_j=\frac{\partial E_d}{ \partial net_j }$，则</p><script type="math/tex; mode=display">\delta_j=a_j(1-a_j)\sum_{k\in Downstream(j)}\delta_kw_{kj}</script></div></div> <h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><p>假设我们三层神经网络，即输入层、隐藏层和输出层，使用$L_2$损失函数。</p><ul><li>输入是<code>x</code>，令<code>a1=x</code>。</li><li>前向传播<ul><li>输入层-隐藏层的前向传播：<code>z2=a1*w1</code></li><li>输入层-隐藏层的激活函数：<code>a2=sigmoid(z2)</code></li><li>隐藏层-输出层的前向传播：<code>z3=a2*w2</code></li><li>隐藏层-输出层的激活函数：<code>h=sigmoid(z3)</code></li></ul></li><li>后向传播<ul><li>输出层-隐藏层的损失：<code>h_err=(h-y)*sigmoid_prime(h)</code></li><li>输出层-隐藏层的梯度：<code>h_delta=h_err*a2</code></li><li>隐藏层-输入层的损失：<code>z2_err=(h_err*w2)*sigmoid_prime(a2)</code></li><li>隐藏层-输入层的梯度：<code>z2_delta=z2_err*a1</code></li><li>更新梯度：<code>w1 -= z2_delta</code>,<code>w2 -= h_delta</code></li></ul></li></ul><h2 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h2><h3 id="Toy-Example"><a href="#Toy-Example" class="headerlink" title="Toy Example"></a><a href="https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python" target="_blank" rel="noopener">Toy Example</a></h3><p>在该例子中，我们将建模一个具有二输入、一输出和一隐藏层的神经网络，该网络用于预测考试成绩基于两个输入：学习时间和睡觉时间。以下为训练样本：</p><div class="table-container"><table><thead><tr><th style="text-align:center">学习时间</th><th style="text-align:center">睡觉时间</th><th style="text-align:center">考试成绩</th></tr></thead><tbody><tr><td style="text-align:center">2</td><td style="text-align:center">9</td><td style="text-align:center">92</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">5</td><td style="text-align:center">86</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">6</td><td style="text-align:center">89</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">8</td><td style="text-align:center">？</td></tr></tbody></table></div><h4 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h4><p>考虑到时间是小时制，而考试成绩的百分制，所以我们想要将数据标准化，故对每一个变量我们都除以它的最大值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.array(([<span class="number">2</span>,<span class="number">9</span>],[<span class="number">1</span>,<span class="number">5</span>],[<span class="number">3</span>,<span class="number">6</span>]),dtype=float)</div><div class="line">y = np.array(([<span class="number">92</span>],[<span class="number">86</span>],[<span class="number">89</span>]),dtype=float)</div><div class="line">x = x/np.max(x,axis=<span class="number">0</span>)</div><div class="line">y = y/<span class="number">100</span></div></pre></td></tr></table></figure><p>然后，我们定义一个类<code>class</code>和初始化函数<code>init</code>，在<code>init</code>函数中我们定义神经网络参数，如输入层，隐藏层和输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neural_Network</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.input_size = <span class="number">2</span></div><div class="line">        self.output_size = <span class="number">1</span></div><div class="line">        self.hidden_size = <span class="number">3</span></div><div class="line">        self.w1 = np.random.randn(self.hidden_size,self.input_size)</div><div class="line">        self.w2 = np.random.randn(self.output_size,self.hidden_size)</div></pre></td></tr></table></figure><p>变量定义好之后，我们便可以写前向传播函数了，在前向传播中，我们要完成的工作有输入与权重的点乘，再应用激活函数；隐层与权重的点乘，再应用一个激活函数得到输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></div><div class="line">    self.z2 = x.dot(self.w1.T) <span class="comment"># x.shape=(n,2) , w1.shape=(3,2),z1.shape=(n,3)</span></div><div class="line">    self.a2 = self.sigmoid(self.z2)</div><div class="line">    self.z3 = self.a2.dot(self.w2.T)</div><div class="line">    o = self.sigmoid(self.z3)</div><div class="line">    <span class="keyword">return</span> o</div></pre></td></tr></table></figure><p>所以，我们还需要定义一个<code>sigmoid</code>激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,z)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</div></pre></td></tr></table></figure><h4 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h4><p>第一步我们需要定义一个损失函数，我们使用<code>MSE</code>作为损失函数</p><script type="math/tex; mode=display">Loss=\frac{1}{2m}\sum_{i=1}^{m}(o_i-y_i)</script><p>其中，$o_i$是对$i_{th}$样本的预测，$y_i$是该样本的真实输出。</p><p>一旦定义好了损失函数，我们的目标就是使损失函数的值不断逼近0，即我们需要优化损失函数，我们需要激活函数的导数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(self,z)</span>:</span></div><div class="line">    <span class="keyword">return</span> z*(<span class="number">1</span>-z)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,x,y,o)</span>:</span></div><div class="line">    self.o_err = o-y <span class="comment">#shape=(n,output_size)</span></div><div class="line">    self.o_delta = self.o_err*self.sigmoid_prime(o) <span class="comment">#shape=(n,output_size)</span></div><div class="line"></div><div class="line">    self.z2_err = self.o_delta.dot(self.w2) <span class="comment">#(n,hidden_size)</span></div><div class="line">    self.z2_delta = self.z2_err*self.sigmoid_prime(self.a2) <span class="comment">#(n,hidden_size)</span></div><div class="line"></div><div class="line">    self.w1 -= self.z2_delta.T.dot(x)</div><div class="line">    self.w2 -= self.o_delta.T.dot(self.a2)</div></pre></td></tr></table></figure><p>最后，我们便可以定义一个训练函数，它完成前向传播和后向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,x,y)</span>:</span></div><div class="line">    o = self.forward(x)</div><div class="line">    self.backward(x,y,o)</div></pre></td></tr></table></figure><p>最后我们定义<code>main</code>函数，训练模型1000次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    nn = Neural_Network()</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">        o = nn.forward(x)</div><div class="line">        <span class="comment"># print("Predicted Output:",o)</span></div><div class="line">        print(<span class="string">"LOSS:"</span>,np.mean(np.square(y-o)))</div><div class="line">        nn.train(x,y)</div></pre></td></tr></table></figure><h3 id="MNIST手写体数字分类"><a href="#MNIST手写体数字分类" class="headerlink" title="MNIST手写体数字分类"></a><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/" target="_blank" rel="noopener">MNIST手写体数字分类</a></h3><p>本例我们介绍如何使用神经网络进行多分类任务，数据集$ex3data1.mat$包含了5000个训练样本，每个样本都是$20\times 20$像素大小的灰度图片，这$20\times 20$大小的 图片被展开成一个400维的向量，也就是说，我们的训练数据集是一个$5000\times 400$的矩阵。同样地，图片的标签被记录在大小为5000维向量$y$里，范围从1-10。</p><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-08-29 at 10.22.09 PM.png" alt="creen Shot 2018-08-29 at 10.22.09 P"></p><p>我们就构建一个最简单的神经网络：输入层-隐藏层-输出层。</p><p><strong>输入层神经元个数</strong>：$input_size=20\times 20 =400$</p><p><strong>隐藏层神经元个数</strong>：$hidden_size=25$</p><p><strong>输出层神经元个数</strong>：$output_size=10$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">data = scipy.io.loadmat(<span class="string">'dataset/ex3data1.mat'</span>)</div><div class="line">x = data[<span class="string">'X'</span>]</div><div class="line">y = data[<span class="string">'y'</span>]</div><div class="line">encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</div><div class="line">y = encoder.fit_transform(y)</div><div class="line"></div><div class="line">input_size = <span class="number">400</span></div><div class="line">hidden_size = <span class="number">25</span></div><div class="line">output_size = <span class="number">10</span></div><div class="line">learning_rate = <span class="number">1</span></div></pre></td></tr></table></figure><p>那么<strong>输入层和隐藏层</strong>之间的参数个数：$\theta_1=hidden_size \times (input_size+1)=25\times 401=10025$</p><p>输出层和隐藏层之间的参数个数：</p><p>$\theta_2=output_size \times (hidden_size+1)=10\times 26=260$</p><blockquote><p>上式中的+1是偏置神经元。</p></blockquote><p>所以这个神经网络的参数总数为：</p><p>$\theta=10025+260=10285$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">theta = np.random.randn(hidden_size*(input_size+<span class="number">1</span>)+output_size*(hidden_size+<span class="number">1</span>))</div><div class="line">theta1 = theta[<span class="number">0</span>:hidden_size*(input_size+<span class="number">1</span>)]</div><div class="line">theta1 = theta1.reshape(hidden_size,input_size+<span class="number">1</span>)</div><div class="line">theta2 = theta[hidden_size*(input_size+<span class="number">1</span>):]</div><div class="line">theta2 = theta2.reshape(output_size,hidden_size+<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>不要将权重参数初始化为0</p></blockquote><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">displayData</span><span class="params">(self,example_width=None, figsize=<span class="params">(<span class="number">10</span>, <span class="number">10</span>)</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Displays 2D data stored in X in a nice grid.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment"># Compute rows, cols</span></div><div class="line">    rand_indices = np.random.choice(len(self.y), <span class="number">100</span>, replace=<span class="keyword">False</span>)</div><div class="line">    X = self.x[rand_indices, :]</div><div class="line">    <span class="keyword">if</span> X.ndim == <span class="number">2</span>:</div><div class="line">        m, n = X.shape</div><div class="line">    <span class="keyword">elif</span> X.ndim == <span class="number">1</span>:</div><div class="line">        n = X.size</div><div class="line">        m = <span class="number">1</span></div><div class="line">        X = X[<span class="keyword">None</span>]  <span class="comment"># Promote to a 2 dimensional array</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">raise</span> IndexError(<span class="string">'Input X should be 1 or 2 dimensional.'</span>)</div><div class="line"></div><div class="line">    example_width = example_width <span class="keyword">or</span> int(np.round(np.sqrt(n)))</div><div class="line"></div><div class="line">    <span class="comment"># Compute number of items to display</span></div><div class="line">    display_rows = int(np.floor(np.sqrt(m)))</div><div class="line">    display_cols = int(np.ceil(m / display_rows))</div><div class="line"></div><div class="line">    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)</div><div class="line">    fig.subplots_adjust(wspace=<span class="number">0.025</span>, hspace=<span class="number">0.025</span>)</div><div class="line"></div><div class="line">    ax_array = [ax_array] <span class="keyword">if</span> m == <span class="number">1</span> <span class="keyword">else</span> ax_array.ravel()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(ax_array):</div><div class="line">        <span class="comment"># Display Image</span></div><div class="line">        h = ax.imshow(X[i].reshape(example_width, example_width, order=<span class="string">'F'</span>),</div><div class="line">                      cmap=<span class="string">'Greys'</span>, extent=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">        ax.axis(<span class="string">'off'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></div></div><h4 id="前向传播-2"><a href="#前向传播-2" class="headerlink" title="前向传播"></a>前向传播</h4><p>输入是$x$，其维度是（$batch_size, 400$），我们需要对其插入一列全1偏置向量，得到新输入$a_1$变成（$batch_size, 401$）维：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">row,col = np.shape</div><div class="line">a1 = np.insert(x,np.ones(row),axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>那么先全连接乘法，再对其使用激活函数：</p><p>$z_2=a1*theta1^T$</p><p>$a_2=sigmoid(z_2)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">z2 = a1.dot(theta1.T)</div><div class="line">a2 = sigmoid(z2)</div></pre></td></tr></table></figure><p>得到隐层输出$a_2$之后，我们计算输出层：</p><p>$a_2$的大小是（$batch_size,25$），我们需要插入偏置向量，变成$(batch_size,26)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">a2 = np.insert(a2,np.ones(row),axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>$z_3=a2*theta2^T$</p><p>$ h=sigmoid(z_3)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">z3 = a2.dot(theta2.T)</div><div class="line">h = sigmoid(z3)</div></pre></td></tr></table></figure><p>这样我们就完成了前向传播的计算。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagate</span><span class="params">(x,theta1,theta2)</span>:</span></div><div class="line">    <span class="comment">#x_shape = (5000,400)</span></div><div class="line">    <span class="comment"># theta1_shape = [hidden_size, input_size+1]</span></div><div class="line">    <span class="comment"># theta1_shape = [num_labels,hidden_size+1]</span></div><div class="line">    row,col = np.shape(x)</div><div class="line">    a1 = np.insert(x,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>)</div><div class="line">    z2 = a1.dot(theta1.T)</div><div class="line">    a2 = sigmoid(z2)</div><div class="line">    a2 = np.insert(a2,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>)</div><div class="line">    z3 = a2.dot(theta2.T)</div><div class="line">    h = sigmoid(z3)</div><div class="line">    <span class="keyword">return</span> a1,z2,a2,z3,h</div></pre></td></tr></table></figure></div></div><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>我们利用神经网络进行分类任务，所以我们使用交叉熵来衡量损失：<a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" target="_blank" rel="noopener">ref</a> </p><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2018-09-02 at 4.03.40 PM.png" alt="creen Shot 2018-09-02 at 4.03.40 P"></p><p>上式是增加了正则化项的损失函数，其中$m$是样本数，即$batch_{size}$；$K$是输出神经元数，即$output_{size}$；$\lambda$是学习速率；需要注意的是，正则化项没有偏置参数。</p><blockquote><p><a href="https://blog.csdn.net/sinat_29819401/article/details/60885679" target="_blank" rel="noopener">why regularization</a> </p><p>除了偏置神经元的权重之外，我们对网络的所有神经元的权重平方和作为正则化项。在求损失函数对权重的梯度时，在原来的梯度计算基础上，加上$\frac{\lambda}{m}\theta$。</p><p>回忆起$L_2$的梯度更新公式：</p><script type="math/tex; mode=display">\frac{\partial Cost}{\partial \theta}=\frac{\partial Cost}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial \theta}=(h-y)\sigma'(h)x</script><p>其中$L_2$的$\frac{\partial Cost}{\partial h}=(h-y)$，但是对于交叉熵而言$\frac{\partial Cost}{\partial h}=\frac{h-y}{h(1-h)}=\frac{h-y}{\sigma’(h)}$</p><p>$\frac{\partial Cost}{\partial h}$代入上式：</p><script type="math/tex; mode=display">\frac{\partial Cost}{\partial \theta}=\frac{\partial Cost}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial \theta}=\frac{h-y}{\sigma'(h)}\sigma'(h)x=(h-y)x</script><p>可以看出，偏导数是由$(h-y)$控制，模型的输出$h$与标签$y$之间的差异越大，偏导数就会越大，学习就会越快。</p></blockquote><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(thetas,x,y)</span>:</span></div><div class="line">    row, col = np.shape(x)</div><div class="line">    theta1 = thetas[:hidden_size*(input_size+<span class="number">1</span>)].reshape(hidden_size,input_size+<span class="number">1</span>)</div><div class="line">    theta2 = thetas[hidden_size*(input_size+<span class="number">1</span>   ):].reshape(num_labels,hidden_size+<span class="number">1</span>)</div><div class="line">    a1, z2, a2, z3, h = forward_propagate(x,theta1,theta2)</div><div class="line">    <span class="comment"># err = (h-y)**2</span></div><div class="line">    <span class="comment"># err = 0.5*sum(sum(err)/len(x))</span></div><div class="line">    l1 = -y*(np.log(h))</div><div class="line">    l2 = (y<span class="number">-1</span>)*np.log(<span class="number">1</span>-h)</div><div class="line">    err = l1+l2</div><div class="line">    reg_err = learning_rate/(<span class="number">2</span>*row)*(np.sum(theta1[:,<span class="number">1</span>:]**<span class="number">2</span>)+np.sum(theta2[:,<span class="number">1</span>:]**<span class="number">2</span>))</div><div class="line">    <span class="keyword">return</span> np.mean(err)+reg_err,a1,z2,a2,z3,h</div></pre></td></tr></table></figure><blockquote><p>初始时，err=0.6383642819248599；reg_err=0.016015625000000002</p></blockquote></div></div><h4 id="反向传播-2"><a href="#反向传播-2" class="headerlink" title="反向传播"></a>反向传播</h4><p>反向传播不断更新参数使样本的损失函数越来越小，不过我们需要一个工具计算激活函数的梯度：</p><script type="math/tex; mode=display">sigmoid(z)=\frac{1}{1+e^{-z}}</script><p>其梯度为：</p><script type="math/tex; mode=display">sigmoid'(z)=z(1-z)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span><span class="params">(z)</span>:</span></div><div class="line">    <span class="keyword">return</span> z*(<span class="number">1</span>-z)</div></pre></td></tr></table></figure><p>接下来我们需要逐层计算损失：</p><p>输出层损失：</p><p>$err_3=(a_3-y)*sigmoid_{gradient}(a_3)$，$shape=(batch_size,output_size)$</p><p>隐藏层与输出层之间的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">z2 = np.insert(z2,<span class="number">0</span>,np.ones(row),axis=<span class="number">1</span>) <span class="comment">#shape = (batch_size,hidden_size+1)</span></div></pre></td></tr></table></figure><p>$\delta_3=[err_3]^T.*z_2$，$shape=(output_size,hidden_size+1)$</p><p>隐藏层损失：</p><p>$err_2=sigmoid_{gradient}(a_2)<em>err_3.  </em>theta_2$,$shape=(batch_size,hidden_size+1)$</p><p>输入层与隐藏层之间的梯度：</p><p>$\delta_2=[err_2]^T.*a_1$，$shape=(hidden_size,input_size+1)$</p><h3 id="KERAS版本实现"><a href="#KERAS版本实现" class="headerlink" title="KERAS版本实现"></a>KERAS版本实现</h3><p><code>100epoch</code>训练之后的正确率为<code>94.56%</code>。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Ann</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.input_size = <span class="number">400</span></div><div class="line">        self.hidden_size = <span class="number">25</span></div><div class="line">        self.output_size = <span class="number">10</span></div><div class="line"></div><div class="line">        opt = Adam(lr=<span class="number">0.0002</span>,beta_1=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">        self.ann = self.make_model()</div><div class="line">        self.ann.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">                         optimizer=opt,</div><div class="line">                         metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(self)</span>:</span></div><div class="line">        inputs = Input((self.input_size,))</div><div class="line">        h = Dense(self.hidden_size,activation=<span class="string">'sigmoid'</span>)(inputs)</div><div class="line">        outputs = Dense(self.output_size,activation=<span class="string">'sigmoid'</span>)(h)</div><div class="line">        model = Model(inputs,outputs)</div><div class="line">        model.summary()</div><div class="line">        <span class="keyword">return</span> model</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></div><div class="line">        data = loadmat(<span class="string">'dataset/ex3data1.mat'</span>)</div><div class="line">        X = data[<span class="string">'X'</span>]</div><div class="line">        y = data[<span class="string">'y'</span>]</div><div class="line">        encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</div><div class="line">        y = encoder.fit_transform(y)</div><div class="line"></div><div class="line">        results = self.ann.fit(X,y,epochs=<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    ann = Ann()</div><div class="line">    ann.train()</div></pre></td></tr></table></figure></div></div><h3 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h3><p>实现了神经网络的前向传播和后向传播之后，为保证我们的代码能正确运行，在进行神经网络训练数据之前，我们需要检查梯度是否正确。最简单的方法是将神经网络的梯度与我们手工计算得梯度进行比较。</p><script type="math/tex; mode=display">\frac{\partial Cost}{\partial \theta}=\frac{Cost(\theta +\epsilon)-Cost(\theta -\epsilon)}{2\epsilon}</script><p>也就是说，反向传播计算出来的梯度应该近似上述式子计算出来的梯度，否则我们的代码有误。</p><p>一般的，我们设置$\epsilon=10^{-7}$，而两个梯度之间的差异应该小于$10^{-7}$。</p><ol><li><a href="https://www.leiphone.com/news/201711/MWEDFvRMdOyN7Evm.html" target="_blank" rel="noopener">神经网络中容易被忽视的基础知识</a> </li></ol><h1 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h1><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayersNetwork</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size, hidden_size, output_size, std=<span class="number">1e-4</span>)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        Initialize the model. Weights are initialized to small random values and</span></div><div class="line"><span class="string">        biases are initialized to zero. Weights and biases are stored in the</span></div><div class="line"><span class="string">        variable self.params, which is a dictionary with the following keys:</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        W1: First layer weights; has shape (D, H)</span></div><div class="line"><span class="string">        b1: First layer biases; has shape (H,)</span></div><div class="line"><span class="string">        W2: Second layer weights; has shape (H, C)</span></div><div class="line"><span class="string">        b2: Second layer biases; has shape (C,)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Inputs:</span></div><div class="line"><span class="string">        - input_size: The dimension D of the input data.</span></div><div class="line"><span class="string">        - hidden_size: The number of neurons H in the hidden layer.</span></div><div class="line"><span class="string">        - output_size: The number of classes C.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        self.params = &#123;&#125;</div><div class="line">        self.params[<span class="string">'W1'</span>] = std * np.random.randn(input_size, hidden_size)</div><div class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</div><div class="line">        self.params[<span class="string">'W2'</span>] = std * np.random.randn(hidden_size, output_size)</div><div class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        Compute the loss and gradients for a two layer fully connected neural</span></div><div class="line"><span class="string">        network.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Inputs:</span></div><div class="line"><span class="string">        - X: Input data of shape (N, D). Each X[i] is a training sample.</span></div><div class="line"><span class="string">        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></div><div class="line"><span class="string">          an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></div><div class="line"><span class="string">          is not passed then we only return scores, and if it is passed then we</span></div><div class="line"><span class="string">          instead return the loss and gradients.</span></div><div class="line"><span class="string">        - reg: Regularization strength.</span></div><div class="line"><span class="string">        - relu activation function</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></div><div class="line"><span class="string">        the score for class c on input X[i].</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        If y is not None, instead return a tuple of:</span></div><div class="line"><span class="string">        - loss: Loss (data loss and regularization loss) for this batch of training</span></div><div class="line"><span class="string">          samples.</span></div><div class="line"><span class="string">        - grads: Dictionary mapping parameter names to gradients of those parameters</span></div><div class="line"><span class="string">          with respect to the loss function; has the same keys as self.params.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="comment"># Unpack variables from the params dictionary</span></div><div class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</div><div class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</div><div class="line">        N, D = X.shape</div><div class="line"></div><div class="line">        <span class="comment"># Compute the forward pass</span></div><div class="line">        scores = <span class="keyword">None</span></div><div class="line"></div><div class="line">        s1 = X.dot(W1) + b1</div><div class="line">        a1 = np.maximum(s1, <span class="number">0</span>)</div><div class="line">        scores = a1.dot(W2) + b2</div><div class="line"></div><div class="line">        <span class="comment"># If the targets are not given then jump out, we're done</span></div><div class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> scores</div><div class="line"></div><div class="line">        <span class="comment"># Compute the loss</span></div><div class="line"></div><div class="line">        normalized_scores = scores - np.max(scores, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">        exp_scores = np.exp(normalized_scores)</div><div class="line">        soft_scores = exp_scores / (np.sum(exp_scores, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>)))</div><div class="line">        target_scores = soft_scores[np.arange(N), y]</div><div class="line">        loss = <span class="number">-1</span> * np.sum(np.log(target_scores)) / N + reg * (np.sum(W1 ** <span class="number">2</span>) + np.sum(W2 ** <span class="number">2</span>))</div><div class="line"></div><div class="line"></div><div class="line">        <span class="comment"># Backward pass: compute gradients</span></div><div class="line">        grads = &#123;&#125;</div><div class="line"></div><div class="line">        dSoftmax = soft_scores</div><div class="line">        dSoftmax[np.arange(N), y] -= <span class="number">1</span></div><div class="line">        dW2 = a1.T.dot(dSoftmax)</div><div class="line">        db2 = np.sum(dSoftmax, axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">        dRelu = np.zeros_like(a1)</div><div class="line">        dRelu[a1 &gt; <span class="number">0</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">        prev = dSoftmax.dot(W2.T)</div><div class="line">        prev *= dRelu</div><div class="line"></div><div class="line">        dW1 = X.T.dot(prev)</div><div class="line">        db1 = np.sum(prev, axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">        grads[<span class="string">'W2'</span>] = dW2 / N + <span class="number">2</span> * reg * W2</div><div class="line">        grads[<span class="string">'b2'</span>] = db2 / N</div><div class="line">        grads[<span class="string">'W1'</span>] = dW1 / N + <span class="number">2</span> * reg * W1</div><div class="line">        grads[<span class="string">'b1'</span>] = db1 / N</div><div class="line"></div><div class="line">        <span class="keyword">return</span> loss, grads</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, X_val, y_val,</span></span></div><div class="line"><span class="function"><span class="params">              learning_rate=<span class="number">1e-3</span>, learning_rate_decay=<span class="number">0.95</span>,</span></span></div><div class="line"><span class="function"><span class="params">              reg=<span class="number">5e-6</span>, num_iters=<span class="number">100</span>,</span></span></div><div class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        Train this neural network using stochastic gradient descent.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Inputs:</span></div><div class="line"><span class="string">        - X: A numpy array of shape (N, D) giving training data.</span></div><div class="line"><span class="string">        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span></div><div class="line"><span class="string">          X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">        - X_val: A numpy array of shape (N_val, D) giving validation data.</span></div><div class="line"><span class="string">        - y_val: A numpy array of shape (N_val,) giving validation labels.</span></div><div class="line"><span class="string">        - learning_rate: Scalar giving learning rate for optimization.</span></div><div class="line"><span class="string">        - learning_rate_decay: Scalar giving factor used to decay the learning rate</span></div><div class="line"><span class="string">          after each epoch.</span></div><div class="line"><span class="string">        - reg: Scalar giving regularization strength.</span></div><div class="line"><span class="string">        - num_iters: Number of steps to take when optimizing.</span></div><div class="line"><span class="string">        - batch_size: Number of training examples to use per step.</span></div><div class="line"><span class="string">        - verbose: boolean; if true print progress during optimization.</span></div><div class="line"><span class="string">        """</span></div><div class="line">        num_train = X.shape[<span class="number">0</span>]</div><div class="line">        iterations_per_epoch = max(num_train / batch_size, <span class="number">1</span>)</div><div class="line"></div><div class="line">        N, D = X.shape</div><div class="line"></div><div class="line">        <span class="comment"># Use SGD to optimize the parameters in self.model</span></div><div class="line">        loss_history = []</div><div class="line">        train_acc_history = []</div><div class="line">        val_acc_history = []</div><div class="line"></div><div class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</div><div class="line"></div><div class="line">            idx = np.random.choice(N, batch_size)</div><div class="line">            X_batch = X[idx]</div><div class="line">            y_batch = y[idx]</div><div class="line"></div><div class="line">            <span class="comment"># Compute loss and gradients using the current minibatch</span></div><div class="line">            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</div><div class="line">            loss_history.append(loss)</div><div class="line"></div><div class="line">            self.params[<span class="string">'W1'</span>] -= learning_rate * grads[<span class="string">'W1'</span>]</div><div class="line">            self.params[<span class="string">'b1'</span>] -= learning_rate * grads[<span class="string">'b1'</span>]</div><div class="line">            self.params[<span class="string">'W2'</span>] -= learning_rate * grads[<span class="string">'W2'</span>]</div><div class="line">            self.params[<span class="string">'b2'</span>] -= learning_rate * grads[<span class="string">'b2'</span>]</div><div class="line"></div><div class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</div><div class="line"></div><div class="line">            <span class="comment"># Every epoch, check train and val accuracy and decay learning rate.</span></div><div class="line">            <span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</div><div class="line">                <span class="comment"># Check accuracy</span></div><div class="line">                train_acc = (self.predict(X_batch) == y_batch).mean()</div><div class="line">                val_acc = (self.predict(X_val) == y_val).mean()</div><div class="line">                train_acc_history.append(train_acc)</div><div class="line">                val_acc_history.append(val_acc)</div><div class="line"></div><div class="line">                <span class="comment"># Decay learning rate</span></div><div class="line">                learning_rate *= learning_rate_decay</div><div class="line"></div><div class="line">        <span class="keyword">return</span> &#123;</div><div class="line">            <span class="string">'loss_history'</span>: loss_history,</div><div class="line">            <span class="string">'train_acc_history'</span>: train_acc_history,</div><div class="line">            <span class="string">'val_acc_history'</span>: val_acc_history,</div><div class="line">        &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        Use the trained weights of this two-layer network to predict labels for</span></div><div class="line"><span class="string">        data points. For each data point we predict scores for each of the C</span></div><div class="line"><span class="string">        classes, and assign each data point to the class with the highest score.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Inputs:</span></div><div class="line"><span class="string">        - X: A numpy array of shape (N, D) giving N D-dimensional data points to</span></div><div class="line"><span class="string">          classify.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        Returns:</span></div><div class="line"><span class="string">        - y_pred: A numpy array of shape (N,) giving predicted labels for each of</span></div><div class="line"><span class="string">          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span></div><div class="line"><span class="string">          to have class c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">        """</span></div><div class="line"></div><div class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</div><div class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</div><div class="line">        s1 = X.dot(W1) + b1</div><div class="line">        a1 = np.maximum(s1, <span class="number">0</span>)</div><div class="line">        scores = a1.dot(W2) + b2</div><div class="line">        y_pred = np.argmax(scores, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> y_pred</div></pre></td></tr></table></figure></div></div><h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p><a href="https://towardsdatascience.com/deep-learning-concepts-part-2-9aed45e5e7ed" target="_blank" rel="noopener">to do</a> </p><p>y为ground_truth, a为预测。</p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>The whole idea of regularization is you try to penalize the complexity of the model, instead of explicitly trying to fit the training data.</p><script type="math/tex; mode=display">L=\frac{1}{N}\sum_{i=1}^{N}\sum_{j\ne y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)+\lambda R(W)</script><p><img src="/2019/06/11/DP-Neural-Network/Screen Shot 2019-03-07 at 4.24.27 PM.png" alt="creen Shot 2019-03-07 at 4.24.27 P"></p><blockquote><p>$L1$ Regularization prefer the sparse weight vector, which </p></blockquote><h1 id="BatchNormalization"><a href="#BatchNormalization" class="headerlink" title="BatchNormalization"></a>BatchNormalization</h1><p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">zhihu</a> <a href="https://blog.csdn.net/whitesilence/article/details/75667002" target="_blank" rel="noopener">过程</a> </p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">ref1</a> <a href="https://www.cnblogs.com/makefile/p/dropout.html" target="_blank" rel="noopener">ref2</a> <a href="https://www.cnblogs.com/santian/p/5457412.html" target="_blank" rel="noopener">ref3</a> <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">ref4</a> <a href="https://yq.aliyun.com/articles/68901" target="_blank" rel="noopener">ref4-ch</a> <a href="http://lib.csdn.net/article/deeplearning/51257" target="_blank" rel="noopener">regularization vs dropout</a> </p><p>dropout的引入是为了防止网络的过拟合，本质是正则化。</p><p>神经元以概率$p$被保留，当一个神经元被drop out，该神经元的输出就被置0</p><p>被drop out的神经元对训练阶段(包括前向传播和后向传播)没有任何贡献，</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP-Word2Vec</title>
      <link href="/2019/05/29/NLP-Word2Vec/"/>
      <url>/2019/05/29/NLP-Word2Vec/</url>
      <content type="html"><![CDATA[<p>Word embedding, i.e., vector representations of a particular word and also called word vectoring, is important in NLP. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. </p><p>Word2Vec, as one of the most popular technique to learn word embeddings using shallow neural network, includes:</p><ul><li>2 algorithms: continuous bag-of-words (CBOW) and skip-gram. CBOW aims to predict a center word from the surrounding context in terms of word vectors. Skip-gram does the opposite, and predicts the distribution (probability) of context words from a center word.</li><li>2 training methods: negative sampling and hierarchical softmax. Negative sampling defines an objective by sampling negative examples, while hierarchical softmax defines an objective using an efficient tree structure to compute probabilities for all the vocabulary.</li></ul><a id="more"></a><h1 id="Why-do-we-need-them"><a href="#Why-do-we-need-them" class="headerlink" title="Why do we need them?"></a>Why do we need them?</h1><p>Consider the following similar sentences: <em>Have a good day</em> and <em>Have a great day.</em> They hardly have different meaning. If we construct an exhaustive vocabulary (let’s call it V), it would have V = {Have, a, good, great, day}.</p><p>Now, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.</p><p>Have = [1,0,0,0,0]<code>; a=[0,1,0,0,0]</code> ; good=[0,0,1,0,0]<code>; great=[0,0,0,1,0]</code> ; day=[0,0,0,0,1]<code>(</code> represents transpose)</p><p>If we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means ‘good’ and ‘great’ are as different as ‘day’ and ‘have’, which is not true.</p><p>Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0.</p><p><img src="/2019/05/29/NLP-Word2Vec/0_XMW5mf81LSHodnTi.png" alt="_XMW5mf81LSHodnT"></p><p>Here comes the idea of generating <em>distributed representations</em>. Intuitively, we introduce some <em>dependence</em> of one word on the other words. The words in context of this word would get a greater share of this <em>dependence.</em> In one hot encoding representations, all the words are <em>independent</em> of each other<em>,</em> as mentioned earlier.</p><h1 id="How-does-Word2Vec-work"><a href="#How-does-Word2Vec-work" class="headerlink" title="How does Word2Vec work?"></a>How does Word2Vec work?</h1><p>Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)</p><h2 id="CBOW-Model"><a href="#CBOW-Model" class="headerlink" title="CBOW Model"></a>CBOW Model</h2><p>This method takes the context of each word as the input and tries to predict the word corresponding to the context.</p><p>Let the input to the Neural Network be the word, <em>great.</em> Notice that here we are trying to predict a target word (<em>d</em>ay<em>)</em> using a single context input word <em>great.</em> More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (<em>d</em>ay). In the process of predicting the target word, we learn the vector representation of the target word.</p><p><img src="/2019/05/29/NLP-Word2Vec/0_3DFDpaXoglalyB4c.png" alt="_3DFDpaXoglalyB4"></p><blockquote><p>Note: input is the one-hot vector of one word.</p></blockquote><p>The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.</p><p>The hidden layer neurons just copy the weighted sum of inputs to the next layer. <strong>There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer.</strong></p><p>But, the above model used a single context word to predict the target. We can use multiple context words to do the same.</p><p><img src="/2019/05/29/NLP-Word2Vec/0_CCsrTAjN80MqswXG.png" alt="_CCsrTAjN80MqswX"></p><h2 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h2><p>We can use the target word (whose representation we want to generate) to predict the context and in the process, we produce the representations.</p><p><img src="/2019/05/29/NLP-Word2Vec/0_Ta3qx5CQsrJloyCA.png" alt="_Ta3qx5CQsrJloyC"></p><p>We input the target word into the network. The model outputs C probability distributions. What does this mean? For each context position, we get C probability distributions of V probabilities, one for each word.</p><h3 id="The-Hidden-Layer"><a href="#The-Hidden-Layer" class="headerlink" title="The Hidden Layer"></a>The Hidden Layer</h3><p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).</p><p><img src="/2019/05/29/NLP-Word2Vec/word2vec_weight_matrix_lookup_table.png" alt="ord2vec_weight_matrix_lookup_tabl"></p><p>So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!</p><p>Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just <em>select</em> the matrix row corresponding to the “1”. Here’s a small example to give you a visual.</p><p><img src="/2019/05/29/NLP-Word2Vec/matrix_mult_w_one_hot.png" alt="atrix_mult_w_one_ho"></p><p>This means that <strong>the hidden layer is operating as a lookup table.</strong> The output of the hidden layer is just the “word vector” for the input word.</p><h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>Ok, are you ready for an exciting bit of insight into this network?</p><p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p><blockquote><p>say we have two words, their word embedding are A and B. The hidden-output weight matrix is <code>HO</code>, so the outputs are <code>A*HO</code> and <code>B*HO</code>. Because these two words have similar contexts, so the groundtruth target should be same. which result in <code>A*HO = B*HO</code>, meaning <code>A</code> and <code>B</code> have to be similar.</p></blockquote><p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p><p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>Say we have a word documents with $T$ words, our task is to predict context words within a window of fixed size $m$, given center word $w_j$:</p><script type="math/tex; mode=display">max \ L(\theta)=max \ \prod_{t=1}^{T} \prod_{-m \leq j \leq m \atop j \neq 0} P\left(w_{t+j} | w_{t} ; \theta\right)</script><p>to simplify it:</p><script type="math/tex; mode=display">min \ J(\theta)=min \ -\frac{1}{T} \log L(\theta)=min \ -\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right)</script><p>Here, we transform max to min by put a negative sign in front of the formula.</p><p>In order to calculate $P\left(w_{t+j} | w_{t} ; \theta\right)$, we need to introduce two vectors for each word in the document:</p><ul><li>$v_w$ when $w$ is a center word</li><li>$u_w$ when $w$ is a context word</li></ul><p>Then for a center word $c$ and a context word $o$:</p><script type="math/tex; mode=display">P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}</script><p>which equals the softmax function if we take $x_i=u_{o}^{T} v_{c}$, </p><script type="math/tex; mode=display">\operatorname{softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}=p_{i}</script><p>The softmax function maps arbitrary values $x_i$ to a probability distribution $p_i$,</p><ul><li>“max” because amplifies probability of largest $x_i$,</li><li>“soft” because still assigns some probability to smaller $x_i$</li></ul><h3 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h3><p>Say we want to calculate the derivative of <strong>center vector</strong> $v_c$, </p><script type="math/tex; mode=display">\\\begin{equation}\begin{aligned}\frac{\partial}{\partial v_c}logP(o | c)&= \frac{\partial}{\partial v_c}log \frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)} \\&=\frac{\partial}{\partial v_c}log{\exp \left(u_{o}^{T} v_{c}\right)}-\frac{\partial}{\partial v_c}log {\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)} \\&=\frac{\partial}{\partial v_c}{ \left(u_{o}^{T} v_{c}\right)} - \frac{1}{\sum_{w \in V}\exp(u_{w}^{T}v_{c})}\frac{\partial}{\partial v_c}{\sum_{x \in V} \exp \left(u_{x}^{T} v_{c}\right)} \\&= u_o - \frac{1}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}{\sum_{x \in V}}\frac{\partial}{\partial v_c}\exp \left(u_{x}^{T} v_{c}\right) \\&=u_o - \frac{1}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}{\sum_{x \in V}}\exp \left(u_{x}^{T} v_{c}\right)\frac{\partial}{\partial v_c}(u_x^Tv_c)  \\&=u_o - \frac{1}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}{\sum_{x \in V}}\exp \left(u_{x}^{T} v_{c}\right)(u_x) \\&=u_o-\sum_{w\in V}P(o|c)u_w\end{aligned}\end{equation}</script><p>Here $\sum_{w\in V}P(o|center)u_w$ is the expected context word by our trained model, and $u_o$ is our target context, it is like the difference between the target and prediction.</p><p>We can treat $u_0$ as we choose the target row from contect matrix $U$, which is $U^T\cdot y$; as for $u_w$, we can see it as we also choose a row from context matrix, but which row? Actually, it is each row of the matrix and average them. Because $P(o|c)$ is the prediction, which is the output of our model, so we can treat it as $\hat y$. Therefor $\sum_{w\in V}P(o|c)u_w=U^T \cdot \hat y$ , so the gradient for center word vector is:</p><script type="math/tex; mode=display">\frac{\partial}{\partial v_c}logP(o | c)= U^T \cdot (\hat y-y)</script><p>where $U$ is one of the learned weights.</p><p>Then we need to calculate the derivative for context vector $u_w$:</p><p>if $w=o$, then:</p><script type="math/tex; mode=display">\begin{array}{c}{\frac{\partial logP(o | c)}{\partial \boldsymbol{u}_{w}}} \\ {=-\boldsymbol{v}_{c}+\frac{1}{\sum_{w=1}^{V} \exp \left(\boldsymbol{u}_{w}^{T} \boldsymbol{v}_{c}\right)} \frac{\partial \sum_{w=1}^{V} \exp \left(\boldsymbol{u}_{w}^{T} \boldsymbol{v}_{c}\right)}{\partial \boldsymbol{u}_{o}}} \\ {=-\boldsymbol{v}_{c}+\frac{1}{\sum_{w=1}^{V} \exp \left(\boldsymbol{u}_{w}^{T} \boldsymbol{v}_{c}\right)} \frac{\partial \exp \left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)}{\partial \boldsymbol{u}_{o}}} \\ {=-\boldsymbol{v}_{c}+\frac{\exp \left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)}{\sum_{w=1}^{V} \exp \left(\boldsymbol{u}_{w}^{T} \boldsymbol{v}_{c}\right)} \boldsymbol{v}_{c}} \\ {=(P(O=o | C=c)-1) \boldsymbol{v}_{c}}\end{array}</script><p>if $w \ne o$, </p><script type="math/tex; mode=display">\begin{array}{c}{\frac{\partial logP(o | c)}{\partial \boldsymbol{u}_{w}}}\\ {=\frac{\exp \left(\boldsymbol{u}_{w}^{T} \boldsymbol{v}_{c}\right)}{\sum_{w=1}^{V} \exp \left(\boldsymbol{u}_{w}^{T} \boldsymbol{v}_{c}\right)} \boldsymbol{v}_{c}} \\ {=P(O=w | C=c) \boldsymbol{v}_{c}}\end{array}</script><p>Similarly, we can combine $P(O=o | C=c)-1$ and $P(O=o | C=c)$ as $\hat y - y$,</p><script type="math/tex; mode=display">\frac{\partial \log p(o | c)}{\partial u_{o}}=(\hat y-y) \cdot v_{c}</script><h2 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h2><p>You may have noticed that the skip-gram neural network contains a huge number of weights… For our example with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each! Training this on a large dataset would be prohibitive.</p><p>The main idea of negative sampling is to train binary logistic regressions for a true pair (center word and word in its context window) versus several noise pairs (the center word paired with a random word)</p><p>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works.</p><p>Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,..,w_k$ and their outside vectors as $u_1,u_2,…,u_k$. Note that $o \notin \{w_1,w_2,…,w_k\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:</p><script type="math/tex; mode=display">J_{\mathrm{neg}-\text { sample }}\left(\boldsymbol{v}_{c}, o, \boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right)</script><p>for a sample $w_1,…,w_k$, where $\sigma(\cdot)$ is the sigmoid function. The idea is to maximize probability that real outside word appears, minimize prob. that random words appear around center word.</p><h3 id="Gradient-1"><a href="#Gradient-1" class="headerlink" title="Gradient"></a>Gradient</h3><p><strong>For center word vector $v_c$:</strong></p><script type="math/tex; mode=display">\begin{array}{c}{\frac{\partial J_{\text {neg-sample}}\left(\boldsymbol{v}_{c}, o, \boldsymbol{U}\right)}{\partial \boldsymbol{v}_{c}} \log \left(\sigma\left(-\boldsymbol{u}_{k}^{T} \boldsymbol{v}_{c}\right)\right)} \\ {=\frac{\partial\left(-\log \left(\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-\boldsymbol{u}_{k}^{T} \boldsymbol{v}_{c}\right)\right)\right)}{\partial \boldsymbol{v}_{c}}} \\ {=-\frac{\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)\left(1-\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)\right)}{\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)} \frac{\partial \boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}}{\partial \boldsymbol{v}_{c}}-\sum_{k=1}^{K} \frac{\partial \log \left(\sigma\left(-\boldsymbol{u}_{k}^{T} \boldsymbol{v}_{c}\right)\right)}{\partial \boldsymbol{v}_{c}}} \\ {=-\left(1-\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)\right) \boldsymbol{u}_{o}+\sum_{k=1}^{K}\left(1-\sigma\left(-\boldsymbol{u}_{k}^{T} \boldsymbol{v}_{c}\right)\right) \boldsymbol{u}_{k}}\end{array}</script><p><strong>For outside target word vector $u_o$:</strong></p><script type="math/tex; mode=display">=\frac{\partial\left(-\log \left(\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)\right)\right.}{\partial \boldsymbol{u}_{o}}=-\left(1-\sigma\left(\boldsymbol{u}_{o}^{T} \boldsymbol{v}_{c}\right)\right) \boldsymbol{v}_{c}</script><p><strong>For sampling outside word vector $u_w$:</strong></p><script type="math/tex; mode=display">=\frac{\partial\left(-\log \left(\sigma\left(-\boldsymbol{u}_{k}^{T} \boldsymbol{v}_{c}\right)\right)\right.}{\partial \boldsymbol{u}_{k}}=\left(1-\sigma\left(-\boldsymbol{u}_{k}^{T} \boldsymbol{v}_{c}\right)\right) \boldsymbol{v}_{c}</script><h1 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">skipgram</span><span class="params">(currentCenterWord, windowSize, outsideWords, word2Ind,</span></span></div><div class="line"><span class="function"><span class="params">             centerWordVectors, outsideVectors, dataset,</span></span></div><div class="line"><span class="function"><span class="params">             word2vecLossAndGradient=naiveSoftmaxLossAndGradient)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">    :param currentCenterWord: one current center word</span></div><div class="line"><span class="string">    :param windowSize: integer, context window size</span></div><div class="line"><span class="string">    :param outsideWords: list of no more than 2*windowSize strings, the outside words</span></div><div class="line"><span class="string">    :param word2Ind: a dictionary that maps words to their indices in</span></div><div class="line"><span class="string">              the word vector list</span></div><div class="line"><span class="string">    :param centerWordVectors: center word vectors (as rows) for all words in vocab</span></div><div class="line"><span class="string">    :param outsideVectors: outside word vectors (as rows) for all words in vocab</span></div><div class="line"><span class="string">    :param dataset:</span></div><div class="line"><span class="string">    :param word2vecLossAndGradient: he loss and gradient function for</span></div><div class="line"><span class="string">                               a prediction vector given the outsideWordIdx</span></div><div class="line"><span class="string">                               word vectors, could be one of the two</span></div><div class="line"><span class="string">                               loss functions you implemented above.</span></div><div class="line"><span class="string">    :return:</span></div><div class="line"><span class="string">    loss -- the loss function value for the skip-gram model</span></div><div class="line"><span class="string">    gradCenterVecs -- the gradient with respect to the center word vectors</span></div><div class="line"><span class="string">    gradOutsideVectors -- the gradient with respect to the outside word vectors</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    loss = <span class="number">0.0</span></div><div class="line">    gradCenterVecs = np.zeros(centerWordVectors.shape)</div><div class="line">    gradOutsideVectors = np.zeros(outsideVectors.shape)</div><div class="line"></div><div class="line">    idx = word2Ind[currentCenterWord]</div><div class="line">    center_word_vector = centerWordVectors[idx]</div><div class="line">    <span class="keyword">for</span> outside_word <span class="keyword">in</span> outsideWords:</div><div class="line">        outside_word_idx = word2Ind[outside_word]</div><div class="line">        loss_, gradCenterVec_, gradOutsideVecs_ = word2vecLossAndGradient(center_word_vector, outside_word_idx,</div><div class="line">                                                                          outsideVectors, dataset)</div><div class="line"></div><div class="line">        loss += loss_</div><div class="line">        gradCenterVecs[idx] += gradCenterVec_</div><div class="line">        gradOutsideVectors += gradOutsideVecs_</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loss, gradCenterVecs, gradOutsideVectors</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naiveSoftmaxLossAndGradient</span><span class="params">(centerWordVec,outsideWordIdx,outsideVectors)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">    :param centerWordVec: shape=(D,)</span></div><div class="line"><span class="string">    :param outsideWordIdx: target word index</span></div><div class="line"><span class="string">    :param outsideVectors: shape=(W,D)</span></div><div class="line"><span class="string">    :return:</span></div><div class="line"><span class="string">    '''</span></div><div class="line"></div><div class="line">    W,D = outsideVectors.shape</div><div class="line">    scores = centerWordVec.dot(outsideVectors.T)</div><div class="line">    prob = softmax(scores)</div><div class="line"></div><div class="line">    loss = <span class="number">-1</span>*np.log(prob)[outsideWordIdx]</div><div class="line"></div><div class="line">    target = np.zeros(W)</div><div class="line">    target[outsideWordIdx] = <span class="number">1</span></div><div class="line">    gradCenterVec = outsideVectors.T.dot(prob-target)</div><div class="line">    gradOutsideVecs = (prob-target).reshape(<span class="number">-1</span>,<span class="number">1</span>).dot(centerWordVec.reshape((<span class="number">1</span>,<span class="number">-1</span>)))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">negSamplingLossAndGradient</span><span class="params">(centerWordVec,outsideWordIdx,outsideVectors,dataset,K=<span class="number">10</span>)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">    :param centerWordVec: size [D,]</span></div><div class="line"><span class="string">    :param outsideWordIdx: size [1,]</span></div><div class="line"><span class="string">    :param outsideVectors: size [W,D]</span></div><div class="line"><span class="string">    :param dataset:</span></div><div class="line"><span class="string">    :param K:</span></div><div class="line"><span class="string">    :return:</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)</div><div class="line"></div><div class="line">    indices = [outsideWordIdx] + negSampleWordIndices</div><div class="line"></div><div class="line">    gradCenterVec = np.zeros_like(centerWordVec)</div><div class="line">    gradOutsideVecs = np.zeros_like(outsideVectors)</div><div class="line">    loss = <span class="number">0</span></div><div class="line"></div><div class="line">    scores1 = centerWordVec.dot(outsideVectors[outsideWordIdx].T)</div><div class="line">    scores2 = np.dot(outsideVectors[outsideWordIdx],centerWordVec)</div><div class="line">    prob = sigmoid(scores2)</div><div class="line">    loss -= np.log(prob)</div><div class="line"></div><div class="line">    gradCenterVec += (prob<span class="number">-1</span>)*outsideVectors[outsideWordIdx]</div><div class="line">    gradOutsideVecs[outsideWordIdx] += (prob<span class="number">-1</span>)* centerWordVec</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</div><div class="line">        idx = indices[i+<span class="number">1</span>]</div><div class="line">        s = centerWordVec.dot(<span class="number">-1</span>*outsideVectors[idx].T)</div><div class="line">        p = sigmoid(s)</div><div class="line">        loss -= np.log(p)</div><div class="line"></div><div class="line">        gradCenterVec += (<span class="number">1</span>-p)*outsideVectors[idx]</div><div class="line">        gradOutsideVecs[idx] += (<span class="number">1</span>-p)*centerWordVec</div><div class="line"></div><div class="line">    <span class="keyword">return</span> loss, gradCenterVec, gradOutsideVecs</div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa" target="_blank" rel="noopener">Introduction to Word Embedding and Word2Vec</a> </p><p><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="noopener">Word2Vec Tutorial - The Skip-Gram Model</a> </p><p><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec</a> - calculation examples</p><p><a href="https://www.cnblogs.com/iloveai/p/cs224d-lecture2-note.html" target="_blank" rel="noopener">Stanford CS224d) Deep Learning and NLP课程笔记（二）：word2vec</a></p>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PlanOne</title>
      <link href="/2019/05/28/PlanOne/"/>
      <url>/2019/05/28/PlanOne/</url>
      <content type="html"><![CDATA[<p>The overall goal is to learn the following stuff:</p><ul><li>CS231N-Convolutional Neural Networks for Visual Recognition</li><li>CS224n: Natural Language Processing with Deep Learning</li><li>Linear Algebra <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/" target="_blank" rel="noopener">ref</a> <a href="https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/resource-index/" target="_blank" rel="noopener">resources</a> </li><li>Probability and Statistics <a href="">ref</a> </li><li>Probabilistic Graphical Models Specialization <a href="https://www.bilibilijj.com/video/av17504453/" target="_blank" rel="noopener">ref</a>  <a href="https://sailinglab.github.io/pgm-spring-2019/" target="_blank" rel="noopener">cmu</a></li><li>Statistics for Applications <a href="https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/" target="_blank" rel="noopener">ref</a> </li><li>Convex Optimization <a href="https://see.stanford.edu/Course/EE364A" target="_blank" rel="noopener">website</a> <a href="https://www.youtube.com/playlist?list=PL3940DD956CDF0622" target="_blank" rel="noopener">video</a> <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener">textbook</a> </li></ul><h1 id="Week-1-28-May-2-June"><a href="#Week-1-28-May-2-June" class="headerlink" title="Week 1 (28 May - 2 June)"></a>Week 1 (28 May - 2 June)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture2 - Image Classification</del></li><li><del>CS224n : Introduction and Word Vectors</del></li></ol><h1 id="Week-2-3-June-9-June"><a href="#Week-2-3-June-9-June" class="headerlink" title="Week 2 (3 June - 9 June)"></a>Week 2 (3 June - 9 June)</h1><ol><li><del>CS231N : Lecture3 - Loss Functions and Optimization</del> </li><li><del>CS224n : Word Vectors 2 and Word Senses</del></li></ol><h1 id="Week-3-10-June-16-June"><a href="#Week-3-10-June-16-June" class="headerlink" title="Week 3 (10 June - 16 June)"></a>Week 3 (10 June - 16 June)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture4 - Introduction to Neural Networks</del> </li><li><del>CS224n : Word Window Classification, Neural Networks, and Matrix Calculus</del> </li></ol><h1 id="Week-4-17-June-23-June"><a href="#Week-4-17-June-23-June" class="headerlink" title="Week 4 (17 June - 23 June)"></a>Week 4 (17 June - 23 June)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture5 - Convolutional Neural Networks</del> </li><li><del>CS224n : Backpropagation and Computation Graphs</del> </li></ol><h1 id="Week-5-24-June-30-June"><a href="#Week-5-24-June-30-June" class="headerlink" title="Week 5 (24 June - 30 June)"></a>Week 5 (24 June - 30 June)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture6 - Training Neural Networks, part I</del> </li><li><del>CS224n : The probability of a sentence? Recurrent Neural Networks and Language Models</del></li></ol><h1 id="Week-6-1-July-7-July"><a href="#Week-6-1-July-7-July" class="headerlink" title="Week 6 (1 July - 7 July)"></a>Week 6 (1 July - 7 July)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture7 - Training Neural Networks, part II</del></li><li>CS224n : Vanishing Gradients and Fancy RNNs </li></ol><h1 id="Week-7-8-July-14-July"><a href="#Week-7-8-July-14-July" class="headerlink" title="Week 7 (8 July - 14 July)"></a>Week 7 (8 July - 14 July)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture9 - CNN Architectures</del></li><li><del>CS224n : Machine Translation, Seq2Seq and Attention</del></li></ol><h1 id="Week-8-15-July-21-July"><a href="#Week-8-15-July-21-July" class="headerlink" title="Week 8 (15 July - 21 July)"></a>Week 8 (15 July - 21 July)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture10 - Recurrent Neural Networks</del></li><li>CS224n : Question Answering and the Default Final Project</li></ol><h1 id="Week-9-22-July-28-July"><a href="#Week-9-22-July-28-July" class="headerlink" title="Week 9 (22 July - 28 July)"></a>Week 9 (22 July - 28 July)</h1><p>The goal is :: </p><ol><li>CS231N : Lecture11 - Detection and Segmentation</li><li><del>CS224n : ConvNets for NLP</del> </li></ol><h1 id="Week-10-29-July-4-Aug"><a href="#Week-10-29-July-4-Aug" class="headerlink" title="Week 10 (29 July - 4 Aug)"></a>Week 10 (29 July - 4 Aug)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture12 - Visualizing and Understanding</del></li><li><del>CS224n : Information from parts of words: Subword Models</del></li></ol><h1 id="Week-11-5-Aug-11-Aug"><a href="#Week-11-5-Aug-11-Aug" class="headerlink" title="Week 11 (5 Aug - 11 Aug)"></a>Week 11 (5 Aug - 11 Aug)</h1><p>The goal is :: </p><ol><li><del>CS231N : Lecture13 - Generative Models</del></li><li><del>CS224n : Modeling contexts of use: Contextual Representations and Pretraining</del> </li></ol><h1 id="Week-12-12-Aug-18-Aug"><a href="#Week-12-12-Aug-18-Aug" class="headerlink" title="Week 12 (12 Aug - 18 Aug)"></a>Week 12 (12 Aug - 18 Aug)</h1><p>The goal is :: </p><ol><li>​</li><li><del>CS224n: Lecture 14  Transformers and Self-Attention</del></li></ol><h1 id="Week-13-19-Aug-15-Aug"><a href="#Week-13-19-Aug-15-Aug" class="headerlink" title="Week 13 (19 Aug - 15 Aug)"></a>Week 13 (19 Aug - 15 Aug)</h1><p>The goal is :: </p><ol><li>​</li><li><del>CS224n: Lecture 15 Natural Language Generation</del></li></ol><h1 id="Week-14"><a href="#Week-14" class="headerlink" title="Week 14"></a>Week 14</h1><p>The goal is :: </p><ol><li>​</li><li><del>CS224n: Lecture 16 – Coreference Resolution</del></li></ol><h1 id="Week-15"><a href="#Week-15" class="headerlink" title="Week 15"></a>Week 15</h1><p>The goal is :: </p><ol><li>​</li><li>CS224n: Lecture 18 – Constituency Parsing, TreeRNNs</li></ol><h1 id="Week-16"><a href="#Week-16" class="headerlink" title="Week 16"></a>Week 16</h1><p>The goal is :: </p><ol><li>​</li><li>​</li></ol>]]></content>
      
      <categories>
          
          <category> Plan </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Plan </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-PyTorch</title>
      <link href="/2019/04/22/DP-PyTorch/"/>
      <url>/2019/04/22/DP-PyTorch/</url>
      <content type="html"><![CDATA[<p>Tutorials of PyTorch and some useful tips.</p><a id="more"></a><h1 id="Pytorch-101"><a href="#Pytorch-101" class="headerlink" title="Pytorch 101"></a>Pytorch 101</h1><h2 id="GPU-vs-CPU"><a href="#GPU-vs-CPU" class="headerlink" title="GPU vs CPU"></a>GPU vs CPU</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> torch.cuda.is_available():</div><div class="line">    device = torch.device(<span class="string">'cuda'</span>)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    device = torch.device(<span class="string">'cpu'</span>)</div><div class="line"><span class="comment"># Move the data to the proper device (GPU or CPU)</span></div><div class="line">x = x.to(device=device, dtype=dtype)</div><div class="line">y = y.to(device=device, dtype=torch.long)</div><div class="line">model = model.to(device=device)  <span class="comment"># move the model parameters to CPU/GPU</span></div></pre></td></tr></table></figure></div></div><h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_weight</span><span class="params">(shape)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Create random Tensors for weights; setting requires_grad=True means that we</span></div><div class="line"><span class="string">    want to compute gradients for these Tensors during the backward pass.</span></div><div class="line"><span class="string">    We use Kaiming normalization: sqrt(2 / fan_in)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">if</span> len(shape) == <span class="number">2</span>:  <span class="comment"># FC weight</span></div><div class="line">        fan_in = shape[<span class="number">0</span>]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        fan_in = np.prod(shape[<span class="number">1</span>:]) <span class="comment"># conv weight [out_channel, in_channel, kH, kW]</span></div><div class="line">    <span class="comment"># randn is standard normal distribution generator. </span></div><div class="line">    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(<span class="number">2.</span> / fan_in)</div><div class="line">    w.requires_grad = <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> w</div><div class="line"><span class="comment"># nn.init.kaiming_normal_(self.fc.weight)</span></div><div class="line">----------------------------------------------------------</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_weight</span><span class="params">(shape)</span>:</span></div><div class="line">    <span class="keyword">return</span> torch.zeros(shape, device=device, dtype=dtype, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># nn.init.constant_(self.conv1.bias, 0)</span></div></pre></td></tr></table></figure></div></div><h1 id="The-basics"><a href="#The-basics" class="headerlink" title="The basics"></a>The basics</h1><p>In this section, we’ll go through the basic ideas of PyTorch starting at tensors and computational graphs and finishing at the Variable class and the PyTorch autograd functionality.</p><h2 id="Computational-graphs"><a href="#Computational-graphs" class="headerlink" title="Computational graphs"></a>Computational graphs</h2><p><a href="https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">link</a></p><p>The first thing to understand about any deep learning library is the idea of a computational graph. A computational graph is a set of calculations, which are called <em>nodes</em>, and these nodes are connected in a directional ordering of computation. In other words, some nodes are dependent on other nodes for their input, and these nodes in turn output the results of their calculations to other nodes. A simple example of a computational graph for the calculation $a=(b+c)*(c+2)$ can be seen below – we can break this calculation up into the following steps/nodes:</p><p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.21.07 AM.png" alt="creen Shot 2019-07-13 at 11.21.07 A"></p><p>The benefits of using a computational graph is that each node is like its own independently functioning piece of code (once it receives all its required inputs). This allows various performance optimizations to be performed in running the calculations such as threading and multiple processing / parallelism. All the major deep learning frameworks (TensorFlow, Theano, PyTorch etc.) involve constructing such computational graphs, through which neural network operations can be built and through which gradients can be back-propagated.</p><h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p>Tensors are matrix-like data structures which are essential components in deep learning libraries and efficient computation. Graphical Processing Units (GPUs) are especially effective at calculating operations between tensors, and this has spurred the surge in deep learning capability in recent times. In PyTorch, tensors can be declared simply in a number of ways:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line">x = torch.Tensor(<span class="number">2</span>,<span class="number">3</span>)</div></pre></td></tr></table></figure><p>This code creates a tensor of size (2, 3) – i.e. 2 rows and 3 columns, filled with zero float values.</p><p>We can also create tensors filled random float values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>Multiplying tensors, adding them and so forth is straight-forward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</div><div class="line">y = torch.ones(<span class="number">2</span>,<span class="number">3</span>) * <span class="number">2</span></div><div class="line">print(x + y)</div><div class="line"><span class="comment"># 3 3 3</span></div><div class="line"><span class="comment"># 3 3 3</span></div></pre></td></tr></table></figure><p>Another great thing is the numpy slice functionality that is available – for instance y[:, 1]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">y[:,<span class="number">1</span>] = y[:,<span class="number">1</span>] + <span class="number">1</span></div><div class="line"><span class="comment"># 2 3 2</span></div><div class="line"><span class="comment"># 2 3 2</span></div></pre></td></tr></table></figure><p><strong>Numpy Bridge</strong></p><p>Converting a Torch Tensor to a NumPy array and vice versa is a breeze. <strong>The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other.</strong></p><p>Converting a Torch Tensor to a NumPy Array</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">a = torch.ones(<span class="number">5</span>)</div><div class="line">print(a) <span class="comment">#tensor([1., 1., 1., 1., 1.])</span></div><div class="line">b = a.numpy()</div><div class="line">print(b) <span class="comment">#[1. 1. 1. 1. 1.]</span></div><div class="line">a.add_(<span class="number">1</span>)</div><div class="line">print(a) <span class="comment">#tensor([2., 2., 2., 2., 2.])</span></div><div class="line">print(b) <span class="comment">#[2. 2. 2. 2. 2.]</span></div></pre></td></tr></table></figure><p>Converting NumPy Array to Torch Tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.ones(<span class="number">5</span>)</div><div class="line">b = torch.from_numpy(a)</div><div class="line">np.add(a, <span class="number">1</span>, out=a)</div><div class="line">print(a) <span class="comment">#[2. 2. 2. 2. 2.]</span></div><div class="line">print(b) <span class="comment">#tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></div></pre></td></tr></table></figure><p><code>torch.Tensor</code> is the central class of the package. If you set its attribute <code>.requires_grad</code> as <code>True</code>, it starts to track all operations on it. When you finish your computation you can call <code>.backward()</code> and have all the gradients computed automatically. The gradient for this tensor will be accumulated into <code>.grad</code> attribute.</p><p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.37.00 AM.png" alt="creen Shot 2019-07-13 at 11.37.00 A"></p><p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.37.25 AM.png" alt="creen Shot 2019-07-13 at 11.37.25 A"></p><p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-07-13 at 11.40.45 AM.png" alt="creen Shot 2019-07-13 at 11.40.45 A"></p><h2 id="Autograd-in-Pytorch"><a href="#Autograd-in-Pytorch" class="headerlink" title="Autograd in Pytorch"></a>Autograd in Pytorch</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank" rel="noopener">link</a></p><p>n any deep learning library, there needs to be a mechanism where error gradients are calculated and back-propagated through the computational graph. This mechanism, called autograd in PyTorch. Pytorch allows automatic gradient computation on the tensor when the .backward() function is called. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line">print(x) </div><div class="line"><span class="comment"># tensor([[1., 1.],</span></div><div class="line"><span class="comment">#         [1., 1.]],requires_grad=True)</span></div></pre></td></tr></table></figure><p>do a tensor operation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">y = x+<span class="number">2</span></div><div class="line">print(y)</div><div class="line"><span class="comment">#tensor([[3., 3.],</span></div><div class="line"><span class="comment">#        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span></div></pre></td></tr></table></figure><p><code>y</code> was created as a result of an operation, so it has a <code>grad_fn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(y.grad_fn) </div><div class="line"><span class="comment"># &lt;AddBackward0 object at 0x7f27857f7c88&gt;</span></div></pre></td></tr></table></figure><p>do more operations on y:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">z = y * y * <span class="number">3</span></div><div class="line">out = z.mean()</div><div class="line">print(z, out)</div><div class="line"><span class="comment">#tensor([[27., 27.],</span></div><div class="line"><span class="comment">#       [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span></div></pre></td></tr></table></figure><p><strong>Gradients</strong></p><p>Let’s backprop now. <strong>Because <code>out</code> contains a single scalar, <code>out.backward()</code> is equivalent to <code>out.backward(torch.tensor(1.))</code>.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">out.backward()</div></pre></td></tr></table></figure><p>Print gradients d(out)/dx</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">print(x.grad)</div><div class="line"><span class="comment">#tensor([[4.5000, 4.5000],</span></div><div class="line"><span class="comment">#        [4.5000, 4.5000]])</span></div></pre></td></tr></table></figure><h1 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a><a href="https://pytorch.org/docs/stable/torch.html#" target="_blank" rel="noopener">Torch</a></h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>The torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. </p><p><img src="/2019/04/22/DP-PyTorch/Screen Shot 2019-05-18 at 11.57.01 AM.png" alt="creen Shot 2019-05-18 at 11.57.01 A"></p><h3 id="Creation-Ops"><a href="#Creation-Ops" class="headerlink" title="Creation Ops"></a>Creation Ops</h3><h4 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.tensor(data, dtype=<span class="keyword">None</span>, device=<span class="keyword">None</span>, requires_grad=<span class="keyword">False</span>, pin_memory=<span class="keyword">False</span>) → Tensor</div></pre></td></tr></table></figure><p>Constructs a tensor with <code>data</code>.</p><h4 id="topk"><a href="#topk" class="headerlink" title="topk"></a>topk</h4><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)</div></pre></td></tr></table></figure><p>Returns the <code>k</code> largest elements of the given <code>input</code> tensor along a given dimension.</p><p>If <code>dim</code> is not given, the last dimension of the input is chosen.</p><p>If <code>largest</code> is <code>False</code> then the k smallest elements are returned.</p><p>A namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.</p><h4 id="view"><a href="#view" class="headerlink" title="view"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" target="_blank" rel="noopener">view</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">view(*shape) → Tensor</div></pre></td></tr></table></figure><p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</p><h4 id="size"><a href="#size" class="headerlink" title="size"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size" target="_blank" rel="noopener">size</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">size() → torch.Size</div></pre></td></tr></table></figure><p>Returns the size of the <code>self</code> tensor. The returned value is a subclass of <a href="https://docs.python.org/3/library/stdtypes.html#tuple" target="_blank" rel="noopener"><code>tuple</code></a>.</p><h4 id="div"><a href="#div" class="headerlink" title="div"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.div" target="_blank" rel="noopener">div</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">div_(value) → Tensor</div></pre></td></tr></table></figure><p>Divides each element of the input <code>input</code> with the scalar <code>value</code> and returns a new resulting tensor.</p><h4 id="mm"><a href="#mm" class="headerlink" title="mm"></a><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mm" target="_blank" rel="noopener">mm</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mm(mat2) → Tensor</div></pre></td></tr></table></figure><p>matrxi multiply. see <a href="https://pytorch.org/docs/stable/torch.html#torch.mm" target="_blank" rel="noopener">torch.mm()</a>. </p><h4 id="torch-from-numpy"><a href="#torch-from-numpy" class="headerlink" title="torch.from_numpy"></a><a href="https://pytorch.org/docs/stable/torch.html#torch.from_numpy" target="_blank" rel="noopener">torch.from_numpy</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.from_numpy(ndarray) → Tensor</div></pre></td></tr></table></figure><p>Creates a <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><code>Tensor</code></a> from a <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" target="_blank" rel="noopener"><code>numpy.ndarray</code></a>.</p><p>The returned tensor and <code>ndarray</code> share the same memory. Modifications to the tensor will be reflected in the <code>ndarray</code> and vice versa. The returned tensor is not resizable.</p><h2 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h2><p><code>torch.autograd</code> provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. </p><h3 id="Variable-deprecated"><a href="#Variable-deprecated" class="headerlink" title="Variable (deprecated)"></a>Variable (deprecated)</h3><p>The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with <code>requires_grad</code> set to <code>True</code>. Below please find a quick guide on what has changed:</p><ul><li><code>Variable(tensor)</code> and <code>Variable(tensor, requires_grad)</code> still work as expected, but they return Tensors instead of Variables.</li><li><code>var.data</code> is the same thing as <code>tensor.data</code>.</li><li>Methods such as <code>var.backward(), var.detach(), var.register_hook()</code> now work on tensors with the same method names.</li></ul><h2 id="max"><a href="#max" class="headerlink" title="max"></a>max</h2><ol><li><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.max(input) -&gt; Tensor</div></pre></td></tr></table></figure><p>Returns the maximum value of all elements in the <code>input</code> tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a</div><div class="line">tensor([[ <span class="number">0.6763</span>,  <span class="number">0.7445</span>, <span class="number">-2.2369</span>]])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a)</div><div class="line">tensor(<span class="number">0.7445</span>)</div></pre></td></tr></table></figure></li><li><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)</div></pre></td></tr></table></figure><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the maximum value of each row of the <code>input</code> tensor in the given dimension <code>dim</code>. And <code>indices</code> is the index location of each maximum value found (argmax).</p><p>If <code>keepdim</code> is <code>True</code>, the output tensors are of the same size as <code>input</code> except in the dimension <code>dim</code> where they are of size 1. Otherwise, <code>dim</code> is squeezed (see <a href="https://pytorch.org/docs/stable/torch.html#torch.squeeze" target="_blank" rel="noopener"><code>torch.squeeze()</code></a>), resulting in the output tensors having 1 fewer dimension than <code>input</code>.</p><ul><li><strong>input</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the input tensor</li><li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) – the dimension to reduce</li><li><strong>keepdim</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – whether the output tensors have <code>dim</code> retained or not. Default: <code>False</code>.</li><li><strong>out</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" target="_blank" rel="noopener"><em>tuple</em></a><em>,</em> <em>optional</em>) – the result tuple of two output tensors (max, max_indices)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a</div><div class="line">tensor([[<span class="number">-1.2360</span>, <span class="number">-0.2942</span>, <span class="number">-0.1222</span>,  <span class="number">0.8475</span>],</div><div class="line">        [ <span class="number">1.1949</span>, <span class="number">-1.1127</span>, <span class="number">-2.2379</span>, <span class="number">-0.6702</span>],</div><div class="line">        [ <span class="number">1.5717</span>, <span class="number">-0.9207</span>,  <span class="number">0.1297</span>, <span class="number">-1.8768</span>],</div><div class="line">        [<span class="number">-0.6172</span>,  <span class="number">1.0036</span>, <span class="number">-0.6060</span>, <span class="number">-0.2432</span>]])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a, <span class="number">1</span>)</div><div class="line">torch.return_types.max(values=tensor([<span class="number">0.8475</span>, <span class="number">1.1949</span>, <span class="number">1.5717</span>, <span class="number">1.0036</span>]), indices=tensor([<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]))</div></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.max(input, other, out=<span class="keyword">None</span>) → Tensor</div></pre></td></tr></table></figure><p>Each element of the tensor <code>input</code> is compared with the corresponding element of the tensor <code>other</code> and an element-wise maximum is taken.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>a</div><div class="line">tensor([ <span class="number">0.2942</span>, <span class="number">-0.7416</span>,  <span class="number">0.2653</span>, <span class="number">-0.1584</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div><div class="line">tensor([ <span class="number">0.8722</span>, <span class="number">-1.7421</span>, <span class="number">-0.4141</span>, <span class="number">-0.5055</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.max(a, b)</div><div class="line">tensor([ <span class="number">0.8722</span>, <span class="number">-0.7416</span>,  <span class="number">0.2653</span>, <span class="number">-0.1584</span>])</div></pre></td></tr></table></figure></li></ol><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a><a href="https://pytorch.org/docs/stable/torch.html#torch.cat" target="_blank" rel="noopener">cat</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.cat(tensors, dim=<span class="number">0</span>, out=<span class="keyword">None</span>) → Tensor</div></pre></td></tr></table></figure><h2 id="multinomial"><a href="#multinomial" class="headerlink" title="multinomial"></a><a href="https://pytorch.org/docs/stable/torch.html#torch.multinomial" target="_blank" rel="noopener">multinomial</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.multinomial(input, num_samples, replacement=<span class="keyword">False</span>, out=<span class="keyword">None</span>) → LongTensor</div></pre></td></tr></table></figure><blockquote><p><a href="https://blog.csdn.net/monchin/article/details/79787621" target="_blank" rel="noopener">understanding</a> </p></blockquote><h2 id="nn"><a href="#nn" class="headerlink" title="nn"></a>nn</h2><h3 id="functional"><a href="#functional" class="headerlink" title="functional"></a><a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">functional</a></h3><h3 id="Linear-layers"><a href="#Linear-layers" class="headerlink" title="Linear layers"></a><a href="https://pytorch.org/docs/stable/nn.html#linear-layers" target="_blank" rel="noopener">Linear layers</a></h3><h4 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.Linear(in_features, out_features, bias=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>Applies a linear transformation to the incoming data: $y=x A^{T}+b$.</p><ul><li><strong>in_features</strong> – size of each input sample</li><li><strong>out_features</strong> – size of each output sample</li><li><strong>bias</strong> – If set to <code>False</code>, the layer will not learn an additive bias. Default: <code>True</code></li></ul><p><strong>Variables</strong> - Linear<strong>.weight</strong> and <strong>Linear.bias</strong> </p><h3 id="Convolution-layers"><a href="#Convolution-layers" class="headerlink" title="Convolution layers"></a><a href="https://pytorch.org/docs/stable/nn.html#convolution-layers" target="_blank" rel="noopener">Convolution layers</a></h3><h4 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a><a href="https://pytorch.org/docs/stable/nn.html#conv2d" target="_blank" rel="noopener">Conv2d</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="keyword">True</span>, padding_mode=<span class="string">'zeros'</span>)</div></pre></td></tr></table></figure><ul><li><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">link</a> has a nice visualization of what <code>dilation</code> does.</li></ul><p><strong>Variables</strong> - <strong>Conv2d.weight</strong> and <strong>Conv2d.bias</strong> </p><h3 id="Non-linear-activations"><a href="#Non-linear-activations" class="headerlink" title="Non-linear activations"></a><a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" target="_blank" rel="noopener">Non-linear activations</a></h3><h4 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.ReLU(inplace=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.Softmax(dim=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><script type="math/tex; mode=display">\operatorname{Softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}\</script><h3 id="LogSoftmax"><a href="#LogSoftmax" class="headerlink" title="LogSoftmax"></a>LogSoftmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.LogSoftmax(dim=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor. </p><h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.CrossEntropyLoss(weight=<span class="keyword">None</span>, size_average=<span class="keyword">None</span>, ignore_index=<span class="number">-100</span>, reduce=<span class="keyword">None</span>, reduction=<span class="string">'mean'</span>)</div></pre></td></tr></table></figure><p>This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.</p><blockquote><p>Therefore, in network architecture, we should not define a softmax layer.</p></blockquote><h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss()"></a>NLLLoss()</h3><blockquote><p>go with LogSoftmax.</p></blockquote><h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding" target="_blank" rel="noopener">Embedding</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="keyword">None</span>, max_norm=<span class="keyword">None</span>, norm_type=<span class="number">2.0</span>, scale_grad_by_freq=<span class="keyword">False</span>, sparse=<span class="keyword">False</span>, _weight=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p><strong>Shape</strong></p><ul><li>Input: (*)(∗), LongTensor of arbitrary shape containing the indices to extract</li><li>Output: (<em>, H)(∗,H), where </em> is the input shape and $H=\text{embedding_dim}$</li></ul><p><strong>Parameters</strong></p><ul><li>num_embeddings(int) - </li><li>embedding_dim(int) - size of each embedding vector</li><li>padding_idx(int,optional) - If given, pads the output with the embedding vector at <code>padding_idx</code>(initialized to zeros) whenever it encounters the index. <a href="http://www.linzehui.me/2018/08/19/%E7%A2%8E%E7%89%87%E7%9F%A5%E8%AF%86/%E5%85%B3%E4%BA%8EPytorch%E4%B8%ADEmbedding%E7%9A%84padding/" target="_blank" rel="noopener">understanding</a> </li></ul><blockquote><p><a href="https://stackoverflow.com/questions/50747947/embedding-in-pytorch" target="_blank" rel="noopener">understanding</a>  </p></blockquote><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.GRU" target="_blank" rel="noopener">GRU</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.nn.GRU(*args, **kwargs)</div></pre></td></tr></table></figure><p><strong>Parameters</strong></p><ul><li>input_size - The number of expected features in the input x</li><li>hidden_szie - The number of features in the hidden state h</li><li>num_layers - Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li></ul><h2 id="optim"><a href="#optim" class="headerlink" title="optim"></a><a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">optim</a></h2><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" target="_blank" rel="noopener">Adam</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>, amsgrad=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><h2 id="Utils"><a href="#Utils" class="headerlink" title="[Utils]"></a>[Utils]</h2><h3 id="DATA"><a href="#DATA" class="headerlink" title="DATA"></a><a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener">DATA</a></h3><h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" target="_blank" rel="noopener">torch.utils.data.DataLoader</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.utils.data.DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="keyword">False</span>, sampler=<span class="keyword">None</span>, batch_sampler=<span class="keyword">None</span>, num_workers=<span class="number">0</span>, collate_fn=&lt;function default_collate&gt;, pin_memory=<span class="keyword">False</span>, drop_last=<span class="keyword">False</span>, timeout=<span class="number">0</span>, worker_init_fn=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" target="_blank" rel="noopener"><em>Dataset</em></a>) – dataset from which to load the data.</li><li><strong>batch_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – how many samples per batch to load (default: <code>1</code>).</li><li><strong>shuffle</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – set to <code>True</code> to have the data reshuffled at every epoch (default: <code>False</code>).</li><li><strong>sampler</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler" target="_blank" rel="noopener"><em>Sampler</em></a><em>,</em> <em>optional</em>) – defines the strategy to draw samples from the dataset. If specified, <code>shuffle</code>must be False.</li><li><strong>batch_sampler</strong> (<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler" target="_blank" rel="noopener"><em>Sampler</em></a><em>,</em> <em>optional</em>) – like sampler, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code>.</li><li><strong>num_workers</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: <code>0</code>)</li></ul><h4 id="torch-utils-data-Sampler"><a href="#torch-utils-data-Sampler" class="headerlink" title="torch.utils.data.Sampler"></a><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler" target="_blank" rel="noopener">torch.utils.data.Sampler</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.utils.data.Sampler(data_source)</div></pre></td></tr></table></figure><p>Base class for all Samplers.</p><p>Every Sampler subclass has to provide an <strong>iter</strong> method, providing a way to iterate over indices of dataset elements, and a <strong>len</strong> method that returns the length of the returned iterators.</p><h4 id="torch-utils-data-SubsetRandomSampler"><a href="#torch-utils-data-SubsetRandomSampler" class="headerlink" title="torch.utils.data.SubsetRandomSampler"></a><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler" target="_blank" rel="noopener">torch.utils.data.SubsetRandomSampler</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.utils.data.SubsetRandomSampler(indices)</div></pre></td></tr></table></figure><p>Samples elements randomly from a given list of indices, without replacement.</p><ul><li><strong>indices</strong> (<em>sequence</em>) – a sequence of indices</li></ul><h1 id="Torchvision"><a href="#Torchvision" class="headerlink" title="Torchvision"></a>Torchvision</h1><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a><a href="https://pytorch.org/docs/stable/torchvision/models.html" target="_blank" rel="noopener">Models</a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</div><div class="line">resnet18 = models.resnet18(pretrained=<span class="keyword">True</span>)</div><div class="line">alexnet = models.alexnet(pretrained=<span class="keyword">True</span>)</div><div class="line">squeezenet = models.squeezenet1_0(pretrained=<span class="keyword">True</span>)</div><div class="line">vgg16 = models.vgg16(pretrained=<span class="keyword">True</span>)</div><div class="line">densenet = models.densenet161(pretrained=<span class="keyword">True</span>)</div><div class="line">inception = models.inception_v3(pretrained=<span class="keyword">True</span>)</div><div class="line">googlenet = models.googlenet(pretrained=<span class="keyword">True</span>)</div><div class="line">shufflenet = models.shufflenetv2(pretrained=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>. You can use the following transform to normalize:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</div><div class="line">                                 std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div></pre></td></tr></table></figure><h3 id="Stop-undating"><a href="#Stop-undating" class="headerlink" title="Stop-undating"></a>Stop-undating</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Download and load the pretrained SqueezeNet model.</span></div><div class="line">model = torchvision.models.squeezenet1_1(pretrained=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># We don't want to train the model, so tell PyTorch not to compute gradients</span></div><div class="line"><span class="comment"># with respect to model parameters.</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div><div class="line">    param.requires_grad = <span class="keyword">False</span></div></pre></td></tr></table></figure><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a><a href="https://pytorch.org/docs/stable/torchvision/datasets.html" target="_blank" rel="noopener">Datasets</a></h2><p>All datasets are subclasses of <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" target="_blank" rel="noopener"><code>torch.utils.data.Dataset</code></a> i.e, they have <code>__getitem__</code> and <code>__len__</code> methods implemented. Hence, they can all be passed to a <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" target="_blank" rel="noopener"><code>torch.utils.data.DataLoader</code></a> which can load multiple samples parallelly using <code>torch.multiprocessing</code> workers. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">imagenet_data = torchvision.datasets.ImageNet(<span class="string">'path/to/imagenet_root/'</span>)</div><div class="line">data_loader = torch.utils.data.DataLoader(imagenet_data,</div><div class="line">                                          batch_size=<span class="number">4</span>,</div><div class="line">                                          shuffle=<span class="keyword">True</span>,</div><div class="line">                                          num_workers=args.nThreads)</div></pre></td></tr></table></figure><p>The following datasets are available: MNIST, COCO (Captions and Detection), LSUN, ImageNet, CIFAR etc.</p><p>All the datasets have almost similar API. They all have two common arguments: <code>transform</code> and <code>target_transform</code> to transform the input and target respectively.</p><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a><a href="https://pytorch.org/docs/stable/torchvision/datasets.html#mnist" target="_blank" rel="noopener">MNIST</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torchvision.datasets.MNIST(root, train=<span class="keyword">True</span>, transform=<span class="keyword">None</span>, target_transform=<span class="keyword">None</span>, download=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><ul><li><strong>root</strong> (<em>string</em>) – Root directory of dataset where <code>MNIST/processed/training.pt</code> and<code>MNIST/processed/test.pt</code> exist.</li><li><strong>train</strong> (bool, <em>optional</em>) – If True, creates dataset from <code>training.pt</code>, otherwise from <code>test.pt</code>.</li><li><strong>download</strong> (bool, <em>optional</em>) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li><li><strong>transform</strong> (callable, <em>optional</em>) – A function/transform that takes in an PIL image and returns a transformed version. E.g, <code>transforms.RandomCrop</code></li><li><strong>target_transform</strong> (callable, <em>optional</em>) – A function/transform that takes in the target and transforms it.</li></ul><h1 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h1><h3 id="Compose"><a href="#Compose" class="headerlink" title="Compose"></a>Compose</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torchvision.transforms.Compose(transforms)</div></pre></td></tr></table></figure><p>Composes several transforms together.</p><ul><li><strong>transforms</strong> (list of <code>Transform</code> objects) – list of transforms to compose.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">transforms.Compose([</div><div class="line">   transforms.CenterCrop(<span class="number">10</span>),</div><div class="line">   transforms.ToTensor(),</div><div class="line">])</div></pre></td></tr></table></figure><h3 id="Resize"><a href="#Resize" class="headerlink" title="Resize"></a>Resize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torchvision.transforms.Resize(size, interpolation=<span class="number">2</span>)</div></pre></td></tr></table></figure><p>Resize the input PIL Image to the given size.</p><ul><li><strong>size</strong> (<em>sequence</em> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>) – Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size)</li><li><strong>interpolation</strong> (<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a><em>,</em> <em>optional</em>) – Desired interpolation. Default is <code>PIL.Image.BILINEAR</code></li></ul><blockquote><p><code>torchvision.transforms.``Scale</code>(<strong>args*, </strong>*kwargs*) is deprecated in favor of Resize.</p></blockquote><h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torchvision.transforms.ToTensor()</div></pre></td></tr></table></figure><p>Convert a <code>PIL Image</code> or <code>numpy.ndarray</code> to tensor.</p><h3 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize"></a>Normalize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torchvision.transforms.Normalize(mean, std, inplace=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>Normalize a tensor image with mean and standard deviation. Given mean: <code>(M1,...,Mn)</code> and std: <code>(S1,..,Sn)</code> for <code>n</code>channels, this transform will normalize each channel of the input <code>torch.*Tensor</code> i.e. <code>input[channel] =(input[channel] - mean[channel]) / std[channel]</code></p><blockquote><p>This transform acts out of place, i.e., it does not mutates the input tensor.</p></blockquote><h3 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torchvision.transforms.Lambda(lambd)</div></pre></td></tr></table></figure><p>Apply a user-defined lambda as a transform.</p><ul><li><strong>lambd</strong> (<em>function</em>) – Lambda/function to be used for transform.</li></ul><h1 id="HyperParameters-Search"><a href="#HyperParameters-Search" class="headerlink" title="HyperParameters Search"></a>HyperParameters Search</h1><p><a href="https://github.com/ray-project/ray/tree/master/python/ray/tune/examples" target="_blank" rel="noopener">source</a> </p><h1 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h1><p><a href="http://visualdl.paddlepaddle.org/documentation/visualdl/en/develop/getting_started/demo/pytorch/TUTORIAL_EN.html" target="_blank" rel="noopener">source</a></p><h1 id="Pytorch-in-Practice"><a href="#Pytorch-in-Practice" class="headerlink" title="Pytorch in Practice"></a>Pytorch in Practice</h1><h2 id="Char-Level-Names-Classification"><a href="#Char-Level-Names-Classification" class="headerlink" title="Char_Level Names Classification"></a>Char_Level Names Classification</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> string,unicodedata</div><div class="line"><span class="keyword">import</span> glob</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">all_letters = string.ascii_letters + <span class="string">" .,;'"</span></div><div class="line">n_letters = len(all_letters)</div><div class="line">print(n_letters, all_letters)</div><div class="line"></div><div class="line"><span class="comment"># Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicode_to_ascii</span><span class="params">(s)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(</div><div class="line">        c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">'NFD'</span>, s)</div><div class="line">        <span class="keyword">if</span> unicodedata.category(c) != <span class="string">'Mn'</span></div><div class="line">        <span class="keyword">and</span> c <span class="keyword">in</span> all_letters</div><div class="line">    )</div><div class="line"></div><div class="line">print(unicode_to_ascii(<span class="string">'Ślusàrski'</span>))</div><div class="line">print(os.getcwd())</div><div class="line"></div><div class="line">all_files = glob.glob(<span class="string">'/content/colabdataset/ColabDataset/names/*.txt'</span>)</div><div class="line">print(all_files)</div><div class="line">Country2Name = defaultdict(list)</div><div class="line">Countries = []</div><div class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</div><div class="line">  country = file.split(<span class="string">'/'</span>)[<span class="number">-1</span>][:<span class="number">-4</span>]</div><div class="line">  Countries.append(country)</div><div class="line">  print(country)</div><div class="line">  f = open(file)</div><div class="line">  lines = f.readlines()</div><div class="line">  names = []</div><div class="line">  <span class="keyword">for</span> line <span class="keyword">in</span> lines:</div><div class="line">    names.append(unicode_to_ascii(line.replace(<span class="string">'\n'</span>,<span class="string">''</span>)))</div><div class="line">  Country2Name[country] = names</div><div class="line">print(Country2Name[<span class="string">'Chinese'</span>][:<span class="number">10</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2tensor</span><span class="params">(name)</span>:</span></div><div class="line">  time_step = len(name)</div><div class="line">  feature_dim = n_letters</div><div class="line">  batch_size = <span class="number">1</span></div><div class="line">  tensor = torch.zeros(time_step,batch_size,feature_dim)</div><div class="line">  <span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(name):</div><div class="line">    tensor[idx,<span class="number">0</span>,all_letters.index(val)] = <span class="number">1</span></div><div class="line">  tensor = tensor.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</div><div class="line">  <span class="keyword">return</span> tensor</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_training_pair</span><span class="params">()</span>:</span></div><div class="line">  country_label = np.random.choice(Countries)</div><div class="line">  names = np.random.choice(Country2Name[country_label])</div><div class="line">  tensor_names = str2tensor(names)</div><div class="line">  tensor_label = torch.tensor([Countries.index(country_label)])</div><div class="line">  <span class="keyword">return</span> tensor_names,tensor_label</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">rnn_classifier</span><span class="params">(nn.Module)</span>:</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,hidden_size,output_size)</span>:</span></div><div class="line">    super(rnn_classifier,self).__init__()</div><div class="line">    self.input_size = input_size</div><div class="line">    self.hidden_size = hidden_size</div><div class="line">    self.output_size = output_size</div><div class="line">    </div><div class="line">    self.embed = nn.Embedding(input_size,hidden_size)</div><div class="line">    <span class="comment"># input of shape (seq_len, batch, input_size)</span></div><div class="line">    <span class="comment"># output of shape (seq_len, batch, num_directions * hidden_size)</span></div><div class="line">    self.lstm = nn.LSTM(self.input_size,self.hidden_size,batch_first=<span class="keyword">True</span>) </div><div class="line">    self.fc = nn.Linear(self.hidden_size,self.output_size)</div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></div><div class="line">    hidden,(_,_) = self.lstm(input) <span class="comment">#AttributeError: 'tuple' object has no attribute 'dim' if hidden = self.lstm(input) </span></div><div class="line">    output = self.fc(hidden)</div><div class="line">    <span class="keyword">return</span> output</div><div class="line"></div><div class="line">n_hidden = <span class="number">128</span></div><div class="line">rnn = rnn_classifier(n_letters,n_hidden,len(Countries))</div><div class="line">criterion = nn.CrossEntropyLoss()</div><div class="line">opt = torch.optim.SGD(rnn.parameters(),lr=<span class="number">0.005</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(input,output)</span>:</span></div><div class="line">  opt.zero_grad()</div><div class="line">  pred = rnn(input)</div><div class="line">  pred = pred[:,<span class="number">-1</span>,:].squeeze(dim=<span class="number">1</span>)</div><div class="line">  loss = criterion(pred,output)</div><div class="line">  loss.backward()</div><div class="line">  opt.step()</div><div class="line">  <span class="keyword">return</span> pred,loss.item()</div><div class="line"></div><div class="line">n_epochs = <span class="number">100000</span></div><div class="line">print_every = <span class="number">5000</span></div><div class="line">current_loss = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</div><div class="line">  input,output = get_training_pair()</div><div class="line">  pred,loss = train(input,output)</div><div class="line">  current_loss += loss</div><div class="line">  <span class="keyword">if</span> epoch % print_every == <span class="number">0</span>:</div><div class="line">    print(epoch,<span class="string">'/'</span>,n_epochs,loss)</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_testing_pair</span><span class="params">()</span>:</span></div><div class="line">  country_label = np.random.choice(Countries)</div><div class="line">  names = np.random.choice(Country2Name[country_label])</div><div class="line">  tensor_names = str2tensor(names)</div><div class="line">  tensor_label = torch.tensor([Countries.index(country_label)])</div><div class="line">  <span class="keyword">return</span> names,country_label,tensor_names,tensor_label</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prediction</span><span class="params">(name_tensor,names,gt)</span>:</span></div><div class="line">  pred = rnn(name_tensor)</div><div class="line">  pred = pred[:,<span class="number">-1</span>,:].squeeze(<span class="number">1</span>)</div><div class="line">  _,idx = torch.max(pred.data,<span class="number">1</span>)</div><div class="line">  <span class="keyword">return</span> Countries[idx]</div><div class="line"></div><div class="line">n = <span class="number">100</span></div><div class="line">c = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">  names,country_label,tensor_names,tensor_label = get_testing_pair() </div><div class="line">  pred = prediction(tensor_names,names,country_label)</div><div class="line">  print(<span class="string">'name is &#123;&#125;, the prediction is &#123;&#125;, while the gound_t is &#123;&#125;'</span>.format(names,pred,country_label))</div><div class="line">  <span class="keyword">if</span> pred==country_label:</div><div class="line">    c += <span class="number">1</span></div><div class="line">print(<span class="string">"Acurracy is &#123;&#125;"</span>.format(c/n))</div></pre></td></tr></table></figure><p>There are several points needed the attention:</p><ol><li><p><strong>Input data and groundtruth</strong></p><p>The input data should be in the size <code>(time_steps, batch_size, feature_dim)</code> for the original <code>LSTM</code> function. But if you specify the parameter <code>batch_first</code> in <code>LSTM</code>, then you need to switch the dimentaion to <code>(batch_size, time_steps, feature_dim)</code>. </p><p>As for the groundtruth, it should be a scalar instead of a one-hot label if we are dealing with the classification problem, whose size should be <code>(batch_size,1)</code>. For example, if the batch_size is 1, so one possilbility could be <code>[1]</code> instead of <code>1</code>.</p><p>In this case, we usually use <code>CrossEntropyLoss</code> or <code>NLLLoss</code>.</p></li><li><p><strong>Loss function</strong></p><p>If in the model definition we have a layer called <code>LogSoftmax</code>, then we should use <code>CrossEntropyLoss</code>.</p></li><li><p><strong>Training function</strong></p><p>For <code>LSTM</code>, the input of shape should be <code>[seq_len, batch, input_size]</code>, while the output shape would be <code>[seq_len, batch, hidden_size]</code>. And then going through a Linear layer, the output size should be <code>[seq_len, batch, output_size]</code>. Here the <code>output_size</code> is class number. But we only need the last element of output, which means output<code>[-1,:,:]</code>. This is because for each time step, there is always a output.</p></li><li><p><strong>Prediction</strong></p><p>When testing, we should reshape the prediction to <code>[batch_size,output_size]</code>, then we can use <code>val,idx = torch.max(prediction,1)</code> for printing.</p></li><li><p>xxx</p></li><li><p>​</p></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://medium.com/@josh_2774/deep-learning-with-pytorch-9574e74d17ad?" target="_blank" rel="noopener">Deep Learning With PyTorch</a></p><p><a href="https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/" target="_blank" rel="noopener">A PyTorch tutorial – deep learning in Python</a></p><p><a href="https://github.com/jcjohnson/pytorch-examples" target="_blank" rel="noopener">jcjohnson’s PyTorch examples</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python-Seaborn</title>
      <link href="/2019/04/19/Python-Seaborn/"/>
      <url>/2019/04/19/Python-Seaborn/</url>
      <content type="html"><![CDATA[<p>A high-level plotting library built on top of matplotlab. Seaborn helps resolve the two major problems faced by Matplotlib; the problems are：</p><ul><li>Default Matplotlib parameters</li><li>Working with data frames</li></ul><a id="more"></a><h1 id="Seaborn-Quick-Guide"><a href="#Seaborn-Quick-Guide" class="headerlink" title="Seaborn Quick Guide"></a>Seaborn Quick Guide</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line">print(sns.get_dataset_names())</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[<span class="string">u'anscombe'</span>, <span class="string">u'attention'</span>, <span class="string">u'brain_networks'</span>, <span class="string">u'car_crashes'</span>, <span class="string">u'dots'</span>, </div><div class="line"><span class="string">u'exercise'</span>, <span class="string">u'flights'</span>, <span class="string">u'fmri'</span>, <span class="string">u'gammas'</span>, <span class="string">u'iris'</span>, <span class="string">u'planets'</span>, <span class="string">u'tips'</span>, </div><div class="line"><span class="string">u'titanic'</span>]</div></pre></td></tr></table></figure><h2 id="Figure-Aesthetic"><a href="#Figure-Aesthetic" class="headerlink" title="Figure Aesthetic"></a>Figure Aesthetic</h2><p>Basically, Seaborn splits the Matplotlib parameters into two groups−</p><ul><li>Plot styles</li><li>Plot scale</li></ul><h3 id="Figure-style"><a href="#Figure-style" class="headerlink" title="Figure style"></a>Figure style</h3><p>Seaborn provides five preset themes: white grid, dark grid, white, dark, and ticks. The interface for manipulating the styles is <strong>set_style()</strong>. </p><h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><h5 id="Darkgrid"><a href="#Darkgrid" class="headerlink" title="Darkgrid"></a>Darkgrid</h5><p>It is the default one.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats, integrate</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line">sns.set()</div><div class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>)</div><div class="line">sns.distplot(x)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419163334245.png" alt="image-20190419163334245"></p><h4 id="whitegrid"><a href="#whitegrid" class="headerlink" title="whitegrid"></a>whitegrid</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sns.set_style(<span class="string">"whitegrid"</span>)</div><div class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>)</div><div class="line">sns.distplot(x)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419163402652.png" alt="image-20190419163402652"></p><h4 id="dark"><a href="#dark" class="headerlink" title="dark"></a>dark</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sns.set_style(<span class="string">"dark"</span>)</div><div class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>)</div><div class="line">sns.distplot(x)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419163435391.png" alt="image-20190419163435391"></p><h4 id="white"><a href="#white" class="headerlink" title="white"></a>white</h4><p><img src="/2019/04/19/Python-Seaborn/image-20190419163501257.png" alt="image-20190419163501257"></p><h4 id="ticks"><a href="#ticks" class="headerlink" title="ticks"></a>ticks</h4><p><img src="/2019/04/19/Python-Seaborn/image-20190419163705284.png" alt="image-20190419163705284"></p><h4 id="Axes"><a href="#Axes" class="headerlink" title="Axes"></a>Axes</h4><h5 id="Remove-axes-spines"><a href="#Remove-axes-spines" class="headerlink" title="Remove axes spines"></a>Remove axes spines</h5><p>You can call <code>despine</code> function to remove them:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sns.set_style(<span class="string">"ticks"</span>)</div><div class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>)</div><div class="line">sns.distplot(x)</div><div class="line">sns.despine()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419163841209.png" alt="image-20190419163841209"></p><p>You can also control which spines are removed with additional arguments to despine:</p><p><img src="/2019/04/19/Python-Seaborn/image-20190419163953491.png" alt="image-20190419163953491"></p><h3 id="Scaling-plot-elements"><a href="#Scaling-plot-elements" class="headerlink" title="Scaling plot elements"></a>Scaling plot elements</h3><p>We also have control on the plot elements and can control the scale of plot using the <strong>set_context()</strong> function. We have four preset templates for contexts, based on relative size, the contexts are named as follows</p><ul><li>Paper</li><li>Notebook</li><li>Talk</li><li>Poster</li></ul><p>By default, context is set to notebook;</p><h2 id="Color-Palette"><a href="#Color-Palette" class="headerlink" title="Color Palette"></a>Color Palette</h2><p>Seaborn provides a function called <strong>color_palette()</strong>, which can be used to give colors to plots and adding more aesthetic value to it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">seaborn.color_palette(palette = <span class="keyword">None</span>, n_colors = <span class="keyword">None</span>, desat = <span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>Return refers to the list of RGB tuples. Following are the readily available Seaborn palettes −</p><ul><li>Deep</li><li>Muted</li><li>Bright</li><li>Pastel</li><li>Dark</li><li>Colorblind</li></ul><h3 id="Qualitative-Color-Palettes"><a href="#Qualitative-Color-Palettes" class="headerlink" title="Qualitative Color Palettes"></a>Qualitative Color Palettes</h3><p>Qualitative or categorical palettes are best suitable to plot the categorical data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">current_palette = sns.color_palette()</div><div class="line">sns.palplot(current_palette)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904212015382.png" alt="mage-20190421201538"></p><p>Here, the <strong>palplot()</strong> is used to plot the array of colors horizontally.</p><h3 id="Sequential-Color-Palettes"><a href="#Sequential-Color-Palettes" class="headerlink" title="Sequential Color Palettes"></a>Sequential Color Palettes</h3><p>Sequential plots are suitable to express the distribution of data ranging from relative lower values to higher values within a range.</p><p>Appending an additional character ‘s’ to the color passed to the color parameter will plot the Sequential plot.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">current_palette = sns.color_palette(<span class="string">"Greens"</span>)</div><div class="line">sns.palplot(current_palette)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904212017206.png" alt="mage-20190421201720"></p><h3 id="Diverging-Color-Palette"><a href="#Diverging-Color-Palette" class="headerlink" title="Diverging Color Palette"></a>Diverging Color Palette</h3><h1 id="Figures"><a href="#Figures" class="headerlink" title="Figures"></a>Figures</h1><h2 id="Histograms-KDE-and-densities"><a href="#Histograms-KDE-and-densities" class="headerlink" title="Histograms, KDE, and densities"></a>Histograms, KDE, and densities</h2><p>In matplotlib, </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats, integrate</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"></div><div class="line">data = np.random.multivariate_normal(mean=[<span class="number">0</span>,<span class="number">0</span>],cov=[[<span class="number">5</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>]],size=<span class="number">2000</span>) <span class="comment">#shape = [2000,2]</span></div><div class="line">data = pd.DataFrame(data,columns=[<span class="string">'x'</span>,<span class="string">'y'</span>])</div><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="string">'xy'</span>:</div><div class="line">    plt.hist(data[col],alpha=<span class="number">0.5</span>) <span class="comment">#alpha define the of transparency of the common area</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419195725160.png" alt="image-20190419195725160"></p><p>Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with <code>sns.kdeplot</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="string">'xy'</span>:</div><div class="line">    sns.kdeplot(data[col],shade=<span class="keyword">True</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419195851654.png" alt="image-20190419195851654"></p><p>Histograms and KDE can be combined using <code>distplot</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="string">'xy'</span>:</div><div class="line">    sns.distplot(data[col])</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419200042425.png" alt="image-20190419200042425"></p><p>If we pass the full two-dimensional dataset to <code>kdeplot</code>, we will get a two-dimensional visualization of the data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.kdeplot(data)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419200208063.png" alt="image-20190419200208063"></p><p>We can see the joint distribution and the marginal distributions together using <code>sns.jointplot</code>. For this plot, we’ll set the style to a white background:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> sns.axes_style(<span class="string">'white'</span>):</div><div class="line">    sns.jointplot(<span class="string">"x"</span>, <span class="string">"y"</span>, data, kind=<span class="string">'kde'</span>);</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419200409974.png" alt="image-20190419200409974"></p><p>There are other parameters that can be passed to <code>jointplot</code>—for example, we can use a hexagonally based histogram instead:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> sns.axes_style(<span class="string">'dark'</span>):</div><div class="line">    sns.jointplot(<span class="string">"x"</span>, <span class="string">"y"</span>, data, kind=<span class="string">'hex'</span>)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-20190419200524568.png" alt="image-20190419200524568"></p><h2 id="Pairwise-plots"><a href="#Pairwise-plots" class="headerlink" title="Pairwise plots"></a>Pairwise plots</h2><p>When you generalize joint plots to datasets of larger dimensions, you end up with <em>pair plots</em>. This is very useful for exploring correlations between multidimensional data, when you’d like to plot all pairs of values against each other.</p><p>We’ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iris = sns.load_dataset(<span class="string">'iris'</span>)</div><div class="line">print(iris.head())</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/Screen Shot 2019-04-20 at 8.47.30 PM.png" alt="creen Shot 2019-04-20 at 8.47.30 P"></p><p>Visualizing the multidimensional relationships among the samples is as easy as calling <code>sns.pairplot</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.pairplot(iris,hue=<span class="string">'species'</span>)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904202049285.png" alt="mage-20190420204928"></p><p>We can see that if we want to do classification, we can use <code>petal_width</code> and <code>petal_length</code> because from figure 15, these three species are distinguished.</p><h2 id="categorical-data"><a href="#categorical-data" class="headerlink" title="categorical data"></a>categorical data</h2><h3 id="stripplot"><a href="#stripplot" class="headerlink" title="stripplot()"></a>stripplot()</h3><p>stripplot() is used when one of the variable under study is categorical. It represents the data in sorted order along any one of the axis.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line">df = sb.load_dataset(<span class="string">'iris'</span>)</div><div class="line">sns.stripplot(x = <span class="string">"species"</span>, y = <span class="string">"petal_length"</span>, data = df)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292101502.png" alt="mage-20190429210150"></p><p>In the above plot, we can clearly see the difference of <strong>petal_length</strong> in each species. But, the major problem with the above scatter plot is that the points on the scatter plot are overlapped. We use the ‘Jitter’ parameter to handle this kind of scenario.</p><p>Jitter adds some random noise to the data. This parameter will adjust the positions along the categorical axis.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.stripplot(x = <span class="string">"species"</span>, y = <span class="string">"petal_length"</span>, data = df,jitter=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292102451.png" alt="mage-20190429210245"></p><p>We can see that the x-axis of point is changed but not y_axis. So that we can see the petal_length of each point without any overlay.</p><h3 id="swarmplot"><a href="#swarmplot" class="headerlink" title="swarmplot()"></a>swarmplot()</h3><p>Another option which can be used as an alternate to ‘Jitter’ is function <strong>swarmplot()</strong>. This function positions each point of scatter plot on the categorical axis and thereby avoids overlapping points </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.swarmplot(x = <span class="string">"species"</span>, y = <span class="string">"petal_length"</span>, data = df)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292105091.png" alt="mage-20190429210509"></p><h2 id="Distribution-of-observations"><a href="#Distribution-of-observations" class="headerlink" title="Distribution of observations"></a>Distribution of observations</h2><h3 id="boxplot"><a href="#boxplot" class="headerlink" title="boxplot()"></a>boxplot()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.boxplot(x = <span class="string">"species"</span>, y = <span class="string">"petal_length"</span>, data = df)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292108136.png" alt="mage-20190429210813"></p><h3 id="violinplot"><a href="#violinplot" class="headerlink" title="violinplot()"></a>violinplot()</h3><p>Violin Plots are a combination of the box plot with the kernel density estimates. So, these plots are easier to analyze and understand the distribution of the data.</p><p>Let us use tips dataset called to learn more into violin plots. This dataset contains the information related to the tips given by the customers in a restaurant.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line">df = sb.load_dataset(<span class="string">'tips'</span>)</div><div class="line">sns.violinplot(x = <span class="string">"day"</span>, y = <span class="string">"total_bill"</span>, data=df)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292109264.png" alt="mage-20190429210926"></p><p>The quartile and whisker values from the boxplot are shown inside the violin. As the violin plot uses KDE, <strong>the wider portion of violin indicates the higher density and narrow region represents relatively lower density</strong>. The Inter-Quartile range in boxplot and higher density portion in kde fall in the same region of each category of violin plot.</p><p>The above plot shows the distribution of total_bill on four days of the week. But, in addition to that, if we want to see how the distribution behaves with respect to sex, lets explore it in below example.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.violinplot(x = <span class="string">"day"</span>, y = <span class="string">"total_bill"</span>,hue = <span class="string">'sex'</span>, data = df)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292111234.png" alt="mage-20190429211123"></p><p>Now we can clearly see the spending behavior between male and female. We can easily say that, men make more bill than women by looking at the plot.</p><p>And, if the hue variable has only two classes, we can beautify the plot by splitting each violin into two instead of two violins on a given day. Either parts of the violin refer to each class in the hue variable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sb.violinplot(x = <span class="string">"day"</span>, y=<span class="string">"total_bill"</span>,hue = <span class="string">'sex'</span>, data = df,split=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p><img src="/2019/04/19/Python-Seaborn/image-201904292114337.png" alt="mage-20190429211433"></p><h1 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.tutorialspoint.com/seaborn/index.htm" target="_blank" rel="noopener">Seaborn Tutorial</a></p><p><a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html" target="_blank" rel="noopener">Visualization with Seaborn</a></p><p><a href="https://elitedatascience.com/python-seaborn-tutorial" target="_blank" rel="noopener"><a href="https://elitedatascience.com/python-seaborn-tutorial" target="_blank" rel="noopener">The Ultimate Python Seaborn Tutorial: Gotta Catch ‘Em All</a></a> </p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-GANImproving</title>
      <link href="/2019/04/14/DP-GANImproving/"/>
      <url>/2019/04/14/DP-GANImproving/</url>
      <content type="html"><![CDATA[<p>It is noted that GAN training is hard and unstable, which results in blury images. In this post, a several techniques are introduced to improve the training stability of GAN.</p><a id="more"></a><h1 id="Multi-scale-discriminator"><a href="#Multi-scale-discriminator" class="headerlink" title="Multi-scale discriminator"></a>Multi-scale discriminator</h1><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>It is first introduced in the paper <a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a>, where several techniques are proposed to enhance the reality of generated images. </p><p>According to the paper, discriminator with a large receptive field can help differentiate high-resolution real and synthesized images. But This would require either a deeper network or larger convolutional kernels, both of which would increase the network capacity and potentially cause overfitting. Also, both choices demand a larger memory footprint for training, which is already a scarce resource for highresolution image generation.</p><p>Therefore, they try to use multi-scale discriminators. They use 3 discriminators that have an <strong>identical network structure</strong> but <strong>operate at different image scales</strong>.<br>We will refer to the discriminators as D1, D2 and D3. Specifically, we <strong>downsample the real and synthesized highresolution images</strong> by a factor of 2 and 4 to create an image pyramid of 3 scales. The discriminators D1, D2 and D3 are then trained to differentiate real and synthesized images at the 3 different scales, respectively. Although the discriminators have an identical architecture, the one that <strong>operates at the coarsest scale has the largest receptive field</strong>. It has a <strong>more global view</strong> of the image and can guide the generator to <strong>generate globally consistent images</strong>. On the other hand, the <strong>discriminator at the finest scale encourages the generator to produce finer details</strong>. This also makes training the coarse-to-fine generator easier, since extending a low re solution model to a higher resolution only requires adding<br>a discriminator at the finest level, rather than retraining from scratch. Without the multi-scale discriminators, we observe that many repeated patterns often appear in the generated images.</p><script type="math/tex; mode=display">\min _{G} \max _{D_{1}, D_{2}, D_{3}} \sum_{k=1,2,3} \mathcal{L}_{\mathrm{GAN}}\left(G, D_{k}\right)</script><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p>Here, <code>BaseDiscriminator</code> implements a single identical discriminator network. Then <code>MultipleDiscriminator</code> receive the parameter  <code>num_of_discriminator</code> and create MultipleDiscriminator network strcuture. Then we can directly use function <code>define_D</code> to use the method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">define_D</span><span class="params">(in_channels=<span class="number">3</span>,num_D=<span class="number">3</span>)</span>:</span></div><div class="line">    net_D = MultipleDiscriminator(in_channels,num_D)</div><div class="line">    <span class="keyword">return</span> net_D</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultipleDiscriminator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,in_channels=<span class="number">3</span>,num_D=<span class="number">3</span>)</span>:</span></div><div class="line">        super(MultipleDiscriminator,self).__init__()</div><div class="line">        self.num_D = num_D</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_D):</div><div class="line">            netD = BaseDiscriminator(in_channels)</div><div class="line">            setattr(self, <span class="string">'layer'</span> + str(i), netD.model)</div><div class="line">        self.downsample = nn.AvgPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=[<span class="number">1</span>, <span class="number">1</span>], count_include_pad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleD_forward</span><span class="params">(self, model, input)</span>:</span></div><div class="line">        <span class="keyword">return</span> [model(input)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></div><div class="line">        num_D = self.num_D</div><div class="line">        result = []</div><div class="line">        input_downsampled = input</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_D):</div><div class="line">            model = getattr(self, <span class="string">'layer'</span> + str(num_D - <span class="number">1</span> - i))</div><div class="line">            result.append(self.singleD_forward(model, input_downsampled))</div><div class="line">            <span class="keyword">if</span> i != (num_D - <span class="number">1</span>):</div><div class="line">                input_downsampled = self.downsample(input_downsampled)</div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseDiscriminator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels=<span class="number">3</span>)</span>:</span></div><div class="line">        super(BaseDiscriminator, self).__init__()</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">discriminator_block</span><span class="params">(in_filters, out_filters, normalization=True)</span>:</span></div><div class="line">            <span class="string">"""Returns downsampling layers of each discriminator block"""</span></div><div class="line">            layers = [nn.Conv2d(in_filters, out_filters, <span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)]</div><div class="line">            <span class="keyword">if</span> normalization:</div><div class="line">                layers.append(nn.InstanceNorm2d(out_filters))</div><div class="line">            layers.append(nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="keyword">True</span>))</div><div class="line">            <span class="keyword">return</span> layers</div><div class="line"></div><div class="line">        self.model = nn.Sequential(</div><div class="line">            *discriminator_block(in_channels*<span class="number">2</span>, <span class="number">64</span>, normalization=<span class="keyword">False</span>),</div><div class="line">            *discriminator_block(<span class="number">64</span>, <span class="number">128</span>),</div><div class="line">            *discriminator_block(<span class="number">128</span>, <span class="number">256</span>),</div><div class="line">            *discriminator_block(<span class="number">256</span>, <span class="number">512</span>),</div><div class="line">            nn.ZeroPad2d((<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)),</div><div class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img_A, img_B)</span>:</span></div><div class="line">        <span class="comment"># Concatenate image and condition image by channels to produce input</span></div><div class="line">        img_input = torch.cat((img_A, img_B), <span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> self.model(img_input)</div></pre></td></tr></table></figure><p>We also need to cusomize our own <code>MSE</code> loss because each discriminator have its own output and corresponding mse loss. We want the sum mse loss of all discriminators.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GANLoss</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_lsgan=True, target_real_label=<span class="number">1.0</span>, target_fake_label=<span class="number">0.0</span>,</span></span></div><div class="line"><span class="function"><span class="params">                 tensor=torch.FloatTensor)</span>:</span></div><div class="line">        super(GANLoss, self).__init__()</div><div class="line">        self.real_label = target_real_label</div><div class="line">        self.fake_label = target_fake_label</div><div class="line">        self.real_label_var = <span class="keyword">None</span></div><div class="line">        self.fake_label_var = <span class="keyword">None</span></div><div class="line">        self.Tensor = tensor</div><div class="line">        <span class="keyword">if</span> use_lsgan:</div><div class="line">            self.loss = nn.MSELoss()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.loss = nn.BCELoss()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_tensor</span><span class="params">(self, input, target_is_real)</span>:</span></div><div class="line">        <span class="keyword">if</span> target_is_real:</div><div class="line">            create_label = ((self.real_label_var <span class="keyword">is</span> <span class="keyword">None</span>) <span class="keyword">or</span></div><div class="line">                            (self.real_label_var.numel() != input.numel()))</div><div class="line">            <span class="keyword">if</span> create_label:</div><div class="line">                real_tensor = self.Tensor(input.size()).fill_(self.real_label)</div><div class="line">                self.real_label_var = Variable(real_tensor, requires_grad=<span class="keyword">False</span>)</div><div class="line">            target_tensor = self.real_label_var</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            create_label = ((self.fake_label_var <span class="keyword">is</span> <span class="keyword">None</span>) <span class="keyword">or</span></div><div class="line">                            (self.fake_label_var.numel() != input.numel()))</div><div class="line">            <span class="keyword">if</span> create_label:</div><div class="line">                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)</div><div class="line">                self.fake_label_var = Variable(fake_tensor, requires_grad=<span class="keyword">False</span>)</div><div class="line">            target_tensor = self.fake_label_var</div><div class="line">        <span class="keyword">return</span> target_tensor</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, input, target_is_real)</span>:</span></div><div class="line">        <span class="keyword">if</span> isinstance(input[<span class="number">0</span>], list):</div><div class="line">            loss = <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> input_i <span class="keyword">in</span> input:</div><div class="line">                pred = input_i[<span class="number">-1</span>]</div><div class="line">                target_tensor = self.get_target_tensor(pred, target_is_real)</div><div class="line">                loss += self.loss(pred, target_tensor)</div><div class="line">            <span class="keyword">return</span> loss</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            target_tensor = self.get_target_tensor(input[<span class="number">-1</span>], target_is_real)</div><div class="line">            <span class="keyword">return</span> self.loss(input[<span class="number">-1</span>], target_tensor)</div></pre></td></tr></table></figure><p>Once we finish the above codes, we can use them. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminate</span><span class="params">(input_label, test_image)</span>:</span></div><div class="line">    input_concat = torch.cat((input_label, test_image.detach()), dim=<span class="number">1</span>)</div><div class="line">    <span class="keyword">return</span> discriminator(input_concat)</div><div class="line"></div><div class="line">cuda = <span class="keyword">True</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="keyword">False</span></div><div class="line">Tensor = torch.cuda.FloatTensor <span class="keyword">if</span> cuda <span class="keyword">else</span> torch.Tensor</div><div class="line">criterion_GAN = GANLoss(use_lsgan=<span class="keyword">not</span> opt.no_lsgan,tensor=Tensor)</div><div class="line">discriminator = define_D(num_D = opt.num_discriminator)</div><div class="line"><span class="keyword">if</span> cuda:</div><div class="line">    discriminator.cuda()</div><div class="line">discriminator.apply(weights_init_normal)</div><div class="line">optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))</div><div class="line"></div><div class="line"><span class="comment"># ---------------------</span></div><div class="line"><span class="comment">#  Train Discriminator</span></div><div class="line"><span class="comment"># ---------------------</span></div><div class="line">optimizer_D.zero_grad()</div><div class="line"><span class="comment"># Real loss</span></div><div class="line">pred_real = discriminate(real_B, real_A)</div><div class="line">loss_real = criterion_GAN(pred_real, <span class="keyword">True</span>)</div><div class="line"><span class="comment"># Fake loss</span></div><div class="line">pred_fake = discriminate(fake_B.detach(), real_A)</div><div class="line">loss_fake = criterion_GAN(pred_fake, <span class="keyword">False</span>)</div><div class="line"><span class="comment"># Total loss</span></div><div class="line">loss_D = <span class="number">0.5</span> * (loss_real + loss_fake)</div><div class="line">loss_D.backward()</div><div class="line">optimizer_D.step()</div></pre></td></tr></table></figure><h1 id="Progressive-growing-of-GANs"><a href="#Progressive-growing-of-GANs" class="headerlink" title="Progressive growing of GANs"></a>Progressive growing of GANs</h1><p><a href="https://towardsdatascience.com/progressively-growing-gans-9cb795caebee" target="_blank" rel="noopener">link1</a> <a href="https://zhuanlan.zhihu.com/p/30637133" target="_blank" rel="noopener">link2</a> </p><p><a href="https://github.com/github-pengge/PyTorch-progressive_growing_of_gans" target="_blank" rel="noopener">pytorch-progan1</a> <a href="https://github.com/facebookresearch/pytorch_GAN_zoo" target="_blank" rel="noopener">2</a> <a href="https://github.com/akanimax/pro_gan_pytorch" target="_blank" rel="noopener">3</a> <a href="https://github.com/zsef123/PGGAN-Pytorch" target="_blank" rel="noopener">4</a> <a href="https://github.com/jeromerony/Progressive_Growing_of_GANs-PyTorch" target="_blank" rel="noopener">5</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Spanish</title>
      <link href="/2019/04/13/Spanish/"/>
      <url>/2019/04/13/Spanish/</url>
      <content type="html"><![CDATA[<p>西班牙语学习！</p><a id="more"></a><h2 id="人称代词"><a href="#人称代词" class="headerlink" title="人称代词"></a>人称代词</h2><p>Yo - I</p><p>Mi - my, me</p><blockquote><p><strong>Mis amigas</strong> siempre necesitan muchos zapatos. = My friends always need a lot of shoes.</p><p>放在介词后面 la camiseta es para mi. = The t-shirt is for me.</p></blockquote><p>T$\acute u$ - You | usted=you | ustedes = you guys</p><blockquote><p>usted 第三人称处理</p></blockquote><p>Tu - yours</p><blockquote><p>Señora Castro, ¿usted necesita sus vestidos ahora? = Mrs. Castro, do you need your dresses now?</p><p>—  because the sentence is in the formal format where you use “Usted”. If the sentence was in the form of “Tu” then you would also write “tus” “necesitas tus vestidos ahora” vs “usted necesita sus vestidos ahora”.</p></blockquote><p>ti - you</p><blockquote><p>放在介词后面</p><p>Estos pantalones son para ti. = These pants are for you.</p><p>A ti no te gusta el pastre. = You don’t like the cake.</p><p>A usted le gusta mucho leer. = You like reading a lot.</p></blockquote><p>Ella - She <e ya=""></e></p><blockquote><p>主语，也可以放在介词后面。</p><p>Este vestido no es para ella. = This dress is not for her.</p></blockquote><p>su - her/his/their/your(usted)/its</p><blockquote><p>“Señor Herrera, ¿estas son sus camisas?” = Mr. Herrera, are these your shirts?</p><p>sus zapatos nuevos. = his new shoes</p></blockquote><p>$\acute El$ - He，him</p><blockquote><p>主语，也可以放在介词后面。</p></blockquote><p>Nosotros / Nosotras somos - we are</p><blockquote><p>主语，也可以放在介词后面。</p><p>Esta fiesta es para nosotros? = Is this party for us?</p></blockquote><p>nuestro/a - our</p><blockquote><p>De donde es nuestra abuela? = where is our grandmother from.</p><p>Nuestro abuelo es italiano = Our grandfather is italian.</p></blockquote><p>ellos | ellas son - they are </p><p>ni$\tilde n$a - girl</p><p>ni$\acute n$o - boy</p><blockquote><p>los ninos = the children</p></blockquote><p>mujer - woman || las mujeres</p><p>hombre - man || los hombres</p><p>chico / chica - boy / girl</p><p>nadie - nobody</p><p>Ellos <strong>nos</strong> necesitan ahora.  = They need us now.</p><p>Tu esposa <strong>te</strong> compra libros? = Does your wife buy you books?</p><p>Cuantas munecas <strong>te</strong> compra tu ebuelo? = How many dolls is your grandfather buying you?</p><p>por que ellos no <strong>te</strong> visitan? = Why do they not visit you?</p><p>物主代词</p><p>mio/a - mine</p><blockquote><p>tu carro o el mio - your car or mine</p></blockquote><p>tuyo/a - yours</p><blockquote><p>mi carro o el tuyo - my car or yours</p></blockquote><p>suyo/a - hers, theirs</p><blockquote><p>Use ‘tuya’ when in 2nd person(tu). Use ‘suyo’ whne in 3rd person(usted,el,ella).</p><p>‘suya’ is more formal.</p></blockquote><p>nuestro/a - ours</p><p><strong>人称代词</strong></p><p>mis - my | mis padres= my parents</p><p>su - his/her  | sus regalos= his gifts</p><p>sus amigas - their friends</p><p><strong>be动词</strong></p><p>somos </p><p>eres(2) - are</p><p>era(1) eras(2) eran(they), eramos(we) - were {eras nino = you were a child}</p><p>fui(1), fuiste(2), fue(3), fueron(they), fuimos(we) - was, went</p><blockquote><p>yo fui maestro por muchos anos = i was a teacher for many years</p><p>fui de vacationes - went on vacationes</p></blockquote><p><strong>For sb:</strong></p><p>para mi - for me</p><p>para ti - for you</p><p>para ustedes - for you guys</p><p>para ella - for her</p><p>para $\acute e$l - for him</p><p>para nosotros - for us</p><p><strong>a sb * gusta</strong></p><p>A mi me gusta - i like</p><p>A usted le gusta / A ti te gusta - you like</p><p>A ustedes les gusta - you guys like</p><p>A el le gusta - he likes</p><p>A ellos les gusta - they like</p><p>A nosotros nos gusta - we like </p><p><strong>宾格(Accusative case | Objective case)</strong></p><p><em>直接宾格</em></p><p>一般直接放在动词前面</p><p>lo - him / it /</p><p>la - her / it /</p><blockquote><p>El vestido es bonito, pero no lo compro.</p><p>Esta tienda, ya la conozco.</p></blockquote><p>los / las - them </p><p><em>间接宾格</em></p><p>le - him/her</p><blockquote><p>Le hago preguntas porque ella es interestante. = I ask her questions because …</p><p>ellos le compran un vestido = they buy her a dress</p><p>mi padre les da mucho dinero = my dad gives them many money</p><p>Ella les manda regalos = She sends gifts to them.</p><p>Ella les manda regalos a sus abuelos = She sends gifts to her grandparents.</p><p>yo le hablo a la mesera en frances = i speak to the waitress in french</p></blockquote><p>me</p><p>(no) lo quiero - I (don’t) want it </p><blockquote><p>ese carro es muy bueno y lo quiero = That car is very good and I want it</p><p>esa falda es bonita, pero no la quiero = That skirt is pretty but I don’t want it</p><p>me gusta esa chaqueta y la quiero comprar</p></blockquote><p>los quiero - I want them</p><blockquote><p>estos pasteles son muy buenos y los quiero. = These cakes are very good and I want them.</p></blockquote><p>Que comida les gusta a ustedes? = What food do you like? (放后面)</p><p>A el que le gusta? = What does he like? (放前面)</p><h2 id="介词"><a href="#介词" class="headerlink" title="介词"></a>介词</h2><p>En - In</p><p>el - on</p><p>los - on | los lunes=on mondays</p><p>a - to</p><p>o - or</p><p>sobre - on, about, around</p><blockquote><p>sobre el escritorio = on the desk | sobre la cama = on the bed</p><p>leo sobre pajaros = read about birds</p><p>sobre las once = around 11.</p></blockquote><p>debajo de - under</p><p>y - and <yi></yi></p><p>con - with <gon></gon></p><p>sin - without</p><p>de - from/of</p><p>para - for, in order to <reason>/<location></location></reason></p><blockquote><p>el turista sale para Peru manana. = The tourist is leaving for Peru tomorrow.</p></blockquote><p>por - for <time>/<transportation>/<reason></reason></transportation></time></p><blockquote><p>tres huevos por dos pesos. = Three eggs for two pesos.</p><p>por tren/ por avión</p><p>ella no quiere conducir por la tormenta - she doesn’t want to drive because of the store</p></blockquote><p>aqui / ac$\acute a$ - here</p><p>all$\acute a$ - there</p><p>alli - there</p><p>D$\acute o$nde - Where</p><p>ad$\acute o$nde - to where </p><p>si - if</p><p>Cu$a$ndo - when </p><p>mientras - while</p><p>A qu$\acute e$ hora - when/what time</p><blockquote><p>A que hora te levantas? = what time do you get up</p><p>A qué hora se levanta usted mañana? = what time are you getting up tomorrow?</p><p>que hora es? = what time is it</p></blockquote><p>Qu$\acute e$ - what</p><blockquote><p>Qu$\acute e$ interestante. = How interesting.</p><p>Qu$\acute e$ vestido tan bonito. = What a pretty dress.</p><p>Qu$\acute e$ divertido eres, Juan. = You are so fun, juan.</p><p>Qu$\acute e$ interestante es tu amigo = Your friend is so interesting.</p></blockquote><p>que - that</p><p>cual - which</p><blockquote><p><a href="http://www.onetoonespanish.co.uk/blog/the-difference-between-que-and-cual-in-spanish.htm" target="_blank" rel="noopener">difference between que and cual</a>  <a href="https://forum.duolingo.com/comment/10165244" target="_blank" rel="noopener">difference</a> </p><p>When you use que, you are asking for a definition, while cual us for asking between options.</p><p>cual es el precio? = what is the price?</p><p>¿Cuáles son tus clases favoritas = what is yoru favorite classes</p></blockquote><p>Qui$\acute e$n - who/whom </p><blockquote><p>quienes son esas jovenes = who are those women</p></blockquote><p>Cu$\acute a$ntos  / Cu$\acute a$ntas - how many</p><p>puede - can you</p><blockquote><p>puede traer un vaso, por favor = can you bring a glass, please</p></blockquote><p>mucho - a lot <mu cho=""> 修饰名词和动词</mu></p><blockquote><p>el necesita mucha agua.</p><p>quiero mucho arroz. </p></blockquote><p>contigo - to you / with you</p><p>conmigo - with me</p><p>los lunes - Monday <lu nei="" s=""></lu></p><blockquote><p>el lunes que viene = next monday</p></blockquote><p>los martes - Tuesday <ma ruo="" dei="" s=""></ma></p><p>los mi$\acute e$rcoles - Wednesday</p><p>los jueves - Thursday <hua ves=""></hua></p><p>viernes - Friday </p><p>los s$\acute a$bado - saturday </p><p>domingo - sunday</p><blockquote><p>el proximo domingo = next sunday</p><p>el domingo que viene = next sunday</p></blockquote><p>semana - week</p><p>fin de semana - weekend  |los fines de semana= on weekends</p><p>el dia - day</p><p>hoy - today </p><p>ma$\tilde n$ana - morning/tomorrow || manzana= apple</p><blockquote><p>en la manana = in the morning.</p></blockquote><p>ayer - yesterday</p><p>antes - before</p><blockquote><p>antes de las nueve = before nine</p></blockquote><p>despu$\acute e$s - after</p><blockquote><p>dos horas despues = two hours later</p></blockquote><p>tan .. como .. - as .. as …</p><blockquote><p>somos tan bonito como tu = we are as pretty as you</p></blockquote><p>tan - so</p><blockquote><p>esta bicicleta es tan vieja. = This bicycle is so old.</p></blockquote><p>pr$\acute o$ximo/a - next</p><blockquote><p>el proxima viernes - next friday</p></blockquote><p>detr$\acute a$s de - behind</p><p>entre - between</p><blockquote><p>entre las cinco y seis de la tarde = between 5 and 6 in the evening</p></blockquote><p>dentro de - inside</p><p>fuera de - outside</p><p>arriba - upstairs</p><p>abajo - downstairs</p><p>atr$\acute a$s - back</p><p>final de - end of </p><p>oye - hey</p><p>excepto - except</p><p>hasta - until</p><p>alrededor - around</p><blockquote><p>alrededor del mundo - around the world</p></blockquote><p>asi - therefore,so</p><p>vaya - wow</p><p>hacia - toward</p><h2 id="系动词"><a href="#系动词" class="headerlink" title="系动词"></a>系动词</h2><p>Un (m), Una (f) - a</p><p>este(male), esta(fem) - this | estos vestidos + estas carteras = these</p><p>estabas(2) , estabamos(we) - were</p><p>esto - this </p><blockquote><p>esto es muy diferente = this is very different.</p></blockquote><p>eso - that</p><blockquote><p>me ayudas con eso = can you help me with that</p></blockquote><p>ese / esa - that <e sei=""> | esos = those</e></p><p>aquel / aquella - that</p><p>aquellos / aquellas - those</p><p>Si - Yes</p><p>soy - am || ser=to be</p><blockquote><p>======Ella está cansada.</p><p>Está is for something temporary and es is for something permanent. In this case, she is tired, at the moment. If she was always tired, it would be es.</p></blockquote><p>est$\acute a$s(2), est$\acute a$(3), estamos(we), estan(they) - be (临时) | estar= to be</p><blockquote><p>tengo que estar en el aeropuerto a las dos. = I have to be at the airport at two.</p></blockquote><p>es, - is， </p><p>eres - are (tu)</p><p>El (pos), La (neg) - the (单数)</p><p>los(mas), las(fem) - the(复数)</p><p>estoy - I am</p><h2 id="动词"><a href="#动词" class="headerlink" title="动词"></a>动词</h2><h3 id="及人动词"><a href="#及人动词" class="headerlink" title="及人动词"></a>及人动词</h3><p><strong>reflexive pronouns </strong> <a href="https://studyspanish.com/grammar/lessons/reflexive2" target="_blank" rel="noopener">link</a></p><p>me(myself)</p><p>te(yourself)</p><p>se(himeself, herself, yourself)</p><p>nos(ourselves)</p><p>os(yourselves)</p><p>se(themselves, yourselves)</p><p> ~rse=to </p><p>pongo(1), pones(2) - put, put on | ponerse= to put on / ponerte</p><blockquote><p>me pongo | se ponen</p><p>Rafael, pon aquellas xx en mi casa. = put those xx at my home</p><p>senor, ponga esas silas. = sir, put those chairs</p></blockquote><p>puse(1) </p><p>cepillo(1), cepilla(3), cepillan(they), cepillamos(we) - brush | cepillarse= to brush</p><blockquote><p>ellas se cepillan el cabello en el bano = they brush their hair in the bathroom</p><p>ella se cepilla su cabello largo = she brushes her long hair.</p></blockquote><p>ducho(1), ducha(3), duchan(they) - shower | ducharse= to shower | ducharte(to ~)</p><blockquote><p>mi perrp nunca se ducha.= My dog never showers.</p><p>ella tiene que ducharse hoy. = she has to shower today</p><p>hijo, tienes que ducharte antes de salir. = Son you have to shower before leaving.</p></blockquote><p>quedo(1), queda(3), quedamos(we) - stay | quedarse= to stay</p><p>digo(1), dice(3), dicen(they), decimos(we) - tell, say | decir= to tell</p><blockquote><p>te voy a decir algo = I am going to tell you something.</p><p>me vas a decir tu nombre? = Are you going to tell me your name?</p></blockquote><p>dije(1),dijiste(2),dijo(3) - said,told</p><p>sit down | sentarse = to ~ | sientate(u), sientese(ud)=!, </p><p>lavo(1), lava(3), lavan(they), lavamos(we) - wash | lavar / lavarnos= to wash | lavate(u), lavese(ud)=~!</p><p>lav$\acute o$(3), lavaron(they) - washed</p><blockquote><p>nos lavamos en el bano del hotel. = We wash ourselves in the hotel bathroom</p><p>mi gato no tiene que lavarse. = My cat doesn’t have to wash itself.</p><p>pedro se lava las manos antes de comer = Pedro wahes his hands before eating</p><p>queremos lavarnos las manos = we want to wash our hands</p><p>Pablo, lavate las manos = Pablo, wash your hands.</p></blockquote><p>despierto(1), despierta(3), despertamos(we) - wake up | despertar=to ~</p><p>acuerdo(1) acuerda(3) | acordar de - to remember</p><blockquote><p>tienes que acordarte de eso = you have to remember that</p></blockquote><p>acuesto(1),acuesta(3),acostamos(we) - | acostar - to go to bed</p><p>preocupo(2) - worry</p><blockquote><p>me preocupo por - I worry about</p></blockquote><p>siento(1), sienta(2), sentamos(we) - sit</p><p>vestir - to dress</p><p>caer - to fall</p><blockquote><p>Ayuda, creo que voy a caerme. = Help, I think I am going to fall.</p></blockquote><p>reir - laugh</p><p>caso(1) casamos(we), casan(they) - get married</p><p>lleva(3) llevamos(we) se llevan(they) - get along with</p><p>subo(1)sube(3) subimos(we) | subir - get in, get on, go up</p><p>volver - become</p><p>apuro(1) apura(3) apuramos apuran - hurry</p><p>quito(1)quita(3)quitamos quitan | quitar - take off</p><h3 id="不及物动词"><a href="#不及物动词" class="headerlink" title="不及物动词"></a>不及物动词</h3><p>manejo(1), meneja(3), manejan(they), manejamos(we) - drive | manejar= to drive</p><blockquote><p>manejo a mi trabajo = drive to my work.</p><p>manejo el carro = drive the car</p></blockquote><p>conduzco(1) - drive | </p><p>voy(1), vas(2), va(3), van(they), vamos(we) - go | ir =to go | iba - used to go</p><p>请求 felipe, ven ahora - come now</p><p>se va(3) - leave | irse, irte - to leave</p><blockquote><p>cuando vas a ir a Espanola = when are you going to spain</p><p>rafael, ve a mi oficina ahora = go to my office now</p><p>senor, vaya mas tarde = sir, go later</p></blockquote><p>viajas(2), - travel | viajar=to travel</p><blockquote><p>Quieres ir al parque hoy? = Do you want to go to the park today?</p><p>viaja a muchos paises = travel to many countries</p></blockquote><p>salgo(1), sales(2), salen(they), salimos(we) - leave | salir=to leave | saliendo=~ing</p><p>salio(3), salieron(they), salimos(we)</p><blockquote><p>salgo de casa = leave the house</p><p>salen a cenar = go out to dinner</p></blockquote><p>llego(1), llegas(2), llegan(they), llegamos(we) - arrive | llegar= to arrive</p><p>llegue(1)</p><blockquote><p>llega al aeropuerto = arrive ar rge airport</p></blockquote><p>llamo(1), llama(2), llaman(they) - name, call | llamar= to call</p><blockquote><p>me llamo | se llama | se llaman</p><p>Como se llaman tus padres? = What are your parents’ names?</p><p>mi esposo llama a su madre. = My husband calls his mother.hist</p><p>vamos a llamar a dos taxis. = we are going to call two taxis.</p></blockquote><p>amo(1), ama(3), amamos(we) - love | amar= to love</p><blockquote><p>A ama a B. = A loves B.</p></blockquote><p>vuelves(2), vuelven(they), volvemos(we) - come back | volver= to go back | vuelva(ud), vuelve(tu) = command</p><blockquote><p>tu vuelves del trabajo a las nueve. = You come back from work at nine</p><p>usted vuelve a casa en la noche = you come back home at night</p></blockquote><p>escucho(1), escuchas(2), escuchan(they), escuchamos(we) - listen to ||escuchar= to listen | escuchando= listening to</p><p>escuchamos(we) - listened to</p><blockquote><p>escucho musica = listen to music </p><p>escucho la radio = listen to the radio</p><p>escucho a la maestra</p></blockquote><p>vengo(1), viene(3), venimos(we) - come | venir= to come</p><p>vine(1), viniste(2), vino(3), vinieron(they), vinimos(we)</p><blockquote><p>quieres venir al cine? = do you want to come to the movies?</p><p>cuando  vienes a casa = when do you come home</p><p>cuando vienen tus amigos de mexico? = when do your friends come from mexico</p></blockquote><p>invito(1), invitan(they), invitamos(we), - we | invitar= to invite</p><p>请求 senor, invite a su esposa. / victoria, invita a tu amiga al partido de futbol.</p><blockquote><p>nosotros invitamos a Maria. = We invite maria.</p><p>ella te invita = she invites you</p><p>me vas a invitar? = are you going to invite me</p><p>ella te invita al baile el viernes = she invites you to the dance on Friday.</p></blockquote><p>ayudo(1), ayudas(1), ayudan(they), ayudamos(we) - help</p><blockquote><p>ellos ayudan a los ninos. = They help the boys.</p><p>te ayudo a limpiar la cocina = i help you to clean the kitchen</p></blockquote><p>aprendo(1), aprendes(2), aprendemos(we) - learn | aprender=to learn | aprendiendo=~ing</p><p>aprendiste(2), aprendio(3), aprendieron(they), aprendimos(we) - </p><blockquote><p>estamos aprendiendo a cocinar = we are learning to cook</p></blockquote><p>ense$\acute n$o(1), ense$\acute n$a(3), ense$\acute n$an(they) ense$\acute n$amos(we) - teach | ense$\acute n$ar= to teach</p><blockquote><p>ensena ingles a Felip = teach felip english. | teach english to felip.</p><p>tu madre te ensena a nadar = does your mother teach you to swim</p></blockquote><p>show</p><blockquote><p>te enseno mi granja? = Can i show you my farm</p></blockquote><p>conozco(1), conoces(3), conocen(they), conocemos(we) - know | conocer= to know</p><blockquote><p>concoces a mi padre. = You know my father || (people)</p><p>mi hijo conoce muchos paises. = My son knows many countries. || (place)</p></blockquote><p>conoci(1), conociste(2), conocimos(we) - knew</p><p>empiezo(1), empieza(3), empiezan(they), empezamos(we) - start | empezar= to start</p><p>empece(1), empezaste(2), empezo(3), empezaron(they)</p><blockquote><p>~ a estudiar japones. = start to study japanese.</p></blockquote><p>pinto(1), pintas(2), pinta(3), pintan(they), pintamos(we) - paint | pintar= to paint | pintado/a - painted</p><blockquote><p>Ella pinta su dormitorio azul = She is painting her blue bedroom.</p><p>Ella pinta su dormitorio de azul = She is painting her bedroom blue.</p><p>esta pared, la pintamos de color azul. = This wall we paint it blue.</p><p>las paredes estan pintadas de azul= the walls are painted blue</p></blockquote><p>giro(1), gira(3), giramos(we) - turn | girar=to ~</p><blockquote><p>senor, gire a la izquierda. = Turn to the left</p></blockquote><p>espero(1), espera(3), esperamos(we), wait | esperar=to ~ | esperando=~ing</p><blockquote><p>~ un minuto = wait a minute</p><p>yo espero a juan en el hotel</p><p>tienes que esperar media hora = You have to wait half an hour</p><p>espero en la fila - wait in the line</p></blockquote><p>elijo(1), elige(3), eligen(they), elegimos(we) - choose | elegir=to ~</p><blockquote><p>tu eliges a los mejores estudiantes = you choose the best students.</p></blockquote><h3 id="及物动词"><a href="#及物动词" class="headerlink" title="及物动词"></a>及物动词</h3><p>cantan(they), cantamos(we) - sing | cantar= to sing</p><blockquote><p>mi mama me canta. = My mother sings to me</p></blockquote><p>vuelo(1) vuela(3) vuelan volamos - fly to | volar= to fly</p><blockquote><p>~ un avion = fly a plane</p><p>ellos vuelan de Boston a Alemaniu</p></blockquote><p>mando(1), manda(3), mandamos(we) - send | mandar=to ~ </p><p>estuve(1), estuviste(2), estuvo(3), estuvieron(they), estuvimos(we) - was/were</p><blockquote><p>ayer estuve en madrid = yesterday i was in madrid</p><p>donde estuviste la semana pasada = where were you last week</p></blockquote><p>como, come, comen - eat  | comer=to eat | comiendo= ~ing</p><p>comi(1), comiste(2), comio(3), comieron(they), comimos(we) - ate</p><blockquote><p>esos sandwiches, ellos no los comen. = Those sandwiches, they are not eating them.</p></blockquote><p>hablo(1), hablas(2), hablamos(we), hablan(they) - speak  | hablar=to speak | hablando= speaking | hable(ud), habla(tu) - command</p><p>hablaste(2)</p><p>necesito(1), necesitas(2),necesita(3) - need | necesitala - need it</p><p>tengo(1), tienes(2), tiene(3), tienen(they), tenemos(we) - have | tener=to have</p><p>tenia(1) - used to have</p><p>tuve(1),tuviste(2),tuvo(3) - had</p><blockquote><p>tengo que estudiar= have to study</p><p>tengo/tiene puesto una ropa. = I (she) have on a clothes.</p><p>tengo/tiene ganas <strong>de</strong> jugar al futbol. = feel like playing soccer</p></blockquote><p>bebo(1),bebes(2), bebe(3) - drink | beber=to drink | bebiendo=~ing | beba(ud), bebe(tu) - command</p><p>bebi(1), bebiste(2), bebimos(we) </p><blockquote><p>senora, beba este jugo, por favor. = Ma’am, drink this juice, please</p></blockquote><p>quiero(第一人称), quieres(2), queremos(we), quieren(you guys)- want </p><blockquote><p>Mi abuelo me quiere mucho. = My grandfather loves me very much.</p></blockquote><p>queria(3), querian(they) - wanted</p><p>pago(1), pagas(2), paga(3), pagan(they), pagamos(we) - pay | pagar=to pay</p><blockquote><p>cuanto pagaste por ese vestido negro = how much did you pay for that black dress</p></blockquote><p>vivo(1), vives(2), viven(they), vivimos(we) - live | vivir= to live</p><p>vivi(1), viviste(2) - lived</p><p>vivire(1) viviras(2), viviremos() viviran(they) - will live</p><p>compro(1), compras(2), compra(3), compran(they), compramos(we) - buy | comprar - to buy</p><blockquote><p>Yo voy de compras. = I go shopping.</p><p>senor and senora lopez, compren estos libros = Mr. and Mrs. Lopez, buy these books. </p></blockquote><p>estudio(1), estudias(2), estudia(3), estudiamos(we), estudian(they) - study | estudiar=to</p><p>study | estudiando= studying | estudia(tu) - command</p><p>estudie(1), estudiaste(2) - studied</p><p>leo(1), lees(2) leen(they) - read <lei ruo=""> | leer = to read | leyendo=~ing | lee(tu) = command</lei></p><p>lei(1) leiste(2),leyo(3), leyeron, leimos</p><p>escribo(1),escribes(2),escriben(they), escribimos(we) - write | escribir= to write | escribiendo=~ing | escriba(ud) = command</p><p>escribio(3), escribieron(they), escribimos(we) - wrote</p><p>comprendo(1), comprende(3), comprenden(they)  - understand | comprender=to ~</p><p>entiendo(1), entiende(3), entienden(they), entendemos(we) - understand | entender= to understand</p><blockquote><p>usted me entiende? = do you understand me?</p></blockquote><p>disfruto(1), disfruta(3), disfrutan(they), disfrutamos(we) - enjoy | disfrutar=to enjoy</p><p>disfrute(1)</p><blockquote><p>Mi plan es disfrutar mucho. = My plan is to enjoy a lot.</p></blockquote><p>trabajo(1),trabaja(3) - work | trabajar=to work | trabaje(ud) = command</p><blockquote><p>Senor Sanchez, trabaje para nosotros. = Mr. Sanchez, work for us.</p></blockquote><p>uso(1), usas(2), usa(3) - use, wear | usar - to use</p><p>duermo(1) duerme(3) duermen(they) dormimos(we) - slept | dormir=to sleep | dormido/a - asleep| durmiendo - sleeping</p><p>siento(1), siente(3), sentimos(we) - feel | sentir - to feel</p><blockquote><p>Me siento casada hoy. = I feel tired today.</p><p>Tu tambien te sientes feliz. = You also feel happy.</p><p>Como se siente tu hermano? = how does your brother feel?</p><p>siento always requires a ‘me’ in front of it. Sientes requires a ‘te’. Siente requires a ‘se’. </p><p>me siento enfermo del estomago = I feel sick to my stomach</p></blockquote><p>gusta - like | te gustaria - would you like to | gustar= to like</p><blockquote><p>me gusta mucho el te = I like tea a lot.</p><p>A ellos les gusta … = They like</p><p>A mi padre le gusta el baloncesto. = My father likes basketball. (Le gusta means it is pleasing to him/her/it. For”A mi padre le gusta el baloncesto, you are literally saying Basketball i pleasing to my father. To my father=A mi padre)</p><p>A el que le gusta?=  What does he like?</p><p>Me gustan estos vinos chilenos. = I like these Chilean wines.</p><p>Esto te va a gustar mucho. </p><p>— There isn’t a Spanish verb for “to like”. Instead, in Spanish you use “gustar”, which literally means “to please”. As a result, the subject and object are reversed. In this case, “Esto te va a gustar mucho” means “This is going to please you a lot”. “This” is the subject and “you” is the object so you have to use “va” instead of “vas” because it is “This” doing the action.</p><p>Creo que te va a gustar = I think you are going to like it</p><p>No me gustaron los platos de este restaurante</p></blockquote><p>encanta - love</p><blockquote><p>me encanta - I love</p><p>te encanta - you love</p><p>nos encanta - we love</p><p>les encanta - they love</p><p>a usted le encanta - you live</p></blockquote><p>enterarse de - find out </p><p>entere(1) enteraste(2) entero(3) enteramos(we) enteraron(they) - found </p><p>camino(1), caminan(they), caminamos(we) - walk | caminar=to walk</p><p>caminamos(we) - walked</p><p>tome(1), toma(3), tomamos(we) - take| tomar=to take</p><blockquote><p>senor, tome el autobus aqui.</p><p>carmen, toma el autobus.</p></blockquote><p>irme - leave</p><p>creo(1) - think</p><blockquote><p>creo que = I think that</p></blockquote><p>miro(1), mira(3), miran(they), miramos(we) - watch | mirar = to watch | mirando=~ing</p><p>mir$\acute o$(3), mirarion(they) - watched</p><blockquote><p>estamos mirando la tele en la casa = we are watching tv in the living room</p><p>miro las fotos = look at the photos</p></blockquote><p>juego(1), juegas(2), juegan(they), jugamos(we) - play | jugar= to play </p><blockquote><p>jugar al baloncesto = play basketball</p><p>jugar al tenis = play tennis</p></blockquote><p>toco(1), toca(3), tocan(they), tocamos(we)- play ||tocar= to play <play instruments=""></play></p><p>nado(1), nada(3), nadan(they), - swim | nadar - to swim</p><p>corro(1), corres(2), corren(they), corremos(we) - run | correr= to run</p><p>corrimos(we)</p><p>dibujo(1), dibujas(2), dibujan(they), dibujamos(we) - draw | dibujar - to draw | dibujando=~ing</p><p>levanto(1), levantas(2), levanta(3) - get up |levantarme /levantarte /levantarse= to get up | levantate(2), levantese(ud)=! </p><blockquote><p>me levanto a las diez. = I get up at 10.</p><p>A veces tu te levantas cansada. = Sometimes you get up tired.</p><p>El se levanta = he gets up.</p><p>Mi esposo nunca se levanta conmigo. = My huaband never gets up with me.</p><p>el no quiere levantarse. = he doesn’t want to get up.</p><p>Tienes que levantarte ahora = you have to get up now</p><p>necesito levantarme temprano manana = I need to get up early tomorrow</p><p>levantate rapido = get up fast</p><p>levantate de la silla = get up from the chair</p></blockquote><p>querria - I would like</p><p>quisiera(1) - would like</p><blockquote><p>quisiera algo de beber = i would like something to drink</p></blockquote><p>traigo(1), traes(2), traemos(we) - bring| traer - to ~</p><p>traje(1), trajiste(2), trajo(3)</p><p>cojo(1), coges(2), cogen(they) - take | coger=to ~</p><blockquote><p>voy a coger el vestido rojo = i am going to take the red dress</p></blockquote><p>limpio(1), limpias(2), limpia(3), limpian(they), limpiamos(we) - clean |limpiar= to clean</p><p>limpi$\acute o$(3), limpiaron(they) - cleaned</p><p>busco(1), buscas(2), busca(3), buscan(they) | buscar= to look for/ search/ |buscando=~ing</p><p>cierra(3), cierran(they), cerramos(we) - close | cerrar= to close | cierra(tu) - command</p><p>cerro(1), cerraron(they) - closed</p><p>abro(1), abre(3), abren(they), abrimos(we) - open| abrir= to open | abriendo=~ing |abre(tu) = command</p><p>abrio(3), abrieron(they), abrimos(we) - opened</p><blockquote><p>estamos abriendo los libros = we are opening the books</p></blockquote><p>puedo probar - try on</p><blockquote><p>Dónde me puedo probar esta camisa? = Where can I try on this shirt?</p></blockquote><p>cuesta(3), cuestan(they) - cost</p><blockquote><p>Todas las camisas cuestan veinte pesos. = All the shirts cost twenty pesos.</p><p>cuanto cuestan los tomates? = How much do the tomatoes cost</p><p>cuanto cuestan las naranjas? </p></blockquote><p>visito(1), visita(3), visitan(they), visitamos(we) - visit | visitar= to visit</p><p>visite(1), visitaste(2), visitamos(we) - visited</p><blockquote><p>visito a sb. = visit sb</p><p>visito el parque = visit park.</p></blockquote><p>veo(1), ve(3), ven(they), vemos(we) - see | ver= to see</p><p>vimos(we)</p><p> veias(2)veia(3),veiamos(we) veian(they) - used to see</p><blockquote><p>tengo que ver a Duo. = I have to see Duo,</p><p>yo veo muchas peliculas. = I watch many movies.</p><p>veo la tele = watch tv</p><p>yo quiero verme muy bien = I want to look very good</p></blockquote><p>monto(1), monta(3), montamos(we) - ride | montar= to ride</p><p>montamos(we) - rode</p><blockquote><p>monto a caballo. = I ride horses.</p><p>monto las bicicletas = ride bikes</p></blockquote><p>hago(1), haces(2), hacen(they), hacemos(we) - make | hacer= to make | haciendo=~ing</p><p>hice(1), hiciste(2), hizo(3), hicieron(they), hicimos(we)</p><p>hacias(2)hacia(3)haciamos(we)hacian - used to do</p><blockquote><p>que hacen ustedes aqui? = What are you doing here</p><p>tu haces un sandwich? = are you making a sandwich?</p><p>hace su cama = make his bed.</p><p>hace la tarea = do the homework.</p><p>hace el trabajo. = do the work</p><p>hace muchas preguntas = ask many questions</p><p>hace cinco anos = 5 years ago</p><p>hace un ano = one year ago</p><p>pedro, haz tu trabajo = do you work</p><p>senor, haga preguntas= sir, ask questions</p></blockquote><p>bailo(1), bailas(2), bailan(they), bailamos(we) -dance | bailar= to dance</p><p>bailamos(we) - danced</p><p>paso(1), pasa(3), pasan(they), pasamos(we) - spend, pass | pasar= to spend</p><p>pasamos(we)</p><blockquote><p>paga una semana en Francia. = Spend a week in France.</p><p>me pasas esa regla? = can you pass me that ruler</p></blockquote><p>almuerza(3), almuerza(they), almorzamos(we) - have lunch | almorzar= to have lunch</p><p>almorce(1) - had lunch</p><p>pruebo(1) ,prueba(3), prueban(they), probamos(we) - try | probar / probarse= to try | pruebe(ud) = command | prob$\acute e$=~ed</p><blockquote><p>Senor, pruebe este vino blanco. = Sir, try this white wine</p><p>la senora necesita probarse esa falda = The lady needs to try that skirt</p><p>vas a probate estos pantalones = are you going to try on these pants</p><p><a href="https://www.spanishdict.com/translate/probarse" target="_blank" rel="noopener">link</a></p></blockquote><p>puedo(1), puede(3), pueden(they), podemos(we) - can, be able to | poder= to be able to</p><blockquote><p>es importante poder trabajar. = It is important to be able to work.</p></blockquote><p>pude(1), pusiste(2),pudo(3) - wan’t able to</p><p>comienzo(1) comienza(3) comienzan(they) | comenzar=to ~</p><blockquote><p>comienza a - start to </p></blockquote><p>pierdes(2), pierden(they), perdemos(we) - lose | perder= to lose</p><p>perdi(1) perdio(3) - missed, </p><p>interesa - be interested in</p><blockquote><p>me interesa | A pablo le interesa </p><p>Este libro me interesa mucho. = I am very interested in this book.</p></blockquote><p>practicamos(we) - | practicar= to practice</p><p>practique(1), practicamos(we)</p><p>s$\acute e$(1), sabes(2), saben(they), sabemos(we) - know | saber= to know</p><blockquote><p>como se dice gato en italiano? = How do you say cat in italian</p><p>como se escribe el nombre? = How you write the name?</p><p>No lo s$\acute e$ = I don’t know</p><p>sabe usted donde esta el cine? = Do you know where the theather is</p><p>yo se que - i know that</p></blockquote><p>supe(1), supiste(2),supo(3) - knew</p><p>lleva(3), llevan(they), llevamos(we) - carry | llevar= to carry</p><blockquote><p>lleva puesto un sombrero(3) - wear a hat</p></blockquote><p>cocino(1), cocinan(they), cocinamos(we) - cook | cocinar= to cook</p><p>cocin$\acute o$(3), cocinaron(they)</p><p>cambia(3),cambiamos(we) - change |cambiar=to change | cambiando=~ing</p><p>pienso(1), piensas(2), piensan(they), pensamos(we) - think | pensar=to~ | pensando=~ing</p><blockquote><p>~ en muchos cosas = think about many things</p><p>~ que = think that</p><p>~ que no = I don’t think so</p><p>pienso en ti = think of you</p></blockquote><p>parece - think</p><blockquote><p>que te parece = what do you think</p></blockquote><p>cenar=to have dinner</p><p>cenamor(we) - had dinner</p><p>desayunar=to have breakfast</p><p> encontra(3) - find | encontrar= to find</p><p>me/se encuentro(1), encuentra(3) - meet up </p><blockquote><p>encontra con - meet up with</p></blockquote><p>recuerdo(1), recuerda(3) - remember| recordar=to remember</p><p>doy(1), da(3), dan(they) - give | dar= to give</p><p>dio(3),dimos(we),dieron(they) - gave</p><p>pregunto(1), pregunta(3), preguntan(they)- ask | preguntar=to ~</p><blockquote><p>por que no le preguntas a Maria ahora = Why don’t you ask Maria now</p></blockquote><p>sigo(1), sigue(3), siguen(uds), seguimos(we) - continue, follow | seguir=to ~</p><blockquote><p>siga por aqui = continue through here</p><p>maria, sigue cinco minutos mas. = maria, continue for 5 more minutes.</p></blockquote><p>olvido(1), olvida(3), olvidamos(we) - forget | olvidar=to ~</p><blockquote><p>ella no tiene que olvidarse de su cita = she must not forget her appointment</p><p>ella se olvida de beber mucha agua = she forgets to drink a lot of water</p></blockquote><p>morir - to die | muriendo=dying</p><p>smoke | fumar=to ~</p><p>respondo(1) responde(3) - answer</p><p>nevar - to snow | nevando - snowing</p><p>llover - to rain | lloviendo - raining</p><p>oigo(1),oye(3),oimos(we) - hear | oir - to hear</p><p>quedo(1), queda(3), con - get together withs</p><p>acabar - to finish</p><p>sueno(1) suena(3) sonamos(we) suenan(they) con - dream about | sonar - to ~</p><p>prefiero(1),prefiere(3),preferimos(we) | preferir - to prefer</p><p>odio(1) odia(3) | odiar - to hate</p><blockquote><p>odio a otras personas = hate other people</p><p>odia la clase  = hate the class</p></blockquote><p>para(3) paramos  | parar de - to stop doing / pararme - to stand up</p><p>haber=to be</p><p>sirvo(1) sirve(3) - serve| servir - to ~</p><p>pido(1) pide(3) - order | pedir=to ~</p><blockquote><p>quiero pedir prestado un poco de dinero - want to borrow a little money</p></blockquote><p>parece - </p><blockquote><p>esto me parece un poco aburrido = this seems a little boring to me</p></blockquote><p>acompano(1),acompana(3)  - go with sb | acompanar</p><p>converso(1), conversamos(we) | conversar</p><p>recibir - to receive</p><p>apaga(3) - turn off | apagar=to ~ / esta apagado/a - is offencender - to turn on | esta</p><p>enciendo(1), encendemos(we) - turn on | encendido/a = be on</p><p>perder - to lose | perdido/a - lost</p><p>deberias(2),deberia(3) - you should</p><p>hay que - one must</p><p>duele(3),duelen(they) - hurt| doler= to ~</p><blockquote><p>me duele le espalda = my back hurts</p><p>me duelen mucho los pies = my feet hurt a lot</p><p>papa tiene dolor de estomago = dad has a stomachache</p><p>no te va a doler nana = it isn’t going to hurt you at all.</p></blockquote><p>mover - to move</p><p>descansas(2), descansamos(we) | descansar - to rest</p><p>cuido(1),cuida(3)cuidan(they) | cuidar - to take care of</p><blockquote><p>su trabajo es cuidar a los enfermos = her job is to take care of sick people</p></blockquote><p>enviar - to send</p><p>aprobar - to pass</p><p>ganar - to win, earn</p><p>romper - to break</p><p>pescar - to fish</p><p>entrar - to enter/go</p><p>reparar - to repair</p><p>pedir - to ask for</p><p>oir- to hear | oyendo-listening to</p><p>repetir - to repeat</p><p>completar - to complete</p><p>creer - to believe</p><p>estar de acuerdo con - agree with</p><p>ocurrir - happen</p><p>preparar - to prepare</p><p>reuni(1), reunimos(we) - met</p><blockquote><p>ayer yo me reuni con mi jefe - yesterday i met with my boss</p></blockquote><p>gastar - spend</p><p>explica(3) - explain | explicar=to ~</p><p>acampa(3), acampamos(we) | acampar - camp</p><p>colecciona(3) coleccionan(they) | coleccionar - collect</p><p>esquiar - ski</p><p>navego(1), navega(3) | navegar - sail</p><p>presto(1) prestar - lend</p><p>robar - steal</p><p>descargar - download</p><p>copiar - copy</p><p>deciden(they) | decidir - decide</p><p>nacer - be born</p><p>naci(1)naciste(2) nacio(3) nacieron(they) - was born</p><p>patinar - skate</p><p>compartir - share</p><p>empacar - pack</p><p>empaque(1) - packed</p><p>alquilar - rent</p><p>ahorrar - save</p><p>termino(1) terminamos de - finish</p><p>acabo(1) acaba(3) acabamos(we) acaban(they) de - just doing</p><p>cancelar - cancel</p><p>seco(1) - dry | secar=to ~</p><p>seque(1),secaste(2),seco(3) - dried</p><p>vende(3) - sell | vender=to ~</p><p>vendio(3),vendieron(they) - sold</p><p>grita(3), gritan(they) a - shout at | gritar=to ~</p><blockquote><p>Ella le grita a su hermano.</p><p>por que gritan los ninos</p></blockquote><p>anado(1) anade(3) - add | anadir=to ~</p><p>sucede(3) - happen | suceder=to ~</p><p>sucedio(3) - happened</p><p>rezar - to pray</p><p>intentar - to try</p><p>llorar - to cry</p><p>reserva(3) | reservar - to reserve</p><p>planeo(1),planea(3) | planear - to plan</p><p>regreso(1) regresa(3) | regresar de - to return from</p><p>surfeo(1) surfea(3) surfean(they) - surf surfear</p><p>creci(1) creciste(2) crecio(3) crecimos(we) crecieron - grew up</p><p>significa(3)significan(they) - mean</p><p>ofrezco(1)ofrece(3)ofrecen(they) - offer | ofrecer</p><p>adivinaremos - will guess</p><p>repruebo(1) repruebas(2) reprobamos reprueban- fail</p><p>corta(3)cortan | cortar - cut</p><p>imprimir - print</p><p>suma(3) - add | sumar</p><p>meno - minus</p><p>escalar - climb</p><p>contactar - contact</p><p>agradezco(1) - thank | agradecer </p><p>deseo(1)desea(3)desean - wish</p><blockquote><p>te deseo un buen viaje - i wish you a good trip</p></blockquote><p>guardo(1) guarda(3)- save,keep | guardar</p><p>touch | tocar </p><p>cubre(3) - cover</p><p>salta(3) saltan - jump</p><h2 id="形容词"><a href="#形容词" class="headerlink" title="形容词"></a>形容词</h2><h3 id="前面"><a href="#前面" class="headerlink" title="前面"></a>前面</h3><p>mismo/a - same</p><blockquote><p>los mismos problemas</p></blockquote><p>ambo / amba - both</p><blockquote><p>me interesan ambos restaurantes. = I am interested in both restaurants.</p></blockquote><p>querido/a - dear</p><blockquote><p>querida amiga ana - dear friend ana</p></blockquote><p>$\acute u$ltimos - last, final</p><blockquote><p>estos son mis ultimos veinte dolares</p></blockquote><p>buen - good</p><blockquote><p>es un buen precio = it is a good price</p></blockquote><p>mal - bad / malisima - really bad</p><blockquote><p>un mal cafe = bad coffee</p></blockquote><p>siguiente - next</p><p>tanto - so many</p><p>Muy - very</p><p>demasiado - very much</p><blockquote><p>papa trabaja demasiado usualmente. = Dad usually works too much.</p></blockquote><p>elegante - elegant <eli gan="" dei=""></eli></p><p>inteligente - inteligent <in te="" li="" hen="" dei=""></in></p><p>bonito/a - pretty <bo ni="" dou=""></bo></p><p>bello/a - beautiful</p><blockquote><p>un dia bello - a beautiful day</p></blockquote><p>hermoso - beautiful</p><p>grande - big </p><blockquote><p>Grande drops the -de before a noun, and the meaning is “great”. so “una mujer grande” is “a big woman” while “una gran mujer” is “a great woman”</p></blockquote><p>peque$\acute n$o/a - small </p><p>pequenito/a - really small</p><p>perfecto - perfect </p><p>interesante - interesting <in dei="" rei="" san="" die=""></in></p><p>caro/a - expensive <ga ruo=""></ga></p><p>barato/a - cheep <ba ra="" do=""></ba></p><p>diferente - different </p><p>igual - same</p><p>favorito, favorita - favorite </p><p>preferida - favorite</p><p>c$\acute o$modo/a - comfortable </p><p>duro - hard</p><p>dificil - difficult <di fi="" ci=""></di></p><p>f$\acute a$cil - easy <fa ci=""></fa></p><blockquote><p>tengo dias faciles = have easy days</p></blockquote><p>cansado(pos), cansada(neg) - tired <gan sa="" da=""></gan></p><p>contento/a , feliz - happy <fei li="" s=""></fei></p><blockquote><p>felices(复数)</p></blockquote><p>divertido - fun</p><p>gracioso/a - funny </p><p>joven - young </p><p>simp$\acute a$tico/a - nice</p><p>triste - sad</p><p>enfermo/a - sick</p><blockquote><p>los enfermos - sick persons</p></blockquote><p>emocionada - excided</p><p>emocionante - exciting</p><p>alto/a - tall</p><p>alto - louder</p><p>bajo/a - short</p><p>bajito - really short</p><p>aburrido/a - bored <a bu="" ri="" ta=""></a></p><p>enojado/a - angry</p><p>preocupado/a - worried</p><p>nuevo/a - new</p><p>viejo/a - old</p><p>sorprendido/a - surprised</p><p>bueno - good</p><p>bunenisimo/a - really good</p><p>moderno / moderna - modern</p><p>mayor - older</p><p>menor - younger</p><p>nervioso/a - nervous</p><p>sucio/a - dirty</p><p>limpio/a - clean</p><p>abierto/a - open</p><p>cerrado/a - closed</p><p>rubio/a - blond</p><p>morena - brunette</p><p>moreno - dark(-skin/-hair)</p><p>guapo/a - handsome</p><p>responsable - responsible</p><p>perezoso - lazy</p><p>estudiosa - studious</p><p>largo/a - long</p><p>nublado - cloudy</p><blockquote><p>No esta nublado hoy. = It’s not cloudy today.</p></blockquote><p>oscuro - dark</p><p>menos - less</p><blockquote><p>Quiero una blusa menos cara. = I want a less expensive blouse.</p><p>Esta silla es menos comoda = This chair is less comfortable</p></blockquote><p>m$\acute a$s - more, </p><blockquote><p>La falda es mas comoda que el vestido. = The skirt is more comfortable than the dress.</p><p>Necesita una abrigo mas grande. = I need a larger coat.</p><p>Estos zapatos son mas comodos = These shoes are more comfortable</p></blockquote><p>el/la $\acute a$m - most</p><blockquote><p>Maris es la mas alta de la clase. = Maria is the tallest in the class,</p></blockquote><p>peor - worse</p><p>el/la peor - worst</p><blockquote><p>el peor del mundo = the worst in the world</p><p>David es el peor jugador de beisbol. = David is the worst baseball player.</p></blockquote><p>mejor - better, best</p><p>famoso/a - famous</p><p>delgado/a - thin</p><p>gordo - fat</p><p>popular - popular</p><p>listo/a - ready</p><p>dulce - sweet</p><p>caliente - hot</p><blockquote><p>necesito agua caliente = i need hot water</p></blockquote><p>r$\acute a$pida - rapid</p><p>picante - spicy</p><p>lenta - slow</p><p>malo/a - bad, poor</p><p>ligero/a - light</p><p>seguro/a - safe</p><blockquote><p>creo que si pero no estoy seguro = i think so but i am not sure</p></blockquote><p>cercano/a - near</p><blockquote><p>esta es la tienda mas cercana</p></blockquote><p>pasado/a - last</p><blockquote><p>la semana pasada = last week</p><p>el ano pasado - last year</p><p>el mes pasado - last month</p></blockquote><p>pobre - poor</p><p>fantastico/a - fantastic</p><p>facilisimo - really easy</p><p>cansadisimo - really tired</p><p>caluroso/a - hot</p><p>posible - possible</p><p>excelente - excellent</p><p>extra - extra</p><p>estupendo/a - wonderful</p><p>saludable - healthy</p><p>vacia - empty</p><p>lleno/a - full</p><blockquote><p>estas tazas estan llenas de agua - these cups are full of water</p></blockquote><p>asado/a - grilled</p><p>amable - kind</p><p>tranquilo/a - calm</p><p>disponible - available</p><p>vegetariano - vegetarian</p><p>bellisimo/a - really lovely</p><p>carisimo/a - really expensive</p><p>especial - special</p><p>genial - cool,great</p><p>bella - lovely</p><p>palido/a - pale</p><p>verdadero/a - true</p><blockquote><p>eso no es verdadero - that is not true</p></blockquote><p>verdad  - true  (la verdad - truth)</p><p>cierto - true</p><p>semanal - weekly</p><p>correcto/a - right</p><p>loco - crazy</p><p>enfadado - angry</p><p>interesado/a - interested</p><p>casada - married</p><p>roto/a - borken</p><p>mojada - wet</p><p>seco - dry</p><p>soleado - sunny</p><p>caluroso - warm</p><p>fuerte - strong</p><p>peligroso/a - dangerous</p><p>util - useful</p><p>pesada - heavy</p><p>gracioso - funny</p><p>internacional - international</p><p>increible - incredible</p><p>silvestre - wild</p><p>conocido/a - well-known</p><p>suave - soft</p><p>diaria - daily</p><p>avanzado/a - advanced</p><p>maravilloso/a - wonderful</p><p>salvaje - wild</p><p>nacional - national</p><p>extrano - strange</p><p>gratis - free</p><p>usual - normal</p><p>alejado/a de - far from</p><p>molesto/a - upset</p><p>muerta - dead</p><p>liso - straight</p><p>profundo - deep</p><p>claro - clear</p><p>calido/a - warm</p><p>agradable - pleasant</p><p>negativo/a - negative</p><blockquote><p>no quiere decir nada negativo. - she doesn’t want to say anything negative</p><p>gente negativa - negative people</p></blockquote><p>soltero/a - single</p><p>ancho/a - wide</p><p>suficiente - enough</p><p>basico - simple</p><p>atractivo - attractive</p><p>extranjero - foreign</p><p>inusual - unusual</p><p>ruidoso/a - noisy</p><h2 id="副词"><a href="#副词" class="headerlink" title="副词"></a>副词</h2><p>incluso - even</p><p>siempre - always <si em="" pre=""></si></p><p>tambien - also</p><p>bien - well</p><p>poco - little</p><p>un poco - a little, somewhat </p><blockquote><p>un poco de sal = a little bit of salt</p><p>entiendo un poco de ingles = i understand a little english</p></blockquote><p>a veces - sometimes</p><p>a menudo - often</p><p>nunca - never</p><blockquote><p><em>Nunca</em> is a negative adverb, the negation must always precede the verb in Spanish.</p></blockquote><p>jamas - never</p><p>tampoco - neither</p><p>otra vez - again</p><p>muchos | muchas - many</p><p>vario/a - several</p><p>unos / unas - some</p><p>alg$\acute u$no/a - some</p><blockquote><p>algun buen restaurante = any good restaurant</p><p>hay alguna playa por aqui = is there any beach around here</p></blockquote><p>ning$\acute u$n - any</p><blockquote><p>no quiero comer ningun queso = I don’t want to eat any cheese</p></blockquote><p>poca - few</p><p>cerca - near | </p><blockquote><p>estas cerca del restaurante ahora. = You are near the restaurant now.</p><p>la estacion de tren esta cerca de los parques. = The train station is close to the parks.</p><p>la universidad esta cerca de aqui. = the university is near here</p></blockquote><p>lejos - far | lejos de=far from </p><p>al lado de - next to</p><p>juntos / juntas - together</p><p>porque - because</p><p>pero - but</p><p>todos / todas - every</p><blockquote><p>todos los lunes = every monday</p><p>todos los dias = every day</p><p>todas las mananas = every morning</p><p>todos los anos = every year</p><p>todos los nninos = all children</p><p>todo el bano = the whole bathroom</p></blockquote><p>cada - every</p><blockquote><p>cada tarde = every evening</p></blockquote><p>algo - something</p><p>nada - nothing</p><blockquote><p>nana es barato en esta tienda = nothing is cheap at this store</p></blockquote><p>ninguno - none</p><blockquote><p>ninguno de nosotros tiene dinero ahora. = None of us has money now</p></blockquote><p>alguien - anyone</p><p>hay - there is/are</p><p>habia - there were</p><blockquote><p>no hay carros. = there aren’t cars.</p></blockquote><p>seria - would be</p><p>usualmente - usually</p><p>normalmente - normally</p><p>solo - only</p><p>temprano - early</p><p>pronto - soon</p><p>despacio - slowly</p><p>ya - already</p><p>luego - later</p><p>derecho - straight</p><p>enfrente de - in front of</p><p>completamente - completely</p><p>generalmente - generally</p><p>frecuentemente - frequently</p><p>encima de - on top of</p><p>de vez en cuando - once in a while</p><p>por aqui - around here</p><p>entonces - then</p><p>desafortunadamente - unfortunately</p><p>durante el dia - during the day</p><p>sin duda - definitely</p><p>probablemente - probably</p><p>lentamente - slowly</p><p>solo/a - alone</p><p>adentro - inside</p><blockquote><p>ellas trabajan adentro porque esta lloviendo = they work inside because it is raining</p></blockquote><p>afuera - outside</p><p>propia / propio - own</p><blockquote><p>la reina tenia su propio dinero - the queen had her own money</p></blockquote><p>por fin - finally</p><p>claramente - clearly</p><p>a lo largo de - along</p><p>bastante - quite</p><p>todavia - yet</p><p>rapidamente - quickly</p><p>gravemente - seriously</p><p>facilmente - easily</p><p>inmediatamente - immediately</p><p>casi - almost</p><p>luego - later</p><p>recien - just</p><p>tal - such</p><p>negativa - negative</p><p>etcetera - etc.</p><p>quizas - perhapes</p><h2 id="短语"><a href="#短语" class="headerlink" title="短语"></a>短语</h2><p>De nada - you are welcome. </p><p>bienvenido/a (a) - welcome (to)</p><p>Perd$\acute o$n / perdona/perdone-  Pardon.</p><p>Lo siento - I am sorry. </p><p>Por favor - Please.</p><p>Disculpe/a - Excuse me.</p><blockquote><p>I believe disculpe is the formal command form, that would be appropriate with a stranger. Disculpa is familiar. So it should be disculpe usted or disculpas tu where the pronoun may not necessarily appear.</p><p>Disculpa, me ayudas con la maleta</p></blockquote><p>Adios - Bye.</p><p>Gracias - Thanks.</p><p>Buenos dias - Good morning.</p><p>Buenas noches - Good night.</p><p>Mucho gusto - Nice to meet you.</p><p>Me llamo .. - My name is .. </p><blockquote><p>ellas se llaman A and B. = Their names are A and B.</p></blockquote><p>Es mi nombre. - It is my name</p><p>no estoy bien.  - I am not okay.</p><p>Est$\acute a$s cansada?  - are you tired?</p><p>Cuantos anos tienen tus padres? - How old are your parents</p><p>De verdad? - For real?</p><p>Bien, y t$\acute u$ - fine, and you <bian></bian></p><p>Si, estoy bien - yes, i am fine</p><p>No, no estoy bien - no, i am not okay.</p><p>como esta usted | c$\acute o$mo est$\acute a$s | que tal estas - how are you</p><p>qu$\acute e$ tal - how are you/ How are things/How are you doing? </p><p>se$\acute n$orita - miss, young lady <sei nue="" ri="" ta=""></sei></p><p>se$\acute n$or - mr | se$\acute n$ores</p><p>se$\acute n$ora - mrs,woman, ma’am <si nue="" ra=""></si></p><p>buenas tardes - good afternoon</p><p>hasta ma$\acute n$ana - see you tomorrow <a sta="" ma="" nia="" na=""></a></p><p>hasta luego - see you later <a sta="" lu="" ei="" go=""></a></p><p>c$\acute o$mo te llamas? - what is your name <m></m></p><p>c$\acute o$mo se llama? - what is your name <female></female></p><p>est$\acute a$s ocupado/ocupada? - are you busy? <o gu="" ba="" da=""></o></p><p>Te llamas … ? - is your name …?</p><p>no estoy mal. - I am not unwell.</p><p>ahora mismo - right now</p><p>una hora mas - one hour more</p><p>todo el mundo - everybody</p><p>mas tiempo - longer</p><p>por supuesto - of course</p><p>el veinte de junio - on june 20th</p><p>sobre las cinco - around 5 pm.</p><p>a las ocho - at 8pm.</p><p>el viernes = on friday</p><p>creo que si - I think so</p><p>creo que no- i don’t think so</p><p>nos vemos - see you</p><p>por ejemplo - for example</p><p>a la misma hora - at the same time</p><p>a la hora del cena - at dinner time</p><p>de nina/nino - as a child</p><p>el especial del dia - the special of the day</p><p>buen provecho - bon appetit</p><p>a lo mejor - maybe</p><p>como te va - how is it going</p><p>como le va, senor - how are you</p><p>el Dia de Accion de Gracias - thanksgiving</p><p>la Nochevieja - new year’s eve</p><p>la Nochebuena - chrismas Eve</p><p>la Navidad - christmas</p><p>el Ano Nuevo - new year</p><p>el resto de - the rest of</p><p>del dos al diez de mayo - from second to tenth</p><p>hablia una vez - once upon a time</p><p>en cualquier lugar - anywhere</p><p>por cierto - by the way</p><p>un poquito de - a little bit of</p><p>para siempre - forever</p><p>en realidad - in reality</p><p>de todas formas - anyway</p><p>un par de - a couple of</p><p>tal vez - perhaps</p><p>menos mal - luckily</p><p>tengo prisa - in a hurry</p><p>a unos - a few</p><h2 id="食物"><a href="#食物" class="headerlink" title="食物"></a>食物</h2><p>la compra - grocery</p><blockquote><p>hago la compra - buy the groceries.</p></blockquote><p>pl$\acute a$tano - banana</p><p>la fruta - fruit</p><p>la uva - grape</p><p>manzanas - apple</p><p>la banana - banana </p><p>el mango - mango</p><p>limonada - lemonade</p><p>el t$\acute e$ - tea | t$\acute e$s - teas</p><p>el vino - wine</p><p>comida - food</p><p>la galleta - cookie</p><p>el pollo - chicken</p><p>el cereal - cereal</p><p>el pan - bread</p><p>el alcohol - alcohol</p><p>el champan - champagne</p><p>el arroz - rice</p><p>el hielo - ice</p><p>la hamburguesa - hamburger</p><p>el aceite - oil</p><p>la miel - honey</p><p>el chocolate - chocolate</p><p>crema - cream</p><p>el pastel - cake</p><p>la cerveza - beer</p><p>el refresco - soda</p><p>frijole - bean</p><p>la bebida - drink</p><p>la botella - bottle</p><p>la patata/papa - potato</p><p>la sopa - soup</p><blockquote><p>el nino no quiere tomar la sopa. = The boy doesn’t want to have soup</p></blockquote><p>frito/a - fried <free ta=""></free></p><p>el postre - dessert</p><p>la pizza - pizza</p><p>el huevo - egg</p><p>la mantequilla - butter</p><p>el vegetale - vegetable</p><p>el chorizo - sausage/chorizo</p><p>la salsa - sauce</p><p>una parrilla - barbecue, grill</p><p>mesa - table </p><p>mesita - little tableF</p><p>persona - person</p><p>la persona adulta - adult</p><p>el bar - bar</p><p>otro/a - another</p><blockquote><p>quiero jugar otros depoortes = i want to play other sports</p><p>esta pelicula es peor que la otra. = This movie is worse than the other.</p></blockquote><p>el restaurante - restaurant </p><p>el agua - water</p><p>el pan - bread </p><p>el pescado - fish </p><p>el pez -fish | los feces </p><p>el s$\acute a$ndwich - the sandwich</p><p>la carne - the meat <ga ruo="" ni=""></ga></p><p>el queso - the cheese <gue so=""></gue></p><p>una hamburguesa - hamburger <han bu="" gue="" sa=""></han></p><p>el jugo - the juice <hu go=""></hu></p><blockquote><p>ellos quieren dos jugos de <strong>mazana</strong>. = They want two apple juices.</p></blockquote><p>la naranja - orange, orange color</p><p>el vaso - the glass <va so=""></va></p><blockquote><p>dos vasos de agua = two glasses of water</p></blockquote><p>el caf$\acute e$ - the coffee</p><p>el az$\acute u$car - sugar <a shu="" car=""></a></p><p>los dulces - candy</p><p>leche - milk <le qie=""></le></p><p>la verdura - vegetable</p><p>ensalada - salad <en sa="" la="" da=""></en></p><p>la sal - salt <sa l=""></sa></p><p>el tomate -  tomato <do ma="" die=""></do></p><p>el men$\acute u$ - menu</p><p>el plato - plate,dish</p><blockquote><p>un plato de pollo = a chicken dish | un plato espanol = a spanish dish</p></blockquote><p>el bol - bowl</p><p>la cuchara - spoon</p><p>el cuchillo - knife</p><p>el tenedor - fork</p><blockquote><p>No tengo tenedor. = I don’t have a fork</p></blockquote><p>hambre - hunger</p><blockquote><p>tengo hambre - i am hungry</p></blockquote><p>sed - thirst</p><blockquote><p>tengo mucha sed. = I am thirsty.</p></blockquote><p>calor - heat, hot</p><blockquote><p>tengo calor = i am hot.</p></blockquote><p>frio - cold</p><blockquote><p>tengo frio = feel/be cold</p><p>tengo un abrigo bueno para el frio. = I have a good coat for cold weather.</p><p>Hace frio esta manana.= It is cold this morning</p></blockquote><p>miedo - afraid</p><blockquote><p>tengo miedo (de tu perro) = i am afriad (of your dog)</p><p>tengo un poco de miedo = i am a little scared</p></blockquote><p>sue$\acute n$o - sleepy</p><blockquote><p>tengo sueno = I am sleepy</p></blockquote><p>el desayuno - breakfast</p><blockquote><p>Tomo el desayuno. = Have breakfast.</p><p>hace el desayuno = make breakfast</p></blockquote><p>el almuerzo - lunch | </p><p>la cena - dinner</p><p>la pasta - pasta</p><p>la tostada - toast</p><p>la mermelada - jam</p><p>el jamon - ham</p><p>la cebolla - onion</p><p>yogur - yogurt</p><p>la pimienta - pepper</p><p>el champinon - mushroom</p><p>lata - can</p><p>la variedad - variety</p><p>una lista - list</p><p>oferta - sale</p><blockquote><p>las frutas estan en oferta - the fruit is on sale</p></blockquote><p>la zanahoria - carrot</p><p>la tortilla - omelet</p><p>jalea - jelly</p><p>el bistec - steak</p><h2 id="玩乐"><a href="#玩乐" class="headerlink" title="玩乐"></a>玩乐</h2><p>una tradicion - tradition</p><p>la invitacion - invitation</p><p>vida - life</p><p>excursion - hike</p><p>montanera - hiking</p><p>el mundo - world</p><p>viaje - trip</p><p>el turista - tourist</p><p>el turismo - tourism</p><p>el artista - artist</p><p>la iglesia - church</p><p>la catedral - cathedral</p><p>el barco - boat</p><p>pa$\acute i$s - country</p><p>la nacionalidad - nationality</p><p>la carrera - race</p><p>idioma/lengua - language</p><p>ortografia - spelling</p><p>los Estados Unidos - the United States <los eis="" da="" dos="" wu="" ni=""></los></p><p>ingl$\acute e$s - English</p><p>americano/a - American | americano:adj</p><p>Espa$\tilde n$a - Spain </p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">I soy de Spain = I am from Spain</div></pre></td></tr></table></figure><p>espanol - Spanish <es ban="" nue=""></es></p><p>China - China <ci na=""></ci></p><p>chino - chinese, 中文</p><p>Francia - France </p><p>francese/a - french</p><p>franc$\acute e$s - french</p><p>M$\acute e$xico - Mexico</p><p>mexicano - mexican <meo hi="" ga="" no=""></meo></p><p>cubano/a - cuban</p><p>Inglaterra - England</p><p>inglesa - English</p><p>Europa - Europe <ei u="" ruo="" pa=""></ei></p><p>europeo/a - European</p><blockquote><p>Nosotros no somos europeos = We are not European</p></blockquote><p>europea - european</p><p>portugu$\acute e$s - portuguese</p><p>Australia</p><p>italiano/a - italian</p><p>Canad$\acute a$ - Canada</p><p>Alemania - Germany</p><p>Alem$\acute a$n - German</p><p>Londres - London</p><p>japon$\acute e$s - japanese</p><p>chileno/a - Chilean </p><p>latino/a - Latin (adj)</p><p>suecia - sweden</p><p>el rio - river</p><p>el lago - lake</p><p>el pasaporte </p><p>el taxi </p><p>el tel$\acute e$fono </p><p>el anden - platform</p><p>la maleta - suitcase</p><p>equipaje -luggage</p><p>el hotel -</p><p>reservacion - reservation</p><p>el neumatico - tire</p><p>el coche - car</p><p>rueda - wheel</p><p>el autobus - </p><blockquote><p>en autobus = by bus</p></blockquote><p>barco - ship</p><p>el tren - train </p><p>tranvia - tram</p><p>el dinero - money</p><p>el oro - gold</p><p>plata - silver</p><p>el avi$\acute o$n - airplane</p><p>vuelo - flight</p><p>patineta - skateboard</p><p>aeropuerto - airport <ei ruo="" buo="" duo=""></ei></p><p>boleto - ticket </p><p>la entrada - ticket, entrance</p><p>salida - exit</p><p>reserva - reservation <dre sei="" va=""></dre></p><p>banco - bank | ~s</p><p>ba$\tilde n$o - bathroom</p><p>el servicio - restroom</p><p>la ducha - shower</p><p>museo - museum </p><p>una calle - street </p><p>basurero - wastebasket</p><p>la senal - sign</p><p>la ruta - route</p><p>la manera - way</p><p>la carretera - highway</p><p>la autopista - highway</p><p>trafico - traffic</p><p>la avenida - avenue</p><p>gasolina - gas</p><p>el puente - bridge</p><p>la cuenta - check </p><blockquote><p>nosotros pagamos la cuenta. = We pay the check.</p></blockquote><p>supermercado - supermarket </p><p>el mercado - market</p><p>ayuda - help</p><p>auxilio - help</p><p>la ciudad - city </p><p>la vista - view</p><p>la aldea - village</p><p>la empresa - company</p><p>el voleibol - volleyball</p><p>el hospital</p><p>la estacion - station | ~es</p><blockquote><p>Donde esta la estacion de autobuses?</p><p>in Spain they tend to use “autobuses” in the plural and “tren” in the singular when talking about stops and stations</p></blockquote><p>la parada - stop</p><blockquote><p>la parada del autobus = the bus stop</p></blockquote><p>la playa - beach | ~s</p><p>la colina - hill</p><p>la isla - island</p><p>el oceano - ocean</p><p>el mar - sea</p><p>la montana - mountaion</p><p>el campo - field</p><p>el parque - park</p><p>puerta - door </p><p>el porton - gate</p><p>la izquierda - left</p><blockquote><p>a la izquierda = to the left</p></blockquote><p>la derecha - right</p><p>el metro - subway</p><p>el lugar - place</p><p>el sitio - place</p><p>camisa - shirt </p><p>la corbata - tie</p><p>la bufanda - scarf</p><p>camiseta - T-shirt</p><p>sueter - sweater</p><p>el traje - suit</p><p>las botas - boots</p><p>el vaquero - jean</p><p>tienda - store </p><p>la toalla - towel</p><p>cuero - leather</p><p>ropa - clothes </p><p>el cinturon - belt</p><p>el guante - glove</p><p>la mochila - backpack</p><p>los calcetines - socks</p><p>color - color</p><blockquote><p>de que color es tu carro? = What color is your car?</p><p>de que color son tus lentes? = what color are your glasses</p></blockquote><p>verde - green <vei ve=""> - Una chaqueta verde (a green jacket)</vei></p><p>azul - blue <a zul=""></a></p><p>gris - grey </p><p>rosa - pink</p><p>rosado/a - pink</p><p>marr$\acute o$n - brown </p><p>castano - brown</p><p>rojo / roja - red </p><p>tinto - red</p><blockquote><p>vino tinto= red wine</p><p>If you said vino rojo, it would mean “wine whose color is red” </p></blockquote><p>blanco - white</p><blockquote><p>vino blanco = white wine</p></blockquote><p>amarillo - yellow</p><p>negro - black</p><p>morado/a - purple</p><p>chaqueta - jacket </p><p>blusa - blouse</p><p>la pijama - pajamas</p><p>vestido - dress | vestir=to ~</p><p>vestido/a - dressed</p><blockquote><p>las ninas ya estan vestidas de negro  = the girls are already dressed in black</p><p>cuando vas a vestirte = when are you going to get dressed</p></blockquote><p>sombrero - hat </p><p>gorra - cap,hat</p><p>falda - skirt</p><p>abrigo - coat </p><p>impermeable - raincoat</p><p>el cintur$\acute o$n - belt <cin tu="" ron=""></cin></p><p>un reloj - watch, clock</p><p>el regalo - gift <ren ga="" lo=""></ren></p><p>cartera - purse <ga dei="" ra=""> | ~s</ga></p><p>par - pair</p><blockquote><p>este par de pantalones = this pair of pants</p><p>tengo muchos pares de zapatos = I have many pairs of shoes</p></blockquote><p>el zapato - shoe | ~s</p><p>el pantalon - pant | ~es</p><p>los pantalones cortos - shorts</p><p>el precio - price</p><p>el probador - fitting room</p><p>talla - size</p><blockquote><p>tenemos tallas grandes y pequenas. = We have big and small sizes.</p></blockquote><p>el d$\acute o$lar - dollar</p><blockquote><p>quince dólares. = Fifteen dollars</p></blockquote><p>euro - euro</p><p>centavo - cent</p><p>efectivo - cash</p><p>cheque - check</p><blockquote><p>pago en efectivo - pay in cash</p></blockquote><p>la tarjeta de credito - credit card</p><blockquote><p>in efectivo o con tarjeta de credito? = Cash or credit?</p></blockquote><p>la tarjeta - card</p><p>la carta - card</p><p>d$\acute o$lar - dollar</p><p>la pelota - ball</p><p>partido - game</p><p>b$\acute e$isbol - baseball </p><p>el f$\acute u$tbol - football</p><p>el b$\acute a$squetbol</p><p>cumplea$\tilde n$os - birthday </p><p>el nacimiento - birth</p><p>deseco - wish</p><p>el teatro - theater</p><p>la obra de teatro - play</p><p>la pelicula - movie </p><p>el cine - movie theater</p><blockquote><p>vas al cine esta noche?= Are you going to the movies tonight?</p></blockquote><p>el baile - dance </p><p>el espectaculo - show</p><p>fiesta - party</p><p>el estadio - stadium</p><p>asiento - seat</p><p>concierto - concert</p><p>la m$\acute u$sica - music</p><p>jazz - jazz</p><p>dibujos animados - cartoons</p><p>dibujo - drawing</p><p>el comic - comic</p><blockquote><p>que tal si leemos estos comics - how about we read these comics</p></blockquote><p>el zool$\acute o$gico - zoo</p><p>la mu$\acute n$eca - doll</p><p>el juguete - toy</p><p>la foto - photo</p><p>conversacion - conversation</p><p>la charla - chat</p><p>el arte - art</p><p>la pintura - painting</p><p>hobby - hobby | hobbies</p><p>el museo - museum</p><p>la radio - radio</p><p>cancion - song</p><p>la ima$\acute a$gen - image</p><p>plan - plan</p><p>la opera - opera</p><p>el ajedrez - chess</p><blockquote><p>jugar al ajedrez - playing chess</p></blockquote><p>el golf - golf</p><p>el gimnasio - gym</p><p>descanso - rest / nos tomamos un descanso</p><p>la iglesia - church</p><p>sorpresa - surprise</p><p>una pena -shame</p><p>campamento - camp</p><p>adventura - adventure</p><p>los juegos de mesa - board games</p><p>cuento - story</p><p>momento - moment</p><p>album - album</p><p>la actividad - activity</p><p>sitio - site</p><p>la magia - magic</p><blockquote><p>puedes hacer magia - can you do magic</p></blockquote><p>el rompecabeza - puzzle</p><p>una postal - postcard</p><p>la cometa - kite</p><p>la raqueta - rocket</p><p>el perfume - perfume</p><p>el patin - skate</p><p>el globo - ballon</p><p>la bateria - drum</p><p>un escuter - scooter</p><p>una cita - date</p><h3 id="数字"><a href="#数字" class="headerlink" title="数字"></a>数字</h3><p>doble - double</p><p>segundo - second (time)</p><p>cero - zero <cei ruo=""></cei></p><p>uno - one </p><blockquote><p>uno de los vestidos = one of the dresses</p></blockquote><p>dos - two </p><blockquote><p>son las dos - it is 2pm.</p></blockquote><p>el tres - three <thre s=""></thre></p><p>cuatro - four <gua duo=""></gua></p><p>cinco - five <cin go=""></cin></p><p>las seis - six </p><p>siete - seven</p><p>las ocho - eight</p><p>las nueve - nine</p><p>las diez - ten</p><p>once - eleven</p><p>doce - twelve</p><blockquote><p>de diez a doce = from ten to twelve</p></blockquote><p>trece - thirteen</p><p>catorce - fourteen</p><p>quince - fifteen</p><p>dieciseis - sixteen</p><p>diecisiete - seventeen</p><p>dieciocho - eighteen</p><p>diecinueve - nineteen</p><p>veinte - twenty</p><p>veinti$\acute u$n / veintiuno- 21</p><p>veintid$\acute o$s - 22</p><p>veintitr$\acute e$s - 23</p><p>veinticuatro - 24</p><p>veinticinco - 25</p><p>veintisiete - 27</p><p>veintiocho - 28</p><p>treinta - 30</p><p>cuarenta - 40</p><p>cincuenta - 50</p><p>sesenta - 60</p><p>setenta - 70</p><p>ochenta - 80</p><p>noventa - 90</p><p>cien - 100</p><blockquote><p>esto cuesta ciento veinte dólares = this costs a hundred and twenty dollars.</p></blockquote><p>doscientos - 200</p><p>cuatrocientos - 400</p><p>quinientos - 500</p><p>seiscientos - 600</p><p>setecientos - 700</p><p>ochocientos - 800</p><p>novecientos - 900</p><p>mil - 1000</p><blockquote><p>tengo mil dolares = i have 1000 dollars</p><p>miles de anos - thousands of years</p></blockquote><p>million - million</p><blockquote><p>cuatro milliones de pesos = 4 million pesos</p><p>un million de dolares = a million dollars</p></blockquote><p>primero/primer - first</p><blockquote><p>en el primer piso = on the first floor</p></blockquote><p>segundo - second</p><blockquote><p>en el segundo piso = on the second floor</p></blockquote><p>tercero/a - third</p><blockquote><p>some words drop the -o when used before a masculine singular noun: primero, tercero, alguno, ninguno, uno, bueno, malo.</p><p>el tercer piso, primer piso, algún vino, ninguno libro, buen día, etc.</p><p>la tercera tienda tiene muy buenos precios. = the third store has very good prices.</p></blockquote><p>metro - meter</p><p>litro - liter</p><blockquote><p>un litro de agua - a liter of water</p><p>esta botella es de un litro - this bottle is one liter</p></blockquote><p>kilogramo - kilogram</p><p>kilometro - kilometer</p><p>grado - degree</p><blockquote><p>la temperature es de quince grados - the temperature is 15 degrees</p></blockquote><p>el horario - schedule</p><p>ahora - now</p><p>el futuro - future</p><p>un retraso - delay</p><p>tiempo - time, weather</p><blockquote><p>paga tiempo en la playa = spend time at beach</p><p>que tiempo hace ahora = what is the weather like now</p><p>hace mal tiempo. = The weather is bad.</p><p>hace buen tiempo hoy. = the weather is nice today</p><p>como esta el tiempo = how is the weather</p><p>todo el tiempo = all the time</p></blockquote><p>la noche - night</p><blockquote><p>en la noche = at night</p><p>en la manana = in the morning</p></blockquote><p>anoche - last night</p><p>la tarde - afternoon, evening, later</p><blockquote><p>come mas tarde = eat later</p><p>sale del trabajo tarde = leave work late</p><p>es tarde o temprano = is it late or early</p></blockquote><p>media - half, thirty</p><p>cuarto - quarter</p><blockquote><p>son las seis menos cuarto. = It is quarter to six.</p></blockquote><p>a las ocho = at eight</p><blockquote><p>a la una = at one o’clock</p><p>a las diez y meida = at half past ten</p><p>a las ocho menos cuarto = at quarter to five.</p></blockquote><p>hora - hour</p><blockquote><p>en media hora = in half an hour</p></blockquote><p>minuto - minute</p><blockquote><p>un minuto </p></blockquote><p>el calendario - calendar</p><p>mes - month</p><p>el enero - January</p><blockquote><p>el proximo enero = next month</p></blockquote><p>febrero - February</p><blockquote><p>en febrero = In February</p><p>el uno de febrero - the first of the February / February 1st</p></blockquote><p>marzo - March</p><p>abril - April</p><p>mayo - May</p><p>junio - June</p><p>julio - July</p><p>agosto - August</p><p>septiembre - September</p><p>octubre - October</p><p>noviembre - November</p><p>diciembre - December</p><blockquote><p>el tres de mayo - on May third</p></blockquote><p>el kilo - kilogram</p><blockquote><p>el kilo de carne es muy caro. = The kilo of meat is very expensive.</p><p>tres kilos de patatas = three kilos of potatoes.</p><p>cuantos kilos de manzanas necesitas? = How many kilos of apples do you need</p><p>cuanto cuesta el kilo de tomates? = How much does the kilo of tomatoes cost</p></blockquote><p>la fecha - date</p><p>primero - first</p><blockquote><p>que hacemos primero = what do we do first</p></blockquote><p>en el ano mil ochocientos cincuenta - in the year 1850</p><p>el ano setecientos noventa y ocho - the year 1798</p><p>el ano dos mil diez - the year 2010</p><p>a las diez de la noche - at 10 p.m.</p><p>al mediodia - at noon</p><p>la medianoche - midnight</p><p>el siglo -century</p><p>el principio - beginning</p><p>principiante - beginner</p><p>vece - time</p><blockquote><p>tres veces - three times</p></blockquote><h3 id="东西"><a href="#东西" class="headerlink" title="东西"></a>东西</h3><p>la cancelacion - cancellation</p><p>dorado/a - gold</p><p>espacio - space</p><p>el cuadrado - square</p><p>la tijera - scissor</p><p>la manta - blanket</p><p>un collar - necklace</p><p>una sortija - ring</p><p>el anillo - ring</p><p>la joya - jewelry</p><p>el brazalete - bracelet</p><p>el arete - earring</p><p>la c$\acute a$mara - camera</p><p>la flor - flower</p><p>la naturaleza - nature</p><p>el bosque - forest</p><p>la madera - wood</p><p>el arbol - tree</p><p>el c$\acute e$sped - grass</p><p>el tipo - type</p><p>el p$\acute a$jaro - bird</p><p>el juego - game</p><p>lente - glass</p><blockquote><p>uso lentes = wear glasses</p></blockquote><p>gafas de sol - sunglasses</p><p>el traje de bano - swimsuit</p><p>el paraguas = umbrella</p><p>la bolsa - bag</p><p>el refrigerador - refrigerator</p><p>la llave - key</p><p>el cepillo de dientes - toothbrush</p><p>el cepillo - brush</p><p>la pizarra - chalkboard</p><p>el pizarron - whiteboard</p><p>la nota - note</p><p>el diario - diary</p><p>el paquete - package</p><p>el papel - paper</p><p>la taza - cup </p><p>el tazon - mug</p><p>una copa - glass,cup</p><p>la caja - box</p><p>rudio - noise</p><p>la lampara - lamp</p><p>la luz - light / las luces - lights</p><p>el mueble - furniture</p><p>portatil - laptop</p><p>la cafetera - coffee maker</p><p>el televisor - television</p><p>canal - channel</p><p>la informacion - information</p><p>noticia - news</p><p>el aviso - notice</p><p>archivo - file</p><p>la carpeta - folder</p><p>documento - document</p><p>anuncio - announcement</p><p>revista - magazine</p><p>la alfombra - carpet</p><p>la comoda - dresser</p><p>estufa - stove</p><p>el espejo - mirror</p><p>lavadora - washing machine</p><p>el sillon - armchair</p><p>el sofa - sofa</p><p>el jabon - soap</p><p>cigarrillo - cigarette</p><p>maquina - machine</p><p>el peine - comb</p><p>champu - shampoo</p><p>la regla - ruler</p><p>la estampilla - stamp</p><p>el velero - sailboat</p><p>la mapa - map</p><p>el ventilador - fan</p><p>el formulario - form</p><h3 id="天气"><a href="#天气" class="headerlink" title="天气"></a>天气</h3><p>la nube - cloud</p><p>el cielo - sky</p><p>la niebla - fog</p><p>el aire - air</p><p>el clima - weather</p><p>primavera - spring</p><p>verano - summer </p><blockquote><p>en el verano = in the summer</p></blockquote><p>oto$\acute n$o - fall</p><p>invierno - winter</p><p>el viento - wind</p><blockquote><p>Hace viento en la ciudad. = it is windy in the city</p></blockquote><p>el sol - sun</p><blockquote><p>hace mucho sol hoy. = It is very sunny today.</p></blockquote><p>la estrella - star</p><p>la luna - moon</p><p>la nieva - snowing</p><p>la nieve - snow</p><p>llueve - raining</p><p>la lluvia - rain</p><p>la tormenta - storm</p><p>el fuego - fire</p><h2 id="人物"><a href="#人物" class="headerlink" title="人物"></a>人物</h2><p>la gente - people</p><blockquote><p>mucha gente = a lot of people</p></blockquote><p>la multitud - crowd</p><p>madre - mother <ma d="" rei=""></ma></p><p>mam$\acute a$ - mom</p><p>esposa - wife</p><p>padre - father </p><blockquote><p>los pardes = parents</p></blockquote><p>pap$\acute a$ - dad</p><p>tio - uncle</p><p>el beb$\acute e$ - baby</p><p>esposo/marido - husband <ei s="" bo="" so=""></ei></p><p>hermana - sister </p><p>hermanito - little sister</p><p>hermano - brother </p><p>hermanito - little brother</p><p>hijo - son <iho></iho></p><p>hija - daughter</p><p>abuelo - grandfather </p><p>abuelito - grandpa</p><p>abuela - grandmother </p><p>abuelito - grandpa</p><p>nieto - grandson</p><p>novio - boyfriend</p><p>novia - girlfriend</p><p>primo/a - cousine</p><p>la familia - family <fa mi="" lia=""></fa></p><p>adulto - adult</p><p>el invitado/a - guest</p><p>el carro - car </p><p>electrico/a - electric</p><p>la pila - battery</p><p>el ascensor - elevator</p><p>el edificio - building</p><p>la mezquita - mosque</p><p>el pasillo / el salon / el vestibulo / el corredor - hall</p><p>el concejo municipal - city council</p><p>un armario - closet</p><p>el alquiler - rent</p><p>la casa - house </p><blockquote><p>ellos leen en casa. = they read at home.</p></blockquote><p>un techo - roof</p><p>el garaje - garage</p><p>cuadra - block</p><p>el norte - north</p><blockquote><p>vamos para el norte? - do we go north?</p></blockquote><p>el sur - south</p><p>oeste - west</p><blockquote><p>voy para el oeste - go west</p></blockquote><p>el este - east</p><p>la esquina - corner</p><p>el patio - backyard</p><p>el centro - downtown</p><p>la escalera - stair</p><p>la parede - wall</p><p>el piso - floor</p><p>el desierto - desert</p><p>la granja - farm</p><p>animal - animal</p><p>el oso - bear</p><p>el insecto - insect</p><p>un dinosaurio - dinosaur</p><p>el gato - the cat </p><p>el gatito - little cat</p><p>la rata - rat</p><p>el raton - mouse</p><p>el perro - dog </p><p>el perrito - little dog</p><p>la oveja - sheep</p><p>la vaca - cow</p><p>el cerdo - pig</p><p>el caballo - horse</p><p>el mono - monkey</p><p>el elefante - elephant</p><p>el conejo - rabbit</p><p>la serpiente - snake</p><p>el leon - lion</p><p>el pato - duck</p><p>un apartamento - apartment </p><p>la bicicleta/bici - bicycle </p><p>la motocicleta - motocycle</p><p>el ciclismo - cycling</p><p>pueblo - town </p><p>la mascota - pet | ~s</p><p>la television - television</p><p>la tele - TV</p><p>el anuncio - advertisement</p><p>la piscina - pool</p><p>la nevera - fridge</p><p>la cocina - kitchen</p><p>el lavamanos - sink</p><p>el fregadero - sink</p><p>la sala - living room</p><p>la cama - bed</p><p>la almohada - pillow</p><p>habitacion / dormitorio/cuarto(el) - bedroom</p><p>la ventana - window</p><p>la cortina - curtain</p><p>la silla - chair</p><p>la estanteria - bookcase,bookshell</p><p>la planta - plant</p><p>el jardin - garden</p><p>el deporte - sport</p><p>balonmano - handball</p><p>el baloncesto - basketball</p><p>el tenis - tennis</p><p>el piano - piano</p><p>la guitarra - guitar </p><p>el tambor - drum</p><p>el violin - violin</p><p>parte - part</p><blockquote><p>en todas partes - everywhere</p></blockquote><p>nombre - name</p><p>camarero/a - waitor</p><p>mesero - waitor</p><p>amigo/a - friend</p><p>m$\acute e$dico/a - doctor</p><blockquote><p>nuestra madre es medica/ = Our mother is a doctor</p></blockquote><p>la medicina - medicine</p><p>la sangre - blood</p><p>el jugador / la jugadora - player | los jugadores = las jugadoras</p><p>la vacaciones - vacation</p><blockquote><p>In spanish, una vacacion (singular) is a day off. Unas vacaciones is several days off in a row, which is what we think of as a “vacation”</p></blockquote><p>el titulo - degree</p><p>la escuela - school </p><p>meta - goal</p><p>objetivo - goal</p><p>ganador / ganadora - winner</p><p>la competencia - competition</p><p>el semestre - semester</p><p>matematicas - math</p><p>la historia - history</p><p>la quimica - chemistry</p><p>geografia - geography</p><p>biologia - biology</p><p>fisica - physics</p><p>cuaderno - notebook</p><p>una prueba - test</p><p>libros de texto - textbooks</p><p>los libros de ejercicios - workbooks</p><p>el diploma - diplioma</p><p>nota - grade </p><p>los premios - prizes</p><p>el aula - classroom</p><p>el patio de juego - playground</p><p>palabra - word</p><p>la oraci$\acute o$n - sentence</p><p>la gramatica - grammar</p><p>la clase - class </p><p>la lecci$\acute o$n - lesson</p><p>asignatura - subject, class</p><p>la visitante - visitor</p><p>visita - visitor</p><p>grupo - group</p><p>el libro - book | los libros - the books</p><p>la p$\acute a$gina - page</p><blockquote><p>la ~ cinco = page five</p><p>abre el libro en la pagina dos. = open the book to page two</p></blockquote><p>la biblioteca - library </p><p>la libreria - bookstore</p><p>colega - colleague</p><p>el proyecto - project</p><p>el programa - program</p><p>en la reunion - at the meeting</p><p>el boligrafo - pen || boligrafos - pens</p><p>un estuche - pencil case</p><p>los l$\acute a$pices - pencils | lapiz= pencil</p><p>el borrador - erasor </p><p>la estudiante - student </p><p>alumno/a - student</p><p>la graduado/a - graduate</p><p>miembro - member</p><p>la actriz / las actrices - actress</p><p>actor - actor</p><p>el recepcionista - receptionist</p><p>pregunto(1),preguntas(2),pregunta(3) - question </p><p>el problema - problem </p><p>un sunto - matter</p><p>el examen - exam </p><p>ciencia - science</p><p>conductor / conductora - driver</p><p>maestro - teacher </p><p>la enfermero/a - nurse</p><p>la dentista - dentist</p><p>la computadora - computer </p><p>el teclado - keyboard</p><p>el celular - cell phone</p><p>una llamada - call</p><p>medicina - medicine </p><p>la universidad - university </p><p>el estudio - study</p><p>el pintor - painter</p><p>gerente/a - manager</p><p>trabajador /trabajadora - worker</p><p>empresario/a - busunessman</p><p>la oficina - office</p><p>el departamento - department, apartment</p><p>la impresora - printer</p><p>direcci$\acute o$n - address </p><p>la indicacion - direction</p><p>n$\acute u$mero - number </p><p>f$\acute a$brica - factory</p><p>el escritorio - desk</p><p>el estante - shelf</p><p>el cajon - drawer</p><p>el periodico - newspaper</p><p>el diario - newspaper</p><p>compania - company</p><p>jefe(mas), jefa(fem) - boss</p><p>obrero/a - worker</p><p>el personal - staff</p><p>el mensaje - message</p><p>la policia - police</p><p>trabajo - job </p><blockquote><p>Tengo dos semanas de mucho trabajo. = I have two weeks of hard work.</p></blockquote><p>la ocupacion - occupation</p><p>ingeniero/a - engineer</p><p>el motor - engine</p><p>secretaria - secretary </p><p>presidente/a - president</p><p>carta - letter | ~s</p><p>correo - mail</p><p>el correo electr$\acute o$nico - email address</p><blockquote><p>los correos electr$\acute o$nicos = emails</p></blockquote><p>negocio - business | </p><p>el curso - course</p><p>la tarea - homework</p><p>la cosa - thing</p><p>la respuesta - answer, response</p><p>la edad - age</p><p>el corazon - heart</p><p>el ojo - eye</p><p>la mano - hand</p><p>el cerebro - brain</p><p>la cara - face</p><p>cabello - hair</p><p>el diente - tooth</p><p>la oreja - ear</p><p>la sonido - sound</p><p>el pie - foot</p><p>la boca - mouth</p><p>el cuerpo - body</p><p>la nariz - nose | narices - noses</p><p>la cabeza - head</p><p>el pelo - hair</p><p>el brazo - arm</p><p>la pierna - leg</p><p>brazo derecho - right arm</p><p>la barba - beard</p><p>la espalda - back</p><p>el dedo - finger</p><p>el cuello - neck</p><p>el estomago - stomach</p><p>salud - health</p><p>el ejercicio - exercise</p><p>dolor de cabeza - headache</p><p>companera(s) de cuarto - roommate(s)</p><p>companeras de clase - classmates</p><p>companero/a - classmate</p><p>el recuerdo - memory</p><p>cita - appointment</p><p>un accidente -accident</p><p>cocinero/a - cook</p><p>director / directora - director</p><p>el agricultor - farmer</p><p>el equipo - team</p><p>corredor/corredora - runner</p><p>la razon - reason</p><p>maquillaje - makeup</p><p>instruccion - instruction</p><p>el detalle - detail</p><p>la diferencia - difference</p><p>consejo - advice</p><p>nivel - level</p><p>peligro - danger</p><p>electricidad - electricity</p><p>la fotografa - photographer</p><p>limpiador / limpiadora - cleaner</p><p>el cantante - singer</p><p>el piloto - pilot</p><p>el recibo - receipt</p><p>vendedor / vendedora - saleperson</p><p>un vendaje - bandage </p><p>en moda - in style</p><p>forma - shape</p><blockquote><p>estoy en forma - i am in shape.</p></blockquote><p>el cuidado - care</p><p>el cambio - change</p><p>ama de casa - homemaker</p><p>adolescente - teenager</p><p>un hecho - fact</p><p>el payaso - clown</p><p>mecanico/a - mechanic</p><p>el rey - king</p><p>la reina - queen</p><p>el castillo - castle</p><p>la bailarina - dancer</p><p>la guerra - war</p><p>el pasajero - passanger</p><p>identificacion - identification</p><p>tengo suerte - lucky</p><p>patada - kick</p><h2 id="问句"><a href="#问句" class="headerlink" title="问句"></a>问句</h2><p>Usted tiene una cartera? - Do you have a purse?</p><p>Estas enferma, Carla? - are you sick?</p><p>dónde estudias tú - where do you study</p><blockquote><p>With content questions (questions that are not yes/no) the verb should be moved to before the subject.</p></blockquote><p>Te llamas Carmen? = Is your name Carmen?</p><p>cual es tu nombre | Como se llama usted? = What is your name? </p><p>Por que estas sorpredida, sonia? = why are you surprised?</p><p>por que te sientes triste? = why do you feel sad</p><p>por que ellos no tu visitan? = why do they not visit you?</p><p>cuando nos visita nuestra abuela? = When do our grandmother visit us?</p><p>es usted camarera? = Are you a waitress?</p><p>como esta tu papa esta manana? = How does your dad this morning?</p><p>como son los restaurante? = What are the restaurants like?</p><p>Senores Castro y Garcia, de donde son ustedes? = Mr. Castro and Mr. Garcia, where are you from?</p><p>Me va a gustar? = Am I going to like it?</p><p>cuando tiempo esperaste = How long did you wait</p><h2 id="句子"><a href="#句子" class="headerlink" title="句子"></a>句子</h2><p>Yo tengo nueve a$\acute n$os. = I am nine years old.</p><p>Las senoritas tienen veintidos anos. = The ladies are twenty-two years old.</p><p>Yo tengo un hijo de nueve a$\acute n$os. = I have a nine years old son.</p><p>Este ni$\acute n$o tiene un a$\acute n$o. = This boy is one year old.</p><p>Esta mujer nunca esta enojada. = This woman is never angry</p><p>Yo estoy en el cafe del peublo. = I am at the town cafe.</p><blockquote><p>del = de el.</p></blockquote><p>Estas camisetas son muy bonitas. = These t-shirts are very pretty.</p><p>Las tardes en la playa son divertidas. = Afternoons on the beach are fun.</p><p>Senor Juan, estas son sus camisas? = Mr.Juan, are these your shirts?</p><p>Esta es la puerta del hotel. = This is the door to the hotel.</p><p>Aqui tiene su tenedor, senor. = Here is your fork, sir.</p><p>Catorce pesos no es mucho dinero. = Fourteen pesos is not much money.</p><p>es importante amar a los ninos. = it is important to love children.</p><p>tan … como … = as … as …</p><p>mas … que … = more … than …</p><p>yo soy el mas joven de la familia. = I am the youngest in the family.</p><p>la fiesta te va a gustar = you are going to like the party</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><h2 id="动词-1"><a href="#动词-1" class="headerlink" title="动词"></a>动词</h2><h3 id="Ser-vs-Estar"><a href="#Ser-vs-Estar" class="headerlink" title="Ser vs Estar"></a>Ser vs Estar</h3><p>两者都是Be动词, 但是前者表示永久/持续的状态，而后者表示暂时的状态。<a href="https://www.spanishdict.com/guide/ser-vs-estar" target="_blank" rel="noopener">link</a> <a href="https://www.spanishdict.com/conjugate/ser" target="_blank" rel="noopener">ser</a> <a href="https://www.spanishdict.com/conjugate/estar" target="_blank" rel="noopener">estar</a> </p><h4 id="Ser"><a href="#Ser" class="headerlink" title="Ser"></a>Ser</h4><p><img src="/2019/04/13/Spanish/Screen Shot 2019-10-02 at 11.40.40 AM.png" alt="creen Shot 2019-10-02 at 11.40.40 A"></p><h4 id="Estar"><a href="#Estar" class="headerlink" title="Estar"></a>Estar</h4><p><img src="/2019/04/13/Spanish/Screen Shot 2019-10-02 at 11.42.05 AM.png" alt="creen Shot 2019-10-02 at 11.42.05 A"></p><h3 id="Imperative-Mood"><a href="#Imperative-Mood" class="headerlink" title="Imperative Mood"></a>Imperative Mood</h3><h4 id="Affirmative-Informal-‘Tu’-Command-link"><a href="#Affirmative-Informal-‘Tu’-Command-link" class="headerlink" title="Affirmative Informal ‘Tu’ Command link"></a>Affirmative Informal ‘Tu’ Command <a href="https://www.spanishdict.com/guide/affirmative-informal-tu-commands/" target="_blank" rel="noopener">link</a></h4><p>规则变化是动词的第三人称。不规则变化如下：</p><p><img src="/2019/04/13/Spanish/Screen Shot 2019-10-02 at 6.07.22 PM.png" alt="creen Shot 2019-10-02 at 6.07.22 P"></p><p><strong>Pronouns are attached to the end of affirmative commands.</strong></p><h3 id="Preterite-Tense-Forms-过去式"><a href="#Preterite-Tense-Forms-过去式" class="headerlink" title="Preterite Tense Forms 过去式"></a>Preterite Tense Forms <a href="https://www.spanishdict.com/guide/spanish-preterite-tense-forms" target="_blank" rel="noopener">过去式</a></h3><p>There are only two sets of endings for regular preterite verbs, one for <strong>-ar</strong> verbs and one for both <strong>-er</strong> and <strong>-ir</strong> verbs. To conjugate a regular verb in the preterite tense, simply remove the infinitive ending (<strong>-ar</strong>, <strong>-er</strong>, or <strong>-ir</strong>) and add the preterite ending that matches the subject. Check out the table of regular preterite endings below.</p><h4 id="Regular-Preterite-Verb-Endings"><a href="#Regular-Preterite-Verb-Endings" class="headerlink" title="Regular Preterite Verb Endings"></a>Regular Preterite Verb Endings</h4><p><img src="/2019/04/13/Spanish/Screen Shot 2019-10-07 at 11.02.22 AM.png" alt="creen Shot 2019-10-07 at 11.02.22 A"></p><blockquote><ol><li><p>Note that the first person singular (<strong>yo</strong>), third person singular (<strong>él, ella</strong>), and second person formal singular (<strong>usted</strong>) preterite forms have <strong>tildes</strong> (<em>written accents</em>) on the final vowel. Keep in mind that one little <strong>tilde</strong> can change both the tense and subject of a sentence. </p><p><img src="/2019/04/13/Spanish/Screen Shot 2019-10-07 at 11.04.02 AM.png" alt="creen Shot 2019-10-07 at 11.04.02 A"></p></li><li><p>以-ar结尾的动词，第一人称变$a$为$\acute e$, 第二人称加$ste$, 第三人称变$a$为$\acute o$, they加$ron$, we加$mos$</p><p>以-er/-ir结尾的动词，第一人称变$e/i$为$\acute i$, 第二人称加$iste$, 第三人称变$e/i$为$\acute io$, they加$ieron$, we加$imos$ </p></li><li><p>xx </p></li></ol></blockquote><h4 id="Irregular-Spanish-Preterite-Forms"><a href="#Irregular-Spanish-Preterite-Forms" class="headerlink" title="Irregular Spanish Preterite Forms"></a>Irregular Spanish Preterite Forms</h4><p><img src="/2019/04/13/Spanish/Screen Shot 2019-10-07 at 11.05.16 AM.png" alt="creen Shot 2019-10-07 at 11.05.16 A"></p><h3 id="Imperfect-tense-forms-不完全时态-link"><a href="#Imperfect-tense-forms-不完全时态-link" class="headerlink" title="Imperfect tense forms (不完全时态) link"></a>Imperfect tense forms (不完全时态) <a href="https://www.spanishdict.com/guide/spanish-imperfect-tense-forms" target="_blank" rel="noopener">link</a></h3><p>It is used to <strong>describe past habitual actions</strong> or to talk about what someone <strong>was doing</strong> when they were interrupted by something else.</p><p>There are only two sets of endings for regular imperfect verbs in Spanish, one for <strong>-ar</strong> verbs and one for both <strong>-er</strong> and <strong>-ir</strong> verbs.</p><h4 id="Regular"><a href="#Regular" class="headerlink" title="Regular"></a>Regular</h4><p><img src="/2019/04/13/Spanish/Screen Shot 2019-11-04 at 5.47.49 PM.png" alt="creen Shot 2019-11-04 at 5.47.49 P"></p><h3 id="Irregular"><a href="#Irregular" class="headerlink" title="Irregular"></a>Irregular</h3><p><img src="/2019/04/13/Spanish/Screen Shot 2019-11-04 at 5.47.57 PM.png" alt="creen Shot 2019-11-04 at 5.47.57 P"></p><h3 id="Present-Participle-现在分词-link-link2"><a href="#Present-Participle-现在分词-link-link2" class="headerlink" title="Present Participle (现在分词) link link2"></a>Present Participle (现在分词) <a href="https://www.spanishdict.com/guide/present-participles-in-spanish" target="_blank" rel="noopener">link</a> <a href="https://www.spanishdict.com/guide/spanish-present-progressive-forms" target="_blank" rel="noopener">link2</a></h3><p>Present participles in Spanish are verb forms used to express continuous or ongoing actions. Spanish present participles end in <strong>-ndo</strong>, which is the equivalent of the English ending <em>-ing</em>.</p><h4 id="Regular-1"><a href="#Regular-1" class="headerlink" title="Regular"></a>Regular</h4><p>To form the <strong>gerundio</strong> of regular verbs, most of the time you just drop the infinitive ending (<strong>-ar</strong>, <strong>-er</strong>, <strong>-ir</strong>) and add <strong>-ando</strong> to the stem of <strong>-ar</strong> verbs and <strong>-iendo</strong> to the stem of <strong>-er</strong> and <strong>-ir</strong> verbs. Check out the table below to see how to form <strong>gerundios</strong>.</p><p><img src="/2019/04/13/Spanish/Screen Shot 2019-12-18 at 11.30.28 AM.png" alt="creen Shot 2019-12-18 at 11.30.28 A"></p><h4 id="Irregular-1"><a href="#Irregular-1" class="headerlink" title="Irregular"></a>Irregular</h4><p><img src="/2019/04/13/Spanish/Screen Shot 2019-12-18 at 11.31.37 AM.png" alt="creen Shot 2019-12-18 at 11.31.37 A"></p><p><img src="/2019/04/13/Spanish/Screen Shot 2019-12-18 at 11.32.01 AM.png" alt="creen Shot 2019-12-18 at 11.32.01 A"></p><p><img src="/2019/04/13/Spanish/Screen Shot 2019-12-18 at 11.32.52 AM.png" alt="creen Shot 2019-12-18 at 11.32.52 A"></p><h3 id="Present-Perfect-现在完成时"><a href="#Present-Perfect-现在完成时" class="headerlink" title="Present Perfect ( 现在完成时)"></a>Present Perfect ( 现在完成时)</h3><p>he(1) , has(2) , ha(3) , hemos(we) , han(they) + Vdo / Vido</p><p>hecho - have done | abierto - have opened | escrito-write | puesto - put | tenido - have</p><p>dicho - have told | visto - have seen |</p><h3 id="（将来时）"><a href="#（将来时）" class="headerlink" title="（将来时）"></a>（将来时）</h3><p>ire(1)iras(2),ira(3) iremos(we) iran- will go</p><p>sere(1) seras(2) sera(3) seremos(we) seran - will be</p><h2 id="名词复数"><a href="#名词复数" class="headerlink" title="名词复数"></a>名词复数</h2><ul><li><p>以元音结尾的名词词尾加 -s</p><p>alumno - alumnos </p><p>chileno - chilenos</p><p>amigo - amigos</p></li><li><p>以辅音结尾的名词词尾加 -es</p></li></ul><h2 id="名词的性"><a href="#名词的性" class="headerlink" title="名词的性"></a>名词的性</h2><ul><li><p>以 “o” 结尾的名词多为阳性</p><p>piso - 层，sitio - 地方，lazo - 纽带，alumno </p></li><li><p>以 “a” 结尾的名词多为阴性</p><p>sopa - 汤，silla - 椅子，mesa - 桌子，alumna</p></li><li><p>许多以辅音结尾的阳性名词，在其词尾添加元音a，就变成了阴性名词</p><p>profesor - profesora  espanol - espanola</p></li><li><p>阳性名词的复数既可以表示多个男性/雄性，也可以指多个男女。但是阴性名词复数只能表示多个女性</p></li></ul><h2 id="主格人称代词"><a href="#主格人称代词" class="headerlink" title="主格人称代词"></a>主格人称代词</h2><p><img src="/2019/04/13/Spanish/Screen Shot 2019-04-22 at 6.01.43 PM.png" alt="creen Shot 2019-04-22 at 6.01.43 P"></p><p><strong>系动词变化</strong></p><p><img src="/2019/04/13/Spanish/Screen Shot 2019-04-22 at 6.02.18 PM.png" alt="creen Shot 2019-04-22 at 6.02.18 P"></p><h2 id="动词变化"><a href="#动词变化" class="headerlink" title="动词变化"></a>动词变化</h2><h3 id="规则变化"><a href="#规则变化" class="headerlink" title="规则变化"></a>规则变化</h3><p><img src="/2019/04/13/Spanish/Screen Shot 2019-06-03 at 3.20.31 PM.png" alt="creen Shot 2019-06-03 at 3.20.31 P"></p><h3 id="不规则变化"><a href="#不规则变化" class="headerlink" title="不规则变化"></a>不规则变化</h3><p><img src="/2019/04/13/Spanish/Screen Shot 2019-06-03 at 3.21.01 PM.png" alt="creen Shot 2019-06-03 at 3.21.01 P"></p><p>========</p><p>carro (car) | carta (letter) | casa (house) | cartera (purse)</p><hr><p>vaso (glass), taza (cup)</p><hr><p>semana (week), manana (tomorrow)</p><hr><p>nueva (new), nueve (nine)</p><hr><p>bajo (short), barato (cheap), </p><hr><p>comoda (comfortable)</p><hr><p>cerca (near)</p><p><a href="https://www.thoughtco.com/noun-masculine-or-feminine-spanish-3079270" target="_blank" rel="noopener">Is That Noun Masculine or Feminine?</a></p><p><a href="https://www.spanishdict.com/guide/adjective-placement" target="_blank" rel="noopener">Adjective Placement</a></p><h1 id="发音"><a href="#发音" class="headerlink" title="发音"></a>发音</h1><p><a href="https://www.bilibili.com/video/av12059587/?p=20&amp;t=76" target="_blank" rel="noopener">西班牙语发音课</a></p><h2 id="西语字母"><a href="#西语字母" class="headerlink" title="西语字母"></a>西语字母</h2><div class="table-container"><table><thead><tr><th style="text-align:center">大写</th><th style="text-align:center">小写</th><th style="text-align:center">读音</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">a</td><td style="text-align:center">a（阿）</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">b</td><td style="text-align:center">be</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">c</td><td style="text-align:center">ce</td></tr><tr><td style="text-align:center">CH</td><td style="text-align:center">ch</td><td style="text-align:center">che（切）</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">d</td><td style="text-align:center">de</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">e</td><td style="text-align:center">e</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">f</td><td style="text-align:center">efe</td></tr><tr><td style="text-align:center">G</td><td style="text-align:center">g</td><td style="text-align:center">he</td></tr><tr><td style="text-align:center">H</td><td style="text-align:center">h</td><td style="text-align:center">a che</td></tr><tr><td style="text-align:center">I</td><td style="text-align:center">i</td><td style="text-align:center">i</td></tr><tr><td style="text-align:center">J</td><td style="text-align:center">j</td><td style="text-align:center">hoda</td></tr><tr><td style="text-align:center">K</td><td style="text-align:center">k</td><td style="text-align:center">ga</td></tr><tr><td style="text-align:center">L</td><td style="text-align:center">l</td><td style="text-align:center">ele</td></tr><tr><td style="text-align:center">LL</td><td style="text-align:center">ll</td><td style="text-align:center">eye</td></tr><tr><td style="text-align:center">M</td><td style="text-align:center">m</td><td style="text-align:center">eme</td></tr><tr><td style="text-align:center">N</td><td style="text-align:center">n</td><td style="text-align:center">ene</td></tr><tr><td style="text-align:center">$\hat N$</td><td style="text-align:center">$\hat n$</td><td style="text-align:center">enie</td></tr><tr><td style="text-align:center">O</td><td style="text-align:center">o</td><td style="text-align:center">o</td></tr><tr><td style="text-align:center">P</td><td style="text-align:center">p</td><td style="text-align:center">bei</td></tr><tr><td style="text-align:center">Q</td><td style="text-align:center">q</td><td style="text-align:center">gu</td></tr><tr><td style="text-align:center">R</td><td style="text-align:center">r</td><td style="text-align:center">ere</td></tr><tr><td style="text-align:center">rr</td><td style="text-align:center"></td><td style="text-align:center">erre</td></tr><tr><td style="text-align:center">S</td><td style="text-align:center">s</td><td style="text-align:center">ese</td></tr><tr><td style="text-align:center">T</td><td style="text-align:center">t</td><td style="text-align:center">de</td></tr><tr><td style="text-align:center">U</td><td style="text-align:center">u</td><td style="text-align:center">wu</td></tr><tr><td style="text-align:center">V</td><td style="text-align:center">v</td><td style="text-align:center">wubei</td></tr><tr><td style="text-align:center">W</td><td style="text-align:center">w</td><td style="text-align:center">double u wubei</td></tr><tr><td style="text-align:center">X</td><td style="text-align:center">x</td><td style="text-align:center">egis</td></tr><tr><td style="text-align:center">Y</td><td style="text-align:center">y</td><td style="text-align:center">ye / ge rui ye ga</td></tr><tr><td style="text-align:center">Z</td><td style="text-align:center">z</td><td style="text-align:center">zeta</td></tr><tr><td style="text-align:center">gu</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">qu</td><td style="text-align:center"></td></tr></tbody></table></div><h2 id="元音字母"><a href="#元音字母" class="headerlink" title="元音字母"></a>元音字母</h2><p>Aa, Ee, Ii, Oo, Uu</p><h3 id="二重元音"><a href="#二重元音" class="headerlink" title="二重元音"></a>二重元音</h3><p>由两个元音组成：强元音+弱元音；弱元音+弱元音。</p><p>强元音： a，e，o</p><p>弱元音：i，u</p><p>共14个，独立为一个音节：</p><p>ai [爱]，ia [yi ya]     || ei，ie        || oi，io </p><p>au，ua   ||  eu，ue    || ou，uo  </p><p>iu，ui</p><h3 id="三重元音"><a href="#三重元音" class="headerlink" title="三重元音"></a>三重元音</h3><p>又三个元音组成，弱元音+强元音+弱元音；字母y在词尾读[i]，所以y也可以和其他两个元音构成三重元音。</p><p>buey - [buo ei i] Uruguay - [wu ru gua i]</p><h2 id="辅音字母"><a href="#辅音字母" class="headerlink" title="辅音字母"></a>辅音字母</h2><p>辅音字母是不起发音作用的，所以需要跟元音字母搭配，才能发音。</p><h3 id="辅音连缀"><a href="#辅音连缀" class="headerlink" title="辅音连缀"></a>辅音连缀</h3><p>又两个辅音组成</p><p>p b c g f t d + l / r，即</p><p>pl, bl, cl, gl, fl</p><p>pr, br, cr, gr, fr, tr, dr</p><p><strong>Mm</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">ma</th><th style="text-align:center">me</th><th style="text-align:center">mi</th><th style="text-align:center">mo</th><th style="text-align:center">mu</th></tr></thead><tbody><tr><td style="text-align:center">ama</td><td style="text-align:center">eme</td><td style="text-align:center">imi</td><td style="text-align:center">omo</td><td style="text-align:center">umu - wu mu</td></tr><tr><td style="text-align:center">am - 暗m</td><td style="text-align:center">em</td><td style="text-align:center">im</td><td style="text-align:center">om</td><td style="text-align:center">um - wu mu</td></tr></tbody></table></div><p><strong>Nn</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">na</th><th style="text-align:center">ne</th><th style="text-align:center">ni</th><th style="text-align:center">no</th><th style="text-align:center">nu</th></tr></thead><tbody><tr><td style="text-align:center">ana</td><td style="text-align:center">ene</td><td style="text-align:center">ini</td><td style="text-align:center">ono</td><td style="text-align:center">unu - wu nu</td></tr><tr><td style="text-align:center">an</td><td style="text-align:center">en</td><td style="text-align:center">in</td><td style="text-align:center">on</td><td style="text-align:center">un - wu en</td></tr></tbody></table></div><p><strong>Ll</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">la</th><th style="text-align:center">le</th><th style="text-align:center">li</th><th style="text-align:center">lo</th><th style="text-align:center">lu</th></tr></thead><tbody><tr><td style="text-align:center">ala</td><td style="text-align:center">ele</td><td style="text-align:center">ili</td><td style="text-align:center">olo</td><td style="text-align:center">ulu</td></tr><tr><td style="text-align:center">al</td><td style="text-align:center">el</td><td style="text-align:center">il</td><td style="text-align:center">ol</td><td style="text-align:center">ul</td></tr></tbody></table></div><blockquote><p>L在词尾的时候，舌尖轻轻触碰上颚，不能卷。</p></blockquote><p><strong>Pp</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">apa</td><td style="text-align:center">epe</td><td style="text-align:center">ipi</td><td style="text-align:center">opo</td><td style="text-align:center">upu</td></tr><tr><td style="text-align:center">pa</td><td style="text-align:center">pe</td><td style="text-align:center">pi</td><td style="text-align:center">po</td><td style="text-align:center">pu</td></tr></tbody></table></div><blockquote><p>清辅音，声带不震动，发be的音。</p></blockquote><p><strong>Tt</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">ata</td><td style="text-align:center">ete</td><td style="text-align:center">iti</td><td style="text-align:center">oto</td><td style="text-align:center">utu</td></tr><tr><td style="text-align:center">ta</td><td style="text-align:center">te</td><td style="text-align:center">ti</td><td style="text-align:center">to - dou</td><td style="text-align:center">tu - du</td></tr></tbody></table></div><blockquote><p>清辅音，不吐气，发de</p></blockquote><p><strong>Bb</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aba - ava</td><td style="text-align:center">ebe - eve</td><td style="text-align:center">ibi - ivi</td><td style="text-align:center">obo - ovo</td><td style="text-align:center">ubu -uvu</td></tr><tr><td style="text-align:center">ba</td><td style="text-align:center">be</td><td style="text-align:center">bi</td><td style="text-align:center">bo</td><td style="text-align:center">bu</td></tr></tbody></table></div><blockquote><p>浊辅音，发音与Pb相同，但是声带需要震动。</p></blockquote><p><strong>Vv</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">ava</td><td style="text-align:center">eve</td><td style="text-align:center">ivi</td><td style="text-align:center">ovo</td><td style="text-align:center">uvu</td></tr><tr><td style="text-align:center">va -ba</td><td style="text-align:center">ve - be</td><td style="text-align:center">vi - bi</td><td style="text-align:center">vo - bo</td><td style="text-align:center">vu - bu</td></tr></tbody></table></div><blockquote><p>Bb和Vv发音是一样的，一般情况读[v]，双唇之间空隙，让气流通过。在以下情况读[b]:</p><ul><li>单词词首</li><li>在字母m, n后面</li></ul></blockquote><p><strong>Dd</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">edad - eda</th><th style="text-align:center">poed - bonei</th><th style="text-align:center">sed - sei</th><th style="text-align:center">comed - gomei</th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">ada</td><td style="text-align:center">ede</td><td style="text-align:center">idi</td><td style="text-align:center">odo</td><td style="text-align:center">udu</td></tr><tr><td style="text-align:center">da</td><td style="text-align:center">de</td><td style="text-align:center">di</td><td style="text-align:center">do</td><td style="text-align:center">du</td></tr></tbody></table></div><blockquote><p>浊辅音，发音与Tt相同，但声带需要震动。当d在字母中间时，需要舌尖微微伸出上牙，让气流通过，声带震动。当d在词尾的时候，舌尖接触上齿，让气流停止通过，不发音。</p></blockquote><p><strong>Ss</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">asa</td><td style="text-align:center">ese</td><td style="text-align:center">isi</td><td style="text-align:center">oso</td><td style="text-align:center">usu</td></tr><tr><td style="text-align:center">sa</td><td style="text-align:center">se</td><td style="text-align:center">si</td><td style="text-align:center">so</td><td style="text-align:center">su</td></tr></tbody></table></div><p><strong>Cc</strong></p><hr><div class="table-container"><table><thead><tr><th style="text-align:center">ce - the</th><th style="text-align:center">ci - thi</th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aga</td><td style="text-align:center">eque</td><td style="text-align:center">equi</td><td style="text-align:center">oco</td><td style="text-align:center">ucu</td></tr><tr><td style="text-align:center">ca - ga</td><td style="text-align:center">que - gei</td><td style="text-align:center">qui - gui</td><td style="text-align:center">co - go</td><td style="text-align:center">cu - gu</td></tr></tbody></table></div><blockquote><p>在密目c, n, t, d之前发轻微的k</p><p>$lecci \acute o n$ -&gt; le ke ci on ; $t \acute e c nico$ -&gt; de ke ni con</p></blockquote><p><strong>Gg</strong></p><hr><div class="table-container"><table><thead><tr><th style="text-align:center">$g \ddot u e$ - wei(喂)</th><th style="text-align:center">$g \ddot u i$ - wei(微)</th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aga</td><td style="text-align:center">egue</td><td style="text-align:center">igui</td><td style="text-align:center">ogo</td><td style="text-align:center">ugu</td></tr><tr><td style="text-align:center">ga - ga</td><td style="text-align:center">gue - gei</td><td style="text-align:center">gui - gui</td><td style="text-align:center">go - go</td><td style="text-align:center">gu - gu</td></tr></tbody></table></div><blockquote><p>浊辅音，发音与Cc相同，但声带震动</p></blockquote><p><strong>Jj</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aja</td><td style="text-align:center">eje</td><td style="text-align:center">iji</td><td style="text-align:center">ojo</td><td style="text-align:center">uju</td></tr><tr><td style="text-align:center">ja - ha</td><td style="text-align:center">je - hei</td><td style="text-align:center">ji - hi</td><td style="text-align:center">jo - ho</td><td style="text-align:center">ju - hu</td></tr></tbody></table></div><blockquote><p>ge = je : hei </p><p>gi = ji : hi</p></blockquote><p><strong>Zz</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aza</td><td style="text-align:center">ece</td><td style="text-align:center">ici</td><td style="text-align:center">oco</td><td style="text-align:center">ucu</td></tr><tr><td style="text-align:center">za - tha</td><td style="text-align:center">ce - the</td><td style="text-align:center">ci -</td><td style="text-align:center">zo</td><td style="text-align:center">zu</td></tr></tbody></table></div><blockquote><p>不存在ze, zi。</p></blockquote><p><strong>Ff</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">afa</td><td style="text-align:center">efe</td><td style="text-align:center">ifi</td><td style="text-align:center">ofo</td><td style="text-align:center">ufu</td></tr><tr><td style="text-align:center">fa</td><td style="text-align:center">fe</td><td style="text-align:center">fi</td><td style="text-align:center">fo</td><td style="text-align:center">fu</td></tr></tbody></table></div><p><strong>Hh</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ha</td><td style="text-align:center">he</td><td style="text-align:center">hi</td><td style="text-align:center">ho</td><td style="text-align:center">hu</td></tr></tbody></table></div><blockquote><p>h不发音。hola - ola.</p></blockquote><p><strong>ch</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">acha</td><td style="text-align:center">eche</td><td style="text-align:center">ichi</td><td style="text-align:center">ocho</td><td style="text-align:center">uchu</td></tr><tr><td style="text-align:center">cha - 掐</td><td style="text-align:center">che - 切</td><td style="text-align:center">chi</td><td style="text-align:center">cho - 球</td><td style="text-align:center">chu</td></tr></tbody></table></div><blockquote><p>ch发切</p></blockquote><p><strong>$\tilde n$</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">a$\tilde n$a</td><td style="text-align:center">e$\tilde n$e</td><td style="text-align:center">i$\tilde n$i</td><td style="text-align:center">o$\tilde n$o</td><td style="text-align:center">u$\tilde n$u</td></tr><tr><td style="text-align:center">$\tilde n$a</td><td style="text-align:center">$\tilde n$e</td><td style="text-align:center">$\tilde n$i</td><td style="text-align:center">$\tilde n$o - niu</td><td style="text-align:center">$\tilde n$u - new</td></tr></tbody></table></div><blockquote><p>$\tilde n$ : ei nie</p></blockquote><p><strong>Yy</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aya</td><td style="text-align:center">eye</td><td style="text-align:center">iyi</td><td style="text-align:center">oyo</td><td style="text-align:center">uyu</td></tr><tr><td style="text-align:center">ya</td><td style="text-align:center">ye</td><td style="text-align:center">yi</td><td style="text-align:center">yo</td><td style="text-align:center">yu</td></tr></tbody></table></div><p><strong>Rrf</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">ara</td><td style="text-align:center">ere</td><td style="text-align:center">iri</td><td style="text-align:center">oro</td><td style="text-align:center">uru</td></tr><tr><td style="text-align:center">ra</td><td style="text-align:center">re</td><td style="text-align:center">ri</td><td style="text-align:center">ro</td><td style="text-align:center">ru</td></tr></tbody></table></div><p><strong>II</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">alla</td><td style="text-align:center">elle</td><td style="text-align:center">illi</td><td style="text-align:center">ollo</td><td style="text-align:center">ullu</td></tr><tr><td style="text-align:center">lla</td><td style="text-align:center">lle</td><td style="text-align:center">lli</td><td style="text-align:center">llo</td><td style="text-align:center">llu</td></tr></tbody></table></div><blockquote><p>发音与y相同。</p></blockquote><p><strong>Xx</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">[x] - Mexico</th><th style="text-align:center">mei hi go</th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">[ks] - axa - akasa</td><td style="text-align:center">exe - ekse</td><td style="text-align:center">ixi - iksi</td><td style="text-align:center">oxo - okso</td><td style="text-align:center">uxu - wukso</td></tr><tr><td style="text-align:center">[s] - xa -sa</td><td style="text-align:center">xe - sei</td><td style="text-align:center">xi -si</td><td style="text-align:center">xo - so</td><td style="text-align:center">xu - su</td></tr></tbody></table></div><p><strong>Ww</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">wa</td><td style="text-align:center">we</td><td style="text-align:center">wi</td><td style="text-align:center">wo</td><td style="text-align:center">wu</td></tr></tbody></table></div><blockquote><p>可以读 [double uve] 或者 [uve double], dob 累 wu wei。</p><p>发音与b, v一样。</p></blockquote><p><strong>Kk</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">aka</td><td style="text-align:center">eke</td><td style="text-align:center">iki</td><td style="text-align:center">oko</td><td style="text-align:center">uku</td></tr><tr><td style="text-align:center">ka</td><td style="text-align:center">ke</td><td style="text-align:center">ki</td><td style="text-align:center">ko</td><td style="text-align:center">ku</td></tr></tbody></table></div><blockquote><p>发音与c相同,发g音</p></blockquote><h2 id="音节划分"><a href="#音节划分" class="headerlink" title="音节划分"></a>音节划分</h2><p>有几个元音就有几个音节。</p><ol><li><p>一个元音构成一个音节</p><p>a，o，e，i，u</p></li><li><p>一个辅音和一个元音</p><p>la， me，bo</p></li><li><p>两个辅音和一个元音</p><p>las，pel</p></li></ol><h2 id="重音规则"><a href="#重音规则" class="headerlink" title="重音规则"></a>重音规则</h2><ol><li><p>以5个元音a, e, i, o, u和n, s结尾的单词，重音在倒数第二个音节</p><p>susana -&gt; su | <strong>sa</strong> | na</p></li><li><p>其他辅音字母结尾的单词，重音在最后一个音节傻上。</p><p>comed -&gt; co | <strong>med</strong></p></li><li><p>有重音符号的单词优先，忽略前面两点</p><p>$pap\acute a$ -&gt; pa | <strong>$p \acute a$</strong></p></li></ol><h2 id="连读"><a href="#连读" class="headerlink" title="连读"></a>连读</h2><p>三种情况：</p><ol><li><p>元音-元音连读，后面一个元音不发音</p><p>Yo me llam<strong>o A</strong>na</p><p>An<strong>a e</strong>s mexican</p></li><li><p>辅音-辅音连读，后面一个辅音不发音</p><p>Ello<strong>s s</strong>on mis amios</p><p>Entonces, somo<strong>s s</strong>us alumnos</p></li><li><p>辅音-元音连读，辅音连读元音</p><p>Somos su<strong>s a</strong>lumnos</p><p>So<strong>n u</strong>stede<strong>s e</strong>spanol单词</p></li></ol>]]></content>
      
      <categories>
          
          <category> Spanish </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spanish </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-FullyConvolutionalSegmentation</title>
      <link href="/2019/04/05/DP-FullyConvolutionalSegmentation/"/>
      <url>/2019/04/05/DP-FullyConvolutionalSegmentation/</url>
      <content type="html"><![CDATA[<p>This post is based on the paper <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a>, which aims to perform image segmentation.</p><a id="more"></a><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p><a href="https://www.jeremyjordan.me/semantic-segmentation/" target="_blank" rel="noopener">ref</a></p><p>More specifically, the goal of semantic image segmentation is to label <em>each pixel</em> of an image with a corresponding <strong>class</strong> of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as <strong>dense prediction</strong>.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-7.42.16-PM.png" alt="creen-Shot-2018-05-17-at-7.42.16-P"></p><p>One important thing to note is that we’re not separating <em>instances</em> of the same class; we only care about the category of each pixel. In other words, if you have two objects of the same category in your input image, the segmentation map does not inherently distinguish these as separate objects. There exists a different class of models, known as <em>instance segmentation</em> models, which <em>do</em> distinguish between separate objects of the same class.</p><p>Segmentation models are useful for a variety of tasks, including:</p><ul><li><p><strong>Autonomous vehicles</strong><br>We need to equip cars with the necessary perception to understand their environment so that self-driving cars can safely integrate into our existing roads.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/deeplabcityscape.gif" alt="eeplabcityscap"></p></li><li><p><strong>Medical image diagnostics</strong></p></li></ul><h1 id="Representing-the-task"><a href="#Representing-the-task" class="headerlink" title="Representing the task"></a>Representing the task</h1><p>Simply, our goal is to take either a RGB color image (height×width×3height×width×3) or a grayscale image (height×width×1height×width×1) and output a segmentation map where each pixel contains a class label represented as an integer (height×width×1height×width×1).</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-17-at-9.02.15-PM.png" alt="creen-Shot-2018-05-17-at-9.02.15-P"></p><p>Similar to how we treat standard categorical values, we’ll create our <strong>target</strong> by one-hot encoding the class labels - essentially creating an output channel for each of the possible classes.<img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-16-at-9.36.00-PM.png" alt="creen-Shot-2018-05-16-at-9.36.00-P"></p><p>A prediction can be collapsed into a segmentation map (as shown in the first image) by taking the <code>argmax</code> of each depth-wise pixel vector.</p><h1 id="Fully-convolutional-networks"><a href="#Fully-convolutional-networks" class="headerlink" title="Fully convolutional networks"></a>Fully convolutional networks</h1><p>In the traditional image classification deep learning architecture, we take a image and pass this image through a number of convolution layers so that we get the high-level features. And the features are connected with several dense layers and output the classification prediction. </p><p>In FCN, image segmentation models is to follow an <strong>encoder/decoder structure</strong> where we <em>downsample</em> the spatial resolution of the input, developing lower-resolution feature mappings which are learned to be highly efficient at discriminating between classes, and the <em>upsample</em> the feature representations into a full-resolution segmentation map.</p><h2 id="Upsampling-ref"><a href="#Upsampling-ref" class="headerlink" title="Upsampling ref"></a>Upsampling <a href="https://towardsdatascience.com/transpose-convolution-77818e55a123" target="_blank" rel="noopener">ref</a></h2><p>There are a few different approaches that we can use to <em>upsample</em> the resolution of a feature map. </p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-19-at-12.54.50-PM.png" alt="creen-Shot-2018-05-19-at-12.54.50-P"></p><p>However, <strong>transpose convolutions</strong> are by far the most popular approach as they allow for us to develop a <em>learned upsampling</em>.</p><p>Before diving into transpose convolutions, we review what convolutions do. Suppose We are applying the convolution to an image of 5x5x1 with a kernel of 3x3, stride 2x2 and padding VALID. </p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/1_wstv4l1dXB8vl8pclcEmTQ-4479112.gif" alt="_wstv4l1dXB8vl8pclcEmTQ-447911"></p><p>As you can see in the left image the output will be a 2x2 image. You can calculate the output size of a convolution operation by using below formula as well:</p><blockquote><p>Convolution Output Size = 1 + (Input Size - Filter size + 2 * Padding) / Stride</p></blockquote><p>Now suppose you want to up-sample this to same dimension as input image. You will use same parameters as for convolution and will first calculate what was the size of Image before down-sampling.</p><blockquote><p><strong>SAME PADDING:</strong><br>Transpose Convolution Size = Input Size <em> Stride<br><strong>VALID PADDING: 0</strong>Transpose Convolution Size = Input Size </em> Stride + max(Filter Size - Stride, 0)</p></blockquote><p>Suppose we have the following image:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.34.21 PM.png" alt="creen Shot 2019-04-05 at 12.34.21 P"></p><p>For transpose convolution, we use $3 \times 3$ filter and stride 2, then we go over each pixel and each single pixel is multiplied by a $3\times 3$ filter and formed a $3 \times 3$ block which is then put in output matrix. Say the initial filter is all-one matrix, so after processing the first pixel, we have:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.48.33 PM.png" alt="creen Shot 2019-04-05 at 12.48.33 P"></p><p>For the second pixel, we have:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.53.29 PM.png" alt="creen Shot 2019-04-05 at 12.53.29 P"></p><p>Since the stride step is 2, so we are going to combine the above two matrix:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-05 at 12.57.06 PM.png" alt="creen Shot 2019-04-05 at 12.57.06 P"></p><p>we get $3 \times 5$ matrix.</p><p>For the third and fouth matrix, we have </p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-09 at 12.44.43 PM.png" alt="creen Shot 2019-04-09 at 12.44.43 P"></p><p>So after the deconvolution, we have $5\times 5$ matrix:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-09 at 12.46.55 PM.png" alt="creen Shot 2019-04-09 at 12.46.55 P"></p><p>To sum up, the convolution transpose is the opposite of convolution. Since for convolution, the new size is  <code>(old_row - k + 2*p ) / s + 1</code>, so for convolution transpose, its size is <code>(old_row - 1) * s - 2*p + k</code>. If we use no padding and kernel size equals stride step, then new size is <code>old_row * s</code>.</p><h2 id="Network-Structures"><a href="#Network-Structures" class="headerlink" title="Network Structures"></a>Network Structures</h2><p>The original paper use well-studied <em>image classification</em> networks (eg. AlexNet) to serve as the encoder module of the network, appending a decoder module with transpose convolutional layers to upsample the coarse feature maps into a full-resolution segmentation map. The encoder structure looks like the following:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-9.53.20-AM.png" alt="creen-Shot-2018-05-20-at-9.53.20-A"></p><p>The full network, as shown below, is trained according to a pixel-wise cross entropy loss.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-16-at-10.34.02-PM.png" alt="creen-Shot-2018-05-16-at-10.34.02-P"></p><p>However, because the encoder module reduces the resolution of the input by a factor of 32, the decoder module <strong>struggles to produce fine-grained segmentations</strong> (as shown below).</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-10.15.09-AM.png" alt="creen-Shot-2018-05-20-at-10.15.09-A"></p><p>The paper’s authors comment eloquently on this struggle:</p><blockquote><p>Semantic segmentation faces an inherent tension between semantics and location: global information resolves <strong>what</strong> while local information resolves <strong>where</strong>… Combining fine layers and coarse layers lets the model make local predictions that respect global structure. ― <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Long et al.</a></p></blockquote><h2 id="Skip-Connections"><a href="#Skip-Connections" class="headerlink" title="Skip Connections"></a>Skip Connections</h2><p>The authors address this tension by slowly upsampling (in stages) the encoded representation, adding “skip connections” from earlier layers, and summing these two feature maps.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-12.26.53-PM.png" alt="creen-Shot-2018-05-20-at-12.26.53-P"></p><p>These skip connections from earlier layers in the network (prior to a downsampling operation) should provide the necessary detail in order to reconstruct accurate shapes for segmentation boundaries. Indeed, we can recover more fine-grain detail with the addition of these skip connections.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-12.10.25-PM.png" alt="creen-Shot-2018-05-20-at-12.10.25-P"></p><p><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">Ronneberger et al.</a> improve upon the “fully convolutional” architecture primarily through <strong>expanding the capacity of the decoder</strong> module of the network. More concretely, they propose the <strong>U-Net architecture</strong> which “consists of a contracting path to capture context and a <strong>symmetric</strong> expanding path that enables precise localization.” This simpler architecture has grown to be very popular and has been adapted for a variety of segmentation problems.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-20-at-1.46.43-PM.png" alt="creen-Shot-2018-05-20-at-1.46.43-P"></p><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>The most commonly used loss function for the task of image segmentation is a <strong>pixel-wise cross entropy loss</strong>. This loss examines <em>each pixel individually</em>, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen-Shot-2018-05-24-at-10.46.16-PM.png" alt="creen-Shot-2018-05-24-at-10.46.16-P"></p><p>Because the cross entropy loss evaluates the class predictions for each pixel vector individually and then averages over all pixels, we’re essentially asserting equal learning to each pixel in the image. This can be a problem if your various classes have unbalanced representation in the image, as training can be dominated by the most prevalent class. <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Long et al.</a> (FCN paper) discuss weighting this loss for each <strong>output channel</strong> in order to counteract a class imbalance present in the dataset.</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>In order to get a full understanding of FCN, we will get our hands on streetview images segmentation. The state of the art dataset is pascal VOC2012, however it requires preprocessing and is kind of large, we choose a <a href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view" target="_blank" rel="noopener">streetview dataset</a> with 12 object classes which includes 367 training dataset and 101 test images. It is perfect for us to feel how FCN works in image segmentation.</p><h2 id="Data-exploration"><a href="#Data-exploration" class="headerlink" title="Data exploration"></a>Data exploration</h2><p>The data directory looks like the following:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/Screen Shot 2019-04-13 at 2.23.41 PM.png" alt="creen Shot 2019-04-13 at 2.23.41 P"></p><p>where in the <code>images_prepped_train</code> there are original images for training, while in <code>annotations_prepped_train</code> is our groundtruth for each image. For a groundtruth image, it is the same size with the original image where each pixel is labeled with a class. It is same with the test images directory.</p><p>The first thing to do is to visualize a single segmentation image. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cv2,os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"></div><div class="line">dir_seg = <span class="string">"/Users/daniel/Desktop/streets/annotations_prepped_train/"</span></div><div class="line">dir_img = <span class="string">"/Users/daniel/Desktop/streets/images_prepped_train/"</span></div><div class="line"></div><div class="line">n_classes = <span class="number">12</span> <span class="comment">#total classes for segmentation</span></div><div class="line"></div><div class="line">ldseg = os.listdir(dir_seg)</div><div class="line">firstimage = ldseg[<span class="number">0</span>] <span class="comment"># choose the first image for visualization</span></div><div class="line"></div><div class="line">seg = cv2.imread(dir_seg+firstimage) <span class="comment"># read segmentation groundtruth</span></div><div class="line">img = cv2.imread(dir_img+firstimage) <span class="comment"># read original image</span></div><div class="line"></div><div class="line">mi,ma = np.min(seg),np.max(seg) <span class="comment">#get the range for classes</span></div><div class="line">total_classes = ma-mi+<span class="number">1</span> </div><div class="line"><span class="comment"># print("The total segmentation classes are : &#123;&#125;".format(ma-mi+1))</span></div><div class="line"></div><div class="line">fig = plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</div><div class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div><div class="line">ax.imshow(img)</div><div class="line">ax.set_title(<span class="string">"Original Image"</span>)</div><div class="line"></div><div class="line">fig = plt.figure(figsize=(<span class="number">15</span>,<span class="number">10</span>))</div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(mi,ma+<span class="number">1</span>):</div><div class="line">    ax = fig.add_subplot(<span class="number">3</span>,total_classes/<span class="number">3</span>,k+<span class="number">1</span>) <span class="comment">#the first digit is the number of rows, the second the number of columns, and the third the index of the subplot.</span></div><div class="line">    ax.imshow((seg==k)*<span class="number">1.0</span>) <span class="comment">#(seg==k) return a True/False list</span></div><div class="line">    ax.set_title(<span class="string">"label=&#123;&#125;"</span>.format(k))</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131430474.png" alt="mage-20190413143047">  </p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131434053.png" alt="mage-20190413143405"></p><p>We can roughly observe from the segmentaion image that the first class is buildings, while the <code>class label=8</code> is cars.</p><p>Next, I will give the different objects with different colors to visualize them in one image.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_colorized</span><span class="params">(seg,n_classes)</span>:</span></div><div class="line">    <span class="keyword">if</span> len(seg.shape) == <span class="number">3</span>:</div><div class="line">        seg = seg[:, :, <span class="number">0</span>]</div><div class="line">    seg_img = np.zeros((seg.shape[<span class="number">0</span>], seg.shape[<span class="number">1</span>], <span class="number">3</span>)).astype(<span class="string">'float'</span>)</div><div class="line">    colors = sns.color_palette(<span class="string">"hls"</span>, n_classes) <span class="comment"># return n_classes colors</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(n_classes):</div><div class="line">        segc = (seg == c)</div><div class="line">        seg_img[:, :, <span class="number">0</span>] += (segc * (colors[c][<span class="number">0</span>]))</div><div class="line">        seg_img[:, :, <span class="number">1</span>] += (segc * (colors[c][<span class="number">1</span>]))</div><div class="line">        seg_img[:, :, <span class="number">2</span>] += (segc * (colors[c][<span class="number">2</span>]))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> (seg_img)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">color_img = img_colorized(seg,n_classes)</div><div class="line">fig = plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</div><div class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div><div class="line">ax.imshow(color_img)</div><div class="line">ax.set_title(<span class="string">"Colored Image"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/image-201904131451012.png" alt="mage-20190413145101"></p><h2 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h2><p>We will resize the image to <code>(224,224)</code>. In the following code, we resize the image and normalize the image to range <code>(-1,1)</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataProcessing</span><span class="params">(path,width,height)</span>:</span></div><div class="line">    img = cv2.imread(path,<span class="number">1</span>)</div><div class="line">    img = np.float32(cv2.resize(img,(width,height)))/<span class="number">127.5</span><span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> img</div></pre></td></tr></table></figure><p>Then considering we are going to predict class label for each pixel, so for each pixel, there are going to be <code>total_classes</code> output. So for a input image with size <code>(row,col)</code>, the output should be <code>(row, col, total_classes)</code>. Therefore, for each groundtruth image, we are going to generate <code>total_classes</code> groundtruth lable images, where in the first image, the pixel belonging to class 0 has value 1 while the rest has value 0. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSegmentationArr</span><span class="params">(path,nClasses,width,height)</span>:</span></div><div class="line">    seg_labels = np.zeros((height,width,nClasses))</div><div class="line">    img = cv2.imread(path,<span class="number">1</span>)</div><div class="line">    img = cv2.resize(img,(width,height))</div><div class="line">    img = img[:,:,<span class="number">0</span>]</div><div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(nClasses):</div><div class="line">        seg_labels[:, :, c] = (img == c).astype(int)</div><div class="line">    <span class="keyword">return</span> seg_labels</div></pre></td></tr></table></figure><p>Then we create our training dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">images = os.listdir(dir_img)</div><div class="line">images.sort()</div><div class="line">segmentations = os.listdir(dir_seg)</div><div class="line">segmentations.sort()</div><div class="line"></div><div class="line">X = []</div><div class="line">Y = []</div><div class="line"><span class="keyword">for</span> im, seg <span class="keyword">in</span> zip(images, segmentations):</div><div class="line">    X.append(dataProcessing(dir_img + im, <span class="number">224</span>, <span class="number">224</span>))</div><div class="line">    Y.append(getSegmentationArr(dir_seg + seg, total_classes, <span class="number">224</span>, <span class="number">224</span>))</div><div class="line"></div><div class="line">X, Y = np.array(X), np.array(Y)</div></pre></td></tr></table></figure><h2 id="Network-architecture"><a href="#Network-architecture" class="headerlink" title="Network architecture"></a>Network architecture</h2><p>Then we can implement the network, here we use the <code>vgg16</code> architecture, where the architecture looks like:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/1_U8uoGoZDs8nwzQE3tOhfkw@2x.png" alt="_U8uoGoZDs8nwzQE3tOhfkw@2"></p><p>Since the <code>vgg16</code> downsample the input image to the original size of $1/2^5=1/32$ due to <code>maxpooling</code>. Therefore we need to ensure the input size can be divided by 32. In the beginning, we implement the structure of <code>vgg16</code>: <code>block1 -&gt; block2 -&gt; block3 -&gt; block4 -&gt; block5</code> .</p><p>In order to keep more information, we directly use output from <code>block3</code> and <code>block4</code> to combine them with the output of <code>block5</code>. To be specific, for the output of <code>block5</code>, we use <code>conv2d_k4096_s7 + conv2d_k4096_s1 + conv2dT_k12_s4_p4</code>; for the output of <code>block4</code>, we need to upsample it by 2, <code>conv2d_k12_s1 + conv2dT_k12_s2_p2</code>; for output of <code>block3</code>, <code>conv2d_k12_s1</code>. And finally, combine them by adding them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> *</div><div class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</div><div class="line"></div><div class="line">vgg_weights = <span class="string">'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">FCN8</span><span class="params">(nClasses,input_height=<span class="number">224</span>,input_width=<span class="number">224</span>)</span>:</span></div><div class="line">    <span class="keyword">assert</span>  input_height%<span class="number">32</span> == <span class="number">0</span></div><div class="line">    <span class="keyword">assert</span> input_width%<span class="number">32</span> == <span class="number">0</span></div><div class="line"></div><div class="line">    img_input = Input(shape=(input_height, input_width, <span class="number">3</span>))  <span class="comment">## Assume 224,224,3</span></div><div class="line"></div><div class="line">    <span class="comment">## Block 1</span></div><div class="line">    x = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block1_conv1'</span>)(</div><div class="line">        img_input)</div><div class="line">    x = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block1_conv2'</span>)(x)</div><div class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block1_pool'</span>)(x)</div><div class="line">    f1 = x</div><div class="line"></div><div class="line">    <span class="comment"># Block 2</span></div><div class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv1'</span>)(x)</div><div class="line">    x = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block2_conv2'</span>)(x)</div><div class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block2_pool'</span>)(x)</div><div class="line">    f2 = x</div><div class="line"></div><div class="line">    <span class="comment"># Block 3</span></div><div class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block3_conv1'</span>)(x)</div><div class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block3_conv2'</span>)(x)</div><div class="line">    x = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block3_conv3'</span>)(x)</div><div class="line">    x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block3_pool'</span>)(x)</div><div class="line">    pool3 = x</div><div class="line"></div><div class="line">    <span class="comment"># Block 4</span></div><div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block4_conv1'</span>)(x)</div><div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block4_conv2'</span>)(x)</div><div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block4_conv3'</span>)(x)</div><div class="line">    pool4 = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block4_pool'</span>)(x)  <span class="comment">## (None, 14, 14, 512)</span></div><div class="line"></div><div class="line">    <span class="comment"># Block 5</span></div><div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block5_conv1'</span>)(pool4)</div><div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block5_conv2'</span>)(x)</div><div class="line">    x = Conv2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">'block5_conv3'</span>)(x)</div><div class="line">    pool5 = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'block5_pool'</span>)(x)  <span class="comment">## (None, 7, 7, 512)</span></div><div class="line"></div><div class="line">    vgg = Model(img_input, pool5)</div><div class="line">    vgg.load_weights(vgg_weights)  <span class="comment">## loading VGG weights for the encoder parts of FCN8</span></div><div class="line"><span class="comment">#########################################################</span></div><div class="line">    n = <span class="number">4096</span></div><div class="line">    o = Conv2D(n, (<span class="number">7</span>, <span class="number">7</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"conv6"</span>)(pool5)</div><div class="line">    conv7 = Conv2D(n, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"conv7"</span>)(o)</div><div class="line"></div><div class="line">    <span class="comment">## 4 times upsamping for pool4 layer</span></div><div class="line">    conv7_4 = Conv2DTranspose(nClasses, kernel_size=(<span class="number">4</span>, <span class="number">4</span>), strides=(<span class="number">4</span>, <span class="number">4</span>), use_bias=<span class="keyword">False</span>)(conv7)</div><div class="line">    <span class="comment">## (None, 224, 224, 10)</span></div><div class="line">    <span class="comment">## 2 times upsampling for pool411</span></div><div class="line">    pool411 = Conv2D(nClasses, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"pool4_11"</span>)(pool4)</div><div class="line">    pool411_2 =Conv2DTranspose(nClasses, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), use_bias=<span class="keyword">False</span>)(pool411)</div><div class="line"></div><div class="line">    pool311 = Conv2D(nClasses, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>, name=<span class="string">"pool3_11"</span>)(pool3)</div><div class="line"></div><div class="line">    o = Add(name=<span class="string">"add"</span>)([pool411_2, pool311, conv7_4])</div><div class="line">    o = Conv2DTranspose(nClasses, kernel_size=(<span class="number">8</span>, <span class="number">8</span>), strides=(<span class="number">8</span>, <span class="number">8</span>), use_bias=<span class="keyword">False</span>)(o)</div><div class="line">    o = Activation(<span class="string">'softmax'</span>)(o)</div><div class="line"></div><div class="line">    model = Model(img_input, o)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> model</div><div class="line"></div><div class="line">model = FCN8(nClasses=n_classes,input_height=<span class="number">224</span>,input_width=<span class="number">224</span>)</div><div class="line">model.summary()</div></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</div><div class="line">train_rate = <span class="number">0.85</span></div><div class="line">index_train = np.random.choice(X.shape[<span class="number">0</span>],int(X.shape[<span class="number">0</span>]*train_rate),replace=<span class="keyword">False</span>)</div><div class="line">index_test  = list(set(range(X.shape[<span class="number">0</span>])) - set(index_train))</div><div class="line">X, Y = shuffle(X,Y)</div><div class="line">X_train, y_train = X[index_train],Y[index_train]</div><div class="line">X_test, y_test = X[index_test],Y[index_test]</div><div class="line"></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</div><div class="line">sgd = optimizers.SGD(lr=<span class="number">1e-2</span>,decay=<span class="number">5</span>**(<span class="number">-4</span>),momentum=<span class="number">0.9</span>,nesterov=<span class="keyword">True</span>)</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=sgd,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),</div><div class="line">                 batch_size=<span class="number">32</span>,epochs=<span class="number">200</span>,verbose=<span class="number">2</span>)</div><div class="line">model.save(<span class="string">"seg_model.h5"</span>)</div></pre></td></tr></table></figure><h2 id="mean-Intersection-over-Union"><a href="#mean-Intersection-over-Union" class="headerlink" title="mean Intersection over Union"></a>mean Intersection over Union</h2><p>Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset. However, Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU. </p><p>More formally, in order to apply Intersection over Union to evaluate an (arbitrary) object detector we need:</p><ol><li>The <em>ground-truth bounding boxes</em> (i.e., the hand labeled bounding boxes from the testing set that specify <em>where</em> in the image our object is).</li><li>The <em>predicted bounding boxes</em> from our model.</li></ol><p>As long as we have these two sets of bounding boxes we can apply Intersection over Union.</p><p>Below I have included a visual example of a ground-truth bounding box versus a predicted bounding box:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/iou_stop_sign.jpg" alt="ou_stop_sig"></p><p>Computing Intersection over Union can therefore be determined via:</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/iou_equation.png" alt="ou_equatio"></p><p>In the numerator we compute the <strong>area of overlap</strong> between the <em>predicted</em> bounding box and the <em>ground-truth</em> bounding box.</p><p>The denominator is the <strong>area of union</strong>, or more simply, the area encompassed by <em>both</em> the predicted bounding box and the ground-truth bounding box.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">IoU</span><span class="params">(Yi, y_predi)</span>:</span></div><div class="line">    <span class="comment">## mean Intersection over Union</span></div><div class="line">    <span class="comment">## Mean IoU = TP/(FN + TP + FP)</span></div><div class="line"></div><div class="line">    IoUs = []</div><div class="line">    Nclass = int(np.max(Yi)) + <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(Nclass):</div><div class="line">        TP = np.sum((Yi == c) &amp; (y_predi == c))</div><div class="line">        FP = np.sum((Yi != c) &amp; (y_predi == c))</div><div class="line">        FN = np.sum((Yi == c) &amp; (y_predi != c))</div><div class="line">        IoU = TP / float(TP + FP + FN)</div><div class="line">        print(<span class="string">"class &#123;:02.0f&#125;: #TP=&#123;:6.0f&#125;, #FP=&#123;:6.0f&#125;, #FN=&#123;:5.0f&#125;, IoU=&#123;:4.3f&#125;"</span>.format(c, TP, FP, FN, IoU))</div><div class="line">        IoUs.append(IoU)</div><div class="line">    mIoU = np.mean(IoUs)</div><div class="line">    print(<span class="string">"_________________"</span>)</div><div class="line">    print(<span class="string">"Mean IoU: &#123;:4.3f&#125;"</span>.format(mIoU))</div><div class="line"></div><div class="line">y_pred = model.predict(X_test)</div><div class="line">y_predi = np.argmax(y_pred, axis=<span class="number">3</span>)</div><div class="line">y_testi = np.argmax(y_test, axis=<span class="number">3</span>)</div><div class="line">IoU(y_testi, y_predi)</div></pre></td></tr></table></figure><p><strong>WHY WE USE IoU?</strong></p><p>In all reality, it’s <em>extremely unlikely</em> that the <em>(x, y)</em>-coordinates of our predicted bounding box are going to <strong>exactly match</strong> the <em>(x, y)</em>-coordinates of the ground-truth bounding box.</p><p>Due to varying parameters of our model (image pyramid scale, sliding window size, feature extraction method, etc.), a complete and total match between predicted and ground-truth bounding boxes is simply unrealistic.</p><p>Because of this, we need to define an evaluation metric that <strong><em>rewards</em> predicted bounding boxes for heavily overlapping with the ground-truth</strong>.</p><p><img src="/2019/04/05/DP-FullyConvolutionalSegmentation/iou_examples.png" alt="ou_example"></p><p>As you can see, predicted bounding boxes that heavily overlap with the ground-truth bounding boxes have higher scores than those with less overlap. This makes Intersection over Union an excellent metric for evaluating custom object detectors.</p><p>We aren’t concerned with an <em>exact</em> match of <em>(x, y)</em>-coordinates, but we do want to ensure that our predicted bounding boxes match as closely as possible — Intersection over Union is able to take this into account.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    img_is = (X_test[i] + <span class="number">1</span>) * (<span class="number">255.0</span> / <span class="number">2</span>)</div><div class="line">    seg = y_predi[i]</div><div class="line">    segtest = y_testi[i]</div><div class="line"></div><div class="line">    fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">30</span>))</div><div class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</div><div class="line">    ax.imshow(img_is / <span class="number">255.0</span>)</div><div class="line">    ax.set_title(<span class="string">"original"</span>)</div><div class="line"></div><div class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</div><div class="line">    ax.imshow(img_colorized(seg, n_classes))</div><div class="line">    ax.set_title(<span class="string">"predicted class"</span>)</div><div class="line"></div><div class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</div><div class="line">    ax.imshow(img_colorized(segtest, n_classes))</div><div class="line">    ax.set_title(<span class="string">"true class"</span>)</div><div class="line">    plt.savefig(<span class="string">'&#123;&#125;.png'</span>.format(i))</div></pre></td></tr></table></figure><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Overall, in this paper, the most important thing is how use convolution transpose to upsample the feature to the original image size and utilize <code>u-net</code> structure to preserve features for acurate pixel classification. And in the convolution network, we only use convolution and its transpose, that is why it is called <strong>Fully Convolution Network</strong>. </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
            <tag> Object Segmentation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NLP-Attention</title>
      <link href="/2019/03/18/NLP-Attention/"/>
      <url>/2019/03/18/NLP-Attention/</url>
      <content type="html"><![CDATA[<p>Attention in DP and NLP.</p><a id="more"></a><h1 id="Attention-in-Seq2seq"><a href="#Attention-in-Seq2seq" class="headerlink" title="Attention in Seq2seq"></a>Attention in Seq2seq</h1><p>Remember that our seq2seq model is made of two parts, an encoder that encodes the input sentence, and a decoder that leverages the information extracted by the decoder to produce the translated sentence. Basically, our input is a sequence of words $x_1, …, x_n$ that we want to translate, and our target sentecne is a sequence of words $y_1,…,y_m$.</p><ol><li><p>Encoder</p><p>Let $(h_1,…,h_n)$ be the hidden vectors representing the input sentence. These vectors are the output of a bi-LSTM for instance, and capture contextual representation of each word in the sentence.</p></li><li><p>Decoder</p><p>We want to computer the hidden states $s_i$ of the decoder using a recursive formula of the form</p><script type="math/tex; mode=display">s_i=f(s_{i-1},y_{y-1},c_i)</script><p>where $s_{i-1}$ is the previous hidden vector, $y_{i-1}$ is the generated word at the previous step, and $c_i$ is a context vector that capture the context from the original sentence that is relevant to the time step $i$ of the decoder.</p><p>The  contect vector $c_i$ captures relevant information for the $i$-th decoding time step. For each hidden vector from the original sentence $h_j$, compute a score</p><script type="math/tex; mode=display">e_{i,j}=a(s_{i-1},h_j)</script><p>where a is any function with values in $R$, for instance a single layer fully-connceted neural network. Then, we end up with a scalar vaules $e_{i,1},…,e_{i,n}$. Normalize these scores into a vector $\alpha_i=(\alpha_{i,1},…,\alpha_{i,n})$, using a softmax layer.</p><script type="math/tex; mode=display">\alpha_{i, j}=\frac{\exp \left(e_{i, j}\right)}{\sum_{k=1}^{n} \exp \left(e_{i, k}\right)}</script><p>Then, compute the context vector $c_i$ as the weighted average of the hidden vectors from the original sentence</p><script type="math/tex; mode=display">c_{i}=\sum_{j=1}^{n} \alpha_{i, j} h_{j}</script><p>Intuitively, this vector captures the relevant contextual information from the original sentence for the $i$-th step of the decoder.</p></li></ol><h1 id="Attention-in-General"><a href="#Attention-in-General" class="headerlink" title="Attention in General"></a>Attention in General</h1><p>In essence, I will describe Attention Parameters as a projection of a query for a series of key-value pairs as in the below image:</p><p><img src="/2019/03/18/NLP-Attention/0_hJfCjjx0r0slacNm.png" alt="_hJfCjjx0r0slacN"></p><p>Calculating attention comes primarily in three steps. First, we take the query and each key and compute the similarity between the two to obtain a weight. Frequently used similarity functions include dot product, splice, detector, etc. The second step is typically to use a softmax function to normalize these weights, and finally to weight these weights in conjunction with the corresponding values and obtain the final Attention.</p><p>In current NLP work, the key and value are frequently the same, therefore key=value.</p><p><img src="/2019/03/18/NLP-Attention/0_HNv9BMtUF85ticoe.png" alt="_HNv9BMtUF85tico"></p><h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p><p><img src="/2019/03/18/NLP-Attention/The_transformer_encoder_decoder_stack.png" alt="he_transformer_encoder_decoder_stac"></p><p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p><p><img src="/2019/03/18/NLP-Attention/Transformer_encoder.png" alt="ransformer_encode">The encoder’s input first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. The outputs of the self-attention layer are fed to a feed-forward neural network. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.</p><p><img src="/2019/03/18/NLP-Attention/Transformer_decoder.png" alt="ransformer_decode"></p><p>In decoder, the target is fed into the self-attention layer. And the outputs from this layer and encoder are sent to encoder-decoder attention layer.</p><h2 id="Bring-the-tensors-into-the-picture"><a href="#Bring-the-tensors-into-the-picture" class="headerlink" title="Bring the tensors into the picture"></a>Bring the tensors into the picture</h2><p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p><p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding layer. </p><p><img src="/2019/03/18/NLP-Attention/embeddings.png" alt="mbedding"></p><p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p><p><img src="/2019/03/18/NLP-Attention/encoder_with_tensors.png" alt="ncoder_with_tensor"></p><p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p><p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</p><p><img src="/2019/03/18/NLP-Attention/transformer_self_attention_vectors.png" alt="ransformer_self_attention_vector"> </p><p>The <strong>second step</strong> in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p><p>The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.</p><p><img src="/2019/03/18/NLP-Attention/transformer_self_attention_score.png" alt="ransformer_self_attention_scor"></p><p>The <strong>third and forth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p><p><img src="/2019/03/18/NLP-Attention/self-attention_softmax.png" alt="elf-attention_softma"></p><p>This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</p><p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p><p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p><p><img src="/2019/03/18/NLP-Attention/self-attention-output.png" alt="elf-attention-outpu"></p><p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</p><h2 id="Matrix-calculation-of-Self-Attention"><a href="#Matrix-calculation-of-Self-Attention" class="headerlink" title="Matrix calculation of Self-Attention"></a>Matrix calculation of Self-Attention</h2><p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).</p><p><img src="/2019/03/18/NLP-Attention/self-attention-matrix-calculation.png" alt="elf-attention-matrix-calculatio"></p><p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p><p><img src="/2019/03/18/NLP-Attention/self-attention-matrix-calculation-2.png" alt="elf-attention-matrix-calculation-"></p><h2 id="Multi-heads"><a href="#Multi-heads" class="headerlink" title="Multi heads"></a>Multi heads</h2><p>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</p><ol><li>It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.</li><li>It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</li></ol><p><img src="/2019/03/18/NLP-Attention/transformer_attention_heads_qkv.png" alt="ransformer_attention_heads_qk"></p><p>If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p><p><img src="/2019/03/18/NLP-Attention/transformer_attention_heads_z.png" alt="ransformer_attention_heads_"></p><p>We concat the matrices then multiple them by an additional weights matrix WO.</p><p><img src="/2019/03/18/NLP-Attention/transformer_attention_heads_weight_matrix_o.png" alt="ransformer_attention_heads_weight_matrix_"></p><p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p><p><img src="/2019/03/18/NLP-Attention/transformer_multi-headed_self-attention-recap.png" alt="ransformer_multi-headed_self-attention-reca"></p><h2 id="Sequence-order"><a href="#Sequence-order" class="headerlink" title="Sequence order"></a>Sequence order</h2><p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p><p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p><p><img src="/2019/03/18/NLP-Attention/transformer_positional_encoding_vectors.png" alt="ransformer_positional_encoding_vector"></p><h2 id="The-residuals"><a href="#The-residuals" class="headerlink" title="The residuals"></a>The residuals</h2><p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.</p><p><img src="/2019/03/18/NLP-Attention/transformer_resideual_layer_norm.png" alt="ransformer_resideual_layer_nor"></p><p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p><p><img src="/2019/03/18/NLP-Attention/transformer_resideual_layer_norm_2.png" alt="ransformer_resideual_layer_norm_"></p><p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p><p><img src="/2019/03/18/NLP-Attention/transformer_resideual_layer_norm_3.png" alt="ransformer_resideual_layer_norm_"></p><h2 id="Decoder-side"><a href="#Decoder-side" class="headerlink" title="Decoder side"></a>Decoder side</h2><p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence</p><p>The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</p><p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p><p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p><p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it <strong>creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</strong></p><h1 id="Attention-in-GAN"><a href="#Attention-in-GAN" class="headerlink" title="Attention in GAN"></a>Attention in GAN</h1><p><a href="https://medium.com/towards-artificial-intelligence/techniques-in-self-attention-generative-adversarial-networks-22f735b22dfb" target="_blank" rel="noopener">REF1</a> <a href="https://medium.com/@jonathan_hui/gan-self-attention-generative-adversarial-networks-sagan-923fccde790c" target="_blank" rel="noopener">REF2</a> <a href="https://towardsdatascience.com/not-just-another-gan-paper-sagan-96e649f01a6b" target="_blank" rel="noopener">great explanation</a> </p><p>Convolutional GANs have difficulty in learning the image distributions of diverse multi-class datasets like Imagenet. It is observed that CGANs could easily generate images with a simpler geometry like Ocean, Sky etc. but failed on images with some specific geometry like dogs, horses and many more. </p><p>This problem is arising because the convolution is a local operation whose receptive field depends on the spatial size of the kernel. In a convolution operation, it is not possible for an output on the top-left position to have any relation to the output at bottom-right.</p><p>You would ask, Can’t we make the spatial size bigger so that it captures more of the image? Yes! of course we can, but it would decrease computational efficiency achieved by smaller filters and make the operation slow. Then you would again ask, Can’t we make a Deep CGAN with smaller filters so that the later layers have a large receptive field? Yes! we can, but it would take too many layers to have a large enough receptive field and too many layers would mean too many parameters. Hence it would make the GAN training more unstable.</p><p>The solutions to keeping computational efficiency and having a large receptive field at the same time is <strong>Self-Attention.</strong> It helps create a balance between efficiency and long-range dependencies(= large receptive fields) by utilizing the famous mechanism from NLP called attention.</p><h2 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h2><p>The work process is as follows</p><p><img src="/2019/03/18/NLP-Attention/1_H29pojIh1fvscvX04gF2Xg.png" alt="_H29pojIh1fvscvX04gF2X"></p><p>On the left of the below image, we get our feature maps from the previous convolutional layer. Let’s suppose it is of the dimension $(512\times 7 \times 7)$ where 512 is the number of channels and 7 is the spatial dimension. We first pass the feature map through three $1\times 1$ convolutions separately. We name the three filters $f, g$ and $h$.</p><p>What $1\times 1$ convolution does is that it reduces the channel number in the image. The filter size of $f$ and $g$ is 64 while the size of $h$ is 512. After the image gets passed through we get three feature maps of dimensions $(64\times 7\times 7)$, $(64\times 7\times 7)$ and $(512\times 7\times 7)$. These three things are our query, key and value pairs. In order to perform self attention onthe complete image, we flatten out last two dimensions and the dimensions become $(64\times 49)$,  $(64\times 49)$,  $(512\times 49)$. We transpose the query and matrix-multiply it by the key and take the softmax on all the rows. So we get an output attention map of shape $(49\times 49)$. Then we multiple the value vector with the attention map, resulting in output with $(512\times 49)$. One last thing that the paper proposes is that to multiply the final output by a learnable scale parameter and add back the input as a residual connection. Let’s say the $x$ is the image and $o$ is the output, we multiple $o$ by a parameter $y$. The final output $O$ is $O=y<em>o+x$. Initially, the paper advises initializing the sacle parameter as zero so that the output is simple old convolution at the beginning. They initialized $y$ with zero because they wanted their network to rely on the cues in the local neighborhood — since that was easier and then gradually the network will learn to assign the value to </em>y* parameter and use self-attention.</p><h1 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h1><p>Language modeling is the task of predicting what word comes next. More formally, given a sequence of words $x^{(1)}, x^{(2)},…,x^{(t)}, $ compute the probability distribution of the next word $x^{(t+1)}$:</p><script type="math/tex; mode=display">P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)</script><p>where $x^{t+1}$ can be any word in the vocabulary $V={w_1,…,w_{|V|}}$. A system that does this is called a <strong>Language Model</strong>.</p><p>We can also think of a Language Model as a system that assigns probability to a piece of text. For example, if we have some text $x^{(1)}, x^{(2)},…,x^{(T)}$, then the probability of this text is:</p><script type="math/tex; mode=display">\begin{aligned} P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}\right) &=P\left(\boldsymbol{x}^{(1)}\right) \times P\left(\boldsymbol{x}^{(2)} | \boldsymbol{x}^{(1)}\right) \times \cdots \times P\left(\boldsymbol{x}^{(T)} | \boldsymbol{x}^{(T-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \\ &=\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \end{aligned}</script><h2 id="N-gram-Language-Models"><a href="#N-gram-Language-Models" class="headerlink" title="N-gram Language Models"></a>N-gram Language Models</h2><p><strong>Definition:</strong> A n-gram is a chunk of n consecutive words.</p><p><strong>Idea:</strong> Collect statistics about how frequent different n-grams are, and use these to predict next word.</p><p><strong>Assumption:</strong> $x^{t=1}$ depends only on the preceding $n-1$ words.</p><script type="math/tex; mode=display">P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)=P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right) \\=\frac{P\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{P\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}</script><p><strong>Counting:</strong> by counting in some large corpus of text, we get these n-gram and (n-1)-gram probabilities.</p><script type="math/tex; mode=display">\approx \frac{\operatorname{count}\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{\operatorname{count}\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}</script><p><strong>Problems:</strong> </p><ol><li><p>Sparsity Problems with n-gram Language Models</p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.06.49 PM.png" alt="creen Shot 2019-08-02 at 5.06.49 P"></p></li><li><p>Storage Problems with n-gram Language Models</p><p>Need to store count for all n-grams you saw in the corpus and increasing n or increasing corpus increases model size!</p></li></ol><h2 id="Neural-Language-Model"><a href="#Neural-Language-Model" class="headerlink" title="Neural Language Model"></a>Neural Language Model</h2><h3 id="A-fixed-window-neural-Language-Model"><a href="#A-fixed-window-neural-Language-Model" class="headerlink" title="A fixed-window neural Language Model"></a>A fixed-window neural Language Model</h3><p>Say we have the sentence: </p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.11.21 PM.png" alt="creen Shot 2019-08-02 at 5.11.21 P">  </p><p>Then we use the neural network to build the model</p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.12.00 PM.png" alt="creen Shot 2019-08-02 at 5.12.00 P"></p><p><strong>Improvements</strong> over n-gram LM:</p><ol><li>No sparsity problem</li><li>Don’t need to store all observed n-grams</li></ol><p><strong>Remaining problems:</strong></p><ol><li>Fixed window is too small</li><li>Enlarging window enlarges $W$ while Window can never be large enough!</li><li>$ x^{(1)}$and $ x^{(2)}$ are multiplied by completely different weights in $W$. No symmetry in how the inputs are processed.</li></ol><p><strong>We need a neural architecture that can process any length input.</strong></p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p><strong>Core idea:</strong> Apply the same weights $W$ repeatedly.</p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.19.53 PM.png" alt="creen Shot 2019-08-02 at 5.19.53 P"></p><p><strong>RNN Advantages:</strong></p><ol><li>Can process any length input</li><li>Computation for step t can (in theory) use information from many steps back</li><li>Model size doesn’t increase for longer input</li><li>Same weights applied on every timestep, so there is symmetry in how inputs are processed.</li></ol><p><strong>RNN Disadvantages:</strong></p><ol><li>Recurrent computation is slow</li><li>In practice, difficult to access information from many steps back</li></ol><h4 id="Training-a-RNN"><a href="#Training-a-RNN" class="headerlink" title="Training a RNN"></a>Training a RNN</h4><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.23.37 PM.png" alt="creen Shot 2019-08-02 at 5.23.37 P"></p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.23.04 PM.png" alt="creen Shot 2019-08-02 at 5.23.04 P"></p><h2 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h2><p>In general, perplexity is a measurement of <strong>how well a probability model predicts a sample</strong>. </p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.37.29 PM.png" alt="creen Shot 2019-08-02 at 5.37.29 P"></p><p><img src="/2019/03/18/NLP-Attention/Screen Shot 2019-08-02 at 5.51.16 PM.png" alt="creen Shot 2019-08-02 at 5.51.16 P"></p><blockquote><p>Normalizing by the length is to avoid the pitfall that bigger corpus have smaller perplexity.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/53682800" target="_blank" rel="noopener">nlp中的Attention注意力机制+Transformer详解</a> </li><li><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">great intriduction</a></li><li><a href="https://dzone.com/articles/self-attention-mechanisms-in-natural-language-proc" target="_blank" rel="noopener">good attention introduction</a> </li><li><a href="https://github.com/johnsmithm/multi-heads-attention-image-classification" target="_blank" rel="noopener">multi-heads-attention-image-classification</a> </li><li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">great explanation on self-attention</a> </li><li><a href="https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/" target="_blank" rel="noopener">self-attention in Pytorch code</a> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">code2</a> </li></ol>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Tensorflow</title>
      <link href="/2019/03/18/DP-Tensorflow/"/>
      <url>/2019/03/18/DP-Tensorflow/</url>
      <content type="html"><![CDATA[<p>Some posts about tensorflow tutorials.</p><a id="more"></a><p>to do </p><p><a href="https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/" target="_blank" rel="noopener">Convolutional Neural Networks Tutorial in TensorFlow</a> </p><p><a href="https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/" target="_blank" rel="noopener">Recurrent neural networks and LSTM tutorial in Python and TensorFlow</a> </p><p><a href="https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/" target="_blank" rel="noopener">Word2Vec word embedding tutorial in Python and TensorFlow</a></p><h1 id="Renferences"><a href="#Renferences" class="headerlink" title="Renferences"></a>Renferences</h1><ol><li><a href="https://adventuresinmachinelearning.com/python-tensorflow-tutorial/" target="_blank" rel="noopener">A good introduction to tensorflow</a></li></ol>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Tensorflow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RL-Deep QLearning</title>
      <link href="/2019/03/10/RL-Deep-QLearning/"/>
      <url>/2019/03/10/RL-Deep-QLearning/</url>
      <content type="html"><![CDATA[<p>Q-Learning needs to maintain a Q-table that an agent uses to find the best action to take given a state. However, producing and updating a Q-table can become ineffective in big state space environments. While in this post, we are going to create a Deep Q Neural Network to improve Q Learning. Instead of using a Q-table, we’ll implement a Neural Network that takes a state and approximates Q-values for each action based on that state. <a href="https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8" target="_blank" rel="noopener">ref</a> </p><p><img src="/2019/03/10/RL-Deep-QLearning/1_w5GuxedZ9ivRYqM_MLUxOQ.png" alt="_w5GuxedZ9ivRYqM_MLUxO"></p><a id="more"></a><h1 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h1><p><img src="/2019/03/10/RL-Deep-QLearning/1_LglEewHrVsuEGpBun8_KTg.png" alt="_LglEewHrVsuEGpBun8_KT"></p><p>Our Deep Q Neural Network takes a stack of four frames as an input. These pass through its network, and output a vector of Q-values for each action possible in the given state. We need to take the biggest Q-value of this vector to find our best action.</p><p>In the beginning, the agent does really badly. But over time, it begins to associate frames (states) with best actions to do.</p><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p><img src="/2019/03/10/RL-Deep-QLearning/1_QgGnC_0BkQEtPqMUftRC6A.png" alt="_QgGnC_0BkQEtPqMUftRC6"></p><p>Preprocessing is an important step. We want to reduce the complexity of our states to reduce the computation time needed for training. First, we can grayscale each of our states. Color does not add important information (in our case, we just need to find the enemy and kill him, and we don’t need color to find him). This is an important saving, since we reduce our three colors channels (RGB) to 1 (grayscale). Then, we crop the frame. In our example, seeing the roof is not really useful. Then we reduce the size of the frame, and we we stack four sub-frames together.</p><h3 id="The-problem-of-temporal-limitation"><a href="#The-problem-of-temporal-limitation" class="headerlink" title="The problem of temporal limitation"></a><strong>The problem of temporal limitation</strong></h3><p><a href="https://medium.com/@awjuliani" target="_blank" rel="noopener">Arthur Juliani</a> gives an awesome explanation about this topic in <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2" target="_blank" rel="noopener">his article</a>. He has a clever idea: using <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM neural networks</a> for handling the problem.</p><p>The first question that you can ask is why we stack frames together? We stack frames together because it helps us to handle the problem of temporal limitation. Let’s take an example, in the game of Pong. When you see this frame:<img src="/2019/03/10/RL-Deep-QLearning/1_0lwyObh4p-jQjk19Q6qyIg.png" alt="_0lwyObh4p-jQjk19Q6qyI"></p><p>Can you tell me where the ball is going? No, because one frame is not enough to have a sense of motion! But what if I add three more frames? Here you can see that the ball is going to the right.</p><p><img src="/2019/03/10/RL-Deep-QLearning/1_MooQJUIkR_FVV2weeVPr8A-2268732.png" alt="_MooQJUIkR_FVV2weeVPr8A-226873"></p><p>That’s the same thing for our Doom agent. If we give him only one frame at a time, it has no idea of motion. And how can it make a correct decision, if it can’t determine where and how fast objects are moving?</p><h2 id="Convolution-network"><a href="#Convolution-network" class="headerlink" title="Convolution network"></a>Convolution network</h2><p>The frames are processed by three convolution layers. These layers allow you to exploit spatial relationships in images. But also, because frames are stacked together, you can exploit some spatial properties across those frames.</p><p>We use one fully connected layer with ELU activation function and one output layer (a fully connected layer with a linear activation function) that produces the Q-value estimation for each action.</p><h1 id="Deep-Q-Learning-algorithm"><a href="#Deep-Q-Learning-algorithm" class="headerlink" title="Deep Q-Learning algorithm"></a>Deep Q-Learning algorithm</h1><p>Remember that we update our Q value for a given state and action using the Bellman equation:<img src="/2019/03/10/RL-Deep-QLearning/1_js8r4Aq2ZZoiLK0mMp_ocg.png" alt="_js8r4Aq2ZZoiLK0mMp_oc"></p><p>In our case, we want to update our neural nets weights to reduce the error.</p><p>The error (or TD error) is calculated by taking the difference between our Q_target (maximum possible value from the next state) and Q_value (our current prediction of the Q-value)</p><p><img src="/2019/03/10/RL-Deep-QLearning/1_Zplt-1wTWu_7BGmZCBFjbQ.png" alt="_Zplt-1wTWu_7BGmZCBFjb"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Initialize Doom Environment E</div><div class="line">Initialize replay Memory M <span class="keyword">with</span> capacity N (= finite capacity)</div><div class="line">Initialize the DQN weights w</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> max_episode:</div><div class="line">    s = Environment state</div><div class="line">    <span class="keyword">for</span> steps <span class="keyword">in</span> max_steps:</div><div class="line">         Choose action a <span class="keyword">from</span> state s using epsilon greedy.</div><div class="line">         Take action a, get r (reward) <span class="keyword">and</span> s<span class="string">' (next state)</span></div><div class="line"><span class="string">         Store experience tuple &lt;s, a, r, s'</span>&gt; <span class="keyword">in</span> M</div><div class="line">         s = s<span class="string">' (state = new_state)</span></div><div class="line"><span class="string">         </span></div><div class="line"><span class="string">         Get random minibatch of exp tuples from M</span></div><div class="line"><span class="string">         Set Q_target = reward(s,a) +  γmaxQ(s'</span>)</div><div class="line">         Update w =  α(Q_target - Q_value) *  ∇w Q_value</div></pre></td></tr></table></figure><p>There are two processes that are happening in this algorithm:</p><ul><li>We sample the environment where we perform actions and store the observed experiences tuples in a replay memory.</li><li>Select the small batch of tuple random and learn from it using a gradient descent update step.</li></ul><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><a href="https://gist.github.com/simoninithomas/7611db5d8a6f3edde269e18b97fa4d0c#file-deep-q-learning-with-doom-ipynb" target="_blank" rel="noopener">Implementation</a></h1><p><a href="https://keon.io/deep-q-learning/" target="_blank" rel="noopener">keras</a> <a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" target="_blank" rel="noopener">pytorch</a> </p><p>In this implementation, we are going to train a Deep Q Learning agent on the CartPole-v0 task. In this task, the agent can take two actions - moving the cart left or right, so that the pole attached to it stays upright.</p><p><img src="/2019/03/10/RL-Deep-QLearning/cartpole1.gif" alt="artpole"></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Reinforcement Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>WebSite Making</title>
      <link href="/2019/03/05/WebSite-Making/"/>
      <url>/2019/03/05/WebSite-Making/</url>
      <content type="html"><![CDATA[<p>We are going to write a Django project so that we can learn the basic concepts and operations. Our website is a basic poll application, which consist of two parts:</p><ul><li>A pyblic site that lets people view polls and vote in them</li><li>An admin site that lets you add, change, and delete polls.</li></ul><a id="more"></a><h1 id="Create-a-project"><a href="#Create-a-project" class="headerlink" title="Create a project"></a>Create a project</h1><p>The first thing we need to do is to create a Django project, which is a collection of settings for an instance of Django, including database configuration, Django-specific options and application-specific settings.</p><p>From the command line, <code>cd</code> into the directory where you would like to store your project, then run the following command:</p><p><code>django-admin startproject MyWebsite</code></p><p>This will result in a directory in your current directory and the directory structure is like:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.29.13 PM.png" alt="creen Shot 2019-03-07 at 9.29.13 P"></p><ol><li>The outter <code>MyWebsite/</code> root directory is just a container for our project. It is name doesn’t matter to Django; so you can rename it whenever you want.</li><li>The inner <code>MyWebsite/</code> directory is the actural Python package for our project.<ul><li><code>MyWebsite/.settings.py</code> is for configuration for this Django project.</li><li><code>MyWebsite/urls.py</code> is for URL declarations for Django project.</li><li><code>MyWebsite/wsgi.py</code> is an entery-point for WSGI-compatible web servers to serve the project.</li></ul></li><li><code>manage.py</code> is a command-line utility that lets you interact with the Django project.</li></ol><h1 id="Development-server"><a href="#Development-server" class="headerlink" title="Development server"></a>Development server</h1><p>Once we created the project, we can verify whether it works by running the following command:</p><p><code>python manage.py runserver</code></p><p>The following output will be on the terminal:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Performing system checks...</div><div class="line">System check identified no issues (<span class="number">0</span> silenced).</div><div class="line">You have unapplied migrations; your app may <span class="keyword">not</span> work properly until they are applied.</div><div class="line">Run <span class="string">'python manage.py migrate'</span> to apply them.</div><div class="line">February <span class="number">28</span>, <span class="number">2019</span> - <span class="number">15</span>:<span class="number">50</span>:<span class="number">53</span></div><div class="line">Django version <span class="number">2.1</span>, using settings <span class="string">'mysite.settings'</span></div><div class="line">Starting development server at http://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">8000</span>/</div><div class="line">Quit the server <span class="keyword">with</span> CONTROL-C.</div></pre></td></tr></table></figure><p>Then we can visit <code>http://127.0.0.1:8000/</code> with browser.</p><p>By default, the runserver command starts the development server on the internal IP at port 8000. If you want to use another port, you can pass it as a command-line argument.</p><p><code>python manage.py runserver 8080</code></p><p>If you want to change the server’s IP, pass it along with the prot.</p><p><code>python manage.py runserver 0:8080</code></p><h1 id="Create-the-Polls-app"><a href="#Create-the-Polls-app" class="headerlink" title="Create the Polls app"></a>Create the Polls app</h1><p>Now that we can create the apps under this project. To be clear, <strong>an app is a Web application</strong> that does something – e.g., a Weblog system, a database of public records or a simple poll app. <strong>A project is a collection of configuration and apps</strong> for a particular website. A project can contain multiple apps. An app can be in multiple projects.</p><p>To create our app, make sure we are in the same directory as <code>manage.py</code> and type the command:</p><p><code>ppython manage.py startapp polls</code></p><p>we will get a directory like this:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-12 at 1.41.00 PM.png" alt="creen Shot 2019-03-12 at 1.41.00 P"></p><h2 id="Write-a-view"><a href="#Write-a-view" class="headerlink" title="Write a view"></a>Write a view</h2><p>Open <code>polls/views.py</code> and put the following code in it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.http <span class="keyword">import</span> HttpResponse</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">(request)</span>:</span></div><div class="line">    <span class="keyword">return</span> HttpResponse(<span class="string">"Hello World!"</span>)</div></pre></td></tr></table></figure><p>To call the view, we need to map it to a URL. Therefore we need to create a <code>urls.py</code> and write the codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.urls <span class="keyword">import</span> path</div><div class="line"><span class="keyword">from</span> . <span class="keyword">import</span> views</div><div class="line">urlpatterns = [</div><div class="line">    path(<span class="string">''</span>,views.index,name=<span class="string">'index'</span>)</div><div class="line">]</div></pre></td></tr></table></figure><p>The next step is to point the root URLconf at the <code>polls.urls</code> module. Open <code>MyWebsite/urls.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.contrib <span class="keyword">import</span> admin</div><div class="line"><span class="keyword">from</span> django.urls <span class="keyword">import</span> include, path</div><div class="line">urlpatterns = [</div><div class="line">    path(<span class="string">'polls/'</span>, include(<span class="string">'polls.urls'</span>)),</div><div class="line">    path(<span class="string">'admin/'</span>, admin.site.urls),</div><div class="line">]</div></pre></td></tr></table></figure><p>Here, the <code>include()</code> function allows referencing other URLconfs. We should always use <code>include()</code> when you include other URL patterns. <code>admin.site.urls</code> is the only exception to this.</p><p>Finally, run the command <code>python manage.py runserver</code> and go to <code>http://127.0.0.1:8000/polls/</code>, we should see the text <code>Hello World!</code>, which is defined in the index view.</p><p>The <code>path()</code> function takes four arguments, two required: <code>route</code> and <code>view</code>, and two optional:<code>kwargs</code> and <code>name.</code></p><ol><li><p><a href="https://docs.djangoproject.com/en/2.1/ref/urls/#django.urls.path" target="_blank" rel="noopener"><code>path()</code></a> argument: <code>route</code></p><p><code>route</code> is a string that contains a URL pattern. When processing a request, Django starts at the first pattern in <code>urlpatterns</code> and makes its way down the list, comparing the requested URL against each pattern until it finds one that matches.</p><p>Patterns don’t search GET and POST parameters, or the domain name. For example, in a request to <code>https://www.example.com/myapp/</code>, the URLconf will look for<code>myapp/</code>. In a request to <code>https://www.example.com/myapp/?page=3</code>, the URLconf will also look for <code>myapp/</code>.</p></li><li><p><a href="https://docs.djangoproject.com/en/2.1/ref/urls/#django.urls.path" target="_blank" rel="noopener"><code>path()</code></a> argument: <code>view</code></p><p>When Django finds a matching pattern, it calls the specified view function with an <a href="https://docs.djangoproject.com/en/2.1/ref/request-response/#django.http.HttpRequest" target="_blank" rel="noopener"><code>HttpRequest</code></a> object as the first argument and any “captured” values from the route as keyword arguments. We’ll give an example of this in a bit.</p></li><li><p><a href="https://docs.djangoproject.com/en/2.1/ref/urls/#django.urls.path" target="_blank" rel="noopener"><code>path()</code></a> argument: <code>kwargs</code></p><p>Arbitrary keyword arguments can be passed in a dictionary to the target view. We aren’t going to use this feature of Django in the tutorial.</p></li><li><p><a href="https://docs.djangoproject.com/en/2.1/ref/urls/#django.urls.path" target="_blank" rel="noopener"><code>path()</code></a> argument: <code>name</code></p><p>Naming your URL lets you refer to it unambiguously from elsewhere in Django, especially from within templates. This powerful feature allows you to make global changes to the URL patterns of your project while only touching a single file.</p></li></ol><h1 id="Database-setup"><a href="#Database-setup" class="headerlink" title="Database setup"></a><a href="https://docs.djangoproject.com/en/2.1/intro/tutorial02/" target="_blank" rel="noopener">Database setup</a></h1><p>By default, the configuration uses SQLite, which is included in Python and thus we won’t need to install anything else to support your database. Now, open <code>Mywebsite/settings.py</code>.We can see that in <code>INSTALLED_APPS</code>, there are several predefined applications as a convenience of common use.</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-14 at 12.26.11 PM.png" alt="creen Shot 2019-03-14 at 12.26.11 P"></p><p>Note that these applications use at least one database table. Then we need to create tables for them before we use them by running the following command:</p><p><code>python manage.py migrate</code></p><p>The <a href="https://docs.djangoproject.com/en/2.1/ref/django-admin/#django-admin-migrate" target="_blank" rel="noopener"><code>migrate</code></a> command looks at the <a href="https://docs.djangoproject.com/en/2.1/ref/settings/#std:setting-INSTALLED_APPS" target="_blank" rel="noopener"><code>INSTALLED_APPS</code></a> setting and creates any necessary database tables according to the database settings in your <code>mysite/settings.py</code> file .</p><h2 id="Create-models"><a href="#Create-models" class="headerlink" title="Create models"></a>Create models</h2><p>A model is the single, definitive source of truth about your data. It contains the essential fields and behaviors of the data you’re storing.</p><p>In our poll application, we are going to create two tables: <strong>Question</strong> and <strong>Choice</strong>. Question model has two fields: question_text and pub_date. Choice has two fields too: choice_text and votes tally. And each Choice is associated with a Question. Open <code>polls/models.py</code> and put the following codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> models</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Question</span><span class="params">(models.Model)</span>:</span></div><div class="line">    question_text = models.CharField(max_length=<span class="number">200</span>)</div><div class="line">    pub_date = models.DateTimeField(<span class="string">'date published'</span>)</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Choice</span><span class="params">(models.Model)</span>:</span></div><div class="line">    question = models.ForeignKey(Question,on_delete=models.CASCADE)</div><div class="line">    choice_text = models.CharField(max_length=<span class="number">200</span>)</div><div class="line">    votes = models.IntegerField(default=<span class="number">0</span>)</div></pre></td></tr></table></figure><p>The code is straightforward. Each model is represented by a class that subclasses <a href="https://docs.djangoproject.com/en/2.1/ref/models/instances/#django.db.models.Model" target="_blank" rel="noopener"><code>django.db.models.Model</code></a>. Each model has a number of class variables, each of which represents a database field in the model.</p><p>Each field is represented by an instance of a <a href="https://docs.djangoproject.com/en/2.1/ref/models/fields/#django.db.models.Field" target="_blank" rel="noopener"><code>Field</code></a> class – e.g., <a href="https://docs.djangoproject.com/en/2.1/ref/models/fields/#django.db.models.CharField" target="_blank" rel="noopener"><code>CharField</code></a> for character fields and <a href="https://docs.djangoproject.com/en/2.1/ref/models/fields/#django.db.models.DateTimeField" target="_blank" rel="noopener"><code>DateTimeField</code></a> for datetimes. This tells Django what type of data each field holds.</p><p>The name of each <a href="https://docs.djangoproject.com/en/2.1/ref/models/fields/#django.db.models.Field" target="_blank" rel="noopener"><code>Field</code></a> instance (e.g. <code>question_text</code> or <code>pub_date</code>) is the field’s name, in machine-friendly format. You’ll use this value in your Python code, and your database will use it as the column name.</p><p>Finally, note a relationship is defined, using <a href="https://docs.djangoproject.com/en/2.1/ref/models/fields/#django.db.models.ForeignKey" target="_blank" rel="noopener"><code>ForeignKey</code></a>. That tells Django each <code>Choice</code> is related to a single <code>Question</code>. Django supports all the common database relationships: many-to-one, many-to-many, and one-to-one.</p><h2 id="Activate-models"><a href="#Activate-models" class="headerlink" title="Activate models"></a>Activate models</h2><p>With the above codes, we can create a database schema (CREATE TABLE statements) for this app and create a Python database-access API for accessing Question and Choice objects. But first we need to tell our project that the <code>polls</code> app is installed. Therefore, we need to add models to <code>INSTALLED_APPS</code> setting. The <code>PollsConfig</code> class is in the <code>polls/apps.py</code> file, so its dotted path is <code>&#39;polls.apps.PollsConfig&#39;</code>. Edit the <code>mysite/settings.py</code> file and add that dotted path to the <a href="https://docs.djangoproject.com/en/2.1/ref/settings/#std:setting-INSTALLED_APPS" target="_blank" rel="noopener"><code>INSTALLED_APPS</code></a> setting. It’ll look like this:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-14 at 12.47.20 PM.png" alt="creen Shot 2019-03-14 at 12.47.20 P"></p><p>Now Django knows to include the <code>polls</code> app. Let’s run another command:</p><p><code>python manage.py makemigrations polls</code></p><p>By running <code>makemigrations</code>, you’re telling Django that you’ve made some changes to your models (in this case, you’ve made new ones) and that you’d like the changes to be stored as a <em>migration</em>.</p><p>Now we can run <code>migrate</code> again to create those model table:</p><p><code>python manage.py migrate</code></p><p>The <a href="https://docs.djangoproject.com/en/2.1/ref/django-admin/#django-admin-migrate" target="_blank" rel="noopener"><code>migrate</code></a> command takes all the migrations that haven’t been applied (Django tracks which ones are applied using a special table in your database called <code>django_migrations</code>) and runs them against your database - essentially, synchronizing the changes you made to your models with the schema in the database.</p><p>In summary, there three steps to make model changes:</p><ul><li>Change your model (in models.py)</li><li>Run <a href="https://docs.djangoproject.com/en/2.1/ref/django-admin/#django-admin-makemigrations" target="_blank" rel="noopener"><code>python manage.py makemigrations</code></a> to create migrations for those changes</li><li>Run <a href="https://docs.djangoproject.com/en/2.1/ref/django-admin/#django-admin-migrate" target="_blank" rel="noopener"><code>python manage.py migrate</code></a> to apply those changes to the database.</li></ul><h2 id="Playing-with-the-API"><a href="#Playing-with-the-API" class="headerlink" title="Playing with the API"></a><a href="https://docs.djangoproject.com/en/2.1/intro/tutorial02/" target="_blank" rel="noopener">Playing with the API</a></h2><p>Now, let’s hop into the interactive Python shell and play around with the free API Django gives you. To invoke the Python shell, use this command:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py shell</div></pre></td></tr></table></figure><p>Once we are in the shell, explore the database API.</p><h1 id="Django-Admin"><a href="#Django-Admin" class="headerlink" title="Django Admin"></a>Django Admin</h1><h2 id="Create-an-admin-user"><a href="#Create-an-admin-user" class="headerlink" title="Create an admin user"></a>Create an admin user</h2><p>Firstly, we need to create a user who can login to the admin site. In order to create such user, we can use the command like this:</p><p><code>python manage.py createsuperuser</code></p><p>Then enter username, email address and password:</p><p><code>Username:admin</code></p><p><code>Email address: admin@example.com</code></p><p><code>Password:admin123</code></p><p>The Django admin site is activated by default. Let’s start server and explore it:</p><p><code>python manage.py runserver</code></p><p>Now, open a web browser and go to “/admin/“ on your local domain, e.g., <code>http://127.0.0.1:8000/admin/</code></p><p><img src="/2019/03/05/WebSite-Making/admin01.png" alt="dmin0"></p><p>Once you login to the website using username and password, we can see:</p><p><img src="/2019/03/05/WebSite-Making/admin02.png" alt="dmin0">We should see a few types of editable content: groups and users. They are provided by <a href="https://docs.djangoproject.com/en/2.1/topics/auth/#module-django.contrib.auth" target="_blank" rel="noopener"><code>django.contrib.auth</code></a>, the authentication framework shipped by Django. </p><h2 id="Make-the-poll-app-modifiable-in-the-admin"><a href="#Make-the-poll-app-modifiable-in-the-admin" class="headerlink" title="Make the poll app modifiable in the admin"></a>Make the poll app modifiable in the admin</h2><p>But where’s our poll app? It’s not displayed on the admin index page.</p><p>Just one thing to do: we need to tell the admin that <code>Question</code> objects have an admin interface. To do this, open the <code>polls/admin.py</code> file, and edit it to look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.contrib <span class="keyword">import</span> admin</div><div class="line"><span class="keyword">from</span> .models <span class="keyword">import</span> Question</div><div class="line">admin.site.register(Question)</div></pre></td></tr></table></figure><p>Now that we’ve registered <code>Question</code>, Django knows that it should be displayed on the admin index page:</p><p><img src="/2019/03/05/WebSite-Making/admin03t.png" alt="dmin03"></p><h1 id="Day1-Basic-Commands"><a href="#Day1-Basic-Commands" class="headerlink" title="Day1 Basic Commands"></a>Day1 Basic Commands</h1><ol><li><p><strong>Create a new Django project</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">django-admin.py startproject project_name</div></pre></td></tr></table></figure></li><li><p><strong>Create a new app</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py startapp app_name</div></pre></td></tr></table></figure><p>Open the Django project directory, then execute the command. Usually a Django project can have multiple app.</p></li><li><p><strong>Run the app</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">python manage.py runserver</div><div class="line"><span class="comment"># use specified port</span></div><div class="line">python manage.py runserver <span class="number">8001</span></div><div class="line">python manage.py runserver <span class="number">9999</span></div><div class="line"><span class="comment"># all available ip</span></div><div class="line">python manage.py runserver <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8000</span></div></pre></td></tr></table></figure></li><li><p><strong>Create a database table</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">python manage.py makemigrations</div><div class="line">python manage.py migrate</div></pre></td></tr></table></figure></li><li><p><strong>Clear the database</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py flush</div></pre></td></tr></table></figure></li><li><p><strong>Import/Export Data</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">python manage.py dumpdata appname &gt; appname.json</div><div class="line">python manage.py loaddata appname.json</div></pre></td></tr></table></figure></li><li><p>Create supermanager</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py createsuperuser</div></pre></td></tr></table></figure></li></ol><h1 id="Day2-View-and-URL"><a href="#Day2-View-and-URL" class="headerlink" title="Day2 View and URL"></a>Day2 View and URL</h1><p>Here we are going to write a “Hello World!” application to illustrate the relationship between views and urls.</p><p>Firstly, use the command to create a project. Of course we can also pycharm to create a project named “MyWebsite”. We can see the project file structures:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.29.13 PM.png" alt="creen Shot 2019-03-07 at 9.29.13 P"></p><p>Then inside this project, we are going to create an app named “FirstDemo”:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py startapp FirstDemo</div></pre></td></tr></table></figure><p>Then, we have the new structures:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.31.49 PM.png" alt="creen Shot 2019-03-07 at 9.31.49 P"></p><p>we can find that there is one added directory “FirstDemo”. After creating a new app, we are going to registering this app to the whole project by add this new app to <code>settings.py</code>:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.34.23 PM.png" alt="creen Shot 2019-03-07 at 9.34.23 P"></p><p>After this, we are going to define a view function. A view function specify what we are going to see when we open the website. Go to “FirstDemo” and open “views.py”:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.37.38 PM.png" alt="creen Shot 2019-03-07 at 9.37.38 P"></p><p>Here, we define a “HelloWorld” function, whose parameter is <code>request</code> and the first parameter can only be <code>request</code>. A <code>request</code> variable includes <code>get</code> and <code>post</code> infomation. This function use <code>HttpResponse</code> to return information to the webpage, like print in python.</p><p>After we define the function of our webpage, but how can be visit it? We are going to define the url that we need to visit the webpage. Open the <code>MyWebsite/MyWebsite/urls.py</code>，we are going to import the view function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> FirstDemo <span class="keyword">import</span> views <span class="keyword">as</span> fd_views</div></pre></td></tr></table></figure><p>then we define the url:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">path(<span class="string">''</span>,fd_views.HelloWorld),</div></pre></td></tr></table></figure><p>The overall modification is like:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.46.20 PM-2016796.png" alt="creen Shot 2019-03-07 at 9.46.20 PM-201679"></p><p>Finally we can run our project:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.47.39 PM.png" alt="creen Shot 2019-03-07 at 9.47.39 P"></p><p>Open the link, we can see:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-07 at 9.48.17 PM.png" alt="creen Shot 2019-03-07 at 9.48.17 P"></p><h1 id="Day3-Advanced-Views-and-Urls"><a href="#Day3-Advanced-Views-and-Urls" class="headerlink" title="Day3 Advanced Views and Urls"></a>Day3 Advanced Views and Urls</h1><p>Last time, we have learned how to display information in django. This time, we are about to implement a little complex function, i.e., addition, and display the addition result.</p><h2 id="add-a-4-amp-b-5-to-visit-webpage"><a href="#add-a-4-amp-b-5-to-visit-webpage" class="headerlink" title="/add/?a=4&amp;b=5 to visit webpage"></a>/add/?a=4&amp;b=5 to visit webpage</h2><p>Firstly, let’s create a new app named <code>Addition</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py startapp Addition</div></pre></td></tr></table></figure><p>We need to define the addition function in <code>views.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> django.shortcuts <span class="keyword">import</span> render</div><div class="line"><span class="keyword">from</span> django.http <span class="keyword">import</span> HttpResponse</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(request)</span>:</span></div><div class="line">    a = request.GET[<span class="string">'a'</span>]</div><div class="line">    b = request.GET[<span class="string">'b'</span>]</div><div class="line">    c = int(a)+int(b)</div><div class="line">    <span class="keyword">return</span> HttpResponse(str(c))</div></pre></td></tr></table></figure><p>Here, <code>request.GET</code> return a dictionary, <code>request.GET[&#39;a&#39;]</code> return <code>a</code> value. A better way is <code>request.GET.get(&#39;a&#39;,0)</code> where a default value 0 is return if <code>a</code> is not defined.</p><p>Then we need to modify <code>urls.py</code>:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-10 at 5.17.57 PM.png" alt="creen Shot 2019-03-10 at 5.17.57 P"></p><p>Finally, we can run our program:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python manage.py runserver <span class="number">8110</span></div></pre></td></tr></table></figure><p>Here, we set the port 8110. And we can use the following url to visit the webpage: </p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http:<span class="comment">//127.0.0.1:8110/add/</span></div></pre></td></tr></table></figure><p>But we found that:<img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-10 at 5.21.18 PM.png" alt="creen Shot 2019-03-10 at 5.21.18 P"></p><p>That is because we didn’t give initial value to <code>a</code> and <code>b</code>.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http:<span class="comment">//127.0.0.1:8110/add/?a=4&amp;b=5</span></div></pre></td></tr></table></figure><p>Using the above url, we can see 9 displayed. </p><h2 id="add-3-4-to-visit-page"><a href="#add-3-4-to-visit-page" class="headerlink" title="/add/3/4 to visit page"></a>/add/3/4 to visit page</h2><p>Here we define another add function in <code>views.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add2</span><span class="params">(request,a,b)</span>:</span></div><div class="line">    c = int(a)+int(b)</div><div class="line">    <span class="keyword">return</span> HttpResponse(str(c))</div></pre></td></tr></table></figure><p>Then, in <code>urls.py</code>:</p><p><img src="/2019/03/05/WebSite-Making/Screen Shot 2019-03-10 at 5.27.19 PM.png" alt="creen Shot 2019-03-10 at 5.27.19 P"></p><p>Finally we run the program and visit the webpage:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">http:<span class="comment">//127.0.0.1:8110/add/3/4/</span></div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://towardsdatascience.com/deploying-a-keras-deep-learning-model-as-a-web-application-in-p-fc0f2354a7ff" target="_blank" rel="noopener">Deploying a Keras Deep Learning Model as a Web Application in Python</a></p><p><a href="https://summerofhpc.prace-ri.eu/bringing-visualisation-to-the-web-with-python-and-bokeh/" target="_blank" rel="noopener">Bringing visualisation to the web with Python and Bokeh</a> </p><p><a href="https://docs.djangoproject.com/en/2.1/intro/tutorial01/" target="_blank" rel="noopener">Django tutorials</a></p>]]></content>
      
      <categories>
          
          <category> Website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Website </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Data Preprocessing</title>
      <link href="/2019/03/04/DP-Data-Preprocessing/"/>
      <url>/2019/03/04/DP-Data-Preprocessing/</url>
      <content type="html"><![CDATA[<p>Data preprocessing including images, texts.</p><a id="more"></a><h1 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h1><h2 id="Tutorial-Datasets"><a href="#Tutorial-Datasets" class="headerlink" title="Tutorial Datasets"></a>Tutorial Datasets</h2><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data/'</span>,one_hot=<span class="keyword">True</span>)</div><div class="line"><span class="comment">#########</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">mnist = tf.keras.datasets.mnist(x_train, y_train),(x_test, y_test) = mnist.load_data()</div><div class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></div></pre></td></tr></table></figure><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> cifar10</div><div class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</div></pre></td></tr></table></figure><ul><li><code>x_train</code> has the shape <code>(num_samples,32,32,3)</code>, <code>y_train</code> has the shape <code>(num_samples,)</code></li></ul><p>There are two choices normalizing between [-1,1] or using (x-mean)/std. We prefer the former when we know different features do not relate to each other. <a href="https://medium.com/@kushajreal/training-alexnet-with-tips-and-checks-on-how-to-train-cnns-practical-cnns-in-pytorch-1-61daa679c74a" target="_blank" rel="noopener">ref</a> </p><p>To accomplish this, we’ll first implement a dedicated Python class to align faces using an affine transformation, where affine transformations are used for rotating, scaling, translating, etc. We’ll then create an example driver Python script to accept an input image, detect faces, and align them.</p><h2 id="Face-Alignment"><a href="#Face-Alignment" class="headerlink" title="Face Alignment"></a>Face Alignment</h2><p>Face images need alignment preprocessing such that all faces:</p><ol><li>Be centered in the image.</li><li>Be rotated that such the eyes lie on a horizontal line (i.e., the face is rotated such that the eyes lie along the same <em>y</em>-coordinates).</li><li>Be scaled such that the size of the faces are approximately identical.</li></ol><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://towardsdatascience.com/building-blocks-text-pre-processing-641cae8ba3bf" target="_blank" rel="noopener">Building Blocks: Text Pre-Processing</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Data Preprocessing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RL-Reinforcement Learning</title>
      <link href="/2019/03/03/RL-Reinforcement-Learning/"/>
      <url>/2019/03/03/RL-Reinforcement-Learning/</url>
      <content type="html"><![CDATA[<p>Reinforcement learning is useful when you have no training data or specific enough expertise about the problem. On a high level, you know WHAT you want, but not really HOW to get there. Luckily, all you need is a reward mechanism, and the reinforcement learning model will figure out how to maximize the reward, if you just let it “play” long enough. This is analogous to teaching a dog to sit down using treats. At first the dog is clueless and tries random things on your command. At some point, it accidentally lands on its butt and gets a sudden reward. As time goes by, and given enough iterations, it’ll figure out the expert strategy of sitting down on cue.</p><a id="more"></a><h1 id="Introduction-link"><a href="#Introduction-link" class="headerlink" title="Introduction link"></a>Introduction <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419" target="_blank" rel="noopener">link</a></h1><p>The idea behind Reinforcement Learning is that an agent will learn from the environment by interacting with it and receiving rewards for performing actions. </p><h2 id="The-Reinforcement-Learning-Process"><a href="#The-Reinforcement-Learning-Process" class="headerlink" title="The Reinforcement Learning Process"></a>The Reinforcement Learning Process</h2><p>Let’s imagine an agent learning to play Super Mario Bros as a working example. The Reinforcement Learning (RL) process can be modeled as a loop that works like this:</p><ul><li>Our Agent receives <strong>state S0</strong> from the <strong>Environment</strong> (In our case we receive the first frame of our game (state) from Super Mario Bros (environment))</li><li>Based on that <strong>state S0,</strong> agent takes an <strong>action A0</strong> (our agent will move right)</li><li>Environment transitions to a <strong>new</strong> <strong>state S1</strong> (new frame)</li><li>Environment gives some <strong>reward R1</strong> to the agent (not dead: +1)</li></ul><p>This RL loop outputs a sequence of <strong>state, action and reward.</strong></p><p>The goal of the agent is to maximize the expected cumulative reward.</p><h2 id="The-central-idea-of-the-Reward-Hypothesis"><a href="#The-central-idea-of-the-Reward-Hypothesis" class="headerlink" title="The central idea of the Reward Hypothesis"></a>The central idea of the Reward Hypothesis</h2><p>Why is the goal of the agent to maximize the expected cumulative reward?</p><p>Well, Reinforcement Learning is based on the idea of the reward hypothesis. All goals can be described by the maximization of the expected cumulative reward.</p><p><strong>That’s why in Reinforcement Learning, to have the best behavior, we need to maximize the expected cumulative reward.</strong></p><p>The cumulative reward at each time step t can be written as:</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/0_ylz4lplMffGQR_g3.gif" alt="_ylz4lplMffGQR_g"></p><p>which is equivalent to:</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_AFAuM1Y8zmso4yB5mOApZA.png" alt="_AFAuM1Y8zmso4yB5mOApZ"></p><p>However, in reality, we can’t just add the rewards like that. The rewards that come sooner (in the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_tciNrjN6pW60-h0PiQRiXg.png" alt="_tciNrjN6pW60-h0PiQRiX"></p><p>Let say your agent is this small mouse and your opponent is the cat. Your goal is to eat the maximum amount of cheese before being eaten by the cat.</p><p>As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).</p><p>As a consequence, the reward near the cat, even if it is bigger (more cheese), will be discounted. We’re not really sure we’ll be able to eat it.</p><p>To discount the rewards, we proceed like this:</p><p>We define a discount rate called gamma. It must be between 0 and 1.</p><ul><li>The larger the gamma, the smaller the discount. This means the learning agent cares more about the long term reward.</li><li>On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).</li></ul><p>Our discounted cumulative expected rewards is:</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_zrzRTXt8rtWF5fX__kZ-yQ.png" alt="_zrzRTXt8rtWF5fX__kZ-y"></p><p>To be simple, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less probable to happen.</p><h2 id="Episodic-or-Continuing-tasks"><a href="#Episodic-or-Continuing-tasks" class="headerlink" title="Episodic or Continuing tasks"></a>Episodic or Continuing tasks</h2><p>A task is an instance of a Reinforcement Learning problem. We can have two types of tasks: episodic and continuous.</p><h3 id="Episodic-task"><a href="#Episodic-task" class="headerlink" title="Episodic task"></a>Episodic task</h3><p>In this case, we have a starting point and an ending point <strong>(a terminal state). This creates an episode</strong>: a list of States, Actions, Rewards, and New States.</p><p>For instance think about Super Mario Bros, an episode begin at the launch of a new Mario and ending: when you’re killed or you’re reach the end of the level.</p><h3 id="Continuing-task"><a href="#Continuing-task" class="headerlink" title="Continuing task"></a>Continuing task</h3><p><strong>These are tasks that continue forever (no terminal state).</strong> In this case, the agent has to learn how to choose the best actions and simultaneously interacts with the environment.</p><p>For instance, an agent that do automated stock trading. For this task, there is no starting point and terminal state. <strong>The agent keeps running until we decide to stop him.</strong></p><h2 id="Monte-Carlo-vs-TD-Learning-methods"><a href="#Monte-Carlo-vs-TD-Learning-methods" class="headerlink" title="Monte Carlo vs TD Learning methods"></a>Monte Carlo vs TD Learning methods</h2><p>We have two ways of learning:</p><ul><li>Collecting the rewards <strong>at the end of the episode</strong> and then calculating the <strong>maximum expected future reward</strong>: <em>Monte Carlo Approach</em></li><li>Estimate <strong>the rewards at each step</strong>: <em>Temporal Difference Learning</em></li></ul><h3 id="Monte-Carlo"><a href="#Monte-Carlo" class="headerlink" title="Monte Carlo"></a>Monte Carlo</h3><p>When the episode ends (the agent reaches a “terminal state”), <strong>the agent looks at the total cumulative reward to see how well it did.</strong> In Monte Carlo approach, rewards are only <strong>received at the end of the game.</strong></p><p>Then, we start a new game with the added knowledge. <strong>The agent makes better decisions with each iteration.</strong></p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_RLLzQl4YadpbhPlxpa5f6A.png" alt="_RLLzQl4YadpbhPlxpa5f6"></p><p>Let’s take an example:</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_tciNrjN6pW60-h0PiQRiXg-2067468.png" alt="_tciNrjN6pW60-h0PiQRiXg-206746"></p><p>If we take the maze environment:</p><ul><li>We always start at the same starting point.</li><li>We terminate the episode if the cat eats us or if we move &gt; 20 steps.</li><li>At the end of the episode, we have a list of State, Actions, Rewards, and New States.</li><li>The agent will sum the total rewards Gt (to see how well it did).</li><li>It will then update V(st) based on the formula above.</li><li>Then start a new game with this new knowledge.</li></ul><p>By running more and more episodes, <strong>the agent will learn to play better and better.</strong></p><h3 id="Temporal-Difference-Learning-learning-at-each-time-step"><a href="#Temporal-Difference-Learning-learning-at-each-time-step" class="headerlink" title="Temporal Difference Learning : learning at each time step"></a>Temporal Difference Learning : learning at each time step</h3><p>TD Learning, on the other hand, will not wait until the end of the episode to update <strong>the maximum expected future reward estimation: it will update its value estimation V for the non-terminal states St occurring at that experience.</strong></p><p>This method is called TD(0) or <strong>one step TD (update the value function after any individual step).</strong></p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_LLfj11fivpkKZkwQ8uPi3A.png" alt="_LLfj11fivpkKZkwQ8uPi3"></p><p>TD methods <strong>only wait until the next time step to update the value estimates.</strong> At time t+1 they immediately <strong>form a TD target using the observed reward Rt+1 and the current estimate V(St+1).</strong></p><p>TD target is an estimation: in fact you update the previous estimate V(St) <strong>by updating it towards a one-step target.</strong></p><h3 id="Exploration-Exploitation-trade-off"><a href="#Exploration-Exploitation-trade-off" class="headerlink" title="Exploration/Exploitation trade-off"></a>Exploration/Exploitation trade-off</h3><p>Before looking at the different strategies to solve Reinforcement Learning problems, we must cover one more very important topic: the exploration/exploitation trade-off.</p><ul><li>Exploration is finding more information about the environment.</li><li>Exploitation is exploiting known information to maximize the reward.</li></ul><p>Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, we can fall into a common trap.</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_APLmZ8CVgu0oY3sQBVYIuw.png" alt="_APLmZ8CVgu0oY3sQBVYIu"></p><p>In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze there is a gigantic sum of cheese (+1000).</p><p>However, if we only focus on reward, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).</p><p>But if our agent does a little bit of exploration, it can find the big reward.</p><p>This is what we call the exploration/exploitation trade off. We must define a rule that helps to handle this trade-off. We’ll see in future articles different ways to handle it.</p><h2 id="Three-approaches-to-RL"><a href="#Three-approaches-to-RL" class="headerlink" title="Three approaches to RL"></a>Three approaches to RL</h2><p>Now that we defined the main elements of Reinforcement Learning, let’s move on to the three approaches to solve a Reinforcement Learning problem. These are value-based, policy-based, and model-based.</p><h3 id="Value-Based"><a href="#Value-Based" class="headerlink" title="Value Based"></a>Value Based</h3><p>In value-based RL, the goal is to optimize the value function <em>V(s)</em>.</p><p>The value function is a function that tells us the maximum expected future reward the agent will get at each state.</p><p><strong>The value of each state is the total amount of the reward an agent can expect to accumulate over the future, starting at that state.</strong></p><p><img src="/2019/03/03/RL-Reinforcement-Learning/0_kvtRAhBZO-h77Iw1.png" alt="_kvtRAhBZO-h77Iw"></p><p>The agent will use this value function to select which state to choose at each step. The agent takes the state with the biggest value.<img src="/2019/03/03/RL-Reinforcement-Learning/1_2_JRk-4O523bcOcSy1u31g.png" alt="_2_JRk-4O523bcOcSy1u31"></p><p>In the maze example, at each step we will take the biggest value: -7, then -6, then -5 (and so on) to attain the goal.</p><h3 id="Policy-Based"><a href="#Policy-Based" class="headerlink" title="Policy Based"></a>Policy Based</h3><p>In the maze example, at each step we will take the biggest value: -7, then -6, then -5 (and so on) to attain the goal.</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/0_8B4cAhvM-K4y9a5U.png" alt="_8B4cAhvM-K4y9a5"></p><p>We learn a policy function. This lets us map each state to the best corresponding action.</p><p>We have two types of policy:</p><ul><li>Deterministic: a policy at a given state will always return the same action.</li><li>Stochastic: output a distribution probability over actions.</li></ul><p><img src="/2019/03/03/RL-Reinforcement-Learning/0_DNiQGeUl1FKunRbb.png" alt="_DNiQGeUl1FKunRb"></p><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_fii7Z01laRGateAJDvloAQ.png" alt="_fii7Z01laRGateAJDvloA"></p><p>We learn a policy function. This lets us map each state to the best corresponding action.</p><p>We have two types of policy:</p><ul><li>Deterministic: a policy at a given state will always return the same action.</li><li>Stochastic: output a distribution probability over actions<strong>.</strong></li></ul><h3 id="Model-Based"><a href="#Model-Based" class="headerlink" title="Model Based"></a>Model Based</h3><p>In model-based RL, we model the environment. This means we create a model of the behavior of the environment. The problem is each environment will need a different model representation. </p><h1 id="The-Q-learning-algorithm"><a href="#The-Q-learning-algorithm" class="headerlink" title="The Q-learning algorithm"></a>The Q-learning algorithm</h1><p><img src="/2019/03/03/RL-Reinforcement-Learning/1_r-F8AfutP0a8gPWs_5BBLQ.png" alt="_r-F8AfutP0a8gPWs_5BBL"></p><blockquote><p>$\lambda$ - determines how much importance we want to give to future rewards. A high value for the discount factor (close to <strong>1</strong>) captures the long-term effective award, whereas, a discount factor of <strong>0</strong> makes our agent consider only immediate reward, hence making it greedy.</p></blockquote><p>It looks a bit intimidating, but what it does is quite simple. We can summarize it as:</p><p><strong>Update the value estimation of an action based on the reward we got and the reward we expect next.</strong></p><p>This is the fundamental thing we are doing. The <strong>learning rate</strong> and <strong>discount</strong>, while required, are just there to tweak the behavior. The discount will define how much we weigh future expected action values over the one we just experienced. The learning rate is sort of an overall gas pedal. Go too fast and you’ll drive past the optimal, go too slow and you’ll never get there.</p><blockquote><p>Why do we need to gamble and take random actions? For the same reason that the accountant got stuck. Since our default strategy is still greedy, that is we take the most lucrative option by default, we need to introduce some stochasticity to ensure all possible \<state, action="">pairs are explored.</state,></p></blockquote><h1 id="Example-Design-Self-Driving-Cab"><a href="#Example-Design-Self-Driving-Cab" class="headerlink" title="Example Design: Self-Driving Cab"></a>Example Design: Self-Driving Cab</h1><h2 id="Problem-illustration"><a href="#Problem-illustration" class="headerlink" title="Problem illustration"></a>Problem illustration</h2><p>In this problem, we try to solve solve a problem with Q-Learning in python with OpenAI Gym. Let’s design a simulation of a self-driving cab. The major goal is to demonstrate, in a simplified environment, how you can use RL techniques to develop an efficient and safe approach for tackling this problem. The Smartcab’s job is to pick up the passenger at one location and drop them off in another. Here are a few things that we’d love our Smartcab to take care of:</p><ul><li>Drop off the passenger to the right location.</li><li>Save passenger’s time by taking minimum time possible to drop off</li><li>Take care of passenger’s safety and traffic rules</li></ul><p>There are different aspects that need to be considered here while modeling an RL solution to this problem: rewards, states, and actions.</p><h3 id="Rewards"><a href="#Rewards" class="headerlink" title="Rewards"></a>Rewards</h3><ul><li>The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired</li><li>The agent should be penalized if it tries to drop off a passenger in wrong locations</li><li>The agent should get a slight negative reward for not making it to the destination after every time-step. “Slight” negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible</li></ul><h3 id="State-Space"><a href="#State-Space" class="headerlink" title="State Space"></a>State Space</h3><p>In Reinforcement Learning, the agent encounters a state, and then takes action according to the state it’s in.</p><p>The <strong>State Space</strong> is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.</p><p>Let’s say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):</p><p><img src="/2019/03/03/RL-Reinforcement-Learning/Reinforcement_Learning_Taxi_Env.width-1200.png" alt="einforcement_Learning_Taxi_Env.width-120"></p><p>Let’s assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).</p><p>You’ll also notice there are four locations that we can pick up and drop off a passenger: R, G, Y, B or <code>[(0,0), (0,4), (4,0), (4,3)]</code> in (row, col) coordinates. Our illustrated passenger is in location <strong>Y</strong> and they wish to go to location <strong>R</strong>.</p><p>When we also account for one additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there’s four destinations and five passenger locations.</p><p>So, our taxi environment has $5\times 5 \times 5 \times 4=500$ total possible states.</p><p>The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.</p><p>In other words, we have six possible actions:</p><ol><li><code>south</code></li><li><code>north</code></li><li><code>east</code></li><li><code>west</code></li><li><code>pickup</code></li><li><code>dropoff</code></li></ol><p>This is the <strong>action space</strong>: the set of all the actions that our agent can take in a given state.</p><p>You’ll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment’s code, we will simply provide a -1 penalty for every wall hit and the taxi won’t move anywhere. This will just rack up penalties causing the taxi to consider going around the wall.</p><h3 id="Action-Space"><a href="#Action-Space" class="headerlink" title="Action Space"></a>Action Space</h3><p>The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.</p><p>In other words, we have six possible actions:</p><ol><li><code>south</code></li><li><code>north</code></li><li><code>east</code></li><li><code>west</code></li><li><code>pickup</code></li><li><code>dropoff</code></li></ol><p>This is the <strong>action space</strong>: the set of all the actions that our agent can take in a given state.</p><p>You’ll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment’s code, we will simply provide a -1 penalty for every wall hit and the taxi won’t move anywhere. This will just rack up penalties causing the taxi to consider going around the wall.</p><h2 id="Implementation-with-Python"><a href="#Implementation-with-Python" class="headerlink" title="Implementation with Python"></a>Implementation with Python</h2><p>Fortunately, <a href="https://gym.openai.com/" target="_blank" rel="noopener">OpenAI Gym</a> has this exact environment already built for us.</p><p>Gym provides different game environments which we can plug into our code and test an agent. The library takes care of API for providing all the information that our agent would require, like possible actions, score, and current state. We just need to focus just on the algorithm part for our agent.</p><p>We’ll be using the Gym environment called <code>Taxi-V2</code>, which all of the details explained above were pulled from. The objectives, rewards, and actions are all the same.</p><h3 id="Gym’s-interface"><a href="#Gym’s-interface" class="headerlink" title="Gym’s interface"></a>Gym’s interface</h3><p>Firstly, we need to load the game environment and render what it looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>).env</div><div class="line">env.render()</div></pre></td></tr></table></figure><p><img src="/2019/03/03/RL-Reinforcement-Learning/Screen Shot 2019-03-04 at 4.50.53 PM.png" alt="creen Shot 2019-03-04 at 4.50.53 P"></p><p>The core gym interface is <code>env</code>, which is the unified environment interface. The following are the <code>env</code>methods that would be quite helpful to us:</p><ul><li><code>env.reset</code>: Resets the environment and returns a random initial state.</li><li><code>env.step(action)</code>: Step the environment by one timestep. Returns<ul><li><strong>observation</strong>: Observations of the environment</li><li><strong>reward</strong>: If your action was beneficial or not</li><li><strong>done</strong>: Indicates if we have successfully picked up and dropped off a passenger, also called one <em>episode</em></li><li><strong>info</strong>: Additional info such as performance and latency for debugging purposes</li></ul></li><li><code>env.render</code>: Renders one frame of the environment (helpful in visualizing the environment)</li></ul><blockquote><p>Note: We are using the <code>.env</code> on the end of <code>make</code> to avoid training stopping at 200 iterations, which is the default for the new version of Gym</p></blockquote><h3 id="Reminder-of-the-problem"><a href="#Reminder-of-the-problem" class="headerlink" title="Reminder of the problem"></a>Reminder of the problem</h3><p>Here’s our restructured problem statement (from Gym docs):</p><p><em>“There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.”</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">env.reset() <span class="comment"># rest environment to a new, random state</span></div><div class="line">env.render()</div><div class="line">print(<span class="string">"Action Space &#123;&#125;"</span>.format(env.action_space))</div><div class="line">print(<span class="string">"State Space &#123;&#125;"</span>.format(env.observation_space))</div></pre></td></tr></table></figure><p><img src="/2019/03/03/RL-Reinforcement-Learning/Screen Shot 2019-03-04 at 4.56.32 PM.png" alt="creen Shot 2019-03-04 at 4.56.32 P"></p><ul><li>The <strong>filled square</strong> represents the taxi, which is yellow without a passenger and green with a passenger.</li><li>The <strong>pipe (“|”)</strong> represents a wall which the taxi cannot cross.</li><li><strong>R, G, Y, B</strong> are the possible pickup and destination locations. The <strong>blue letter</strong> represents the current passenger pick-up location, and the <strong>purple letter</strong> is the current destination.</li></ul><p>As verified by the prints, we have an <strong>Action Space</strong> of size 6 and a <strong>State Space</strong> of size 500. As you’ll see, our RL algorithm won’t need any more information than these two things. All we need is a way to identify a state uniquely by assigning a unique number to every possible state, and RL learns to choose an action number from 0-5 where:</p><ul><li>0 = south</li><li>1 = north</li><li>2 = east</li><li>3 = west</li><li>4 = pickup</li><li>5 = dropoff</li></ul><p><strong>Recall that the 500 states correspond to a encoding of the taxi’s location, the passenger’s location, and the destination location.</strong></p><p>Reinforcement Learning will learn a mapping of <strong>states</strong> to the optimal <strong>action</strong> to perform in that state by <em>exploration</em>, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.</p><p>The optimal action for each state is the action that has the <strong>highest cumulative long-term reward</strong>.</p><p>We can actually take our illustration above, encode its state, and give it to the environment to render in Gym. Recall that we have the taxi at row 3, column 1, our passenger is at location 2, and our destination is location 0. Using the Taxi-v2 state encoding method, we can do the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">state = env.encode(<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) <span class="comment">#(taxi_row,taxi_column,passenger_position,destination_index)</span></div><div class="line">print(<span class="string">"State: "</span>,state)</div><div class="line">env.s = state</div><div class="line">env.render()</div></pre></td></tr></table></figure><p><img src="/2019/03/03/RL-Reinforcement-Learning/Screen Shot 2019-03-04 at 5.07.33 PM.png" alt="creen Shot 2019-03-04 at 5.07.33 P"></p><p>We are using our illustration’s coordinates to generate a number corresponding to a state between 0 and 499, which turns out to be <strong>328</strong> for our illustration’s state.</p><p>Then we can set the environment’s state manually with <code>env.env.s</code> using that encoded number. You can play around with the numbers and you’ll see the taxi, passenger, and destination move around.</p><h3 id="The-reward-table"><a href="#The-reward-table" class="headerlink" title="The reward table"></a>The reward table</h3><p>When the Taxi environment is created, there is an initial Reward table that’s also created, called <code>P</code>. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a $states \times actions$.</p><p>Since every state is in this matrix, we can see the default reward values assigned to our illustration’s state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env.P[<span class="number">328</span>]</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="number">0</span>: [(<span class="number">1.0</span>, <span class="number">428</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div><div class="line"> <span class="number">1</span>: [(<span class="number">1.0</span>, <span class="number">228</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div><div class="line"> <span class="number">2</span>: [(<span class="number">1.0</span>, <span class="number">348</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div><div class="line"> <span class="number">3</span>: [(<span class="number">1.0</span>, <span class="number">328</span>, <span class="number">-1</span>, <span class="keyword">False</span>)],</div><div class="line"> <span class="number">4</span>: [(<span class="number">1.0</span>, <span class="number">328</span>, <span class="number">-10</span>, <span class="keyword">False</span>)],</div><div class="line"> <span class="number">5</span>: [(<span class="number">1.0</span>, <span class="number">328</span>, <span class="number">-10</span>, <span class="keyword">False</span>)]&#125;</div></pre></td></tr></table></figure><p>This dictionary has the structure <code>{action: [(probability, nextstate, reward, done)]}</code>.</p><p>A few things to note:</p><ul><li>The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.</li><li>In this env, <code>probability</code> is always 1.0.</li><li>The <code>nextstate</code> is the state we would be in if we take the action at this index of the dict</li><li>All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)</li><li><code>done</code> is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an <strong>episode</strong></li></ul><p>Note that if our agent chose to explore action two in this state it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the <strong>long-term reward</strong>.</p><h3 id="Without-Reinforcement-Learning"><a href="#Without-Reinforcement-Learning" class="headerlink" title="Without Reinforcement Learning"></a>Without Reinforcement Learning</h3><p>Let’s see what would happen if we try to brute-force our way to solving the problem without RL.</p><p>Since we have our <code>P</code> table for default rewards in each state, we can try to have our taxi navigate just using that.</p><p>We’ll create an infinite loop which runs until one passenger reaches one destination (one <strong>episode</strong>), or in other words, when the received reward is 20. The <code>env.action_space.sample()</code> method automatically selects one random action from set of all possible actions.</p><p>Let’s see what happens:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">env.s = <span class="number">328</span> <span class="comment"># set environment to illustration's state</span></div><div class="line">epochs = <span class="number">0</span></div><div class="line">penalties, reward = <span class="number">0</span>,<span class="number">0</span></div><div class="line">frames = [] <span class="comment"># store each step for visualization</span></div><div class="line">done = <span class="keyword">False</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">not</span> done:</div><div class="line">    action = env.action_space.sample() <span class="comment"># choose next step randomly </span></div><div class="line">    state,reward,done,info = env.step(action)</div><div class="line">    <span class="keyword">if</span> reward == <span class="number">-10</span>: <span class="comment"># illegal pick-up and drop-off actions</span></div><div class="line">        penalties += <span class="number">1</span></div><div class="line">    <span class="comment"># put each rendered frame into dict for visualization    </span></div><div class="line">    frames.append(&#123;</div><div class="line">            <span class="string">'frame'</span>:env.render(mode=<span class="string">'ansi'</span>),</div><div class="line">            <span class="string">'state'</span>:state,</div><div class="line">            <span class="string">'action'</span>:action,</div><div class="line">            <span class="string">'reward'</span>:reward</div><div class="line">        &#125;)</div><div class="line">    epochs += <span class="number">1</span></div><div class="line">print(<span class="string">"Timesteps taken: &#123;&#125;"</span>.format(epochs))</div><div class="line">print(<span class="string">"Penalties incurred: &#123;&#125;"</span>.format(penalties))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Timesteps taken: <span class="number">1348</span></div><div class="line">Penalties incurred: <span class="number">431</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</div><div class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_frames</span><span class="params">(frames)</span>:</span></div><div class="line">    <span class="keyword">for</span> i,frame <span class="keyword">in</span> enumerate(frames):</div><div class="line">        clear_output(wait=<span class="keyword">True</span>)</div><div class="line">        print(frame[<span class="string">'frame'</span>])</div><div class="line">        print(<span class="string">f"Timestep: <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>"</span>)</div><div class="line">        print(<span class="string">f"State: <span class="subst">&#123;frame[<span class="string">'state'</span>]&#125;</span>"</span>)</div><div class="line">        print(<span class="string">f"Action: <span class="subst">&#123;frame[<span class="string">'action'</span>]&#125;</span>"</span>)</div><div class="line">        print(<span class="string">f"Reward: <span class="subst">&#123;frame[<span class="string">'reward'</span>]&#125;</span>"</span>)</div><div class="line">        sleep(<span class="number">.1</span>)</div><div class="line">print_frames(frames)</div></pre></td></tr></table></figure><p><img src="/2019/03/03/RL-Reinforcement-Learning/OpenAI_Gym_Taxi_-Animation-1830599.gif" alt="penAI_Gym_Taxi_-Animation-183059"></p><p>Not good. Our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.</p><p>This is because we aren’t <em>learning</em> from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us.</p><h3 id="Enter-Reinforcement-Learning"><a href="#Enter-Reinforcement-Learning" class="headerlink" title="Enter Reinforcement Learning"></a>Enter Reinforcement Learning</h3><h4 id="Summing-up-the-Q-Learning-Process"><a href="#Summing-up-the-Q-Learning-Process" class="headerlink" title="Summing up the Q-Learning Process"></a>Summing up the Q-Learning Process</h4><p>Breaking it down into steps, we get</p><ul><li>Initialize the Q-table by all zeros.</li><li>Start exploring actions: For each state, select any one among all possible actions for the current state (S).</li><li>Travel to the next state (S’) as a result of that action (a).</li><li>For all possible actions from the state (S’) select the one with the highest Q-value.</li><li>Update Q-table values using the equation.</li><li>Set the next state as the current state.</li><li>If goal state is reached, then end and repeat the process.</li></ul><p>After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.</p><p>There’s a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we’ll be introducing another parameter called $\epsilon$ “epsilon” to cater to this during training.</p><p>Instead of just selecting the best learned Q-value action, we’ll sometimes favor exploring the action space further. Larger epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions.</p><h4 id="Training-the-agent"><a href="#Training-the-agent" class="headerlink" title="Training the agent"></a>Training the agent</h4><p>First, we’ll initialize the Q-table to a 500×6500×6 matrix of zeros:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">q_table = np.zeros([env.observation_space.n,env.action_space.n])</div></pre></td></tr></table></figure><p>In the first part of <code>while not done</code>, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the <code>epsilon</code> value and comparing it to the <code>random.uniform(0, 1)</code> function, which returns an arbitrary number between 0 and 1.</p><p>We execute the chosen action in the environment to obtain the <code>next_state</code> and the <code>reward</code> from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the <code>next_state</code>, and with that, we can easily update our Q-value to the <code>new_q_value</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line">alpha = <span class="number">0.1</span> <span class="comment">#learning rate</span></div><div class="line">lambda_ = <span class="number">0.6</span> <span class="comment"># discount</span></div><div class="line">epsilon = <span class="number">0.1</span></div><div class="line">all_epochs = []</div><div class="line">all_penalties = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">100001</span>):</div><div class="line">    state = env.reset()</div><div class="line">    epochs,penalties,reward = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></div><div class="line">    done = <span class="keyword">False</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</div><div class="line">        <span class="keyword">if</span> random.uniform(<span class="number">0</span>,<span class="number">1</span>)&lt;epsilon:</div><div class="line">            action = env.action_space.sample() <span class="comment">#explore action space</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            action = np.argmax(q_table[state]) <span class="comment">#exploit learned values</span></div><div class="line">        next_state, reward, done, info = env.step(action)</div><div class="line">        old_value = q_table[state,action]</div><div class="line">        next_max = np.max(q_table[next_state])</div><div class="line">        new_value = (<span class="number">1</span>-alpha)*old_value + alpha*(reward+lambda_*next_max)</div><div class="line">        q_table[state,action] = new_value</div><div class="line">        <span class="keyword">if</span> reward == <span class="number">-10</span>:</div><div class="line">            penalties += <span class="number">1</span></div><div class="line">        state = next_state</div><div class="line">        epochs += <span class="number">1</span></div><div class="line">    <span class="keyword">if</span> i%<span class="number">100</span>==<span class="number">0</span>:</div><div class="line">        clear_output(wait=<span class="keyword">True</span>)</div><div class="line">        print(<span class="string">f"Episode:<span class="subst">&#123;i&#125;</span>"</span>)</div><div class="line">print(<span class="string">"Training finished.\n"</span>)</div></pre></td></tr></table></figure><p>Now that the Q-table has been established over 100,000 episodes, let’s see what the Q-values are at our illustration’s state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">q_table[<span class="number">328</span>]</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">array([ <span class="number">-2.41338735</span>,  <span class="number">-2.27325184</span>,  <span class="number">-2.41222668</span>,  <span class="number">-2.36038871</span>,</div><div class="line">       <span class="number">-11.15090102</span>, <span class="number">-10.99517141</span>])</div></pre></td></tr></table></figure><p>The max Q-value is “north” (-2.27325184), so it looks like Q-learning has effectively learned the best action to take in our illustration’s state!</p><h4 id="Evaluating-the-agent"><a href="#Evaluating-the-agent" class="headerlink" title="Evaluating the agent"></a>Evaluating the agent</h4><p>Let’s evaluate the performance of our agent. We don’t need to explore actions any further, so now the next action is always selected using the best Q-value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">total_epochs, total_penalties = <span class="number">0</span>, <span class="number">0</span></div><div class="line">episodes = <span class="number">100</span></div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(episodes):</div><div class="line">    state = env.reset()</div><div class="line">    epochs, penalties, reward = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></div><div class="line">    done = <span class="keyword">False</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</div><div class="line">        action = np.argmax(q_table[state])</div><div class="line">        state, reward, done, info = env.step(action)</div><div class="line">        <span class="keyword">if</span> reward == <span class="number">-10</span>:</div><div class="line">            penalties += <span class="number">1</span></div><div class="line">        epochs += <span class="number">1</span></div><div class="line">    total_penalties += penalties</div><div class="line">    total_epochs += epochs</div><div class="line">print(<span class="string">f"Results after <span class="subst">&#123;episodes&#125;</span> episodes:"</span>)</div><div class="line">print(<span class="string">f"Average timesteps per episode: <span class="subst">&#123;total_epochs / episodes&#125;</span>"</span>)</div><div class="line">print(<span class="string">f"Average penalties per episode: <span class="subst">&#123;total_penalties / episodes&#125;</span>"</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Results after <span class="number">100</span> episodes:</div><div class="line">Average timesteps per episode: <span class="number">12.55</span></div><div class="line">Average penalties per episode: <span class="number">0.0</span></div></pre></td></tr></table></figure><p>We can see from the evaluation, the agent’s performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers.</p><h4 id="Comparison-between-learning-w-o-rl-and-w-rl"><a href="#Comparison-between-learning-w-o-rl-and-w-rl" class="headerlink" title="Comparison between learning w/o rl and w rl"></a>Comparison between learning w/o rl and w rl</h4><p>With Q-learning agent commits errors initially during exploration but once it has explored enough (seen most of the states), it can act wisely maximizing the rewards making smart moves. Let’s see how much better our Q-learning solution is when compared to the agent making just random moves.</p><p>We evaluate our agents according to the following metrics,</p><ul><li><strong>Average number of penalties per episode:</strong> The smaller the number, the better the performance of our agent. Ideally, we would like this metric to be zero or very close to zero.</li><li><strong>Average number of timesteps per trip:</strong> We want a small number of timesteps per episode as well since we want our agent to take minimum steps(i.e. the shortest path) to reach the destination.</li><li><strong>Average rewards per move:</strong> The larger the reward means the agent is doing the right thing. That’s why deciding rewards is a crucial part of Reinforcement Learning. In our case, as both timesteps and penalties are negatively rewarded, a higher average reward would mean that the agent reaches the destination as fast as possible with the least penalties”</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">Measure</th><th style="text-align:center">Random agent’s performance</th><th style="text-align:center">Random agent’s performance</th></tr></thead><tbody><tr><td style="text-align:center">Average rewards per move</td><td style="text-align:center">-3.9012092102214075</td><td style="text-align:center">0.6962843295638126</td></tr><tr><td style="text-align:center">Average number of penalties per episode</td><td style="text-align:center">920.45</td><td style="text-align:center">0.0</td></tr><tr><td style="text-align:center">Average number of timesteps per trip</td><td style="text-align:center">2848.14</td><td style="text-align:center">12.38</td></tr></tbody></table></div><h4 id="Hyperparameters-and-optimizations"><a href="#Hyperparameters-and-optimizations" class="headerlink" title="Hyperparameters and optimizations"></a>Hyperparameters and optimizations</h4><p>The values of <code>alpha</code>, <code>lambda</code>, and <code>epsilon</code> were mostly based on intuition and some “hit and trial”, but there are better ways to come up with good values.</p><p>Ideally, all three should decrease over time because as the agent continues to learn, it actually builds up more resilient priors;</p><ul><li>$\alpha$: (the learning rate) should decrease as you continue to gain a larger and larger knowledge base.</li><li>$\lambda$: as you get closer and closer to the deadline, your preference for near-term reward should increase, as you won’t be around long enough to get the long-term reward, which means your gamma should decrease.</li><li>$\epsilon$: as we develop our strategy, we have less need of exploration and more exploitation to get more utility from our policy, so as trials increase, epsilon should decrease.</li></ul><h1 id="OpenAI-GYM"><a href="#OpenAI-GYM" class="headerlink" title="OpenAI-GYM"></a>OpenAI-GYM</h1><p><strong>Gym is a toolkit for developing and comparing reinforcement learning algorithms.</strong> The <a href="https://github.com/openai/gym" target="_blank" rel="noopener">gym</a> library is a collection of test problems — <strong>environments</strong> — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.</p><h2 id="Environments"><a href="#Environments" class="headerlink" title="Environments"></a>Environments</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>)</div><div class="line">env.reset()</div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    env.render()</div><div class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></div></pre></td></tr></table></figure><ul><li><code>gym.make()</code>: create an environment</li><li><code>reset()</code>: reset the environment and returns a random initial state.</li><li><code>render()</code>: print out the current environmnet</li><li><code>env.step(action)</code>: step the environment by one timestep. returns<ul><li><strong>observation(object)</strong>: an environment-specific object representing your observation of the environment. </li><li><strong>reward(float)</strong>: amount of reward achieved by the previous action.</li><li><strong>done(boolean)</strong>: whether it’s time to <code>reset</code> the environment again. Most (but not all) tasks are divided up into well-defined episodes, and <code>done</code> being <code>True</code> indicates the episode has terminated. </li><li><strong>info(dict)</strong>: Additional info such as performance and latency for debugging purposes</li></ul></li></ul><p>This is just an implementation of the classic “agent-environment loop”. Each timestep, the agent chooses an <code>action</code>, and the environment returns an <code>observation</code> and a <code>reward</code>. The process gets started by calling <code>reset()</code>, which returns an initial <code>observation</code>. So a more proper way of writing the previous code would be to respect the <code>done</code> flag:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>)</div><div class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</div><div class="line">    observation =  env.reset()</div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</div><div class="line">        env.render()</div><div class="line">        print(observation)</div><div class="line">        action = env.action_space.sample()</div><div class="line">        observation,reward,done,info = env.step(action)</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            print(print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>)))</div><div class="line">            <span class="keyword">break</span></div></pre></td></tr></table></figure><h2 id="Spaces"><a href="#Spaces" class="headerlink" title="Spaces"></a>Spaces</h2><p>In the examples above, we’ve been sampling random actions from the environment’s action space. But what actually are those actions? Every environment comes with an <code>action_space</code> and an <code>observation_space</code>. These attributes are of type <a href="https://github.com/openai/gym/blob/master/gym/core.py" target="_blank" rel="noopener"><code>Space</code></a>, and they describe the format of valid actions and observations:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">"Taxi-v2"</span>)</div><div class="line">print(env.action_space)</div><div class="line"><span class="comment">#&gt; Discrete(6)</span></div><div class="line">print(env.observation_space)</div><div class="line"><span class="comment">#&gt; Discrete(500)</span></div></pre></td></tr></table></figure><p>The <a href="https://github.com/openai/gym/blob/master/gym/spaces/discrete.py" target="_blank" rel="noopener"><code>Discrete</code></a> space allows a fixed range of non-negative numbers.</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://towardsdatascience.com/reinforcement-learning-tutorial-part-1-q-learning-cadb36998b28" target="_blank" rel="noopener">Reinforcement Learning Tutorial Part 1: Q-Learning</a> </p><p><a href="https://medium.freecodecamp.org/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc" target="_blank" rel="noopener">An introduction to Q-Learning: reinforcement learning</a> </p><p><a href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/#" target="_blank" rel="noopener">Reinforcement Q-Learning from Scratch in Python with OpenAI Gym</a> </p>]]></content>
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Reinforcement Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RL-Double Q-Learning</title>
      <link href="/2019/03/03/RL-Double-Q-Learning/"/>
      <url>/2019/03/03/RL-Double-Q-Learning/</url>
      <content type="html"><![CDATA[<p>Double Q-Learning in Reinforcement Learning.</p><a id="more"></a><h1 id="Problem-in-Q-Learning"><a href="#Problem-in-Q-Learning" class="headerlink" title="Problem in Q-Learning"></a>Problem in Q-Learning</h1><p>Consider an MDP having four states two of which are terminal states. State A is always considered at start state, and has two actions, either Right or Left. The Right action gives zero reward and lands in terminal state C. The Left action moves the agent to state B with zero reward.</p><p><img src="/2019/03/03/RL-Double-Q-Learning/1_HmGJAGiZG8coo-B4Q7m7-g-1628173.png" alt="_HmGJAGiZG8coo-B4Q7m7-g-162817"></p><p>State B has a number of actions, they move the agent to the terminal state D. However (this is important) the reward R of each action from B to D has a <strong>random value that follows a normal distribution with mean -0.5 and a variance 1.0</strong>.</p><p>The expected value of R is known to be negative (-0.5). This means that over a large number of experiments the average value of R is less than zero.</p><p>Based on this assumption, it is clear that moving left from A is always a bad idea. However because some of the values of R are positive, Q-Learning will be tricked to consider that moving left from A maximises the reward. In reality this is a bad decision, because even if it works for some episodes, on the long run it is guaranteed to be a negative reward.</p>]]></content>
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Double Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Object Detection</title>
      <link href="/2019/03/02/DP-Object-Detection/"/>
      <url>/2019/03/02/DP-Object-Detection/</url>
      <content type="html"><![CDATA[<p>Object detection is one of the popular computer vision tasks, i.e., image classification, object detection, object tracking, image segmentation, image caption and image generation. The main of object detection is to find out all the objects in a image, their positions and corresponding confidence.</p><p>In brief, in order to detect objects, we first need to generate region proposals, then to classify the object class and detect the bounding box.</p><a id="more"></a><h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><ol><li>First, R-CNN uses selective search by [2] to <strong>generate about 2K region proposals</strong>, i.e. bounding boxes for image classification.</li><li>Then, for each bounding box, image classification is done through CNN.</li><li>Finally, each bounding box can be refined using regression.</li></ol><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>It is the first fully differentiable object-detection model that was proposed.</p><p>In Faster R-CNN, RPN using SS is replaced by RPN using CNN. And this CNN is shared with detection network. This CNN can be ZFNet or VGGNet in the paper.** Thus, the overall network is as below:</p><p><img src="/2019/03/02/DP-Object-Detection/../DP-Object-Detection/1_e6dx5qzUKWwasIVGSuCyDA-1587215.png" alt="_e6dx5qzUKWwasIVGSuCyDA-158721"></p><ol><li>First, the picture goes through conv layers and feature maps are extracted.</li><li>Then a <strong>sliding window</strong> is used in RPN for each location over the feature map.</li><li>For each location, <strong>k (k=9) anchor</strong> boxes are used (<strong>3 scales of 128, 256 and 512, and 3 aspect ratios of 1:1, 1:2, 2:1</strong>) for generating region proposals.</li><li>A <strong>cls</strong> layer outputs <em>2k</em> scores <strong>whether there is object or not</strong> for <em>k</em> boxes.</li><li>A <strong>reg</strong> layer outputs <em>4k</em> for the <strong>coordinates</strong> (box center coordinates, width and height) of <em>k</em> boxes.</li><li>With a size of <em>W**</em>×<em>**H</em> feature map, there are <em>WHk</em> anchors in total.</li></ol><h1 id="YOLO-ref"><a href="#YOLO-ref" class="headerlink" title="YOLO ref"></a>YOLO <a href="https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/" target="_blank" rel="noopener">ref</a></h1><p>The obove object detection methods are usually two-stage detectot.</p><ol><li>In R-CNN and Faster R-CNN, an object detector that required an algorithm such as <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_blank" rel="noopener">Selective Search</a> (or equivalent) is used to propose candidate bounding boxes that could contain objects.</li><li>These regions were then passed into a CNN for classification, ultimately leading to one of the first deep learning-based object detectors.</li></ol><p>The problem with the standard R-CNN method was that it was <em>painfully slow</em> and not a complete end-to-end object detector. Until the proposal of Faster R-CNN, which is an end-to-end deep learning framework</p><p><strong>While R-CNNs tend to very accurate, the biggest problem with the R-CNN family of networks is their speed — they were incredibly slow.</strong> To help increase the speed of deep learning-based object detectors, both Single Shot Detectors (SSDs) and YOLO use a <strong>one-stage detector strategy</strong>. These algorithms treat object detection as a regression problem, taking a given input image and simultaneously learning bounding box coordinates and corresponding class label probabilities.</p><p><strong>In general, single-stage detectors tend to be less accurate than two-stage detectors but are significantly faster.</strong></p><p>A great explanation about <a href="https://hackernoon.com/understanding-yolo-f5a74bbc7967" target="_blank" rel="noopener">YOLO</a>.</p><p><img src="/2019/03/02/DP-Object-Detection/../DP-Object-Detection/1_OuMJUWo2rXYA-GYU63NUGw.jpeg" alt="_OuMJUWo2rXYA-GYU63NUG"></p><p><img src="/2019/03/02/DP-Object-Detection/../DP-Object-Detection/1_0IPktA65WxOBfP_ULQWcmw.png" alt="_0IPktA65WxOBfP_ULQWcm"></p><p><img src="/2019/03/02/DP-Object-Detection/../DP-Object-Detection/1_9ER4GVUtQGVA2Y0skC9OQQ.png" alt="_9ER4GVUtQGVA2Y0skC9OQ"></p><blockquote><p>Some convolution layers use $1 \times 1$ reduction layers alternatively to reduce the depth of the features maps.</p></blockquote><p>A comprehensive review <a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" target="_blank" rel="noopener">Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3</a>. </p><p><a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/" target="_blank" rel="noopener">How to implement a YOLO (v3) object detector from scratch in PyTorch</a> </p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" target="_blank" rel="noopener">Review: Faster R-CNN (Object Detection)</a> </p><p><a href="https://zhuanlan.zhihu.com/p/32404424" target="_blank" rel="noopener">从编程实现角度学习Faster R-CNN（附极简实现）</a> </p><p><a href="https://www.cnblogs.com/wangyong/p/8513563.html" target="_blank" rel="noopener">Faster RCNN 学习笔记</a></p><p><a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_blank" rel="noopener">Faster R-CNN: Down the rabbit hole of modern object detection</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-RNN</title>
      <link href="/2019/03/02/DP-RNN/"/>
      <url>/2019/03/02/DP-RNN/</url>
      <content type="html"><![CDATA[<p>Recurrent Neural Networks, which are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, or numerical times series data emanating from sensors, stock markets and government agencies. These algorithms take time and sequence into account, they have a temporal dimension.</p><a id="more"></a><h1 id="Problems-with-Vanilla-NN-link"><a href="#Problems-with-Vanilla-NN-link" class="headerlink" title="Problems with Vanilla NN link"></a>Problems with Vanilla NN <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">link</a></h1><p>A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes).</p><p><img src="/2019/03/02/DP-RNN/diags-1578838.jpeg" alt="iags-157883"></p><p>Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN’s state (more on this soon). From left to right: <strong>(1)</strong> Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). <strong>(2)</strong> Sequence output (e.g. image captioning takes an image and outputs a sentence of words). <strong>(3)</strong> Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). <strong>(4)</strong> Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). <strong>(5)</strong> Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.</p><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>At a high level, a<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener"> recurrent neural network</a> (RNN) processes sequences — whether daily stock prices, sentences, or sensor measurements — one element at a time while retaining a <em>memory</em> (called a state) of what has come previously in the sequence.</p><p><a href="https://arxiv.org/pdf/1610.02583.pdf" target="_blank" rel="noopener">A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation</a> </p><p>given an observation sequence $\mathbf{x}=\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{T}\right\}$ and its corresponding label $y=\left\{y_{1}, y_{2}, \ldots, y_{T}\right\}$, we want to learn a map $f : \mathbf{x} \mapsto y$. </p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-19 at 12.15.53 PM.png" alt="creen Shot 2019-06-19 at 12.15.53 P"></p><p>Suppose that we have the following RNN model, such that </p><script type="math/tex; mode=display">\begin{array}{l}{\mathbf{h}_{t}=\tanh \left(W_{h h} \mathbf{h}_{t-1}+W_{x h} \mathbf{x}_{t}+\mathbf{b}_{\mathbf{h}}\right)} \\ {z_{t}=\operatorname{softmax}\left(W_{h z} \mathbf{h}_{t}+\mathbf{b}_{z}\right)}\end{array}</script><p>where $z_t$ is the prediction at the time step $t$.</p><p>We can minimize the negative log likelihood objective function:</p><script type="math/tex; mode=display">\mathcal{L}(\mathrm{x}, \mathrm{y})=-\sum_{t} y_{t} \log z_{t}</script><p>In the following, we will use notation $L$ as the objective function for simplicity. And further we will use $L(t + 1)$ to indicate the output at the time step t + 1, s.t. $L(t + 1) = -y_{t+1}logz_{t+1}$. </p><p>Let’s set $\alpha_{t}=W_{h z} \mathbf{h}_{t}+\mathbf{b}_{z}$ and then we have $z_t=softmax(\alpha_t)$. By taking the derivative with respect to $ \alpha_t$, we get the following:</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial \alpha_{t}}=-\left(y_{t}-z_{t}\right)</script><p>Note the weight $W_{hz}$ is shared across all time sequence, thus we can dierentiate to it at each time</p><p>step and sum all together</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial W_{h z}}=\sum_{t} \frac{\partial \mathcal{L}}{\partial z_{t}} \frac{\partial z_{t}}{\partial W_{h z}}</script><p>Similarly, we can get the gradient w.r.t. bias $b_z$</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial b_{z}}=\sum_{t} \frac{\partial \mathcal{L}}{\partial z_{t}} \frac{\partial z_{t}}{\partial b_{z}}</script><p>Now let’s go through the details to derive the gradient w.r.t. $W_{hh}$. Considering at the time step $t \to t+1$ in figure1, </p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(t+1)}{\partial W_{h h}}=\frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{h h}}</script><p>where we only consider one step $t \to t+1$. And because the hidden state $h_{t+1}$ partially dependents</p><p>on $h_t$, so we can use backpropagation to compute the above partial derivative. Think further $W_{hh}$ is shared cross the whole time sequence. Thus, at the time step $(t -1) \to t$, we can further get the partial derivative w.r.t. $W_{hh}$ as follows</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(t+1)}{\partial W_{h h}}=\frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{t}} \frac{\partial \mathbf{h}_{t}}{\partial W_{h h}}</script><p>Thus, at the time step $t+1$, we can compute gradient w.r.t. $z_{t+1}$ and further use backpropagation</p><p>through time (BPTT) from t to 0 to calculate gradient w.r.t. $W_{hh}$, shown as the red chain in Fig. 1.</p><p>Thus, if we only consider the output $z_{t+1}$ at the time step $t+1$, we can yield the following gradient</p><p>w.r.t. $W_{hh}$</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(t+1)}{\partial W_{h h}}=\sum_{k=1}^{t} \frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{h h}}</script><p>Aggregate the gradients w.r.t. $W_{hh}$ over the whole time sequence with back propagation, we can</p><p>finally yield the following gradient w.r.t. $W_{hh}$</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial W_{h h}}=\sum_{t} \sum_{k=1}^{t+1} \frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{h h}}</script><p>Now we turn to derive the gradient w.r.t. $W_{xh}$. Similarly, we consider the time step $t + 1$ (only</p><p>contribution from $x_{t+1}$) and calculate the gradient w.r.t. to $W_{xh}$ as follows</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(t+1)}{\partial W_{x h}}=\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{x h}}</script><p>Because $h_t$ and $x_{t+1}$ both make contribution to $h_{t+1}$, we need to backpropagte to $h_t$ as well. If we</p><p>consider the contribution from the time step $t$, we can further get</p><script type="math/tex; mode=display">\begin{aligned} & \frac{\partial \mathcal{L}(t+1)}{\partial W_{x h}}=\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{x h}}+\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t}} \frac{\partial \mathbf{h}_{t}}{\partial W_{x h}} \\=& \frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial W_{x h}}+\frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{t}} \frac{\partial \mathbf{h}_{t}}{\partial W_{x h}} \end{aligned}</script><p>Thus, summing up all contributions from $t$ to 0 via backpropagation, we can yield the gradient at</p><p>the time step $t + 1$</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}(t+1)}{\partial W_{x h}}=\sum_{k=1}^{t+1} \frac{\partial \mathcal{L}(t+1)}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{x h}}</script><p>Further, we can take derivative w.r.t. $W_{xh}$ over the whole sequence as</p><script type="math/tex; mode=display">\frac{\partial \mathcal{L}}{\partial W_{x h}}=\sum_{t} \sum_{k=1}^{t+1} \frac{\partial \mathcal{L}(t+1)}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial \mathbf{h}_{t+1}} \frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathbf{h}_{k}}{\partial W_{x h}}</script><p>However, there are gradient vanishing or exploding problems to RNNs. Notice that $\frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{h}_{k}}$ indicates matrix multiplication over the sequence. Because RNNs need to backpropagate gradients over a long sequence (with small values in the matrix multiplication), gradient value will shrink layer over layer, and eventually vanish after a few time steps. Thus, the states that are far away from the current time step does not contribute to the parameters’ gradient computing (or parameters that RNNs is learning). Another direction is the gradient exploding, which attributed to large values in matrix multiplication.</p><h2 id="Forward-propagation"><a href="#Forward-propagation" class="headerlink" title="Forward propagation"></a>Forward propagation</h2><p><img src="/2019/03/02/DP-RNN/1042406-20170306142253375-175971779.png" alt="042406-20170306142253375-17597177"></p><p>$x$: input, $h$: hidden layer, $o$: output, $y$: target label, $L$: loss function, $t$: time </p><p>At time t, we have hidden state:</p><script type="math/tex; mode=display">h^{(t)}=\phi(Ux^{(t)}+Wh^{(t)}+b)</script><p>where $\phi()$ is activation function, typically $tanh()$, and $b$ is the bias.</p><p>The output is at time t is:</p><script type="math/tex; mode=display">o^{(t)}=Vh^{(t)}+c</script><p>Then the prediction is:</p><script type="math/tex; mode=display">\hat{y}^{(t)}=\sigma(o^{(t)})</script><p>where $\sigma()$ is the activation function, typically $softmax()$.</p><h2 id="Back-Propagation-through-Time"><a href="#Back-Propagation-through-Time" class="headerlink" title="Back Propagation through Time"></a>Back Propagation through Time</h2><p><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="noopener">ref</a> <a href="https://peterroelants.github.io/posts/rnn-implementation-part01/" target="_blank" rel="noopener">ref2</a> <a href="https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d" target="_blank" rel="noopener">ref4</a></p><p>Let’s quickly recap the basic equations of our RNN.</p><script type="math/tex; mode=display">\begin{aligned} s_{t} &=\tanh \left(U x_{t}+W s_{t-1}\right) \\ \hat{y}_{t} &=\operatorname{softmax}\left(V s_{t}\right) \end{aligned}</script><p>We also defined our <em>loss</em>, or error, to be the cross entropy loss, given by:</p><script type="math/tex; mode=display">\begin{aligned} E_{t}\left(y_{t}, \hat{y}_{t}\right) &=-y_{t} \log \hat{y}_{t} \\ E(y, \hat{y}) &=\sum_{t} E_{t}\left(y_{t}, \hat{y}_{t}\right) \\ &=-\sum_{t} y_{t} \log \hat{y}_{t} \end{aligned}</script><p>Here, $y_t$ is the correct word at time step $t$, and $\hat y_t$ is our prediction. We typically treat the full sequence (sentence) as one training example, so the total error is just the sum of the errors at each time step (word).</p><p><img src="/2019/03/02/DP-RNN/rnn-bptt1.png" alt="nn-bptt"></p><p>Remember that our goal is to calculate the gradients of the error with respect to our parameters $U$, $V$ and $W$. </p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial E_{3}}{\partial V} &=\frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial V} \\ &=\frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial z_{3}} \frac{\partial z_{3}}{\partial V} \\ &=\left(\hat{y}_{3}-y_{3}\right) \otimes s_{3} \end{aligned}</script><p>In the above, $z_{3}=V s_{3}$ and $\oplus$ is the outer product of two vectors. We can find that $\frac{\partial E_{3}}{\partial V}$ only depends on the values at the current time step, $\hat{y}_{3}, y_{3}, s_{3}$. </p><p>But the story is different for $\frac{\partial E_{3}}{\partial W}$ (and for $U$). To see why, we write out the chain rule, just as above:</p><script type="math/tex; mode=display">\frac{\partial E_{3}}{\partial W}=\frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial s_{3}} \frac{\partial s_{3}}{\partial W}</script><p>Now, note that $s_{3}=\tanh \left(U x_{t}+W s_{2}\right)$ depends on $s_2$, which depends on $W$ and $s_1$, and so on. So if we take the derivative with respect to $W$ we can’t simply treat $s_2$ as a constant! We need to apply the chain rule again and what we really have is this:</p><script type="math/tex; mode=display">\frac{\partial E_{3}}{\partial W}=\sum_{k=0}^{3} \frac{\partial E_{3}}{\partial \hat{y}_{3}} \frac{\partial \hat{y}_{3}}{\partial s_{3}} \frac{\partial s_{3}}{\partial s_{k}} \frac{\partial s_{k}}{\partial W}</script><p>We sum up the contributions of each time step to the gradient. In other words, because $W$ is used in every step up to the output we care about, we need to backpropagate gradients from $t=3$ through the network all the way to $t=0$.</p><p><img src="/2019/03/02/DP-RNN/rnn-bptt-with-gradients.png" alt="nn-bptt-with-gradient"></p><h2 id="Hand-Written-RNN"><a href="#Hand-Written-RNN" class="headerlink" title="Hand-Written RNN"></a>Hand-Written RNN</h2><p> <a href="https://towardsdatascience.com/only-numpy-vanilla-recurrent-neural-network-back-propagation-practice-math-956fbea32704" target="_blank" rel="noopener">hand-writtrn-deduction-1</a> <a href="https://medium.com/@SeoJaeDuk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316" target="_blank" rel="noopener">hand-writtrn-deduction-2</a></p><p>Here I am going to give an simple problem about how RNN can be used to solve problems and aim to have a better understanding of how forward and backward propagation in RNN work.</p><p>Anyways here we go. The problem is very simple, we are going to use RNN to count how many ones there are in the given data.</p><script type="math/tex; mode=display">\begin{array}{l}{x=[ } \\ {[0,0,1]} \\ {[0,1,1]} \\ {[1,1,1]} \\ {\text { ]}}\end{array}\\y = [1,2,3]</script><p>As seen above, the training data is $x$  and $y$ is the groundtruth. </p><p>The corresponding RNN architecture looks like the following:</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-18 at 9.36.36 PM.png" alt="creen Shot 2019-06-18 at 9.36.36 P"></p><p>which is the unrolled version of our network architecture.</p><p>For this network, we have two weight matrixs, $W_x$ and $W_{rec}$, and the forward propagation is:</p><script type="math/tex; mode=display">S_{1}=S_0 \cdot W_{r e c}+X_{1} \cdot W_{x} \\S_{2}=S_1 \cdot W_{r e c}+X_{2} \cdot W_{x}\\S_{3}=S_2 \cdot W_{r e c}+X_{3} \cdot W_{x}</script><p>We define the MSE loss function:</p><script type="math/tex; mode=display">\cos t=\frac{1}{m}\left(S_{3}-y\right)^{2}</script><p>Now lets perform back propagation through time. We have to get derivative respect to $W_x$ and $W_{rec}$ for each state.</p><p><strong>State3:</strong></p><script type="math/tex; mode=display">\frac{d \xi}{d W_{x}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d W_x}=\frac{2}{m}\left(S_{3}-y\right) \cdot x_{3}\\\frac{d \xi}{d W_{r e c}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d W r e c}=\frac{2}{m}\left(S_{3}-y\right) \cdot S_{2}</script><p><strong>State2:</strong></p><script type="math/tex; mode=display">\frac{d \xi}{d W_{x}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d W_x}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot x_{2}\\\frac{d \xi}{d W_{rec}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d W_{rec}}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot S_{1}\\</script><p><strong>State1:</strong></p><script type="math/tex; mode=display">\frac{d \xi}{d W_{x}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d S_1}\cdot \frac{d S_{1}}{d W_x}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot W_{rec}\cdot x_{2}\\\frac{d \xi}{d W_{rec}}=\frac{d \xi}{d S_{1}} \cdot \frac{d S_{3}}{d S_2}\cdot \frac{d S_{2}}{d S_1}\cdot \frac{d S_{1}}{d W_{rec}}=\frac{2}{m}\left(S_{3}-y\right) \cdot W_{rec}\cdot W_{rec}\cdot S_{1}\\</script><p>So that’s it! The simple math behind training RNN.</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-18 at 9.53.34 PM.png" alt="creen Shot 2019-06-18 at 9.53.34 P"></p><h2 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(x, prev_h, Wx, Wh, b)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh</span></div><div class="line"><span class="string">    activation function.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">    a minibatch size of N.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - x: Input data for this timestep, of shape (N, D).</span></div><div class="line"><span class="string">    - prev_h: Hidden state from previous timestep, of shape (N, H)</span></div><div class="line"><span class="string">    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">    - b: Biases of shape (H,)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">    - cache: Tuple of values needed for the backward pass.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    next_h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement a single forward step for the vanilla RNN. Store the next  #</span></div><div class="line">    <span class="comment"># hidden state and any values you need for the backward pass in the next_h   #</span></div><div class="line">    <span class="comment"># and cache variables respectively.                                          #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">    raw_h = x.dot(Wx) + prev_h.dot(Wh) + b</div><div class="line">    next_h = np.tanh(raw_h)</div><div class="line">    cache = (x, prev_h, Wx, Wh, raw_h, next_h)</div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="keyword">return</span> next_h, cache</div></pre></td></tr></table></figure></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dnext_h, cache)</span>:</span></div><div class="line"></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Backward pass for a single timestep of a vanilla RNN.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Inputs:</span></div><div class="line"><span class="string">- dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">- cache: Cache object from the forward pass</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Returns a tuple of:</span></div><div class="line"><span class="string">- dx: Gradients of input data, of shape (N, D)</span></div><div class="line"><span class="string">- dprev_h: Gradients of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">- dWx: Gradients of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">- dWh: Gradients of hidden-to-hidden weights, of shape (H, H)</span></div><div class="line"><span class="string">- db: Gradients of bias vector, of shape (H,)</span></div><div class="line"><span class="string">"""</span></div><div class="line">dx, dprev_h, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for a single step of a vanilla RNN.      #</span></div><div class="line"><span class="comment">#                                                                            #</span></div><div class="line"><span class="comment"># HINT: For the tanh function, you can compute the local derivative in terms #</span></div><div class="line"><span class="comment"># of the output value from tanh.                                             #</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">x, prev_h, Wx, Wh, raw_h, next_h = cache</div><div class="line"></div><div class="line">draw_h = (<span class="number">1</span>-next_h**<span class="number">2</span>)*dnext_h</div><div class="line">db = np.sum(draw_h,axis=<span class="number">0</span>)</div><div class="line">dx = draw_h.dot(Wx.T)</div><div class="line">dWx = x.T.dot(draw_h)</div><div class="line">dprev_h = draw_h.dot(Wh.T)</div><div class="line">dWh = prev_h.T.dot(draw_h)</div><div class="line"></div><div class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="keyword">return</span> dx, dprev_h, dWx, dWh, db</div></pre></td></tr></table></figure></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line"></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Run a vanilla RNN forward on an entire sequence of data. We assume an input</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The RNN uses a hidden</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    the RNN forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Inputs:</span></div><div class="line"><span class="string">- x: Input data for the entire timeseries, of shape (N, T, D).</span></div><div class="line"><span class="string">- h0: Initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">- Wx: Weight matrix for input-to-hidden connections, of shape (D, H)</span></div><div class="line"><span class="string">- Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</span></div><div class="line"><span class="string">- b: Biases of shape (H,)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Returns a tuple of:</span></div><div class="line"><span class="string">- h: Hidden states for the entire timeseries, of shape (N, T, H).</span></div><div class="line"><span class="string">- cache: Values needed in the backward pass</span></div><div class="line"><span class="string">"""</span></div><div class="line">h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement forward pass for a vanilla RNN running on a sequence of    #</span></div><div class="line"><span class="comment"># input data. You should use the rnn_step_forward function that you defined  #</span></div><div class="line"><span class="comment"># above. You can use a for loop to help compute the forward pass.            #</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">N, T, D = x.shape</div><div class="line">_, H = h0.shape</div><div class="line">h = np.zeros((N,T,H))</div><div class="line">cache = &#123;&#125;</div><div class="line">prev_h = h0</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(T):</div><div class="line">    x_ = x[:,i,:]</div><div class="line">    next_h, cache_ = rnn_step_forward(x_, prev_h, Wx, Wh, b)</div><div class="line">    prev_h = next_h</div><div class="line">    h[:,i,:] = next_h</div><div class="line">    cache[i] = cache_</div><div class="line"></div><div class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="keyword">return</span> h, cache</div></pre></td></tr></table></figure></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Compute the backward pass for a vanilla RNN over an entire sequence of data.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - dh: Upstream gradients of all hidden states, of shape (N, T, H). </span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    NOTE: 'dh' contains the upstream gradients produced by the </span></div><div class="line"><span class="string">    individual loss functions at each timestep, *not* the gradients</span></div><div class="line"><span class="string">    being passed between timesteps (which you'll have to compute yourself</span></div><div class="line"><span class="string">    by calling rnn_step_backward in a loop).</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - dx: Gradient of inputs, of shape (N, T, D)</span></div><div class="line"><span class="string">    - dh0: Gradient of initial hidden state, of shape (N, H)</span></div><div class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, H)</span></div><div class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)</span></div><div class="line"><span class="string">    - db: Gradient of biases, of shape (H,)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for a vanilla RNN running an entire      #</span></div><div class="line">    <span class="comment"># sequence of data. You should use the rnn_step_backward function that you   #</span></div><div class="line">    <span class="comment"># defined above. You can use a for loop to help compute the backward pass.   #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    </div><div class="line">    N, T, H = dh.shape</div><div class="line">    x = cache[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">    _,D  = x.shape</div><div class="line">    dx = np.zeros((N, T, D))</div><div class="line">    dh0 = np.zeros((N, H))</div><div class="line">    dWx = np.zeros((D, H))</div><div class="line">    dWh = np.zeros((H, H))</div><div class="line">    db = np.zeros(H)</div><div class="line">    </div><div class="line">    dprev_h = np.zeros((N, H))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(T)):</div><div class="line">        dnext_h = dh[:,i,:] + dprev_h</div><div class="line">        dx[:, i, :], dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dnext_h, cache[i])</div><div class="line">        dWx += dWx_</div><div class="line">        dWh += dWh_</div><div class="line">        db += db_</div><div class="line">    dh0 = dprev_h</div><div class="line">        </div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure></div></div><h2 id="RNN-Vanishing-Gradients-Problem"><a href="#RNN-Vanishing-Gradients-Problem" class="headerlink" title="RNN Vanishing Gradients Problem"></a>RNN Vanishing Gradients Problem</h2><h3 id="What-is-gradients-vanishing"><a href="#What-is-gradients-vanishing" class="headerlink" title="What is gradients vanishing?"></a>What is gradients vanishing?</h3><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 5.58.02 PM.png" alt="creen Shot 2019-08-04 at 5.58.02 P"></p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 6.01.14 PM.png" alt="creen Shot 2019-08-04 at 6.01.14 P"></p><h3 id="Why-it-is-a-problem"><a href="#Why-it-is-a-problem" class="headerlink" title="Why it is a problem?"></a>Why it is a problem?</h3><p>The information from long-term timesteps is gone.</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 6.11.11 PM.png" alt="creen Shot 2019-08-04 at 6.11.11 P"></p><p>If the gradient becomes vanishingly small over longer distances (step t to step t+n), then we can’t tell whether:</p><ol><li>There’s no dependency between step t and t+n in the data</li><li>We have wrong parameters to capture the true dependency between t and t+n</li></ol><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 6.18.31 PM.png" alt="creen Shot 2019-08-04 at 6.18.31 P"></p><h1 id="Long-Short-Term-Memory-networks"><a href="#Long-Short-Term-Memory-networks" class="headerlink" title="Long Short Term Memory networks"></a><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Long Short Term Memory networks</a></h1><p>It is usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.</p><p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-SimpleRNN.png" alt="STM3-SimpleRN"></p><p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-chain.png" alt="STM3-chai"></p><p>For now, let’s just try to get comfortable with the notation we’ll be using.</p><p><img src="/2019/03/02/DP-RNN/LSTM2-notation.png" alt="STM2-notatio"></p><p>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.</p><h2 id="The-core-idea-behind-LSTMs"><a href="#The-core-idea-behind-LSTMs" class="headerlink" title="The core idea behind LSTMs"></a>The core idea behind LSTMs</h2><p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.</p><p>The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-C-line.png" alt="STM3-C-lin"></p><p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</p><p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.<img src="/2019/03/02/DP-RNN/LSTM3-gate.png" alt="STM3-gat"></p><p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!” An LSTM has three of these gates, to protect and control the cell state.</p><h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”</p><p>Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-focus-f.png" alt="STM3-focus-"></p><p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, $\tilde C$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.</p><p>In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-focus-i.png" alt="STM3-focus-"></p><p>It’s now time to update the old cell state, $C_{t-1}$, into the new cell state $C_t$. The previous steps already decided what to do, we just need to actually do it.</p><p>We multiply the old state by $f_t$, forgetting the things we decided to forget earlier. Then we add $i_t* \tilde C_{t}$.</p><p> This is the new candidate values, scaled by how much we decided to update each state value.</p><p>In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-focus-C.png" alt="STM3-focus-"></p><p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between -1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</p><p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.</p><p><img src="/2019/03/02/DP-RNN/LSTM3-focus-o-2079403.png" alt="STM3-focus-o-207940"></p><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>Recall that the forward pass of LSTM is like:</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-07-01 at 4.30.58 PM.png" alt="creen Shot 2019-07-01 at 4.30.58 P"></p><p>The unrolled network during the forward pass is shown above. The cell state at time T, $c^T$ is  responsible for computing h as well as the next cell state $c^{T+1}$. At each time step, the cell output h is shown to be passed to some more layers on which cost function C is computed, as the way an LSTM would be used in a typical application like captioning or language modeling. <a href="http://arunmallya.github.io/writeups/nn/lstm/index.html#/6" target="_blank" rel="noopener">source</a></p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-07-01 at 4.34.08 PM.png" alt="creen Shot 2019-07-01 at 4.34.08 P"></p><p>The unrolled network during the backward pass is shown below. All the arrows in the previous slide have now changed their direction. The cell state at time T, $c^{T}$ receive gradients from $h^T$ as well as the next cell state $c^{T+1}$. </p><p>Note that for the last node, since it dosen’t have a next time stamp, it receive no gradients from $c^{T+1}$ and $h^T$, which means $d_{next_state} = 0$ and $d_{next_hidden}=0$. </p><h2 id="Numpy-Implementation-1"><a href="#Numpy-Implementation-1" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_forward</span><span class="params">(x, prev_h, prev_c, Wx, Wh, b)</span>:</span></div><div class="line"></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Forward pass for a single timestep of an LSTM.</span></div><div class="line"><span class="string">The input data has dimension D, the hidden state has dimension H, and we use</span></div><div class="line"><span class="string">a minibatch size of N.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Note that a sigmoid() function has already been provided for you in this file.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Inputs:</span></div><div class="line"><span class="string">- x: Input data, of shape (N, D)</span></div><div class="line"><span class="string">- prev_h: Previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">- prev_c: previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">- Wx: Input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">- Wh: Hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">- b: Biases, of shape (4H,)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Returns a tuple of:</span></div><div class="line"><span class="string">- next_h: Next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">- next_c: Next cell state, of shape (N, H)</span></div><div class="line"><span class="string">- cache: Tuple of values needed for backward pass.</span></div><div class="line"><span class="string">"""</span></div><div class="line">next_h, next_c, cache = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line"><span class="comment">#############################################################################</span></div><div class="line"><span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for a single timestep of an LSTM.        #</span></div><div class="line"><span class="comment"># You may want to use the numerically stable sigmoid implementation above.  #</span></div><div class="line"><span class="comment">#############################################################################</span></div><div class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">N,D = x.shape</div><div class="line">_,H = prev_h.shape</div><div class="line"></div><div class="line">gates = x.dot(Wx) + prev_h.dot(Wh) + b <span class="comment"># (N, 4H)</span></div><div class="line">gate_i = sigmoid(gates[:,:H]) <span class="comment"># (N,H)</span></div><div class="line">gate_f = sigmoid(gates[:,H:<span class="number">2</span>*H])</div><div class="line">gate_o = sigmoid(gates[:,<span class="number">2</span>*H:<span class="number">3</span>*H])</div><div class="line">gate_g = np.tanh(gates[:,<span class="number">3</span>*H:])</div><div class="line"></div><div class="line">next_c = prev_c*gate_f + gate_g*gate_i</div><div class="line">next_h = gate_o*np.tanh(next_c)</div><div class="line"></div><div class="line">cache = (x, prev_h, prev_c, Wx, Wh, gate_i, gate_f, gate_o, gate_g, next_c)</div><div class="line"></div><div class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"><span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line"><span class="comment">##############################################################################</span></div><div class="line"></div><div class="line"><span class="keyword">return</span> next_h, next_c, cache</div></pre></td></tr></table></figure></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_step_backward</span><span class="params">(dnext_h, dnext_c, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Backward pass for a single timestep of an LSTM.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - dnext_h: Gradients of next hidden state, of shape (N, H)</span></div><div class="line"><span class="string">    - dnext_c: Gradients of next cell state, of shape (N, H)</span></div><div class="line"><span class="string">    - cache: Values from the forward pass</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - dx: Gradient of input data, of shape (N, D)</span></div><div class="line"><span class="string">    - dprev_h: Gradient of previous hidden state, of shape (N, H)</span></div><div class="line"><span class="string">    - dprev_c: Gradient of previous cell state, of shape (N, H)</span></div><div class="line"><span class="string">    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)</span></div><div class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)</span></div><div class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    dx, dprev_h, dprev_c, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="comment">#############################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for a single timestep of an LSTM.       #</span></div><div class="line">    <span class="comment">#                                                                           #</span></div><div class="line">    <span class="comment"># HINT: For sigmoid and tanh you can compute local derivatives in terms of  #</span></div><div class="line">    <span class="comment"># the output value from the nonlinearity.                                   #</span></div><div class="line">    <span class="comment">#############################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    </div><div class="line">    (x, prev_h, prev_c, Wx, Wh, gate_i, gate_f, gate_o, gate_g, next_c) = cache</div><div class="line">    N,H = dnext_h.shape</div><div class="line">    _,D = x.shape</div><div class="line">    </div><div class="line">    dgate_o = dnext_h*np.tanh(next_c)</div><div class="line">    dtanh_next_c = dnext_h*gate_o <span class="comment">#(N, H)</span></div><div class="line">    </div><div class="line">    dnext_c_ = dtanh_next_c*(<span class="number">1</span>-np.tanh(next_c)**<span class="number">2</span>)</div><div class="line">    dnext_c = dnext_c_ + dnext_c</div><div class="line">    </div><div class="line">    dgate_f = dnext_c*prev_c</div><div class="line">    dprev_c = dnext_c*gate_f</div><div class="line">    </div><div class="line">    dgate_g = dnext_c*gate_i</div><div class="line">    dgate_i = dnext_c*gate_g</div><div class="line">    </div><div class="line">    dgate_g = dgate_g*(<span class="number">1</span>-gate_g**<span class="number">2</span>)</div><div class="line">    dgate_o = dgate_o*gate_o*(<span class="number">1</span>-gate_o)</div><div class="line">    dgate_f = dgate_f*gate_f*(<span class="number">1</span>-gate_f)</div><div class="line">    dgate_i = dgate_i*gate_i*(<span class="number">1</span>-gate_i)</div><div class="line">    </div><div class="line">    dgate = np.concatenate([dgate_i,dgate_f,dgate_o,dgate_g],axis=<span class="number">1</span>)</div><div class="line">    db = np.sum(dgate,axis=<span class="number">0</span>)</div><div class="line">    dx = dgate.dot(Wx.T)</div><div class="line">    dprev_h = dgate.dot(Wh.T)</div><div class="line">    dWx = x.T.dot(dgate)</div><div class="line">    dWh = prev_h.T.dot(dgate)</div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> dx, dprev_h, dprev_c, dWx, dWh, db</div></pre></td></tr></table></figure></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, h0, Wx, Wh, b)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Forward pass for an LSTM over an entire sequence of data. We assume an input</span></div><div class="line"><span class="string">    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden</span></div><div class="line"><span class="string">    size of H, and we work over a minibatch containing N sequences. After running</span></div><div class="line"><span class="string">    the LSTM forward, we return the hidden states for all timesteps.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Note that the initial cell state is passed as input, but the initial cell</span></div><div class="line"><span class="string">    state is set to zero. Also note that the cell state is not returned; it is</span></div><div class="line"><span class="string">    an internal variable to the LSTM and is not accessed from outside.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - x: Input data of shape (N, T, D)</span></div><div class="line"><span class="string">    - h0: Initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)</span></div><div class="line"><span class="string">    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)</span></div><div class="line"><span class="string">    - b: Biases of shape (4H,)</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)</span></div><div class="line"><span class="string">    - cache: Values needed for the backward pass.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    h, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="comment">#############################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for an LSTM over an entire timeseries.   #</span></div><div class="line">    <span class="comment"># You should use the lstm_step_forward function that you just defined.      #</span></div><div class="line">    <span class="comment">#############################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    cache = &#123;&#125;</div><div class="line">    N, T, D = x.shape</div><div class="line">    _, H = h0.shape</div><div class="line">    h = np.zeros((N,T,H))</div><div class="line">    prev_h = h0</div><div class="line">    prev_c = np.zeros_like(h0)</div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</div><div class="line">        prev_h, prev_c, cache[t] = lstm_step_forward(x[:,t,:], prev_h, prev_c, Wx, Wh, b)</div><div class="line">        h[:,t,:] = prev_h</div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> h, cache</div></pre></td></tr></table></figure></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(dh, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Backward pass for an LSTM over an entire sequence of data.]</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - dh: Upstream gradients of hidden states, of shape (N, T, H)</span></div><div class="line"><span class="string">    - cache: Values from the forward pass</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - dx: Gradient of input data of shape (N, T, D)</span></div><div class="line"><span class="string">    - dh0: Gradient of initial hidden state of shape (N, H)</span></div><div class="line"><span class="string">    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)</span></div><div class="line"><span class="string">    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)</span></div><div class="line"><span class="string">    - db: Gradient of biases, of shape (4H,)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    dx, dh0, dWx, dWh, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="comment">#############################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for an LSTM over an entire timeseries.  #</span></div><div class="line">    <span class="comment"># You should use the lstm_step_backward function that you just defined.     #</span></div><div class="line">    <span class="comment">#############################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">    N, T, H = dh.shape</div><div class="line">    _, D = cache[<span class="number">0</span>][<span class="number">0</span>].shape</div><div class="line">    dx = np.zeros((N,T,D))</div><div class="line">    dWx = np.zeros((D,<span class="number">4</span>*H))</div><div class="line">    dWh = np.zeros((H,<span class="number">4</span>*H))</div><div class="line">    db = np.zeros((<span class="number">4</span>*H))</div><div class="line">    dprev_c = np.zeros((N, H)) </div><div class="line">    dprev_h = np.zeros((N, H)) </div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</div><div class="line">        dnext_h = dprev_h + dh[:,t,:]</div><div class="line">        dnext_c = dprev_c</div><div class="line">        dx[:,t,:], dprev_h, dprev_c, dWx_, dWh_, db_ = lstm_step_backward(dnext_h, dnext_c, cache[t])</div><div class="line">        dWx += dWx_</div><div class="line">        dWh += dWh_</div><div class="line">        db += db_</div><div class="line">    dh0 = dprev_h</div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line">    <span class="comment">#                               END OF YOUR CODE                             #</span></div><div class="line">    <span class="comment">##############################################################################</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> dx, dh0, dWx, dWh, db</div></pre></td></tr></table></figure></div></div><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a><a href="https://www.d2l.ai/chapter_recurrent-neural-networks/gru.html" target="_blank" rel="noopener">GRU</a></h1><p>So now we know how an LSTM work, let’s briefly look at the GRU. The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-20 at 12.39.27 PM.png" alt="creen Shot 2019-06-20 at 12.39.27 P"></p><p><strong>Reset Gate</strong></p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-20 at 12.49.39 PM.png" alt="creen Shot 2019-06-20 at 12.49.39 P"></p><script type="math/tex; mode=display">\mathbf{R}_{t}=\sigma\left(\mathbf{X}_{t} \mathbf{W}_{x r}+\mathbf{H}_{t-1} \mathbf{W}_{h r}+\mathbf{b}_{r}\right)</script><script type="math/tex; mode=display">\mathbf{Z}_{t}=\sigma\left(\mathbf{X}_{t} \mathbf{W}_{x z}+\mathbf{H}_{t-1} \mathbf{W}_{h z}+\mathbf{b}_{z}\right)</script><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-06-20 at 12.51.45 PM.png" alt="creen Shot 2019-06-20 at 12.51.45 P"></p><script type="math/tex; mode=display">\tilde{\mathbf{H}}_{t}=\tanh \left(\mathbf{X}_{t} \mathbf{W}_{x h}+\left(\mathbf{R}_{t} \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{h h}+\mathbf{b}_{h}\right)</script><script type="math/tex; mode=display">\mathbf{H}_{t}=\mathbf{Z}_{t} \odot \mathbf{H}_{t-1}+\left(1-\mathbf{Z}_{t}\right) \odot \tilde{\mathbf{H}}_{t}</script><h1 id="How-LSTM-GRU-Solve-Vanishing-Gradients"><a href="#How-LSTM-GRU-Solve-Vanishing-Gradients" class="headerlink" title="How LSTM/GRU Solve Vanishing Gradients"></a>How LSTM/GRU Solve Vanishing Gradients</h1><p>In order to understand this question, we need to introdcue shortcut firstly. In ResNet, the shortcut is like:</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 8.51.23 PM.png" alt="creen Shot 2019-08-04 at 8.51.23 P"></p><p>which is $x^{(t)}=x^{(t-1)}+F(x^{(t-1)})$. The next output is the combination of two parts: shortcut($x^{(t-1)}$) and non-linear transformation of $x^{(t-1)}$. Because of the shortcut, there is always a “1” gradient flowing back in backpropagation. In some extent, it sovles the problem of vanishing gradient.</p><p>Now back to our fancy RNN, take GRU as example, the updating rules of GRU is:</p><p><img src="/2019/03/02/DP-RNN/Screen Shot 2019-08-04 at 8.51.07 PM.png" alt="creen Shot 2019-08-04 at 8.51.07 P"></p><p>Let’s focus on how we get the new hidden state $h^{(t)}$ and let’s roll out the formulation:</p><script type="math/tex; mode=display">\boldsymbol{h}^{(t)}=\boldsymbol{h}^{(t-1)}+ \left(\tilde{\boldsymbol{h}}^{(t)}-\boldsymbol{h}^{(t-1)}\right)  \circ  \boldsymbol{u}^{(t)}</script><p>which is very similar to the ResNet becasue there is a direct flow from previous state to the next state.</p><h1 id="Keras-Learn-the-Alphabet"><a href="#Keras-Learn-the-Alphabet" class="headerlink" title="Keras - Learn the Alphabet"></a>Keras - Learn the Alphabet</h1><p>In this implementation, we are going to develop and constrast a number of different LSTM networks. The task we are going to perform is that given a letter of the alphabet, predict the next letter. This is a simple sequence prediction problem that once understood can be generalized to other sequence prediction problems like time series prediction and sequence classification.</p><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line">np.random.seed(<span class="number">7</span>)</div><div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div><div class="line">char2int = defaultdict(int)</div><div class="line">int2char = defaultdict(str)</div><div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div><div class="line">    char2int[val] = idx</div><div class="line">    int2char[idx] = val</div></pre></td></tr></table></figure><p>Because neural network can only process number, we map the letters to integer value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">seq_length = <span class="number">1</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div><div class="line">    seq_in = alphabet[i:i+seq_length]</div><div class="line">    seq_out = alphabet[i+seq_length]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div></pre></td></tr></table></figure><p>Here, we create our input dataset and corresponding output dataset; we use an input length of 1. Running the code will produce the following output:</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">A -&gt; B</div><div class="line">B -&gt; C</div><div class="line">C -&gt; D</div><div class="line">...</div><div class="line">X -&gt; Y</div><div class="line">Y -&gt; Z</div></pre></td></tr></table></figure><p>Then, we need to reshape our data into a format expected by the LSTM networks, that is, <code>[data_size,time_steps,feature_num]</code>. Then we can normalize the input integers to the range <code>[0,1]</code>. Finally, we can think of this problem as a sequence classification task, where each of the 26 letters represents a different class. As such, we can convert the output (y) to a one hot encoding, using the Keras built-in function <strong>to_categorical()</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line">y = np_utils.to_categorical(dataY)</div></pre></td></tr></table></figure><h2 id="One-Char-to-One-Char"><a href="#One-Char-to-One-Char" class="headerlink" title="One-Char to One-Char"></a>One-Char to One-Char</h2><p>Let’s start off by designing a simple LSTM to learn how to predict the next character in the alphabet given the context of just one character.</p><p>Let’s define an LSTM network with 32 units and an output layer with a softmax activation function for making predictions. Because this is a multi-class classification problem, we can use the log loss function (called “<strong>categorical_crossentropy</strong>” in Keras), and optimize the network using the ADAM optimization function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line">model.add(LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div><div class="line">model.add(Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(X,y,epochs=<span class="number">500</span>,batch_size=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>After we fit the model we can evaluate and summarize the performance on the entire training dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scores = model.evaluate(X,y)</div><div class="line">print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div></pre></td></tr></table></figure><p>We can then re-run the training data through the network and generate predictions, converting both the input and output pairs back into their original character format to get a visual idea of how well the network learned the problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div><div class="line">    x = np.reshape(pattern,(<span class="number">1</span>,len(pattern),<span class="number">1</span>))</div><div class="line">    x = x/float(len(alphabet))</div><div class="line">    prediction = model.predict(x)</div><div class="line">    index = np.argmax(prediction)</div><div class="line">    result = int2char[index]</div><div class="line">    seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,result)</div></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">84.00</span>%</div><div class="line">['A'] -&gt; B</div><div class="line">['B'] -&gt; C</div><div class="line">['C'] -&gt; D</div><div class="line">['D'] -&gt; E</div><div class="line">['E'] -&gt; F</div><div class="line">['F'] -&gt; G</div><div class="line">['G'] -&gt; H</div><div class="line">['H'] -&gt; I</div><div class="line">['I'] -&gt; J</div><div class="line">['J'] -&gt; K</div><div class="line">['K'] -&gt; L</div><div class="line">['L'] -&gt; M</div><div class="line">['M'] -&gt; N</div><div class="line">['N'] -&gt; O</div><div class="line">['O'] -&gt; P</div><div class="line">['P'] -&gt; Q</div><div class="line">['Q'] -&gt; R</div><div class="line">['R'] -&gt; S</div><div class="line">['S'] -&gt; T</div><div class="line">['T'] -&gt; U</div><div class="line">['U'] -&gt; W</div><div class="line">['V'] -&gt; Y</div><div class="line">['W'] -&gt; Z</div><div class="line">['X'] -&gt; Z</div><div class="line">['Y'] -&gt; Z</div></pre></td></tr></table></figure><p>We can see that this problem is indeed difficult for the network to learn. The reason is, the poor LSTM units do not have any context to work with. Each input-output pattern is shown to the network in a random order and the state of the network is reset after each pattern (each batch where each batch contains one pattern). This is abuse of the LSTM network architecture, treating it like a standard multilayer Perceptron. Next, let’s try a different framing of the problem in order to provide more sequence to the network from which to learn.</p><h2 id="A-Three-Char-Feature-Window-to-One-Char-Mapping"><a href="#A-Three-Char-Feature-Window-to-One-Char-Mapping" class="headerlink" title="A Three-Char Feature Window to One-Char Mapping"></a>A Three-Char Feature Window to One-Char Mapping</h2><p>A popular approach to adding more context to data for multilayer Perceptrons is to use the window method. We can do this by increasing the input sequence length length from 1 to 3, for example:<code>seq_length=3</code>, which creates training patterns like:</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ABC -&gt; D</div><div class="line">BCD -&gt; E</div><div class="line">CDE -&gt; F</div></pre></td></tr></table></figure><p>Each element in the sequence is then provided as a new input feature to the network. This requires a modification of how the input sequences reshaped in the data preparation step:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># reshape X to be #[batch_size,time_stamps,features]</span></div><div class="line">X = numpy.reshape(dataX, (len(dataX), <span class="number">1</span>, seq_length))</div></pre></td></tr></table></figure><p>The entire code is provided below for completeness.</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"></div><div class="line">np.random.seed(<span class="number">7</span>)</div><div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div><div class="line">char2int = defaultdict(int)</div><div class="line">int2char = defaultdict(str)</div><div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div><div class="line">    char2int[val] = idx</div><div class="line">    int2char[idx] = val</div><div class="line"></div><div class="line">seq_length = <span class="number">3</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div><div class="line">    seq_in = alphabet[i:i+seq_length]</div><div class="line">    seq_out = alphabet[i+seq_length]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div><div class="line"></div><div class="line">input_shape = (len(dataX),<span class="number">1</span>,seq_length)</div><div class="line"><span class="comment"># input_shape = (len(dataX),1,seq_length)</span></div><div class="line"></div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line"></div><div class="line">y = np_utils.to_categorical(dataY)</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div><div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div><div class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div><div class="line">        x = np.reshape(pattern,(<span class="number">1</span>,<span class="number">1</span>,len(pattern)))</div><div class="line">        x = x/float(len(alphabet))</div><div class="line">        prediction = model.predict(x)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div><div class="line">        print(seq_in,<span class="string">'-&gt;'</span>,result)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div><div class="line">    model.fit(X,y,epochs=<span class="number">500</span>,batch_size=<span class="number">1</span>,verbose=<span class="number">1</span>)</div><div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div><div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div><div class="line">    predict(model)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    naive_lstm()</div></pre></td></tr></table></figure></div></div><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">86.96</span>%</div><div class="line">['A', 'B', 'C'] -&gt; D</div><div class="line">['B', 'C', 'D'] -&gt; E</div><div class="line">...</div><div class="line">['T', 'U', 'V'] -&gt; X</div><div class="line">['U', 'V', 'W'] -&gt; Z</div><div class="line">['V', 'W', 'X'] -&gt; Z</div><div class="line">['W', 'X', 'Y'] -&gt; Z</div></pre></td></tr></table></figure><p>We can see a little improvement in the performance that may or may not be true.</p><p>Again, this is a misuse of the LSTM network by a poor framing of the problem. Indeed, the sequences of letters are time steps of one feature rather than one time step of separate features. We have given more context to the network, but not more sequence as it expected.</p><p>In the next section, we will give more context to the network in the form of time steps.</p><h2 id="A-Three-Char-Time-Step-Window-to-One-Char-Mapping"><a href="#A-Three-Char-Time-Step-Window-to-One-Char-Mapping" class="headerlink" title="A Three-Char Time Step Window to One-Char Mapping"></a>A Three-Char Time Step Window to One-Char Mapping</h2><p>We still take as input a sequence with length being 3, <code>seq_length=3</code>. The difference is that the reshaping of the input data takes the sequence as a time step sequence of one feature, rather than a single time step of multiple features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div></pre></td></tr></table></figure><p>This is the correct intended use of providing sequence context to your LSTM in Keras. The full code example is provided below for completeness.</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"></div><div class="line">np.random.seed(<span class="number">7</span>)</div><div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div><div class="line">char2int = defaultdict(int)</div><div class="line">int2char = defaultdict(str)</div><div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div><div class="line">    char2int[val] = idx</div><div class="line">    int2char[idx] = val</div><div class="line"></div><div class="line">seq_length = <span class="number">3</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div><div class="line">    seq_in = alphabet[i:i+seq_length]</div><div class="line">    seq_out = alphabet[i+seq_length]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div><div class="line"></div><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line"></div><div class="line">y = np_utils.to_categorical(dataY)</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div><div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div><div class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div><div class="line">        x = np.reshape(pattern,(<span class="number">1</span>,len(pattern),<span class="number">1</span>))</div><div class="line">        x = x/float(len(alphabet))</div><div class="line">        prediction = model.predict(x)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div><div class="line">        print(seq_in,<span class="string">'-&gt;'</span>,result)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div><div class="line">    model.fit(X,y,epochs=<span class="number">500</span>,batch_size=<span class="number">1</span>,verbose=<span class="number">1</span>)</div><div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div><div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div><div class="line">    predict(model)</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    naive_lstm()</div></pre></td></tr></table></figure></div></div><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">100.00</span>%</div><div class="line">['A', 'B', 'C'] -&gt; D</div><div class="line">...</div><div class="line">['W', 'X', 'Y'] -&gt; Z</div></pre></td></tr></table></figure><p>We can see that the model learns the problem perfectly as evidenced by the model evaluation and the example predictions.</p><h2 id="LSTM-State-Within-A-Batch"><a href="#LSTM-State-Within-A-Batch" class="headerlink" title="LSTM State Within A Batch"></a>LSTM State Within A Batch</h2><p>The Keras implementation of LSTMs resets the state of the network after each batch.</p><p>This suggests that if we had a batch size large enough to hold all input patterns and if all the input patterns were ordered sequentially, that the LSTM could use the context of the sequence within the batch to better learn the sequence.</p><p>We can demonstrate this easily by modifying the first example for learning a one-to-one mapping and increasing the batch size from 1 to the size of the training dataset.</p><p>Additionally, Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.</p><p>And the training epoch becomes 5000 from 500.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X,y,epochs=<span class="number">5000</span>,batch_size=len(X),verbose=<span class="number">1</span>)</div></pre></td></tr></table></figure><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line">np.random.seed(<span class="number">7</span>)</div><div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div><div class="line">char2int = defaultdict(int)</div><div class="line">int2char = defaultdict(str)</div><div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div><div class="line">    char2int[val] = idx</div><div class="line">    int2char[idx] = val</div><div class="line">seq_length = <span class="number">1</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div><div class="line">    seq_in = alphabet[i:i+seq_length]</div><div class="line">    seq_out = alphabet[i+seq_length]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div><div class="line">dataX = pad_sequences(dataX,maxlen=seq_length,dtype=<span class="string">'float32'</span>)</div><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line">y = np_utils.to_categorical(dataY)</div><div class="line">model = Sequential()</div><div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div><div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div><div class="line">    <span class="keyword">for</span> pattern <span class="keyword">in</span> dataX:</div><div class="line">        x = np.reshape(pattern,(<span class="number">1</span>,len(pattern),<span class="number">1</span>))</div><div class="line">        x = x/float(len(alphabet))</div><div class="line">        prediction = model.predict(x)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div><div class="line">        print(seq_in,<span class="string">'-&gt;'</span>,result)</div><div class="line">    print(<span class="string">"Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</div><div class="line">        idx = np.random.randint(len(X))</div><div class="line">        pattern = dataX[idx] <span class="comment"># X is normalized float but dataX is integer.</span></div><div class="line">        x = np.reshape(pattern, (<span class="number">1</span>, len(pattern), <span class="number">1</span>))</div><div class="line">        x = x / float(len(alphabet))</div><div class="line">        prediction = model.predict(x)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        seq_in = [int2char[id] <span class="keyword">for</span> id <span class="keyword">in</span> pattern]</div><div class="line">        print(seq_in, <span class="string">'-&gt;'</span>, result)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div><div class="line">    model.fit(X,y,epochs=<span class="number">5000</span>,batch_size=len(X),verbose=<span class="number">1</span>)</div><div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div><div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div><div class="line">    predict(model)</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    naive_lstm()</div></pre></td></tr></table></figure></div></div><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">100.00</span>%</div><div class="line">['A'] -&gt; B</div><div class="line">['B'] -&gt; C</div><div class="line">...</div><div class="line">['X'] -&gt; Y</div><div class="line">['Y'] -&gt; Z</div><div class="line">Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</div><div class="line">['T'] -&gt; U</div><div class="line">['N'] -&gt; O</div><div class="line">...</div><div class="line">['S'] -&gt; T</div><div class="line">['T'] -&gt; U</div><div class="line">['R'] -&gt; S</div></pre></td></tr></table></figure><p>As we expected, the network is able to use the within-sequence context to learn the alphabet, achieving 100% accuracy on the training data.</p><p>Importantly, the network can make accurate predictions for the next letter in the alphabet for randomly selected characters. </p><h2 id="Stateful-LSTM-for-a-One-Char-to-One-Char-Mapping"><a href="#Stateful-LSTM-for-a-One-Char-to-One-Char-Mapping" class="headerlink" title="Stateful LSTM for a One-Char to One-Char Mapping"></a>Stateful LSTM for a One-Char to One-Char Mapping</h2><p>Ideally, we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem.</p><p>We can do this in Keras by making the LSTM layers stateful and manually resetting the state of the network at the end of the epoch, which is also the end of the training sequence.</p><p>This is truly how the LSTM networks are intended to be used. We find that by allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half).</p><p>We first need to define our LSTM layer as stateful. In so doing, we must explicitly specify the batch size as a dimension on the input shape. This also means that when we evaluate the network or make predictions, we must also specify and adhere to this same batch size. This is not a problem now as we are using a batch size of 1. This could introduce difficulties when making predictions when the batch size is not one as predictions will need to be made in batch and in sequence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">batch_size = <span class="number">1</span></div><div class="line">model.add(LSTM(<span class="number">50</span>, batch_input_shape=(batch_size, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>]), stateful=<span class="keyword">True</span>))</div></pre></td></tr></table></figure><p>An important difference in training the stateful LSTM is that we train it manually one epoch at a time and reset the state after each epoch. We can do this in a for loop. Again, we do not shuffle the input, preserving the sequence in which the input training data was created.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">model.fit(X, y, epochs=<span class="number">1</span>, batch_size=batch_size, verbose=<span class="number">2</span>, shuffle=<span class="keyword">False</span>)</div><div class="line">model.reset_states()</div></pre></td></tr></table></figure><p>As mentioned, we specify the batch size when evaluating the performance of the network on the entire training dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># summarize performance of the model</span></div><div class="line">scores = model.evaluate(X, y, batch_size=batch_size, verbose=<span class="number">0</span>)</div><div class="line">model.reset_states()</div><div class="line">print(<span class="string">"Model Accuracy: %.2f%%"</span> % (scores[<span class="number">1</span>]*<span class="number">100</span>))</div></pre></td></tr></table></figure><p>Finally, we can demonstrate that the network has indeed learned the entire alphabet. We can seed it with the first letter “A”, request a prediction, feed the prediction back in as an input, and repeat the process all the way to “Z”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># demonstrate some model predictions</span></div><div class="line">seed = [char_to_int[alphabet[<span class="number">0</span>]]]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphabet)<span class="number">-1</span>):</div><div class="line">x = numpy.reshape(seed, (<span class="number">1</span>, len(seed), <span class="number">1</span>))</div><div class="line">x = x / float(len(alphabet))</div><div class="line">prediction = model.predict(x, verbose=<span class="number">0</span>)</div><div class="line">index = numpy.argmax(prediction)</div><div class="line">print(int_to_char[seed[<span class="number">0</span>]], <span class="string">"-&gt;"</span>, int_to_char[index])</div><div class="line">seed = [index]</div><div class="line">model.reset_states()</div></pre></td></tr></table></figure><p>We can also see if the network can make predictions starting from an arbitrary letter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># demonstrate a random starting point</span></div><div class="line">letter = <span class="string">"K"</span></div><div class="line">seed = [char_to_int[letter]]</div><div class="line">print(<span class="string">"New start: "</span>, letter)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">5</span>):</div><div class="line">x = numpy.reshape(seed, (<span class="number">1</span>, len(seed), <span class="number">1</span>))</div><div class="line">x = x / float(len(alphabet))</div><div class="line">prediction = model.predict(x, verbose=<span class="number">0</span>)</div><div class="line">index = numpy.argmax(prediction)</div><div class="line">print(int_to_char[seed[<span class="number">0</span>]], <span class="string">"-&gt;"</span>, int_to_char[index])</div><div class="line">seed = [index]</div><div class="line">model.reset_states()</div></pre></td></tr></table></figure><p>The entire code listing is provided below for completeness.</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line">np.random.seed(<span class="number">7</span>)</div><div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div><div class="line">char2int = defaultdict(int)</div><div class="line">int2char = defaultdict(str)</div><div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div><div class="line">    char2int[val] = idx</div><div class="line">    int2char[idx] = val</div><div class="line"></div><div class="line">seq_length = <span class="number">1</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(alphabet)-seq_length):</div><div class="line">    seq_in = alphabet[i:i+seq_length]</div><div class="line">    seq_out = alphabet[i+seq_length]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div><div class="line">dataX = pad_sequences(dataX,maxlen=seq_length,dtype=<span class="string">'float32'</span>)</div><div class="line">input_shape = (len(dataX),seq_length,<span class="number">1</span>)</div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line"></div><div class="line">y = np_utils.to_categorical(dataY)</div><div class="line"></div><div class="line">batch_size = <span class="number">1</span></div><div class="line">model = Sequential()</div><div class="line">model.add(layers.LSTM(<span class="number">50</span>,batch_input_shape=(batch_size,X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>]),stateful=<span class="keyword">True</span>))</div><div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div><div class="line">    seed = [char2int[alphabet[<span class="number">0</span>]]]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(alphabet)<span class="number">-1</span>):</div><div class="line">        x = np.reshape(seed,(<span class="number">1</span>,len(seed),<span class="number">1</span>))</div><div class="line">        x = x/float(len(alphabet))</div><div class="line">        prediction = model.predict(x)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        print(int2char[seed[<span class="number">0</span>]],<span class="string">'-&gt;'</span>,result)</div><div class="line">        seed = [index]</div><div class="line">    model.reset_states()</div><div class="line"></div><div class="line">    print(<span class="string">"Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"</span>)</div><div class="line">    seed = [char2int[<span class="string">'K'</span>]]</div><div class="line">    print(<span class="string">"New start: "</span>, <span class="string">'K'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">        x = np.reshape(seed, (<span class="number">1</span>, len(seed), <span class="number">1</span>))</div><div class="line">        x = x / float(len(alphabet))</div><div class="line">        prediction = model.predict(x)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        print(int2char[seed[<span class="number">0</span>]], <span class="string">'-&gt;'</span>, result)</div><div class="line">        seed = [index]</div><div class="line">    model.reset_states()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">        model.fit(X,y,epochs=<span class="number">1</span>,batch_size=batch_size,verbose=<span class="number">1</span>,shuffle=<span class="keyword">False</span>)</div><div class="line">        model.reset_states()</div><div class="line">    scores = model.evaluate(X,y,batch_size=batch_size,verbose=<span class="number">1</span>)</div><div class="line">    model.reset_states()</div><div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div><div class="line">    predict(model)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    naive_lstm()</div></pre></td></tr></table></figure></div></div><p>Running the example provides the following output.</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">96.00</span>%</div><div class="line">A -&gt; B</div><div class="line">B -&gt; C</div><div class="line">...</div><div class="line">W -&gt; X</div><div class="line">X -&gt; Y</div><div class="line">Y -&gt; Y</div><div class="line">Random Test &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</div><div class="line">New start:  K</div><div class="line">K -&gt; B</div><div class="line">B -&gt; C</div><div class="line">C -&gt; D</div><div class="line">D -&gt; E</div><div class="line">E -&gt; F</div></pre></td></tr></table></figure><p>We can see that the network has memorized the entire alphabet perfectly. It used the context of the samples themselves and learned whatever dependency it needed to predict the next character in the sequence.</p><p>We can also see that if we seed the network with the first letter, that it can correctly rattle off the rest of the alphabet.</p><p>We can also see that it has only learned the full alphabet sequence and that from a cold start. When asked to predict the next letter from “K” that it predicts “B” and falls back into regurgitating the entire alphabet.</p><p>To truly predict “K” the state of the network would need to be warmed up iteratively fed the letters from “A” to “J”. This tells us that we could achieve the same effect with a “stateless” LSTM by preparing training data like:</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">---a -&gt; b</div><div class="line">--ab -&gt; c</div><div class="line">-abc -&gt; d</div><div class="line">abcd -&gt; e</div></pre></td></tr></table></figure><p>Where the input sequence is fixed at 25 (a-to-y to predict z) and patterns are prefixed with zero-padding.</p><p>Finally, this raises the question of training an LSTM network using variable length input sequences to predict the next character.</p><h2 id="LSTM-with-Variable-Length-Input-to-One-Char-Output"><a href="#LSTM-with-Variable-Length-Input-to-One-Char-Output" class="headerlink" title="LSTM with Variable-Length Input to One-Char Output"></a>LSTM with Variable-Length Input to One-Char Output</h2><p>In the previous section, we discovered that the Keras “stateful” LSTM was really only a shortcut to replaying the first n-sequences, but didn’t really help us learn a generic model of the alphabet.</p><p>In this section we explore a variation of the “stateless” LSTM that learns random subsequences of the alphabet and an effort to build a model that can be given arbitrary letters or subsequences of letters and predict the next letter in the alphabet.</p><p>Firstly, we are changing the framing of the problem. To simplify we will define a maximum input sequence length and set it to a small value like 5 to speed up training. This defines the maximum length of subsequences of the alphabet will be drawn for training. In extensions, this could just as set to the full alphabet (26) or longer if we allow looping back to the start of the sequence.</p><p>We also need to define the number of random sequences to create, in this case 1000. This too could be more or less. I expect less patterns are actually required.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">num_inputs = <span class="number">1000</span></div><div class="line">max_len = <span class="number">5</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_inputs):</div><div class="line">    start = np.random.randint(len(alphabet)<span class="number">-2</span>)</div><div class="line">    end = np.random.randint(start,min(start+max_len,len(alphabet)<span class="number">-1</span>))</div><div class="line">    seq_in = alphabet[start:end+<span class="number">1</span>]</div><div class="line">    seq_out = alphabet[end+<span class="number">1</span>]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div><div class="line">dataX = pad_sequences(dataX,maxlen=max_len,dtype=<span class="string">'float32'</span>)</div><div class="line">input_shape = (len(dataX),max_len,<span class="number">1</span>)</div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line">y = np_utils.to_categorical(dataY)</div></pre></td></tr></table></figure><p>Running the code, we create input patterns that look like the following:</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">STUV -&gt; W</div><div class="line">IJKLM -&gt; N</div><div class="line">STUV -&gt; W</div><div class="line">TUVWX -&gt; Y</div><div class="line">RSTU -&gt; V</div></pre></td></tr></table></figure><p>The input sequences vary in length between 1 and <strong>max_len</strong> and therefore require zero padding. Here, we use left-hand-side (prefix) padding with the Keras built in <strong>pad_sequences()</strong> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dataX = pad_sequences(dataX,maxlen=max_len,dtype=<span class="string">'float32'</span>)</div></pre></td></tr></table></figure><p>The trained model is evaluated on randomly selected input patterns. This could just as easily be new randomly generated sequences of characters. I also believe this could also be a linear sequence seeded with “A” with outputs fes back in as single character inputs.</p><p>The full code listing is provided below for completeness.</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line">np.random.seed(<span class="number">7</span>)</div><div class="line">alphabet = <span class="string">"ABCDEFGHIJKLMNOPQRSTUVWXYZ"</span></div><div class="line">char2int = defaultdict(int)</div><div class="line">int2char = defaultdict(str)</div><div class="line"><span class="keyword">for</span> idx,val <span class="keyword">in</span> enumerate(alphabet):</div><div class="line">    char2int[val] = idx</div><div class="line">    int2char[idx] = val</div><div class="line"></div><div class="line">num_inputs = <span class="number">1000</span></div><div class="line">max_len = <span class="number">5</span></div><div class="line">dataX = []</div><div class="line">dataY = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_inputs):</div><div class="line">    start = np.random.randint(len(alphabet)<span class="number">-2</span>)</div><div class="line">    end = np.random.randint(start,min(start+max_len,len(alphabet)<span class="number">-1</span>))</div><div class="line">    seq_in = alphabet[start:end+<span class="number">1</span>]</div><div class="line">    seq_out = alphabet[end+<span class="number">1</span>]</div><div class="line">    dataX.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_in])</div><div class="line">    dataY.append([char2int[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> seq_out])</div><div class="line">    print(seq_in,<span class="string">'-&gt;'</span>,seq_out)</div><div class="line">dataX = pad_sequences(dataX,maxlen=max_len,dtype=<span class="string">'float32'</span>)</div><div class="line">input_shape = (len(dataX),max_len,<span class="number">1</span>)</div><div class="line"><span class="comment">#[batch_size,time_stamps,features]</span></div><div class="line">X = np.reshape(dataX,input_shape)</div><div class="line">X = X/float(len(alphabet))</div><div class="line">y = np_utils.to_categorical(dataY)</div><div class="line"></div><div class="line">batch_size = <span class="number">1</span></div><div class="line">model = Sequential()</div><div class="line">model.add(layers.LSTM(<span class="number">32</span>,input_shape=(X.shape[<span class="number">1</span>],X.shape[<span class="number">2</span>])))</div><div class="line">model.add(layers.Dense(y.shape[<span class="number">1</span>],activation=<span class="string">'softmax'</span>))</div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</div><div class="line">        pattern_index = np.random.randint(len(dataX))</div><div class="line">        pattern = dataX[pattern_index]</div><div class="line">        x = pad_sequences([pattern], maxlen=max_len, dtype=<span class="string">'float32'</span>)</div><div class="line">        x = np.reshape(x, (<span class="number">1</span>, max_len, <span class="number">1</span>))</div><div class="line">        x = x / float(len(alphabet))</div><div class="line">        prediction = model.predict(x, verbose=<span class="number">0</span>)</div><div class="line">        index = np.argmax(prediction)</div><div class="line">        result = int2char[index]</div><div class="line">        seq_in = [int2char[value] <span class="keyword">for</span> value <span class="keyword">in</span> pattern]</div><div class="line">        print(seq_in, <span class="string">"-&gt;"</span>, result)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_lstm</span><span class="params">()</span>:</span></div><div class="line">    model.fit(X,y,epochs=<span class="number">500</span>,batch_size=batch_size,verbose=<span class="number">1</span>)</div><div class="line">    model.reset_states()</div><div class="line">    scores = model.evaluate(X,y,verbose=<span class="number">1</span>)</div><div class="line">    model.reset_states()</div><div class="line">    print(<span class="string">"Model Accuracy: %.2f%%"</span>%(scores[<span class="number">1</span>]*<span class="number">100</span>))</div><div class="line">    predict(model)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    naive_lstm()</div></pre></td></tr></table></figure></div></div><p>Running this code produces the following output:</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">Model Accuracy: <span class="number">98.90</span>%</div><div class="line">['Q', 'R'] -&gt; S</div><div class="line">['W', 'X'] -&gt; Y</div><div class="line">['W', 'X'] -&gt; Y</div><div class="line">['C', 'D'] -&gt; E</div><div class="line">['E'] -&gt; F</div><div class="line">['S', 'T', 'U'] -&gt; V</div><div class="line">['G', 'H', 'I', 'J', 'K'] -&gt; L</div><div class="line">['O', 'P', 'Q', 'R', 'S'] -&gt; T</div><div class="line">['C', 'D'] -&gt; E</div><div class="line">['O'] -&gt; P</div><div class="line">['N', 'O', 'P'] -&gt; Q</div><div class="line">['D', 'E', 'F', 'G', 'H'] -&gt; I</div><div class="line">['X'] -&gt; Y</div><div class="line">['K'] -&gt; L</div><div class="line">['M'] -&gt; N</div><div class="line">['R'] -&gt; T</div><div class="line">['K'] -&gt; L</div><div class="line">['E', 'F', 'G'] -&gt; H</div><div class="line">['Q'] -&gt; R</div><div class="line">['Q', 'R', 'S'] -&gt; T</div></pre></td></tr></table></figure><p>We can see that although the model did not learn the alphabet perfectly from the randomly generated subsequences, it did very well. The model was not tuned and may require more training or a larger network, or both (an exercise for the reader).</p><p>This is a good natural extension to the “<em>all sequential input examples in each batch</em>” alphabet model learned above in that it can handle ad hoc queries, but this time of arbitrary sequence length (up to the max length).</p><h1 id="PyTorch-Classify-Names-with-a-Character-Level-RNN"><a href="#PyTorch-Classify-Names-with-a-Character-Level-RNN" class="headerlink" title="PyTorch - Classify Names with a Character-Level RNN"></a>PyTorch - Classify Names with a Character-Level RNN</h1><h2 id="Preparing-data"><a href="#Preparing-data" class="headerlink" title="Preparing data"></a>Preparing data</h2><p>Each line contains a name and we need to convert them from Unicode to ASCII.</p><p>Once we have read all files, we need to create two dataset: <code>languages=[]</code> containing all target categories ,<code>languages2names={}</code>, mapping each languages to corresponding names.</p><p>Then we use one-hot tensor to represent each letter, whose size is <code>&lt;1,n_letters&gt;</code>. Therefore, a name is represented as <code>&lt;name_len, 1, n_letters&gt;</code>.</p><p>After that,  we are going to define a RNN structure like the following:</p><p><img src="/2019/03/02/DP-RNN/68747470733a2f2f692e696d6775722e636f6d2f5a32786279534f2e706e67.png" alt="8747470733a2f2f692e696d6775722e636f6d2f5a32786279534f2e706e6"></p><p>Considering the output of prediction is merely a number, we need to convert this numerical value into a language category.</p><p>The last step before we dive into the training, we have to write a training data sampling function.</p><h2 id="Train-the-Network"><a href="#Train-the-Network" class="headerlink" title="Train the Network"></a>Train the Network</h2><p>We use <code>nn.NLLLoss()</code> (negative log likelihood loss) as loss function and SGD with lr=0.005 as optimizater.</p><h1 id="Referneces"><a href="#Referneces" class="headerlink" title="Referneces"></a>Referneces</h1><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a> </p><p><a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="noopener">minimal character-level RNN language model in Python/numpy</a> </p><p><a href="http://www.sohu.com/a/118587343_487514" target="_blank" rel="noopener">零基础入门深度学习(四)：循环神经网络 </a> </p><p><a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">RNN</a> </p><p><a href="https://www.cnblogs.com/zhbzz2007/p/6339346.html" target="_blank" rel="noopener">BPTT Python implementation</a> </p><p><a href="https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470" target="_blank" rel="noopener">Recurrent Neural Networks by Example in Python</a> </p><p><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras</a>  </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GAN Metrics</title>
      <link href="/2019/02/28/GAN-Metrics/"/>
      <url>/2019/02/28/GAN-Metrics/</url>
      <content type="html"><![CDATA[<p>Several metrics to evaluate GAN, including Inception Score.</p><p><a href="https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732" target="_blank" rel="noopener">GAN — How to measure GAN performance?</a> </p><a id="more"></a><h1 id="Inception-Score"><a href="#Inception-Score" class="headerlink" title="Inception Score"></a><a href="https://sudomake.ai/inception-score-explained/" target="_blank" rel="noopener">Inception Score</a></h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Consider this setting: a zoo of GANs you’ve trained has generated several sets of images: $X_1, X_2, …, X_N$ that are trying to mimic the original set $X_{real}$. If you had a perfect way to rank the realism if these sets, let’s say, a function $\rho(X)$, then $\rho(X_{real})$ would, obviously, be the highest of all. The real question is:</p><p><strong>How can such function be formulated in terms of statistics / information theory?</strong></p><p>The answer depends on the requirements for the images. Two criteria immediately come to mind:</p><ol><li>A human, looking at a separate image, would be able to confidently determine what’s in there (<strong>saliency</strong>).</li><li>A human, looking at a set of various images, would say that the set has lots of different objects (<strong>diversity</strong>).</li></ol><p>At least, that’s what everyone expects from a good generative model. Those who have tried training GANs themselves have immediately noted the fact that usually you end up getting only one criterion covered.</p><h2 id="Saliency-vs-diversity"><a href="#Saliency-vs-diversity" class="headerlink" title="Saliency vs. diversity"></a>Saliency vs. diversity</h2><p>Broadly speaking, these two criteria are represented by two components of the formula:</p><ol><li>Saliency is expressed as $p(y|x)$ — a distribution of classes for any individual image should have low entropy (think of it as a single high score and the rest very low).</li><li>Diversity is expressed is $p(x)$ — overall distribution of classes across the sampled data should have high entropy, which would mean the absense of dominating classes and something closer to a well-balanced training set.</li></ol><h2 id="Kullback-Leibler-distance-ref"><a href="#Kullback-Leibler-distance-ref" class="headerlink" title="Kullback-Leibler distance ref"></a>Kullback-Leibler distance <a href="https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68" target="_blank" rel="noopener">ref</a></h2><p>Since we are going to explain the KL divergence from the information theory point of view, let us review what is entropy and cross-entropy.</p><h3 id="Entropy-and-Cross-Entropy-ref"><a href="#Entropy-and-Cross-Entropy-ref" class="headerlink" title="Entropy and Cross-Entropy ref"></a>Entropy and Cross-Entropy <a href="https://medium.com/@vijendra1125/understanding-entropy-cross-entropy-and-softmax-3b79d9b23c8a" target="_blank" rel="noopener">ref</a></h3><p>First of all, we need to know <strong>Surprisal: Degree to which you are surprised to see the result</strong>. This means we will be more surprised to see an outcome with low probability in comparison to an outcome with high probability. Now, if $y_i$ is the probability of ith outcome then we could represent surprisal (s) as:</p><script type="math/tex; mode=display">s=-\log \left( y_{i}\right)</script><p><strong>Entropy</strong></p><p>Since I know surprisal for individual outcomes, I would like to know surprisal for the event. It would be intuitive to take a weighted average of surprisals. Now the question is what weight to choose? Hmmm…since I know the probability of each outcome, taking probability as weight makes sense because this is how likely each outcome is supposed to occur. This weighted average of surprisal is nothing but Entropy (e) and if there are <em>n</em>outcomes then it could be written as:</p><script type="math/tex; mode=display">e=\sum_{0}^{n} y_{i} \log \left(1 / y_{i}\right)</script><p><strong>Cross-Entropy</strong></p><p>Now, what if each outcome’s actual probability is $p_i$ but someone is estimating probability as $q_i$. In this case, each event will occur with the probability of $p_i$ but surprisal will be given by $q_i$ in its formula (since that person will be surprised thinking that probability of the outcome is $q_i$). Now, weighted average surprisal, in this case, is nothing but cross entropy(c) and it could be scribbled as:</p><script type="math/tex; mode=display">c=\sum_{0}^{n} p_{i} \log \left(1 / q_{i}\right)</script><p>Cross-entropy is always larger than entropy and it will be same as entropy only when $p_i=q_i$. You could digest the last sentence after seeing really nice plot given by <a href="https://www.desmos.com/calculator/zytm2sf56e" target="_blank" rel="noopener">desmos.com</a></p><p><strong>Cross-Entropy Loss</strong></p><p><img src="/2019/02/28/GAN-Metrics/1.png" alt="_NJTe1lQSz21vQU0xX0PLm"></p><p>In the plot I mentioned above, you will notice that as estimated probability distribution moves away from actual/desired probability distribution, cross entropy increases and vice-versa. Hence, we could say that minimizing cross entropy will move us closer to actual/desired distribution and that is what we want. This is why we try to reduce cross entropy so that our predicted probability distribution end up being close to the actual one. Hence, we get the formula of cross-entropy loss as:</p><script type="math/tex; mode=display">c=\sum_{0}^{n} p_{i} \log \left(1 / q_{i}\right)</script><p>And in the case of binary classification problem where we have only two classes, we name it as binary cross-entropy loss and above formula becomes:</p><script type="math/tex; mode=display">c=\sum_{0}^{1} p_{i} \log \left(1 / q_{i}\right)=p_{0} \log \left(1 / q_{0}\right)+p_{1} \log \left(1 / q_{1}\right)=p_{0} \log \left(1 / q_{0}\right)+\left(1-p_{0}\right) \log \left(1 /\left(1-q_{0}\right)\right)</script><h2 id="Kullback-Leibler-distance"><a href="#Kullback-Leibler-distance" class="headerlink" title="Kullback-Leibler distance"></a>Kullback-Leibler distance</h2><p>The KL divergence tells us how well the probability distribution $Q$ approximates the<br>probability distribution $P$ by calculating the cross-entropy minus the entropy.</p><script type="math/tex; mode=display">D_{K L}(P \| Q)=H(P, Q)-H(P)</script><p>As a reminder, I put the cross-entropy and the entropy formula as below:</p><script type="math/tex; mode=display">\begin{aligned} H(P, Q) &=\mathbb{E}_{x \sim P}[-\log Q(x)] \\ H(P) &=\mathbb{E}_{x \sim P}[-\log P(x)] \end{aligned}</script><p>The KL divergence can also be expressed in the expectation form as follows:</p><script type="math/tex; mode=display">\begin{aligned} D_{K L}(P \| Q) &=\mathbb{E}_{x \sim P}[-\log Q(x)]-\mathbb{E}_{x \sim P}[-\log P(x)] \\ &=\mathbb{E}_{x \sim P}[-\log Q(x)-(-\log P(x))] \\ &=\mathbb{E}_{x \sim P}[-\log Q(x)+\log P(x)] \\ &=\mathbb{E}_{x \sim P}[\log P(x)-\log Q(x)] \\ &=\mathbb{E}_{x \sim P}\left[\log \frac{P(x)}{Q(x)}\right] \end{aligned}</script><p>The expectation formula can be expressed in the discrete summation form or in the<br>continuous integration form:</p><script type="math/tex; mode=display">\begin{aligned} D_{K L}(P \| Q) &=\sum_{i} P(i) \log \frac{P(i)}{Q(i)} \\ D_{K L}(P \| Q) &=\int P(x) \log \frac{P(x)}{Q(x)} d x \end{aligned}</script><p>So, what does the KL divergence measure? It measures the similarity (or dissimilarity)<br>between two probability distributions.</p><h3 id="KL-divergence-is-non-negative"><a href="#KL-divergence-is-non-negative" class="headerlink" title="KL divergence is non-negative"></a>KL divergence is non-negative</h3><p>The KL divergence is non-negative. An intuitive proof is that:</p><p>if $P=Q$, the KL divergence is zero as:</p><script type="math/tex; mode=display">log \frac{P}{Q} = log1=0</script><p>if $P \ne Q$, the KL divergence is positive because the entropy is the minimum average<br>lossless encoding size.</p><p>So, the KL divergence is a non-negative value that indicates how close two probability<br>distributions are.</p><h3 id="KL-devergence-is-asymmetric"><a href="#KL-devergence-is-asymmetric" class="headerlink" title="KL devergence is asymmetric"></a>KL devergence is asymmetric</h3><p>The KL divergence is not symmetric: $D_{KL}(P||Q)\ne D_{KL}(Q||P)$.</p><p>It can be deduced from the fact that the cross-entropy itself is asymmetric. The crossentropy $H(P, Q)$ uses the probability distribution $P$ to calculate the expectation. The<br>cross-entropy $H(Q, P)$ uses the probability distribution $Q$ to calculate the expectation.<br>So, the KL divergence cannot be a distance measure as a distance measure should be<br>symmetric.</p><h3 id="Modeling-a-true-distribution"><a href="#Modeling-a-true-distribution" class="headerlink" title="Modeling a true distribution"></a>Modeling a true distribution</h3><p>By approximating a probability distribution with a well-known distribution like the<br>normal distribution, binomial distribution, etc., we are modeling the true distribution<br>with a known one.<br>This is when we are using the below formula:</p><script type="math/tex; mode=display">D_{K L}(P \| Q)=\mathbb{E}_{x \sim P}\left[\log \frac{P(x)}{Q(x)}\right]</script><p>Calculating the KL divergence, we can find the model (the distribution and the<br>parameters) that fits the true distribution well.</p><p>And that’s it. One important thing to keep in mind (and, actually, the most fascinating among these), is that to compute this score for a set of generated images you need a good image classifier. Hence the name of the metric — for calculating the distributions the authors used a pretrained Inception.</p><p><strong>Higher Inception score indicates better image quality</strong>.</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> inception_v3</div><div class="line"></div><div class="line">net = inception_v3(pretrained=<span class="keyword">True</span>).cuda()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_score</span><span class="params">(images, batch_size=<span class="number">5</span>)</span>:</span></div><div class="line">    scores = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(int(math.ceil(float(len(images)) / float(batch_size)))):</div><div class="line">        batch = Variable(torch.cat(images[i * batch_size: (i + <span class="number">1</span>) * batch_size], <span class="number">0</span>))</div><div class="line">        s, _ = net(batch)  <span class="comment"># skipping aux logits</span></div><div class="line">        scores.append(s)</div><div class="line">    p_yx = F.softmax(torch.cat(scores, <span class="number">0</span>), <span class="number">1</span>)</div><div class="line">    p_y = p_yx.mean(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).expand(p_yx.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">    KL_d = p_yx * (torch.log(p_yx) - torch.log(p_y))</div><div class="line">    final_score = KL_d.mean()</div><div class="line">    <span class="keyword">return</span> final_score</div></pre></td></tr></table></figure><p><a href="http://bluewidz.blogspot.com/2017/12/inception-score.html" target="_blank" rel="noopener">keras-implementation</a> </p><p><a href="https://zhuanlan.zhihu.com/p/54146307" target="_blank" rel="noopener">Inception Score 的原理和局限性</a> </p><h1 id="Frechet-Inception-Distance"><a href="#Frechet-Inception-Distance" class="headerlink" title="Fréchet Inception Distance"></a>Fréchet Inception Distance</h1><p><a href="https://nealjean.com/ml/frechet-inception-distance/" target="_blank" rel="noopener">ref1</a> <a href="https://zhuanlan.zhihu.com/p/54213305" target="_blank" rel="noopener">Fréchet Inception Distance (FID)</a> </p><p>The FID is supposed to improve on the IS by actually <em>comparing</em> the statistics of generated samples to real samples, instead of evaluating generated samples in a vacuum.</p><script type="math/tex; mode=display">\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr} (\Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{1/2}),</script><p>where $X_r \sim \mathcal{N} (\mu_r, \Sigma_r)$ and $X_g \sim \mathcal{N} (\mu_g, \Sigma_g)$ are the 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively. <strong>Lower FID is better</strong>, corresponding to more similar real and generated samples as measured by the distance between their activation distributions.</p><p><a href="https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/" target="_blank" rel="noopener">implementation</a> </p><h1 id="SSD-Score"><a href="#SSD-Score" class="headerlink" title="SSD-Score"></a>SSD-Score</h1><h2 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h2><p><a href="https://github.com/AliaksandrSiarohin/pose-gan/blob/master/ssd_score/compute_ssd_score.py" target="_blank" rel="noopener">ref1</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>BroadReading</title>
      <link href="/2019/02/08/BroadReading/"/>
      <url>/2019/02/08/BroadReading/</url>
      <content type="html"><![CDATA[<p>Here are lists of some interesting knowledge I picked up in daily study.</p><a id="more"></a><h1 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h1><h2 id="Siamese-Network"><a href="#Siamese-Network" class="headerlink" title="Siamese Network"></a>Siamese Network</h2><ol><li><a href="https://www.cnblogs.com/bentuwuying/p/8186364.html" target="_blank" rel="noopener">siamese network理解</a> </li><li><a href="https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8" target="_blank" rel="noopener">Siamese Network &amp; Triplet Loss</a> </li><li><a href="https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5" target="_blank" rel="noopener">CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more ….</a> </li></ol><h2 id="Super-Resolution-GAN"><a href="#Super-Resolution-GAN" class="headerlink" title="Super Resolution GAN"></a>Super Resolution GAN</h2><ol><li><a href="https://arxiv.org/pdf/1609.04802.pdf" target="_blank" rel="noopener">Photo-Realistic Single Image Super-Resolution Using a Generative AdversarialNetwork</a> </li><li><a href="https://medium.com/@jonathan_hui/gan-super-resolution-gan-srgan-b471da7270ec" target="_blank" rel="noopener">GAN — Super Resolution GAN (SRGAN)</a> </li><li>​</li></ol><h2 id="Network-Visualization"><a href="#Network-Visualization" class="headerlink" title="Network Visualization"></a>Network Visualization</h2><ol><li><a href="https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b" target="_blank" rel="noopener">Understanding your Convolution network with Visualizations</a> </li><li><a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM" target="_blank" rel="noopener">Deep Visualization Toolbox</a> </li><li><a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks</a> </li><li><a href="https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/" target="_blank" rel="noopener">The vanishing gradient problem and ReLUs – a TensorFlow investigation</a> </li></ol><h2 id="Fine-tune"><a href="#Fine-tune" class="headerlink" title="Fine-tune"></a>Fine-tune</h2><ol><li><a href="https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html" target="_blank" rel="noopener">A Comprehensive guide to Fine-tuning Deep Learning Models in Keras (Part II)</a> </li><li><a href="https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2" target="_blank" rel="noopener">How to debug neural networks. Manual</a> </li><li>​</li></ol><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><ol><li><p><a href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" target="_blank" rel="noopener">Review: GoogLeNet (Inception v1)— Winner of ILSVRC 2014 (Image Classification)</a> </p></li><li><p><a href="https://stats.stackexchange.com/questions/306574/which-elements-of-a-neural-network-can-lead-to-overfitting" target="_blank" rel="noopener">more layers leads to overfitting</a> </p><blockquote><p>Increasing the number of hidden units and/or layers may lead to overfitting because it will make it easier for the neural network to memorize the training set, that is to learn a function that perfectly separates the training set but that does not generalize to unseen data.</p></blockquote></li><li><p><a href="https://drive.google.com/drive/folders/1OqhNDeFnl-Mwvd6ONQaLs3u3cOhw1ejr" target="_blank" rel="noopener">PPT</a></p></li><li><p><a href="https://gist.github.com/joelouismarino/a2ede9ab3928f999575423b9887abd14" target="_blank" rel="noopener">keras code</a> </p></li></ol><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><ol><li><p><a href="https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce" target="_blank" rel="noopener">Recurrent Neural Networks</a> </p></li><li><p><a href="https://medium.com/@florijan.stamenkovic_99541/rnn-language-modelling-with-pytorch-packed-batching-and-tied-weights-9d8952db35a9" target="_blank" rel="noopener">RNN Language Modelling with PyTorch — Packed Batching and Tied Weights</a> </p></li><li><p><a href="https://medium.com/explore-artificial-intelligence/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba" target="_blank" rel="noopener">Word2Vec — a baby step in Deep Learning but a giant leap towards Natural Language Processing</a> </p></li><li><p><a href="https://towardsdatascience.com/attention-models-in-nlp-a-quick-introduction-2593c1fe35eb" target="_blank" rel="noopener">Attention models in NLP a quick introduction</a> </p></li><li><p>​</p><p>​</p></li></ol><h1 id="Learning-Network"><a href="#Learning-Network" class="headerlink" title="Learning Network"></a>Learning Network</h1><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><ol><li><a href="https://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="noopener">[机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent）</a> </li><li>​</li></ol><h2 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h2><ol><li><a href="https://towardsdatascience.com/measuring-actual-gpu-usage-for-deep-learning-training-e2bf3654bcfd" target="_blank" rel="noopener">Monitor and Improve GPU Usage for Training Deep Learning Models</a></li><li>​</li></ol><h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><ol><li><a href="https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html" target="_blank" rel="noopener">5 Things You Need to Know about Reinforcement Learning</a> </li><li><a href="https://blog.csdn.net/songrotek/article/details/50580904" target="_blank" rel="noopener">Deep Reinforcement Learning 基础知识（DQN方面）</a> </li></ol><h1 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h1><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><ol><li><a href="https://towardsdatascience.com/introduction-to-video-classification-6c6acbc57356" target="_blank" rel="noopener">Introduction to Video Classification</a> </li></ol><h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><ol><li><a href="https://towardsdatascience.com/10-python-file-system-methods-you-should-know-799f90ef13c2" target="_blank" rel="noopener">10 Python File System Methods You Should Know</a> </li><li>​</li></ol><h1 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h1><ol><li><a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" target="_blank" rel="noopener">R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms</a> </li><li><a href="https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a" target="_blank" rel="noopener">Faster R-CNN (object detection) implemented by Keras for custom data from Google’s Open Images Dataset V4</a> </li><li>​</li></ol><h1 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h1><ol><li><a href="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef" target="_blank" rel="noopener">How to do Semantic Segmentation using Deep learning</a> </li></ol><h1 id="DeepLearning-Framework"><a href="#DeepLearning-Framework" class="headerlink" title="DeepLearning Framework"></a>DeepLearning Framework</h1><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><ol><li><a href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#finetuning-torchvision-models" target="_blank" rel="noopener">FINETUNING TORCHVISION MODELS</a> </li></ol><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><ol><li><a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1" target="_blank" rel="noopener">Learning Rate Schedules and A daptive Learning Rate Methods for Deep Learning</a> </li><li><a href="https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/" target="_blank" rel="noopener">Using Learning Rate Schedules for Deep Learning Models in Python with Keras</a> </li><li>​</li></ol><h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><ol><li><a href="https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3" target="_blank" rel="noopener">Introduction to Genetic Algorithms — Including Example Code</a> </li><li><a href="https://slideplayer.com/slide/11955816/" target="_blank" rel="noopener">Genetic Algorithm with Knapsack Problem</a></li><li>​</li></ol><h1 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/25415549" target="_blank" rel="noopener">有哪些相见恨晚的高效学习方法？</a> </li><li>​</li></ol>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Daily Paper Reading</title>
      <link href="/2018/10/23/Daily-Paper-Reading/"/>
      <url>/2018/10/23/Daily-Paper-Reading/</url>
      <content type="html"><![CDATA[<p>Some interesting papers that I read or am about to read.</p><a id="more"></a><h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><h1 id="January"><a href="#January" class="headerlink" title="January"></a>January</h1><ol><li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Image Style Transfer Using Convolutional Neural Networks</a> </li><li><a href="https://arxiv.org/pdf/1505.04597.pdf" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a> </li><li>​</li></ol><h1 id="CVPR2019"><a href="#CVPR2019" class="headerlink" title="CVPR2019"></a>CVPR2019</h1><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p><del>[Progressive Pose Attention Transfer for Person Image Generation]</del>(<a href="https://arxiv.org/pdf/1904.03349.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.03349.pdf</a>) <a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">code</a> </p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qiao_MirrorGAN_Learning_Text-To-Image_Generation_by_Redescription_CVPR_2019_paper.pdf" target="_blank" rel="noopener">MirrorGAN: Learning Text-to-image Generation by Redescription</a> </p><blockquote><p>Text -&gt; Image, then Image -&gt; Text. </p></blockquote><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Joint_Discriminative_and_Generative_Learning_for_Person_Re-Identification_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Joint Discriminative and Generative Learning for Person Re-identification</a> </p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Semantics_Disentangling_for_Text-To-Image_Generation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Semantics Disentangling for Text-to-Image Generation</a> </p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Song_Unsupervised_Person_Image_Generation_With_Semantic_Parsing_Transformation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Unsupervised Person Image Generation with Semantic Parsing Transformation</a></p><h1 id="2019-05-16"><a href="#2019-05-16" class="headerlink" title="2019-05-16"></a>2019-05-16</h1><p><a href="https://arxiv.org/pdf/1812.02784.pdf" target="_blank" rel="noopener">StoryGAN: A Sequential Conditional GAN for Story Visualization</a></p><p><a href="https://arxiv.org/pdf/1902.10740.pdf" target="_blank" rel="noopener">Object-driven Text-to-Image Synthesis via Adversarial Training</a></p><p><a href="https://arxiv.org/pdf/1809.01110.pdf" target="_blank" rel="noopener">Text2Scene: Generating Compositional Scenes from Textual Descriptions</a></p><p><a href="https://arxiv.org/pdf/1904.01310.pdf" target="_blank" rel="noopener">DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis</a></p><p><a href="https://arxiv.org/pdf/1811.11389.pdf" target="_blank" rel="noopener">Image Generation from Layout</a></p><p><a href="https://arxiv.org/pdf/1904.05118.pdf" target="_blank" rel="noopener">Text Guided Person Image Synthesis</a> </p><p>Inferring poses from the text and then taking one person image, text and inferred pose as input to generate person images.</p><p><a href="https://arxiv.org/pdf/1904.01480.pdf" target="_blank" rel="noopener">Semantics Disentangling for Text-to-Image Generation</a></p><p><a href="http://vireo.cs.cityu.edu.hk/papers/R2GAN.pdf" target="_blank" rel="noopener">R2GAN: Cross-modal Recipe Retrieval with Generative Adversarial Network</a></p><h1 id="2019-05-15"><a href="#2019-05-15" class="headerlink" title="2019-05-15"></a>2019-05-15</h1><p><a href="https://arxiv.org/pdf/1904.07460.pdf" target="_blank" rel="noopener">Fashion-AttGAN: Attribute-Aware Fashion Editing with Multi-Objective GAN</a></p><p>Improvement work based on their previous <a href="https://arxiv.org/pdf/1711.10678.pdf" target="_blank" rel="noopener">Attgan: Facial attribute editing by only changing what you want</a>.</p><h1 id="2019-05-14"><a href="#2019-05-14" class="headerlink" title="2019-05-14"></a>2019-05-14</h1><p><a href="https://arxiv.org/pdf/1811.11212.pdf" target="_blank" rel="noopener"><del>Self-Supervised GANs via Auxiliary Rotation Loss</del></a></p><p>Traditioanal CGAN with rotation angle loss being supervising loss. </p><p><a href="https://arxiv.org/pdf/1901.09764.pdf" target="_blank" rel="noopener">CollaGAN: Collaborative GAN for Missing Image Data Imputation</a></p><p><a href="https://arxiv.org/pdf/1903.05628.pdf" target="_blank" rel="noopener">Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis</a></p><p><strong>Solve the cGAN mode collapse problem by introducing a regularization term.</strong></p><p><a href="https://arxiv.org/pdf/1903.05854.pdf" target="_blank" rel="noopener">MirrorGAN: Learning Text-to-image Generation by Redescription</a></p><h1 id="Videos"><a href="#Videos" class="headerlink" title="Videos"></a>Videos</h1><p><a href="https://arxiv.org/pdf/1505.06250v1.pdf" target="_blank" rel="noopener">Efficient Large Scale Video Classification</a> </p><p><a href="https://arxiv.org/pdf/1711.11217v2.pdf" target="_blank" rel="noopener">Future Person Localization in First-Person Videos</a> </p><h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p>2019-03-01 <a href="https://arxiv.org/pdf/1711.09151v1.pdf" target="_blank" rel="noopener">Convolutional Image Captioning</a> Image captioning: describe the content observed in an image.</p><p><a href="https://arxiv.org/pdf/1803.11438v1.pdf" target="_blank" rel="noopener">Reconstruction Network for Video Captioning</a></p><p><a href="https://arxiv.org/pdf/1712.09382v1.pdf" target="_blank" rel="noopener">Audio to Body Dynamics</a> </p><p><a href="https://arxiv.org/pdf/1712.02036v1.pdf" target="_blank" rel="noopener">Learning Semantic Concepts and Order for Image and Sentence Matching</a> </p><p><a href="https://arxiv.org/pdf/1709.07192v1.pdf" target="_blank" rel="noopener">Visual Question Generation as Dual Task of Visual Question Answering</a> </p><p>2019-02-28 <a href="https://arxiv.org/pdf/1711.07613v1.pdf" target="_blank" rel="noopener">Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning</a> </p><p><a href="https://arxiv.org/pdf/1712.01381v3.pdf" target="_blank" rel="noopener">A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts</a> </p><p><a href="https://arxiv.org/pdf/1803.10892v1.pdf" target="_blank" rel="noopener">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</a> </p><p><a href="https://arxiv.org/pdf/1803.07485v1.pdf" target="_blank" rel="noopener">Actor and Action Video Segmentation from a Sentence</a></p><p><a href="https://arxiv.org/pdf/1703.09529v3.pdf" target="_blank" rel="noopener">Objects as context for detecting their semantic parts</a> </p><p><a href="https://arxiv.org/pdf/1706.03872v1.pdf" target="_blank" rel="noopener">Six Challenges for Neural Machine Translation</a> </p><p><a href="https://arxiv.org/pdf/1711.10485.pdf" target="_blank" rel="noopener">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</a> </p><h1 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h1><p>2019-03-01 <a href="https://arxiv.org/pdf/1805.08318.pdf" target="_blank" rel="noopener">Self-Attention Generative Adversarial Networks</a> </p><p>2019-03-02 <a href="https://arxiv.org/pdf/1612.05363v2.pdf" target="_blank" rel="noopener">Learning Residual Images for Face Attribute Manipulation</a> <a href="https://github.com/MingtaoGuo/Learning-Residual-Images-for-Face-Attribute-Manipulation" target="_blank" rel="noopener">tensorflow</a> <a href="https://github.com/sav132/Face-attributes-GAN" target="_blank" rel="noopener">keras</a> </p><p>2019-03-03 <a href="https://arxiv.org/pdf/1711.07410v2.pdf" target="_blank" rel="noopener">Disentangling Factors of Variation by Mixing Them</a> </p><p>2019-03-03 <a href="https://arxiv.org/pdf/1803.03345v2.pdf" target="_blank" rel="noopener">Deep Semantic Face Deblurring</a> </p><p><a href="https://arxiv.org/pdf/1712.02330v1.pdf" target="_blank" rel="noopener">SGAN: An Alternative Training of Generative Adversarial Networks</a> </p><p><a href="https://arxiv.org/pdf/1711.07064v4.pdf" target="_blank" rel="noopener">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</a> </p><p>2019-03-03 <a href="https://arxiv.org/pdf/1702.01983v2.pdf" target="_blank" rel="noopener">Face Aging With Conditional Generative Adversarial Networks</a> </p><p>2019-03-04  <a href="http://web.eecs.utk.edu/~zzhang61/docs/papers/2017_CVPR_Age.pdf" target="_blank" rel="noopener">Age Progression/Regression by Conditional Adversarial Autoencoder</a> </p><p>2019-03-04 <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Face_Aging_With_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Face Aging with Identity-Preserved Conditional Generative Adversarial Networks</a> </p><p>2019-03-05 <a href="https://arxiv.org/pdf/1803.11182v1.pdf" target="_blank" rel="noopener">Towards Open-Set Identity Preserving Face Synthesis</a> </p><p><a href="https://arxiv.org/pdf/1711.08565v1.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a> </p><p>2019-03-08 <a href="https://arxiv.org/pdf/1711.06454v5.pdf" target="_blank" rel="noopener">Separating Style and Content for Generalized Style Transfer</a> </p><p><a href="https://arxiv.org/pdf/1803.00839v1.pdf" target="_blank" rel="noopener">Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</a> </p><p><a href="https://arxiv.org/pdf/1804.08882v1.pdf" target="_blank" rel="noopener">Mask-aware Photorealistic Face Attribute Manipulation</a> </p><p>2019-03-10 <a href="https://arxiv.org/pdf/1706.09138v1.pdf" target="_blank" rel="noopener">Perceptual Adversarial Networks for Image-to-Image Transformation</a> </p><p><a href="https://arxiv.org/pdf/1804.00819v1.pdf" target="_blank" rel="noopener">End-to-End Dense Video Captioning with Masked Transformer</a> </p><p>2019-03-01 <a href="https://arxiv.org/pdf/1712.04350v1.pdf" target="_blank" rel="noopener">Predicting Yelp Star Reviews Based on Network Structure with Deep Learning</a> </p><p><a href="https://arxiv.org/pdf/1711.06448v1.pdf" target="_blank" rel="noopener">Chinese Typeface Transformation with Hierarchical Adversarial Network</a> </p><p><a href="https://arxiv.org/pdf/1711.06420v1.pdf" target="_blank" rel="noopener">Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models</a> </p><p><a href="https://arxiv.org/pdf/1712.01928v2.pdf" target="_blank" rel="noopener">Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Networks</a></p><p><a href="https://arxiv.org/pdf/1804.09578v1.pdf" target="_blank" rel="noopener">Unsupervised Domain Adaptation with Adversarial Residual Transform Networks</a> </p><p><a href="https://arxiv.org/pdf/1804.03390v2.pdf" target="_blank" rel="noopener">Learning Pose Specific Representations by Predicting Different Views</a> </p><p><a href="https://arxiv.org/pdf/1801.01415v1.pdf" target="_blank" rel="noopener">What have we learned from deep representations for action recognition?</a> </p><p><a href="https://arxiv.org/pdf/1511.02799v4.pdf" target="_blank" rel="noopener">Neural Module Networks</a> </p><p><a href="https://arxiv.org/pdf/1801.04356v2.pdf" target="_blank" rel="noopener">Feature Space Transfer for Data Augmentation</a> </p><p><a href="https://arxiv.org/pdf/1706.04306v1.pdf" target="_blank" rel="noopener">Photo-realistic Facial Texture Transfer</a></p><p><a href="https://arxiv.org/pdf/1804.07455v1.pdf" target="_blank" rel="noopener">Generating a Fusion Image: One’s Identity and Another’s Shape</a> </p><p><a href="https://arxiv.org/pdf/1805.11202v1.pdf" target="_blank" rel="noopener">FairGAN: Fairness-aware Generative Adversarial Networks</a></p><p><a href="https://arxiv.org/pdf/1804.00582v1.pdf" target="_blank" rel="noopener">Learning Intrinsic Image Decomposition from Watching the World</a> </p><p><a href="https://arxiv.org/pdf/1802.08797v2.pdf" target="_blank" rel="noopener">Residual Dense Network for Image Super-Resolution</a> </p><p>2019-03-20 <a href="https://arxiv.org/pdf/1704.07333v3.pdf" target="_blank" rel="noopener">Detecting and Recognizing Human-Object Interactions</a> </p><p><a href="https://arxiv.org/pdf/1611.09961v1.pdf" target="_blank" rel="noopener">Semantic Facial Expression Editing using Autoencoded Flow</a> </p><p>  <a href="https://arxiv.org/pdf/1712.00516v1.pdf" target="_blank" rel="noopener">Multi-Content GAN for Few-Shot Font Style Transfer</a>  </p><p>  <a href="https://arxiv.org/pdf/1802.07447v2.pdf" target="_blank" rel="noopener">Load Balanced GANs for Multi-view Face Image Synthesis</a> </p><p>  <a href="https://arxiv.org/pdf/1804.03487v1.pdf" target="_blank" rel="noopener">Exploring Disentangled Feature Representation Beyond Face Identification</a> </p><p>  <a href="https://arxiv.org/pdf/1707.00737v1.pdf" target="_blank" rel="noopener">High-Quality Face Image SR Using Conditional Generative Adversarial Networks</a> </p><p>  <a href="https://arxiv.org/pdf/1710.00962v2.pdf" target="_blank" rel="noopener">GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks</a> </p><p>  <a href="https://arxiv.org/pdf/1803.09722v2.pdf" target="_blank" rel="noopener">3D Human Pose Estimation in the Wild by Adversarial Learning</a> </p><p>  <a href="https://arxiv.org/pdf/1801.07892v2.pdf" target="_blank" rel="noopener">Generative Image Inpainting with Contextual Attention</a> </p><p> <a href="https://arxiv.org/pdf/1706.02823.pdf" target="_blank" rel="noopener">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</a> </p><p> <a href="https://arxiv.org/pdf/1604.04382.pdf" target="_blank" rel="noopener">Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks</a> </p><p>2019-01-24</p><p><a href="https://arxiv.org/pdf/1605.05396.pdf" target="_blank" rel="noopener">Generative Adversarial Text to Image Synthesis</a> </p><p>2019-02-28 <a href="https://arxiv.org/pdf/1612.03242.pdf" target="_blank" rel="noopener">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a></p><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Sangkloy_Scribbler_Controlling_Deep_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Scribbler: Controlling Deep Image Synthesis with Sketch and Color</a> </p><p><a href="https://arxiv.org/pdf/1701.07274.pdf" target="_blank" rel="noopener">DEEP REINFORCEMENT LEARNING: AN OVERVIEW</a> </p><p><a href="https://arxiv.org/pdf/1611.06355.pdf" target="_blank" rel="noopener">Invertible Conditional GANs for image editing</a> </p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.pdf" target="_blank" rel="noopener">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</a> </p><p><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a> </p><p><a href="https://arxiv.org/pdf/1805.10416.pdf" target="_blank" rel="noopener">Human Action Generation with Generative Adversarial Networks</a> </p><p><a href="https://arxiv.org/pdf/1711.08682.pdf" target="_blank" rel="noopener">Deep Video Generation, Prediction and Completion of Human Action Sequences</a> skeleton generation</p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ceyuan_Yang_Pose_Guided_Human_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Pose Guided Human Video Generation</a> skeleton generation</p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Amit_Raj_SwapNet_Garment_Transfer_ECCV_2018_paper.pdf" target="_blank" rel="noopener">SwapNet: Image Based Garment Transfer</a> </p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Esser_A_Variational_U-Net_CVPR_2018_paper.pdf" target="_blank" rel="noopener">A Variational U-Net for Conditional Appearance and Shape Generation</a> </p><p>2019-01-04</p><p><a href="https://arxiv.org/pdf/1711.10684.pdf" target="_blank" rel="noopener">Residual U-net</a></p><p><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks</a> </p><p><a href="https://arxiv.org/pdf/1803.04469.pdf" target="_blank" rel="noopener">An Introduction to Image Synthesis with Generative Adversarial Nets</a> </p><p><a href="https://arxiv.org/pdf/1708.00315.pdf" target="_blank" rel="noopener">Generative Semantic Manipulation with Contrasting GAN</a> </p><p>In this paper, distance loss is used to measure the difference between real/fake images.</p><p><a href="https://arxiv.org/pdf/1808.06601.pdf" target="_blank" rel="noopener">Video-to-Video Synthesis</a></p><p>2018-11-16</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Disentangled_Representation_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Disentangled Representation Learning GAN for Pose-Invariant Face Recognition</a> </p><p>The authors try to solve the problem of Pose-Invariant Face Recognition.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 1.48.00 PM.png" alt="creen Shot 2018-11-18 at 1.48.00 P"></p><blockquote><p>By controlling $c$ and $z$, we can diversify the generated images. </p></blockquote><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 1.48.27 PM.png" alt="creen Shot 2018-11-18 at 1.48.27 P"></p><blockquote><p>Discriminator has three tasks: classify real images as real ones, fake images as fake ones, the identities and poses.</p></blockquote><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 1.51.32 PM.png" alt="creen Shot 2018-11-18 at 1.51.32 P"></p><blockquote><p>Generator tries to maximize the accuracy of generated images being classified to the  true identities and poses.</p></blockquote><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-18 at 2.00.22 PM.png" alt="creen Shot 2018-11-18 at 2.00.22 P"></p><blockquote><p>There are $n$ input images and each one has a corresponding generated image; and there is one generated image. For each generated image, there are two losses.</p></blockquote><p><a href="https://arxiv.org/pdf/1611.03383.pdf" target="_blank" rel="noopener">Disentangling factors of variation in deep representations using adversarial training</a></p><p>2018-11-15</p><p><a href="https://arxiv.org/pdf/1810.11610.pdf" target="_blank" rel="noopener">Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis</a> </p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Pose Transferrable Person Re-Identification</a> </p><p><a href="http://www.robots.ox.ac.uk/~tvg/publications/2018/W21P20.pdf" target="_blank" rel="noopener">A Semi-supervised Deep Generative Model for Human Body Analysis</a> </p><p><a href="https://arxiv.org/pdf/1808.06847.pdf?utm_campaign=Awesome%20Computer%20Science&amp;utm_medium=email&amp;utm_source=Revue%20newsletter" target="_blank" rel="noopener">Deep Video-Based Performance Cloning</a> </p><p><a href="http://www.cs.sfu.ca/~mori/research/papers/zhai-bmvc18.pdf" target="_blank" rel="noopener">Adaptive Appearance Rendering</a> </p><p><a href="https://arxiv.org/pdf/1804.04779.pdf" target="_blank" rel="noopener">A Hybrid Model for Identity Obfuscation by Face Replacement</a> </p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/XU_YANG_Shuffle-Then-Assemble_Learning_Object-Agnostic_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship Features</a> </p><p><a href="https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf" target="_blank" rel="noopener">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a> </p><p>2018-11-10</p><p><a href="https://arxiv.org/pdf/1612.03242.pdf" target="_blank" rel="noopener">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a> </p><p><a href="https://arxiv.org/pdf/1703.09695.pdf" target="_blank" rel="noopener">Semi and Weakly Supervised Semantic Segmentation Using Generative Adversarial Network</a> </p><p><a href="https://arxiv.org/pdf/1704.03414.pdf" target="_blank" rel="noopener">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</a> </p><p><a href="https://www.ijcai.org/proceedings/2017/0404.pdf" target="_blank" rel="noopener">Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering</a> </p><p><a href="https://arxiv.org/pdf/1711.11585.pdf" target="_blank" rel="noopener">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a> </p><h1 id="2018-11-08"><a href="#2018-11-08" class="headerlink" title="2018-11-08"></a>2018-11-08</h1><p><a href="https://arxiv.org/pdf/1706.02823v3.pdf" target="_blank" rel="noopener">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</a></p><p><a href="https://arxiv.org/pdf/1804.04273v1.pdf" target="_blank" rel="noopener">VITAL: VIsual Tracking via Adversarial Learning</a> </p><p>Problem setting : tracking-by-detection, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-09 at 10.15.46 AM.png" alt="creen Shot 2018-11-09 at 10.15.46 A"></p><p><a href="https://arxiv.org/pdf/1712.02478v1.pdf" target="_blank" rel="noopener">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal</a> </p><p><a href="https://arxiv.org/pdf/1708.00159v1.pdf" target="_blank" rel="noopener">Image Denoising via CNNs: An Adversarial Approach</a> </p><p>Image denoising is a fundamental image processing problem whose objective is to remove the noise while preserving the original image structure.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-12 at 9.19.33 AM.png" alt="creen Shot 2018-11-12 at 9.19.33 A"></p><p><a href="https://arxiv.org/pdf/1711.09554v2.pdf" target="_blank" rel="noopener">Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation</a> </p><p><a href="https://arxiv.org/pdf/1705.08824v2.pdf" target="_blank" rel="noopener">From source to target and back: symmetric bi-directional adaptive GAN</a> </p><p><a href="https://arxiv.org/pdf/1805.00251v1.pdf" target="_blank" rel="noopener">Conditional Image-to-Image Translation</a> </p><p><a href="https://arxiv.org/pdf/1712.00268v4.pdf" target="_blank" rel="noopener">Deformable Shape Completion with Graph Convolutional Autoencoders</a> </p><h1 id="2018-11-05"><a href="#2018-11-05" class="headerlink" title="2018-11-05"></a>2018-11-05</h1><p><a href="https://papers.nips.cc/paper/5845-deep-visual-analogy-making.pdf" target="_blank" rel="noopener">Deep Visual Analogy-Making</a> <a href="https://github.com/carpedm20/visual-analogy-tensorflow" target="_blank" rel="noopener">tf code</a> </p><p>Given a pair of images $(a,b)$ and a query image $c$, we try to generate $d$, the corresponding image of $c$, such that $a$ is to $b$ as $c$ is to $d$.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-06 at 9.25.11 PM.png" alt="creen Shot 2018-11-06 at 9.25.11 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-06 at 9.38.23 PM.png" alt="creen Shot 2018-11-06 at 9.38.23 P"></p><p><a href="https://arxiv.org/pdf/1711.08565v1.pdf" target="_blank" rel="noopener">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</a> <a href="https://github.com/pkuvmc/PTGAN" target="_blank" rel="noopener">code</a> </p><p>A new Multi-Scene Multi-Time person ReID dataset (MSMT17) is proposed. </p><p><strong>[transfer styles but keep identies]</strong> A method is proposed to bridge the domain gap by transferring persons in dataset A to another dataset B. The transferred persons from A are desired to keep their identities, meanwhile present similar styles, e.g., backgrounds, lightings, etc., with persons in B.</p><p>To keep identity, a identities loss is introduced where the mask region of generated images and gt images should be similar.</p><p><a href="https://arxiv.org/pdf/1711.06454v5.pdf" target="_blank" rel="noopener">Separating Style and Content for Generalized Style Transfer</a> </p><p><a href="https://arxiv.org/pdf/1712.01066v1.pdf" target="_blank" rel="noopener">Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images</a> <a href="https://github.com/tribhuvanesh/visual_redactions" target="_blank" rel="noopener">code</a> </p><p><a href="https://arxiv.org/pdf/1712.07262v2.pdf" target="_blank" rel="noopener">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</a> </p><p><a href="https://arxiv.org/pdf/1802.06713v3.pdf" target="_blank" rel="noopener">Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment</a></p><p><a href="https://arxiv.org/pdf/1703.03492v3.pdf" target="_blank" rel="noopener">A New Representation of Skeleton Sequences for 3D Action Recognition</a> </p><h1 id="2018-11-04"><a href="#2018-11-04" class="headerlink" title="2018-11-04"></a>2018-11-04</h1><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1978.pdf" target="_blank" rel="noopener">Synthesizing Images of Humans in Unseen Poses</a></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-04 at 11.17.55 PM.png" alt="creen Shot 2018-11-04 at 11.17.55 P"></p><p>Our model is trained on (example, label) tuples of the form $((I_s, p_s, p_t), I_t)$, where $I_s$, $p_s$ and $p_t$ are the source image, source 2D pose and target 2D pose, and $I_t$ is the target image.</p><p>our model first segments the scene into foreground and background layers. It further segments the person’s body into different part layers such as the arms and legs, allowing each part to then be moved independently of the others.<img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-04 at 11.22.15 PM.png" alt="creen Shot 2018-11-04 at 11.22.15 P"></p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3569.pdf" target="_blank" rel="noopener">Generating a Fusion Image: One’s Identity and Another’s Shape</a></p><p>Given two rgb images $x $ and $y$, we try to generate a new image which is the combination of the identity $x$ and the shape or pose of $y$.</p><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lassner_A_Generative_Model_ICCV_2017_paper.pdf" target="_blank" rel="noopener">A Generative Model of People in Clothing</a> </p><p>The authors try to generate different people with different clothes but with the same specified shapes.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-05 at 10.58.10 AM.png" alt="creen Shot 2018-11-05 at 10.58.10 A"></p><h1 id="2018-11-2"><a href="#2018-11-2" class="headerlink" title="2018-11-2"></a>2018-11-2</h1><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.pdf" target="_blank" rel="noopener">5</a> </p><p><a href="https://arxiv.org/pdf/1507.06821.pdf" target="_blank" rel="noopener">Multimodal Deep Learning for Robust RGB-D Object Recognition</a> </p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Weiyue_Wang_Depth-aware_CNN_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Depth-aware CNN for RGB-D Segmentation</a> </p><p><a href="https://arxiv.org/pdf/1407.5736.pdf" target="_blank" rel="noopener">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</a> </p><h2 id="Deep-Bilinear-Learning-for-RGB-D-Action-Recognition"><a href="#Deep-Bilinear-Learning-for-RGB-D-Action-Recognition" class="headerlink" title="Deep Bilinear Learning for RGB-D Action Recognition"></a>Deep Bilinear Learning for RGB-D Action Recognition</h2><p><a href="http://isee.sysu.edu.cn/~hujianfang/pdfFiles/ECCV2018.pdf" target="_blank" rel="noopener">PAPER</a></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-02 at 4.30.19 PM.png" alt="creen Shot 2018-11-02 at 4.30.19 P"></p><p>In this paper, we present a novel tensor-structured cube feature【The multi-modal sequences with temporal information can be regarded as a tensor， structured with two different dimensions (temporal and modality)】, and propose to learn time-varying information from multi-modal action history sequences for RGB-D action recognition.</p><p>In this paper, we address this challenge by proposing a novel deep bilinear framework, where a bilinear block consisting of two linear pooling layers (modality pooling layer and temporal pooling layer) is defined to pool the input tensor along the modality and temporal directions, separately. In this way, the structures along the temporal and modal dimensions are both preserved. By stacking the proposed bilinear blocks and other network layers (e.g., Relu and softmax), we develop our deep bilinear model to jointly learn the action history and modality information in videos. Results have shown that learning modality-temporal mutual information is beneficial for the recognition of RGB-D actions.</p><h1 id="2018-10-31"><a href="#2018-10-31" class="headerlink" title="2018-10-31"></a>2018-10-31</h1><h2 id="A-Pose-Sensitive-Embedding-for-Person-Re-Identification-with-Expanded-Cross-Neighborhood-Re-Ranking"><a href="#A-Pose-Sensitive-Embedding-for-Person-Re-Identification-with-Expanded-Cross-Neighborhood-Re-Ranking" class="headerlink" title="A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1114.pdf" target="_blank" rel="noopener">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</a></h2><p>coarse pose : front, back, side of a person’s orientation to the camera.</p><p>fine pose : joint skeleton</p><h2 id="Everybody-Dance-Now"><a href="#Everybody-Dance-Now" class="headerlink" title="Everybody Dance Now"></a><a href="https://arxiv.org/pdf/1808.07371.pdf" target="_blank" rel="noopener">Everybody Dance Now</a></h2><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-31 at 11.17.32 PM.png" alt="creen Shot 2018-10-31 at 11.17.32 P"></p><p>The task is to generate a action video conditioned on the figure and source video. </p><p>The problem is you have to train a generate G model for each new person.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-14 at 10.55.48 PM.png" alt="creen Shot 2018-11-14 at 10.55.48 P"></p><p>$L_{VGG}$ loss: instead of using per-pixel loss functions depending only on low-level pixel information, we train our networks using perceptual loss functions that depend on high-level features from a pretrained loss network. During training, perceptual losses measure<br>image similarities more robustly than per-pixel losses</p><h2 id="CR-GAN-Learning-Complete-Representations-for-Multi-view-Generation"><a href="#CR-GAN-Learning-Complete-Representations-for-Multi-view-Generation" class="headerlink" title="CR-GAN: Learning Complete Representations for Multi-view Generation"></a><a href="https://www.ijcai.org/proceedings/2018/0131.pdf" target="_blank" rel="noopener">CR-GAN: Learning Complete Representations for Multi-view Generation</a></h2><ol><li>learn complete representation to handle unseen data problem.</li></ol><p><img src="/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%2010.55.28%20AM.png" alt="creen Shot 2018-11-01 at 10.55.28 A"></p><p>The authors aim to geneate multi-view images of a figure given one image of that person. </p><h2 id="Geometry-Contrastive-GAN-for-Facial-Expression-Transfer"><a href="#Geometry-Contrastive-GAN-for-Facial-Expression-Transfer" class="headerlink" title="Geometry-Contrastive GAN for Facial Expression Transfer"></a><a href="https://arxiv.org/pdf/1802.01822.pdf" target="_blank" rel="noopener">Geometry-Contrastive GAN for Facial Expression Transfer</a></h2><ol><li>handle the misalignment across different subjects or facial expressions.</li></ol><p><img src="/2018/10/23/Daily-Paper-Reading/Screen%20Shot%202018-11-01%20at%2011.22.36%20AM.png" alt="creen Shot 2018-11-01 at 11.22.36 A"></p><h2 id="Pose-Guided-Human-Video-Generation"><a href="#Pose-Guided-Human-Video-Generation" class="headerlink" title="Pose Guided Human Video Generation"></a><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ceyuan_Yang_Pose_Guided_Human_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Pose Guided Human Video Generation</a></h2><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-01 at 2.17.07 PM.png" alt="creen Shot 2018-11-01 at 2.17.07 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-01 at 2.20.38 PM.png" alt="creen Shot 2018-11-01 at 2.20.38 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-11-01 at 2.51.41 PM.png" alt="creen Shot 2018-11-01 at 2.51.41 P"></p><h2 id="DDDDDDDIFFICULTLearning-to-Forecast-and-Refine-Residual-Motion-for-Image-to-Video-Generation"><a href="#DDDDDDDIFFICULTLearning-to-Forecast-and-Refine-Residual-Motion-for-Image-to-Video-Generation" class="headerlink" title="DDDDDDDIFFICULTLearning to Forecast and Refine Residual Motion for Image-to-Video Generation"></a>DDDDDDDIFFICULT<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Long_Zhao_Learning_to_Forecast_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Learning to Forecast and Refine Residual Motion for Image-to-Video Generation</a></h2><p>we study a form of classic problems in video generation that can be framed as<br>image-to-video translation tasks, where a system receives one or more images<br>as the input and translates it into a video containing realistic motions of a<br>single object.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-31 at 6.12.16 PM.png" alt="creen Shot 2018-10-31 at 6.12.16 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-31 at 10.45.04 PM.png" alt="creen Shot 2018-10-31 at 10.45.04 P"></p><h1 id="2018-10-29"><a href="#2018-10-29" class="headerlink" title="2018-10-29"></a>2018-10-29</h1><h2 id="Pose-Normalized-Image-Generation-for-Person-Re-identification"><a href="#Pose-Normalized-Image-Generation-for-Person-Re-identification" class="headerlink" title="Pose-Normalized Image Generation for Person Re-identification"></a><a href="https://arxiv.org/pdf/1712.02225.pdf" target="_blank" rel="noopener">Pose-Normalized Image Generation for Person Re-identification</a></h2><p>Critically, once trained, the model can be applied to a new dataset without any model fine-tuning as long as the test image’s pose is also normalized.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.46.55 PM.png" alt="creen Shot 2018-10-29 at 9.46.55 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.45.51 PM.png" alt="creen Shot 2018-10-29 at 9.45.51 P"></p><p>we train two re-id models. One model is trained using the original images in a training set to extract identity-invariant features in the presence of pose variation. The other is trained using the synthesized images with normalized poses using our PN-GAN to compute re-id features free of pose variation. They are then fused as the final feature representat.</p><h2 id="VITON-An-Image-based-Virtual-Try-on-Network"><a href="#VITON-An-Image-based-Virtual-Try-on-Network" class="headerlink" title="VITON: An Image-based Virtual Try-on Network"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf" target="_blank" rel="noopener">VITON: An Image-based Virtual Try-on Network</a></h2><p>We present an image-based virtual try-on approach, relying merely on plain RGB images without leveraging any 3D information. we propose a virtual try-on network (VITON), a coarse-to-fine framework that seamlessly transfers a target clothing item in a product image to the corresponding region of a clothed person in a 2D image.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.06.30 PM.png" alt="creen Shot 2018-10-29 at 10.06.30 P"></p><p>The mask is then used as a guidance to warp the target clothing item to account for deformations.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.33.18 PM.png" alt="creen Shot 2018-10-29 at 10.33.18 P"></p><h2 id="Disentangled-Person-Image-Generation"><a href="#Disentangled-Person-Image-Generation" class="headerlink" title="Disentangled Person Image Generation"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Disentangled Person Image Generation</a></h2><p>Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.53.41 PM.png" alt="creen Shot 2018-10-29 at 10.53.41 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.54.11 PM.png" alt="creen Shot 2018-10-29 at 10.54.11 P"></p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.57.45 PM.png" alt="creen Shot 2018-10-29 at 10.57.45 P"></p><p>In stage one, a real image is used to train 3 independent encoders, i.e., Pose Encoder, Foreground Encoder, and Background Encoder. </p><p>In stage two, we can smaple features from 3 encoders respectively to get pose features, foreground features and background features. And combining these three features to generate imagse.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.08.43 PM.png" alt="creen Shot 2018-10-29 at 11.08.43 P"></p><p>In particular, we aim at sampling from a standard distribution, e.g. a Gaussian<br>distribution, to first generate new embedding features and from them generate new images</p><h2 id="Natural-and-Effective-Obfuscation-by-Head-Inpainting"><a href="#Natural-and-Effective-Obfuscation-by-Head-Inpainting" class="headerlink" title="Natural and Effective Obfuscation by Head Inpainting"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Natural_and_Effective_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Natural and Effective Obfuscation by Head Inpainting</a></h2><ol><li>detecting 68 facial keypoints using the python dlib toolbox <a href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf" target="_blank" rel="noopener">paper</a> </li><li>​</li></ol><p>We focus on the scenario where the user wants to obfuscate some identities in a social media photo by inpainting new heads for them. We use facial landmarks to provide strong guidance for the head inpainter. We factor the head inpainting task into two stages: (1) landmark detection or generation and (2) head inpainting conditioned on body context and landmarks.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.19.36 PM.png" alt="creen Shot 2018-10-29 at 11.19.36 P"></p><p>It takes <strong>either the original or blackhead image as input</strong>, in order to give flexibility to deal with cases where the original images are not available.</p><p>Given original or headobfuscated input, stage-I detects or generates landmarks,<br>respectively. Stage-II takes the blackhead image and landmarks as input, and outputs the generated image.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.23.53 PM.png" alt="creen Shot 2018-10-29 at 11.23.53 P"></p><h2 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Deformable GANs for Pose-based Human Image Generation</a></h2><p>Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with <strong>pixel-to-pixel misalignments caused by the pose differences</strong>, we introduce deformable skip connections in the generator of our Generative Adversarial Network. </p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-30 at 10.00.26 AM.png" alt="creen Shot 2018-10-30 at 10.00.26 A"></p><h1 id="2018-10-27"><a href="#2018-10-27" class="headerlink" title="2018-10-27"></a>2018-10-27</h1><h2 id="Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training"><a href="#Cross-Modality-Person-Re-Identification-with-Generative-Adversarial-Training" class="headerlink" title="Cross-Modality Person Re-Identification with Generative Adversarial Training"></a><a href="https://www.ijcai.org/proceedings/2018/0094.pdf" target="_blank" rel="noopener">Cross-Modality Person Re-Identification with Generative Adversarial Training</a></h2><p>studied the Re-ID between infrared and RGB images, which is essentially a cross-modality problem and widely encountered in real-world scenarios.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 9.40.42 AM.png" alt="creen Shot 2018-10-29 at 9.40.42 A"></p><h2 id="Predicting-Human-Interaction-via-Relative-Attention-Model"><a href="#Predicting-Human-Interaction-via-Relative-Attention-Model" class="headerlink" title="Predicting Human Interaction via Relative Attention Model"></a><a href="https://arxiv.org/pdf/1705.09467.pdf" target="_blank" rel="noopener">Predicting Human Interaction via Relative Attention Model</a></h2><p>Essentially, a good algorithm should effectively model the mutual influence between the two interacting subjects. Also, only a small region in the scene is discriminative for identifying the on-going interaction.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 10.22.19 AM.png" alt="creen Shot 2018-10-29 at 10.22.19 A"></p><h2 id="An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data"><a href="#An-End-to-End-Spatio-Temporal-Attention-Model-for-Human-Action-Recognition-from-Skeleton-Data" class="headerlink" title="An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data"></a><a href="https://arxiv.org/pdf/1611.06067.pdf" target="_blank" rel="noopener">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</a></h2><p>We build our model on top of the Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which learns to selectively focus on <strong>discriminative joints</strong> of skeleton <strong>within each frame</strong> of the inputs and pays <strong>different levels of attention</strong> to the outputs of <strong>different frames</strong>.</p><p>For spatial joints of skeleton, we propose a spatial attention module which conducts automatic mining of discriminative joints. A certain type of action is usually only associated with and characterized by the combinations of a subset of kinematic joints. </p><p>For a sequence, the amount of valuable information provided by different frames is in general not equal. Only some of the frames (key frames) contain the most discriminative information while the other frames provide context information.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-28 at 11.13.33 AM.png" alt="creen Shot 2018-10-28 at 11.13.33 A"></p><h2 id="Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation"><a href="#Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition-and-Detection-with-Hierarchical-Aggregation" class="headerlink" title="Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation"></a><a href="https://www.ijcai.org/proceedings/2018/0109.pdf" target="_blank" rel="noopener">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</a></h2><p>focus on the problem of skeleton-based human action recognition and detection.</p><p>By investigating the convolution operation, we may decompose it into two steps, i.e. local feature aggregation across the spatial domain (width and height) and global feature aggregation across channels.</p><p>The input is skeleton sequences and skeleton temporal differences.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.15.10 AM.png" alt="creen Shot 2018-10-29 at 11.15.10 A"></p><p>For multiple persons, inputs of multiple persons go through the same subnetwork and their conv6 feature maps are merged with either concatenation along channels or element-wise maximum / mean operation.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-29 at 11.17.51 AM.png" alt="creen Shot 2018-10-29 at 11.17.51 A"></p><p>Action detection</p><h2 id="Pose-Guided-Person-Image-Generation"><a href="#Pose-Guided-Person-Image-Generation" class="headerlink" title="Pose Guided Person Image Generation"></a><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a></h2><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-28 at 10.05.47 PM.png" alt="creen Shot 2018-10-28 at 10.05.47 P"></p><p><a href="https://arxiv.org/pdf/1601.01006.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.01006.pdf</a></p><h2 id="A2g-GAN"><a href="#A2g-GAN" class="headerlink" title="A2g-GAN"></a><a href="https://shaoanlu.wordpress.com/2018/09/12/lets-train-gans-to-play-guitar/" target="_blank" rel="noopener">A2g-GAN</a></h2><h1 id="2018-10-26"><a href="#2018-10-26" class="headerlink" title="2018-10-26"></a>2018-10-26</h1><h2 id="IJCAI-2018"><a href="#IJCAI-2018" class="headerlink" title="IJCAI 2018"></a><a href="https://github.com/CSer-Tang-hao/Papers-Reading-Recording" target="_blank" rel="noopener">IJCAI 2018</a></h2><h2 id="Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks"><a href="#Exploiting-Images-for-Video-Recognition-with-Hierarchical-Generative-Adversarial-Networks" class="headerlink" title="Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks"></a><a href="https://arxiv.org/pdf/1805.04384.pdf" target="_blank" rel="noopener">Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks</a></h2><p>The two-level HiGAN is designed to have a low-level conditional GAN and a high-level conditional GAN. The low-level conditional GAN is built to connect videos and their corresponding video frames by learning a mapping function from frame features to video features in the target domain. The high-level conditional GAN, on the other hand, is modeled to bridge the gap between source images and target videos by formulating a mapping function from video features to image-frame features.</p><h2 id="Memory-Attention-Networks-for-Skeleton-based-Action-Recognition"><a href="#Memory-Attention-Networks-for-Skeleton-based-Action-Recognition" class="headerlink" title="Memory Attention Networks for Skeleton-based Action Recognition"></a><a href="https://arxiv.org/pdf/1804.08254.pdf" target="_blank" rel="noopener">Memory Attention Networks for Skeleton-based Action Recognition</a></h2><h1 id="2018-10-25"><a href="#2018-10-25" class="headerlink" title="2018-10-25"></a>2018-10-25</h1><h2 id="StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation"><a href="#StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"></a><a href="https://arxiv.org/pdf/1711.09020.pdf" target="_blank" rel="noopener">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></h2><h2 id="Pose-Guided-Person-Image-Generation-1"><a href="#Pose-Guided-Person-Image-Generation-1" class="headerlink" title="Pose Guided Person Image Generation"></a><a href="https://papers.nips.cc/paper/6644-pose-guided-person-image-generation.pdf" target="_blank" rel="noopener">Pose Guided Person Image Generation</a></h2><h2 id="Disentangled-Person-Image-Generation-1"><a href="#Disentangled-Person-Image-Generation-1" class="headerlink" title="Disentangled Person Image Generation"></a><a href="https://arxiv.org/pdf/1712.02621.pdf" target="_blank" rel="noopener">Disentangled Person Image Generation</a></h2><h2 id="Deformable-GANs-for-Pose-based-Human-Image-Generation-1"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation-1" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation"></a><a href="https://arxiv.org/pdf/1801.00055.pdf" target="_blank" rel="noopener">Deformable GANs for Pose-based Human Image Generation</a></h2><h1 id="2018-10-23"><a href="#2018-10-23" class="headerlink" title="2018-10-23"></a>2018-10-23</h1><h2 id="Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs"><a href="#Generating-Realistic-Videos-from-Keyframes-with-Concatenated-GANs" class="headerlink" title="Generating Realistic Videos from Keyframes with Concatenated GANs"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8451971" target="_blank" rel="noopener">Generating Realistic Videos from Keyframes with Concatenated GANs</a></h2><p>Given two video frames X0 and Xn+1, we aim to generate a series of intermediate frames Y1, Y2, · · · , Yn, such that the resulting video consisting of frames X0, Y1-Yn, Xn+1 appears realistic to a human watcher.</p><h2 id="Human-Action-Generation-with-Generative-Adversarial-Networks"><a href="#Human-Action-Generation-with-Generative-Adversarial-Networks" class="headerlink" title="Human Action Generation with Generative Adversarial Networks"></a><a href="https://arxiv.org/pdf/1805.10416.pdf" target="_blank" rel="noopener">Human Action Generation with Generative Adversarial Networks</a></h2><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-23 at 11.02.50 PM.png" alt="creen Shot 2018-10-23 at 11.02.50 P"></p><h2 id="Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences"><a href="#Deep-Video-Generation-Prediction-and-Completion-of-Human-Action-Sequences" class="headerlink" title="Deep Video Generation, Prediction and Completion of Human Action Sequences"></a><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chunyan_Bai_Deep_Video_Generation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Deep Video Generation, Prediction and Completion of Human Action Sequences</a></h2><p>The model itself</p><p>is originally desi gne d for video generation, i.e., generating human action videos</p><p>from random noise. We split the generation process into two stages: ﬁrst, we</p><p>generate human skeleton sequences from random noise, and then we t r an sf orm</p><p>from the skeleton images to the real pixel-level images.</p><p><img src="/2018/10/23/Daily-Paper-Reading/Screen Shot 2018-10-26 at 9.15.48 AM.png" alt="creen Shot 2018-10-26 at 9.15.48 A"></p><p>The model is independent of training subjests, where we train the model using some subjects but test it using totally different subjects.</p><h2 id="Multiple-Granularity-Group-Interaction-Prediction"><a href="#Multiple-Granularity-Group-Interaction-Prediction" class="headerlink" title="Multiple Granularity Group Interaction Prediction"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0721.pdf" target="_blank" rel="noopener">Multiple Granularity Group Interaction Prediction</a></h2><h2 id="GestureGAN-for-Hand-Gesture-to-Gesture-Translation"><a href="#GestureGAN-for-Hand-Gesture-to-Gesture-Translation" class="headerlink" title="GestureGAN for Hand Gesture-to-Gesture Translation"></a><a href="https://arxiv.org/pdf/1808.04859.pdf" target="_blank" rel="noopener">GestureGAN for Hand Gesture-to-Gesture Translation</a></h2><h2 id="Human-Motion-Generation-via-Cross-Space-Constrained-Sampling"><a href="#Human-Motion-Generation-via-Cross-Space-Constrained-Sampling" class="headerlink" title="Human Motion Generation via Cross-Space Constrained Sampling"></a><a href="https://www.ijcai.org/proceedings/2018/0105.pdf" target="_blank" rel="noopener">Human Motion Generation via Cross-Space Constrained Sampling</a></h2>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Paper Everyday </tag>
            
            <tag> Generative adversarial networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Cooking</title>
      <link href="/2018/10/20/Cooking/"/>
      <url>/2018/10/20/Cooking/</url>
      <content type="html"><![CDATA[<h1 id="肉"><a href="#肉" class="headerlink" title="肉"></a>肉</h1><h1 id="土豆炖排骨"><a href="#土豆炖排骨" class="headerlink" title="土豆炖排骨"></a>土豆炖排骨</h1><h3 id="材料"><a href="#材料" class="headerlink" title="材料"></a>材料</h3><p>排骨，土豆，豆瓣酱，酱油，料酒，姜末，大料，花椒</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol><li>将剁成小块的猪排骨用沸水焯变色，洗去待用，郫县豆瓣酱剁碎待用</li><li>土豆去皮切成小块，用清水浸泡片刻，除去变面淀粉</li><li>锅中放入1大匙（15ml）油烧热，将土豆放入煎至金黄，盛出待用</li><li>炒锅烧热，放入2大匙（30ml）油，放入焯过水的排骨炒至金黄</li><li>放入剁碎的豆瓣酱和姜末炒匀，炒出香味，加入酱油、料酒炒匀</li><li>加入适量开水（没过排骨），烧开，加入大料和花椒，加盖转小火烧20分钟至排骨酥烂</li><li>加入煎黄的土豆块，烧5分钟至汤汁收干，用盐、糖、蚝油调味即可</li><li>​</li></ol><h2 id="土豆炖牛肉"><a href="#土豆炖牛肉" class="headerlink" title="土豆炖牛肉"></a>土豆炖牛肉</h2><h3 id="材料-1"><a href="#材料-1" class="headerlink" title="材料"></a>材料</h3><p>牛肉（牛腩），土豆，洋葱，胡萝卜，葱姜，老抽，料酒，醋，糖，番茄沙司，黑胡椒，油 盐</p><h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><ol><li>牛肉切块冷水下锅焯2分钟，全程用温水清洗干净，</li><li>锅放油烧热，先下牛肉煸炒一下，烹人老抽，料酒和一小勺白醋，</li><li>接着下洋葱西红柿，撒一勺白糖继续翻炒，</li><li>挤人番茄沙司（多挤一些），再磨一些胡椒粒炒匀，</li><li>放葱姜，倒入足够的热水，中大火烧20分钟改小火炖1小时左右，</li><li>出锅前放土豆胡萝卜块加盐烧熟，等到肉烂，汤汁粘稠就可以调味出锅了。</li></ol><h2 id="鱼香肉丝"><a href="#鱼香肉丝" class="headerlink" title="鱼香肉丝"></a>鱼香肉丝</h2><h3 id="材料-2"><a href="#材料-2" class="headerlink" title="材料"></a>材料</h3><p>食材：肉、胡萝卜、青椒、黑木耳</p><h3 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h3><ol><li>使用盐、胡椒粉、料酒、蛋清和淀粉腌制肉10min</li><li>白糖、香醋、料酒、酱油、（盐）、微量清水、淀粉兑成芡汁</li><li>下油炒肉至变色，再入胡萝卜，入豆瓣酱于一边炒出红油，下葱姜蒜末炒熟</li><li>下肉丝，最后倒入兑好的芡汁炒均匀出锅</li></ol><h1 id="饭"><a href="#饭" class="headerlink" title="饭"></a>饭</h1><h2 id="土豆焖饭"><a href="#土豆焖饭" class="headerlink" title="土豆焖饭"></a>土豆焖饭</h2><h3 id="材料-3"><a href="#材料-3" class="headerlink" title="材料"></a>材料</h3><p>米、土豆、蒜、料酒、豆瓣酱、老抽、生抽、糖、葱、香油。</p><h3 id="步骤-3"><a href="#步骤-3" class="headerlink" title="步骤"></a>步骤</h3><ol><li>土豆削皮切小块，泡在清水里防止氧化，待下锅前在沥干</li><li>炒锅入油，油烧热后倒入大蒜片爆香，倒入沥干后的土豆块，翻炒至边角边边略略焦黄，转小火，放入豆瓣酱炒香，再加入料酒、生抽、老抽、糖翻炒均匀上色，关火（这个时候土豆还是生的，不用炒熟）</li><li>把炒过的土豆、大蒜片都倒进生米里，盖上电饭煲，煮饭模式</li><li>饭煮好后开盖把土豆和饭轻轻搅匀，撒入葱花，焖10分钟，准备盛出时放一些鸡精和香油拌匀即可。</li></ol><h1 id="蔬菜"><a href="#蔬菜" class="headerlink" title="蔬菜"></a>蔬菜</h1><h2 id="金针菇日本豆腐"><a href="#金针菇日本豆腐" class="headerlink" title="金针菇日本豆腐"></a>金针菇日本豆腐</h2><h3 id="材料-4"><a href="#材料-4" class="headerlink" title="材料"></a>材料</h3><p>日本豆腐，金针菇，大葱，蒜，盐，酱油，耗油。</p><h3 id="步骤-4"><a href="#步骤-4" class="headerlink" title="步骤"></a>步骤</h3><ol><li>日本豆腐横切成块，</li><li>锅中倒油烧热，加入日本豆腐煎至表皮焦黄，沥干捞起。剩下的油再加入蒜以及大葱爆香，加入金针菇，</li><li>翻炒至金针菇软身，加入一勺酱油，一勺耗油翻炒均匀后加入日本豆腐在金针菇上面，此时再加入一勺酱油及耗油覆盖豆腐上面，盖上盖焖几分钟（2分钟左右）</li><li>打开锅盖，倒入芡汁，略微翻均匀，再盖上焖一小会儿，待汁收得差不多。</li></ol>]]></content>
      
      <categories>
          
          <category> Life Style </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Life Style </tag>
            
            <tag> Cooking </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Principal Component Analysis</title>
      <link href="/2018/10/20/Principal-Component-Analysis/"/>
      <url>/2018/10/20/Principal-Component-Analysis/</url>
      <content type="html"><![CDATA[<p>机器学习中，在高维情形下出现的数据样本稀疏、距离计算困难等问题，被称为“维数灾难”（curse of dimensionality）。而缓解该问题的一个重要途径是降维（dimension reduction），即通过某种数学变换将原始高维空间转换为一个低维“子空间”（subspace），在这个子空间中样本密度大幅度提高，距离计算也变得更加容易。</p><p>之所以可以进行降维，是因为在许多时候，人们观测或者收集到的数据样本虽然是高维的，但是与学习任务相关的也许仅仅是某个低维分布。</p><a id="more"></a><p><img src="/2018/10/20/Principal-Component-Analysis/Screen Shot 2018-10-24 at 12.56.10 PM-0403807.png" alt="creen Shot 2018-10-24 at 12.56.10 PM-040380"></p><p>成分数目选择</p><p>PCA降维一个重要的步骤是预先确定成分数目来描述数据，一种方法是计算cumulative explained variance ratio,</p><script type="math/tex; mode=display">\frac{\sum_{i=1}^{d'}\lambda_i}{\sum_{i=1}^{d}\lambda_i}</script><p>其中，$d$是原始数据维度，$d’$是数据降解之后的维度，$\lambda_i$是第$i$个特征值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">component_number</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">    digits = load_digits()  <span class="comment"># digits.data.shape=(1797, 64)</span></div><div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">    pca = PCA()</div><div class="line">    component = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">64</span>)]</div><div class="line">    pca.fit(digits.data)</div><div class="line">    plt.plot(component,np.cumsum(pca.explained_variance_ratio_))</div><div class="line">    plt.xlabel(<span class="string">"number of components"</span>)</div><div class="line">    plt.ylabel(<span class="string">"cumulative explained variance"</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810261921009.png" alt="mage-20181026192100"></p><blockquote><p>This curve quantifies how much of the total, 64-dimensional variance is contained within the first NN components. For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.</p><p>Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that we’d need about 20 components to retain 90% of the variance. Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.</p></blockquote><p><a href="https://github.com/makang101/machinelearning/blob/master/chapter10dimred/PCA.ipynb" target="_blank" rel="noopener">PCA from Scratch</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(self)</span>:</span></div><div class="line">    self.data = pd.read_csv(<span class="string">'testSet.txt'</span>,sep=<span class="string">'\t'</span>,header=<span class="keyword">None</span>,names=[<span class="string">'x1'</span>,<span class="string">'x2'</span>]).values</div><div class="line">    self.data = np.array(self.data)</div><div class="line">    self.ori_data = pd.read_csv(<span class="string">'testSet.txt'</span>,sep=<span class="string">'\t'</span>,header=<span class="keyword">None</span>,names=[<span class="string">'x1'</span>,<span class="string">'x2'</span>]).values</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(self)</span>:</span></div><div class="line">    self.d = <span class="number">1</span> <span class="comment"># dimension after reduction</span></div><div class="line">    self.load_data()</div><div class="line">    row,col = self.data.shape</div><div class="line"></div><div class="line">    <span class="comment"># Step 1: data centralization</span></div><div class="line">    col_sum = np.sum(self.data,axis=<span class="number">0</span>)</div><div class="line">    col_mean = col_sum/row</div><div class="line">    self.data -= col_mean</div><div class="line"></div><div class="line">    <span class="comment"># Step 2: calculating covariance matrix</span></div><div class="line">    cov = self.data.T.dot(self.data)</div><div class="line"></div><div class="line">    <span class="comment"># Step 3: feature factorization</span></div><div class="line">    feature_val, feature_vec = np.linalg.eig(cov)</div><div class="line"></div><div class="line">    <span class="comment"># Step 4: top n feature value and vector</span></div><div class="line">    sorted_eig = np.argsort(feature_val)</div><div class="line">    sorted_eig = sorted_eig[:-(self.d+<span class="number">1</span>):<span class="number">-1</span>] <span class="comment"># pic d values from the end to the front</span></div><div class="line">    w = feature_vec[:,sorted_eig]</div><div class="line"></div><div class="line">    <span class="comment"># Step 5: calculate low-dimension data</span></div><div class="line">    self.lowdim_data = self.data.dot(w)+col_mean</div><div class="line">    self.vis()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis</span><span class="params">(self)</span>:</span></div><div class="line">    x = self.ori_data[:,<span class="number">0</span>] <span class="comment">#(1000,2)</span></div><div class="line">    y = self.ori_data[:,<span class="number">1</span>]</div><div class="line">    fig = plt.figure()</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line">    ax.scatter(x, y)</div><div class="line">    x0 = self.lowdim_data[:, <span class="number">0</span>] <span class="comment">#(1000,2)</span></div><div class="line">    y0 = self.lowdim_data[:, <span class="number">1</span>]</div><div class="line">    ax.scatter(x0, y0, marker=<span class="string">'o'</span>, c=<span class="string">'r'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810241415598.png" alt="mage-20181024141559"></p><p><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html" target="_blank" rel="noopener">PCA Exploration</a> </p><p>我们利用<code>sklearn</code>中的方法来探索pca，首先生成样本数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(self)</span>:</span></div><div class="line">        rng = np.random.RandomState(<span class="number">1</span>)</div><div class="line">        <span class="comment"># rng.rand(2,2) - sample 2*2 from uniform distribution</span></div><div class="line">        <span class="comment"># rng.randn(2,200) - sample 2*200 from normal distribution</span></div><div class="line">        x = np.dot(rng.rand(<span class="number">2</span>,<span class="number">2</span>),rng.randn(<span class="number">2</span>,<span class="number">200</span>)).T <span class="comment"># get 2*200 mixture sample</span></div><div class="line">        plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>])</div><div class="line">        plt.axis(<span class="string">'equal'</span>)</div><div class="line">        plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810202030142.png" alt="mage-20181020203014"></p><p>明显的，<code>x</code>和<code>y</code>存在类似线性的关系，但是不像线性回归去建模得到一个从<code>x</code>到<code>y</code>的预测函数，<code>PCA</code>尝试学习<code>x</code>与<code>y</code>的关系，这种关系是由一组特征向量和特征值表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sk_pca</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">    pca = PCA(n_components=<span class="number">2</span>)</div><div class="line">    pca.fit(self.x)</div><div class="line">    print(pca.components_)</div><div class="line">    <span class="comment">### [[ 0.94446029  0.32862557]</span></div><div class="line">    <span class="comment">### [ 0.32862557 -0.94446029]]</span></div><div class="line">    print(pca.explained_variance_)</div><div class="line">    <span class="comment">### [ 0.75871884  0.01838551]</span></div></pre></td></tr></table></figure><p>其中<code>pca.components_</code>是特征向量，<code>pca.explained_variance_</code>是特征值。为了了解它们的意义，我们将之可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_component</span><span class="params">(self,v0,v1)</span>:</span></div><div class="line">    ax = plt.gca()</div><div class="line">    arrowprops = dict(arrowstyle=<span class="string">'-&gt;'</span>,linewidth=<span class="number">2</span>)</div><div class="line">    ax.annotate(<span class="string">'mean point'</span>,v1,v0,arrowprops=arrowprops) <span class="comment">#x1是箭头的终点</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sk_pca</span><span class="params">(self)</span>:</span></div><div class="line">    rng = np.random.RandomState(<span class="number">1</span>)</div><div class="line">    self.x = np.dot(rng.rand(<span class="number">2</span>,<span class="number">2</span>),rng.randn(<span class="number">2</span>,<span class="number">200</span>)).T <span class="comment"># get 2*200 mixture sample</span></div><div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">    pca = PCA(n_components=<span class="number">2</span>)</div><div class="line">    pca.fit(self.x)</div><div class="line">    plt.scatter(self.x[:,<span class="number">0</span>],self.x[:,<span class="number">1</span>])</div><div class="line">    <span class="keyword">for</span> length,vector <span class="keyword">in</span> zip(pca.explained_variance_,pca.components_):</div><div class="line">        v = vector*<span class="number">3</span>*np.sqrt(length)</div><div class="line">        self.visualize_component(pca.mean_,pca.mean_+v)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810252123309.png" alt="mage-20181025212330"></p><p>这些向量表征数据的主轴，而向量的长度描述了数据在该方向分布的重要性。</p><blockquote><p>More precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the “principal components” of the data.</p></blockquote><h5 id="PCA-in-Dimension-Reducation"><a href="#PCA-in-Dimension-Reducation" class="headerlink" title="PCA in Dimension Reducation"></a>PCA in Dimension Reducation</h5><p>在维度削减中，PCA剔除一个或多个最小的主成分，使数据在低维上的映射尽可能保存数据的多样性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dim_reduction</span><span class="params">(self)</span>:</span></div><div class="line">    rng = np.random.RandomState(<span class="number">1</span>)</div><div class="line">    x = np.dot(rng.rand(<span class="number">2</span>, <span class="number">2</span>), rng.randn(<span class="number">2</span>, <span class="number">200</span>)).T  <span class="comment"># get 2*200 mixture sample</span></div><div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">    pca = PCA(n_components=<span class="number">1</span>)</div><div class="line">    pca.fit(x)</div><div class="line">    x_pca = pca.transform(x)</div><div class="line">    print(<span class="string">"original shape:   "</span>, x.shape) <span class="comment">#original shape:    (200, 2)</span></div><div class="line">    print(<span class="string">"transformed shape:"</span>, x_pca.shape) <span class="comment">#transformed shape: (200, 1)</span></div></pre></td></tr></table></figure><p>数据被降维到1维空间，为了更加直观的理解降维效果，我们将降维后的数据还原到初始空间，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dim_reduction</span><span class="params">(self)</span>:</span></div><div class="line">    rng = np.random.RandomState(<span class="number">1</span>)</div><div class="line">    x = np.dot(rng.rand(<span class="number">2</span>, <span class="number">2</span>), rng.randn(<span class="number">2</span>, <span class="number">200</span>)).T  <span class="comment"># get 2*200 mixture sample</span></div><div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">    pca = PCA(n_components=<span class="number">1</span>)</div><div class="line">    pca.fit(x)</div><div class="line">    x_pca = pca.transform(x)</div><div class="line">    x_new = pca.inverse_transform(x_pca)</div><div class="line">    plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],label=<span class="string">'original data'</span>)</div><div class="line">    plt.scatter(x_new[:,<span class="number">0</span>],x_new[:,<span class="number">1</span>],label=<span class="string">'inverse data'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810252140521.png" alt="mage-20181025214052"></p><p>从图中可以看出PCA降维的原理：分布在比较不重要的轴的信息被移除，留下分布在重要轴的信息。</p><p>PCA for Visualization</p><p>这一次我们使用高维的数据进行可视化，先载入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits() <span class="comment">#digits.data.shape=(1797, 64)</span></div></pre></td></tr></table></figure><p>手写体数字是<code>8*8</code>图片，故有64个维度，为了理解这些数据点之间的关系，我们将之降维到平面，即2维空间：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">pca = PCA(n_components=<span class="number">2</span>)</div><div class="line">pca.fit(digits.data)</div><div class="line">projected = pca.transform(digits.data) <span class="comment">#projected.shape = (1797, 2)</span></div></pre></td></tr></table></figure><p>最终将它们可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_digits</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">    digits = load_digits() <span class="comment">#digits.data.shape=(1797, 64)</span></div><div class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">    pca = PCA(n_components=<span class="number">2</span>)</div><div class="line">    pca.fit(digits.data)</div><div class="line">    projected = pca.transform(digits.data) <span class="comment">#projected.shape = (1797, 2)</span></div><div class="line">    plt.scatter(projected[:,<span class="number">0</span>],projected[:,<span class="number">1</span>],</div><div class="line">                c=digits.target,cmap=<span class="string">'jet'</span>)</div><div class="line">    plt.xlabel(<span class="string">'component 1'</span>)</div><div class="line">    plt.ylabel(<span class="string">'component 2'</span>)</div><div class="line">    plt.colorbar()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810261632428.png" alt="mage-20181026163242"></p><p>PCA as Noise Filtering</p><blockquote><p>PCA can also be used as a filtering approach for noisy data. The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise. So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.</p></blockquote><p>下面我们以手写体数字为例，探索PCA作为噪声过滤器的效果，首先我们画出无噪声的手写体数字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits().data</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(data)</span>:</span></div><div class="line">    fig,axes = plt.subplots(<span class="number">4</span>,<span class="number">10</span>,figsize=(<span class="number">10</span>,<span class="number">4</span>),sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">            axes[i][j].imshow(data[i*<span class="number">10</span>+j].reshape(<span class="number">8</span>,<span class="number">8</span>),cmap=<span class="string">'binary'</span>)</div><div class="line">    plt.show()</div><div class="line">plot_digits(digits)</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272228567.png" alt="mage-20181027222856"></p><p>现在我们队手写体数字添加噪声：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">np.random.seed(<span class="number">42</span>)</div><div class="line">noisy = np.random.normal(digits,<span class="number">4</span>)</div><div class="line">plot_digits(noisy)</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272231320.png" alt="mage-20181027223132"></p><p>显然，添加了噪声的数据很难辨别，现在我们就去训练一个PCA进行去噪：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">pca = PCA(<span class="number">0.5</span>)</div><div class="line">pca.fit(noisy)</div><div class="line">print(pca.n_components_) <span class="comment">#12</span></div></pre></td></tr></table></figure><p>我们想要模型解释至少50%的变化，根据模型显示，需要12个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">components = pca.transform(noisy)</div><div class="line">filtered = pca.inverse_transform(components)</div><div class="line">plot_digits(filtered)</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272239305.png" alt="mage-20181027223930"></p><blockquote><p>This signal preserving/noise filtering property makes PCA a very useful feature selection routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</div><div class="line">faces = fetch_lfw_people(min_faces_per_person=<span class="number">60</span>)</div><div class="line">print(faces.data.shape) <span class="comment">#(1348, 2914)</span></div></pre></td></tr></table></figure><p>可以看到每一张图片的维度是2941=62*47，由于数据维度比较大，这次我们使用<code>RandomizedPCA</code>而非标准<code>PCA</code></p><blockquote><p>In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as “eigenvectors,” so these types of images are often called “eigenfaces”). As you can see in this figure, they are as creepy as they sound:</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> RandomizedPCA</div><div class="line">pca = RandomizedPCA(<span class="number">150</span>)</div><div class="line">pca.fit(faces.data)</div><div class="line">fig,axes = plt.subplots(<span class="number">3</span>,<span class="number">8</span>,figsize=(<span class="number">9</span>,<span class="number">4</span>),sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">for</span> i,ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">    ax.imshow(pca.components_[i].reshape(<span class="number">62</span>,<span class="number">47</span>),cmap=<span class="string">'bone'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272307483.png" alt="mage-20181027230748"></p><blockquote><p>The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let’s take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plt.plot(range(<span class="number">150</span>),np.cumsum(pca.explained_variance_ratio_))</div><div class="line">plt.xlabel(<span class="string">'number of components'</span>)</div><div class="line">plt.ylabel(<span class="string">'cumulative explained variance'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272310030.png" alt="mage-20181027231003"></p><blockquote><p>We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">components = pca.transform(faces.data)</div><div class="line">projected = pca.inverse_transform(components)</div><div class="line">fig, ax = plt.subplots(<span class="number">2</span>, <span class="number">10</span>, figsize=(<span class="number">10</span>, <span class="number">2.5</span>),</div><div class="line">                       subplot_kw=&#123;<span class="string">'xticks'</span>: [], <span class="string">'yticks'</span>: []&#125;,</div><div class="line">                       gridspec_kw=dict(hspace=<span class="number">0.1</span>, wspace=<span class="number">0.1</span>))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    ax[<span class="number">0</span>, i].imshow(faces.data[i].reshape(<span class="number">62</span>, <span class="number">47</span>), cmap=<span class="string">'binary_r'</span>)</div><div class="line">    ax[<span class="number">1</span>, i].imshow(projected[i].reshape(<span class="number">62</span>, <span class="number">47</span>), cmap=<span class="string">'binary_r'</span>)</div><div class="line"></div><div class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_ylabel(<span class="string">'full-dim\ninput'</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].set_ylabel(<span class="string">'150-dim\nreconstruction'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/20/Principal-Component-Analysis/image-201810272311402.png" alt="mage-20181027231140"></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Principal Component Analysis </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Clustering</title>
      <link href="/2018/10/12/Clustering/"/>
      <url>/2018/10/12/Clustering/</url>
      <content type="html"><![CDATA[<a id="more"></a><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>在无监督学习中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据内在性质及规律，此类学习中研究最多、应用最广的是“聚类(Clustering)”。</p><p>聚类试图将数据集中的样本划分为若干个不相交的子集，每个子集称为一个“簇(cluster)”。形式化地说，假定样本集$D=\{x_1,x_2,…,x_m\}$包含$m$个无标记的样本，每个样本$x_i=\{x_i^{1},x_i^{2},…,x_i^{n}\}$是一个$n$维特征向量，则聚类算法将$D$划分为$k$个不相交的簇。</p><h2 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>给定样本集$D=\{x_1,x_2,…,x_m\}$，“k均值”算法针对聚类所得到簇划分$C=\{C_1,C_2,…,C_k\}$最小化平方误差：</p><script type="math/tex; mode=display">E=\sum_{i=1}^{k}\sum_{x\in C_i}||x-\mu_i||_{2}^{2}</script><p>其中，$u_i=\frac{1}{|C_i|}\sum_{x\in C_i}x$是簇$C_i$的均值向量。上式在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，E值越小则簇内样本相似度越高。</p><p><img src="/2018/10/12/Clustering/Screen Shot 2018-10-15 at 10.12.21 PM.png" alt="Screen Shot 2018-10-15 at 10.12.21 PM"></p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ol><li>Cons<ul><li>需要指定k值<a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html" target="_blank" rel="noopener">Selecting the number of clusters with silhouette analysis on KMeans clustering</a> </li><li>局限于线性边界的划分<a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html" target="_blank" rel="noopener">kernelized k-means</a>   </li><li>数据量大时效率低</li></ul></li></ol><h3 id="Python例子"><a href="#Python例子" class="headerlink" title="Python例子"></a>Python例子</h3><h4 id="K-means-From-Sratch"><a href="#K-means-From-Sratch" class="headerlink" title="K-means From Sratch"></a>K-means From Sratch</h4><p>下面我们以一个例子来演示k均值算法的学习过程，数据如下所示：</p><p><img src="/2018/10/12/Clustering/Screen Shot 2018-10-15 at 10.14.19 PM.png" alt="Screen Shot 2018-10-15 at 10.14.19 PM"></p><p>记第$i$个样本称为$x_i$，每个样本包含两个属性“密度”和“含糖率”，属于二值向量。</p><p><strong>STEP1:数据读取</strong>。</p><p>我们使用panda的<code>read_csv()</code>函数读取<code>.csv</code>文件，<code>header=None</code>表示第一行也被读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(self)</span>:</span></div><div class="line">    df = pd.read_csv(self.data_path,encoding = <span class="string">'unicode_escape'</span>,header=<span class="keyword">None</span>)</div><div class="line">    self.data = df[[<span class="number">1</span>,<span class="number">2</span>]].values</div><div class="line">    self.sample_num,self.attr_num = self.data.shape</div></pre></td></tr></table></figure><p><strong>STEP2:选取k个簇均值</strong></p><p>这里我们k设置为3，随机从数据样本中选择3个样本作为初始簇均值向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.data_path = <span class="string">'watermelon4_0.csv'</span></div><div class="line">    self.cluster_center_change = <span class="keyword">True</span></div><div class="line">    self.cluser_number = <span class="number">3</span></div><div class="line">    self.epoch = <span class="number">10000</span></div><div class="line">    self.load_data()</div><div class="line"></div><div class="line">    <span class="comment">#choose 3 random smaples as initial cluster centers</span></div><div class="line">    idx = np.random.randint(self.sample_num,size=self.cluser_number)</div><div class="line">    self.cluster_center = self.data[idx]</div></pre></td></tr></table></figure><p>从上面看到，我们还设置了其他参数，如最大迭代次数10000，<code>self.cluser_number</code>就是<code>k</code>值。</p><p><strong>STPE3:处理每一个样本</strong></p><p>对每个样本，我们计算它与三个簇均值的欧氏距离，选择距离最小的簇作为该样本的归属簇。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#process each samples to find out the belonging cluster</span></div><div class="line"><span class="keyword">for</span> i,sample <span class="keyword">in</span> enumerate(self.data):</div><div class="line">    dis = []</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">        dis.append(np.sqrt(np.sum(np.square(sample - self.cluster_center[j]))))</div><div class="line">    label = dis.index(min(dis))</div><div class="line">    clusters[label].append(i)</div></pre></td></tr></table></figure><p>一旦处理完所有样本之后，我们就要更新每个簇的均值向量了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#update the cluster center</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">    temp_cluster_center = sum(self.data[clusters[i]])/len(clusters[i])</div><div class="line">    <span class="keyword">if</span> any(self.cluster_center[i]-temp_cluster_center):</div><div class="line">        self.cluster_center[i] = temp_cluster_center</div><div class="line">self.cluster = clusters</div></pre></td></tr></table></figure><p>组合在一起之后，我们的clustering更新如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">clustering</span><span class="params">(self)</span>:</span></div><div class="line">    count = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> count&lt;self.epoch:</div><div class="line">        count += <span class="number">1</span></div><div class="line">        clusters = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">            clusters.append([])</div><div class="line"></div><div class="line">        <span class="comment">#process each samples to find out the belonging cluster</span></div><div class="line">        <span class="keyword">for</span> i,sample <span class="keyword">in</span> enumerate(self.data):</div><div class="line">            dis = []</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">                dis.append(np.sqrt(np.sum(np.square(sample - self.cluster_center[j]))))</div><div class="line">            label = dis.index(min(dis))</div><div class="line">            clusters[label].append(i)</div><div class="line"></div><div class="line">        <span class="comment">#update the cluster center</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">            temp_cluster_center = sum(self.data[clusters[i]])/len(clusters[i])</div><div class="line">            <span class="keyword">if</span> any(self.cluster_center[i]-temp_cluster_center):</div><div class="line">                self.cluster_center[i] = temp_cluster_center</div><div class="line">        self.cluster = clusters</div><div class="line">        print(<span class="string">"Done with the %d clustering"</span>%count)</div></pre></td></tr></table></figure><p><strong>STEP4:每个簇内样本可视化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(self)</span>:</span></div><div class="line">    x_axis = []</div><div class="line">    y_axis = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">        cluster_data = self.data[self.cluster[i]]</div><div class="line">        tx_axis = []</div><div class="line">        ty_axis = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(cluster_data)):</div><div class="line">            tx_axis.append(cluster_data[i][<span class="number">0</span>])</div><div class="line">            ty_axis.append(cluster_data[i][<span class="number">1</span>])</div><div class="line">        x_axis.append(tx_axis)</div><div class="line">        y_axis.append(ty_axis)</div><div class="line">    cluster_center_x = []</div><div class="line">    cluster_center_y = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">        cluster_center_x.append(self.cluster_center[i][<span class="number">0</span>])</div><div class="line">        cluster_center_y.append(self.cluster_center[i][<span class="number">1</span>])</div><div class="line">    <span class="comment">#plot cluster center</span></div><div class="line">    plt.scatter(cluster_center_x,cluster_center_y,marker=<span class="string">'+'</span>)</div><div class="line">    plt.scatter(x_axis[<span class="number">0</span>],y_axis[<span class="number">0</span>],marker=<span class="string">'o'</span>,c=<span class="string">'red'</span>)</div><div class="line">    plt.scatter(x_axis[<span class="number">1</span>], y_axis[<span class="number">1</span>], marker=<span class="string">'s'</span>, c=<span class="string">'green'</span>)</div><div class="line">    plt.scatter(x_axis[<span class="number">2</span>], y_axis[<span class="number">2</span>], marker=<span class="string">'^'</span>, c=<span class="string">'blue'</span>)</div><div class="line">    plt.xlabel(<span class="string">'density'</span>)</div><div class="line">    plt.ylabel(<span class="string">'ratio-sugar'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-20181012102651677.png" alt="image-20181012102651677"></p><h4 id="K-means-in-SKlearn"><a href="#K-means-in-SKlearn" class="headerlink" title="K-means in SKlearn"></a>K-means in SKlearn</h4><p><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html" target="_blank" rel="noopener">In Depth: k-Means Clustering</a> </p><p>sklearn已经实现了k均值算法，我们直接使用它来解决问题。首先我们先生成4组二维无标签数据样本，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sklearn</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</div><div class="line">    x,y = make_blobs(n_samples=<span class="number">300</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,cluster_std=<span class="number">0.6</span>,random_state=<span class="number">0</span>)</div><div class="line">    plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],s=<span class="number">50</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-20181016113124346.png" alt="image-20181016113124346"></p><p>接下来，我们使用sklearn内置k-mean方法对上面的数据聚类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line">kmeans = KMeans(n_clusters=<span class="number">4</span>)</div><div class="line">kmeans.fit(x)</div><div class="line">y_pre = kmeans.predict(x)</div><div class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c = y_pre,s=<span class="number">50</span>,cmap=<span class="string">'jet'</span>)</div><div class="line"></div><div class="line">cluster_cenetr = kmeans.cluster_centers_</div><div class="line">plt.scatter(cluster_cenetr[:,<span class="number">0</span>],cluster_cenetr[:,<span class="number">1</span>],c=<span class="string">'k'</span>,s=<span class="number">60</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-20181016114128997.png" alt="image-20181016114128997"></p><h4 id="K-means-in-Digits-Clustering"><a href="#K-means-in-Digits-Clustering" class="headerlink" title="K-means in Digits Clustering"></a>K-means in Digits Clustering</h4><p>给定数字图，我们将它分为10个类。</p><p>首先我们载入数据，总共是1797个$8\times 8$的图片，这样每个样本就是64个特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">digits</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">    digits = load_digits()</div><div class="line">    <span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line">    kmeans = KMeans(n_clusters=<span class="number">10</span>)</div><div class="line">    y_pred = kmeans.fit_predict(digits.data)</div><div class="line"></div><div class="line">    <span class="comment">#可视化聚类中心</span></div><div class="line">    clusters_center = kmeans.cluster_centers_.reshape([<span class="number">10</span>,<span class="number">8</span>,<span class="number">8</span>])</div><div class="line">    (fig,ax) = plt.subplots(<span class="number">2</span>,<span class="number">5</span>,figsize=(<span class="number">8</span>,<span class="number">3</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">            idx = (i+<span class="number">1</span>)*(j+<span class="number">1</span>)<span class="number">-1</span></div><div class="line">            ax[i,j].imshow(clusters_center[idx])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-20181016121826220.png" alt="image-20181016121826220"></p><h4 id="k-means-for-color-compression"><a href="#k-means-for-color-compression" class="headerlink" title="k-means for color compression"></a><em>k</em>-means for color compression</h4><h2 id="高斯混合聚类"><a href="#高斯混合聚类" class="headerlink" title="高斯混合聚类"></a>高斯混合聚类</h2><p><code>k-means</code>聚类属于hard assignments，即每一个点一定要被分配到一个类中；但是如果两个真实的簇相互重叠，在重叠区域的点可以同时属于两个类，但是<code>k-means</code>只会分配点到更近的簇中。而高斯混合模型聚类(Gaussian Mixture Models)解决这个问题。我们首先回顾高斯分布，然后介绍高斯混合模型，以及模型求解的方法EM算法。</p><p><strong>一元高斯分布</strong>的密度函数为：</p><script type="math/tex; mode=display">N(x;\mu, \sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-u)^2}{2\sigma^2}}</script><p><strong>多元高斯分布</strong>的密度函数：</p><p>一元高斯分布函数中的$x$是标量，多元高斯分布函数中的$x$是一个向量。</p><script type="math/tex; mode=display">N(x;\mu,\sum)=\frac{1}{(2\pi)^{\frac{n}{2}}|\sum|^\frac{1}{2}}e^{\frac{1}{2}(x-u)^T}(\sum)^{-1}(x-u)</script><p>其中$\mu$是$n$维均值向量，$\sum$是$n\times n$的协方差矩阵。</p><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p><a href="https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/" target="_blank" rel="noopener">ref1</a> <a href="http://statweb.stanford.edu/~tibs/stat315a/LECTURES/em.pdf" target="_blank" rel="noopener">ref2</a> <a href="https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/mixture.html" target="_blank" rel="noopener">ref3</a> <a href="http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/clustering.pdf" target="_blank" rel="noopener">ref4</a> <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html" target="_blank" rel="noopener">ref5</a> </p><p>假设数据点来自几个不同的高斯分布，这样所有样本点混合一起，构成了一个高斯混合分布：</p><script type="math/tex; mode=display">P_M(x)=\sum_{i=1}^{k}\alpha_i\cdotp(x|\mu_i,(\sum)_{i})</script><p>该分布共由$k$个混合分布组成，每个混合分布对应一个高斯分布，其中$u_i$与$\sum_i$是第$i$个高斯混合成分的参数，而$\alpha_i&gt;0$为相应的混合系数，$\sum_{i=1}^{k}=1$。</p><p>假设样本的生成过程由高斯混合分布给出：首先，根据$\alpha_1, \alpha_2,…,\alpha_k$定义的先验分布选择高斯分布混合成分，其中$\alpha_i$为选择第$i$个混合成分的概率；然后，根据被选中的混合成分的概率密度函数进行采样，从而生成相应的样本。</p><p><img src="/2018/10/12/Clustering/Screen Shot 2018-10-17 at 12.45.09 PM.png" alt="creen Shot 2018-10-17 at 12.45.09 P"></p><h3 id="Python例子-1"><a href="#Python例子-1" class="headerlink" title="Python例子"></a>Python例子</h3><h4 id="GMM-from-scratch"><a href="#GMM-from-scratch" class="headerlink" title="GMM from scratch"></a>GMM from scratch</h4><p>同样以上面的西瓜数据集为例，令高斯混合成分的个数$k=3$。算法开始时，假定高斯混合分布的模型参数初始化为：$\alpha_1=\alpha_2=\alpha_3=\frac{1}{3}$, $\sum_1=\sum_2=\sum_3=(\begin{matrix}0.1 &amp; 0.0   \\0.0 &amp; 0.1  \end{matrix})$ ， 随机选择$k$个样本点作为初始聚类中心。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.data_path = <span class="string">'watermelon4_0.csv'</span></div><div class="line">    self.cluster_center_change = <span class="keyword">True</span></div><div class="line">    self.cluser_number = <span class="number">3</span></div><div class="line">    self.max_iter = <span class="number">10000</span></div><div class="line">    self.cluster = []</div><div class="line">    self.load_data()</div><div class="line"></div><div class="line">    <span class="comment"># choose 3 random smaples as initial cluster centers</span></div><div class="line">    idx = np.random.randint(self.sample_num, size=self.cluser_number)</div><div class="line">    self.mean = self.data[idx]</div><div class="line">    self.sigma = np.array([[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]],[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]],[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(self)</span>:</span></div><div class="line">    df = pd.read_csv(self.data_path, encoding=<span class="string">'unicode_escape'</span>, header=<span class="keyword">None</span>)</div><div class="line">    self.data = df[[<span class="number">1</span>, <span class="number">2</span>]].values</div><div class="line">    self.sample_num, self.attr_num = self.data.shape</div></pre></td></tr></table></figure><p>根据多元高斯分布密度函数公式，我们完成密度函数计算：</p><script type="math/tex; mode=display">N(x;\mu,\sum)=\frac{1}{(2\pi)^{\frac{n}{2}}|\sum|^\frac{1}{2}}e^{\frac{1}{2}(x-u)^T}(\sum)^{-1}(x-u)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_pdf</span><span class="params">(self,x,u,sigma)</span>:</span></div><div class="line">    n = np.shape(x)[<span class="number">0</span>]</div><div class="line">    expOn = <span class="number">-0.5</span>*(x-u).dot(np.linalg.inv(sigma)).dot((x-u).T)</div><div class="line">    divBy = (<span class="number">2</span>*np.pi)**(n/<span class="number">2</span>)*(np.linalg.det(sigma))**(<span class="number">.5</span>)</div><div class="line">    <span class="keyword">return</span> pow(np.e,expOn)/divBy</div></pre></td></tr></table></figure><p>E-step: 针对每个样本计算属于各个混合成分的概率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">clustering</span><span class="params">(self)</span>:</span></div><div class="line">    self.alpha = [<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>]</div><div class="line">    gamma = np.zeros((self.sample_num,self.cluser_number))</div><div class="line">    iter = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> (iter&lt;self.max_iter):</div><div class="line">        iter += <span class="number">1</span></div><div class="line">        <span class="comment"># E step</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.sample_num):</div><div class="line">            sumAlpha = <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.cluser_number): <span class="comment">#for each sample, process the prob for each cluster</span></div><div class="line">                gamma[i,k] = self.alpha[k]*self.gaussian_pdf(self.data[i],self.mean[k],self.sigma[k])</div><div class="line">                sumAlpha += gamma[i,k]</div><div class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">                gamma[i,k] /= sumAlpha</div><div class="line">        sumGamma = np.sum(gamma,axis=<span class="number">0</span>)</div></pre></td></tr></table></figure><p>M-step: 更新高斯分布参数$u$和$\sum_i$, 以及混合系数$\alpha$ </p><script type="math/tex; mode=display">\mu_{i}^{'}=\frac{\sum_{j=1}^{m}\gamma_{ji}x_j}{\sum_{j=1}^{m}\gamma_{ji}} \\{\sum}^{'}_{i}=\frac{\sum_{j=1}^{m}\gamma_{ji}(x_j-\mu_{i}^{'})(x_j-\mu_{i}^{'})^T}{\sum_{j=1}^{m}\gamma_{ji}}\\\alpha_{i}^{'}=\frac{\sum_{j=1}^{m}\gamma_{ji}}{m}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(self.cluser_number):</div><div class="line">    self.mean[k] = np.zeros((<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">    self.sigma[k] = np.zeros((<span class="number">2</span>,<span class="number">2</span>))</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(self.sample_num):</div><div class="line">        self.mean[k] += gamma[j,k]*self.data[j]</div><div class="line">    self.mean[k] = self.mean[k] / sumGamma[k]</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(self.sample_num):</div><div class="line">        self.sigma[k] += gamma[j,k]*(self.data[j]-self.mean[k]).T*(self.data[j]-self.mean[k])</div><div class="line">    self.sigma[k] /= sumGamma[k]</div><div class="line">    self.alpha[k] = sumGamma[k]/self.sample_num</div><div class="line">cluster = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.cluser_number)]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(self.sample_num):</div><div class="line">    maxP = list(gamma[i]).index(max(gamma[i]))</div><div class="line">    cluster[maxP].append(i)</div><div class="line">self.cluster = cluster</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810192108345.png" alt="mage-20181019210834"></p><p>完整代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"> <span class="comment"># 预处理数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filename)</span>:</span></div><div class="line">    df = pd.read_csv(<span class="string">'watermelon4_0.csv'</span>, encoding=<span class="string">'unicode_escape'</span>, header=<span class="keyword">None</span>)</div><div class="line">    data = df[[<span class="number">1</span>, <span class="number">2</span>]].values</div><div class="line">    <span class="keyword">return</span> data</div><div class="line"></div><div class="line"> <span class="comment"># 高斯分布的概率密度函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob</span><span class="params">(x, mu, sigma)</span>:</span></div><div class="line">     n = np.shape(x)[<span class="number">1</span>]</div><div class="line">     expOn = float(<span class="number">-0.5</span> * (x - mu) * (sigma.I) * ((x - mu).T))</div><div class="line">     divBy = pow(<span class="number">2</span> * np.pi, n / <span class="number">2</span>) * pow(np.linalg.det(sigma), <span class="number">0.5</span>)  <span class="comment"># np.linalg.det 计算矩阵的行列式</span></div><div class="line">     <span class="comment"># print(pow(np.e, expOn) / divBy)</span></div><div class="line">     <span class="keyword">return</span> pow(np.e, expOn) / divBy</div><div class="line"></div><div class="line"> <span class="comment"># EM算法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span><span class="params">(dataMat, maxIter=<span class="number">50</span>)</span>:</span></div><div class="line">     m, n = np.shape(dataMat)</div><div class="line">     <span class="comment"># 1.初始化各高斯混合成分参数</span></div><div class="line">     alpha = [<span class="number">1</span> / <span class="number">3</span>, <span class="number">1</span> / <span class="number">3</span>, <span class="number">1</span> / <span class="number">3</span>]   <span class="comment"># 1.1初始化 alpha1=alpha2=alpha3=1/3</span></div><div class="line">     mu = [dataMat[<span class="number">5</span>, :], dataMat[<span class="number">21</span>, :], dataMat[<span class="number">26</span>, :]] <span class="comment"># 1.2初始化 mu1=x6,mu2=x22,mu3=x27</span></div><div class="line">     sigma = [np.mat([[<span class="number">0.1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>]]) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>)]    <span class="comment"># 1.3初始化协方差矩阵</span></div><div class="line">     gamma = np.mat(np.zeros((m, <span class="number">3</span>)))</div><div class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(maxIter):</div><div class="line">         <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</div><div class="line">             sumAlphaMulP = <span class="number">0</span></div><div class="line">             <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">                 gamma[j, k] = alpha[k] * prob(dataMat[j, :], mu[k], sigma[k]) <span class="comment"># 4.计算混合成分生成的后验概率，即gamma</span></div><div class="line">                 sumAlphaMulP += gamma[j, k]</div><div class="line">             <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">                 gamma[j, k] /= sumAlphaMulP</div><div class="line">         sumGamma = np.sum(gamma, axis=<span class="number">0</span>)</div><div class="line">         <span class="comment"># print(sumGamma)</span></div><div class="line"></div><div class="line">         <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">             mu[k] = np.mat(np.zeros((<span class="number">1</span>, n)))</div><div class="line">             sigma[k] = np.mat(np.zeros((n, n)))</div><div class="line">             <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</div><div class="line">                 mu[k] += gamma[j, k] * dataMat[j, :]</div><div class="line">             mu[k] /= sumGamma[<span class="number">0</span>, k] <span class="comment">#  7.计算新均值向量</span></div><div class="line">             <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</div><div class="line">                 sigma[k] += gamma[j, k] * (dataMat[j, :] - mu[k]).T *(dataMat[j, :] - mu[k])</div><div class="line">             sigma[k] /= sumGamma[<span class="number">0</span>, k]  <span class="comment"># 8. 计算新的协方差矩阵</span></div><div class="line">             alpha[k] = sumGamma[<span class="number">0</span>, k] / m   <span class="comment"># 9. 计算新混合系数</span></div><div class="line">             <span class="comment"># print(mu)</span></div><div class="line">     <span class="keyword">return</span> gamma</div><div class="line"></div><div class="line"></div><div class="line"> <span class="comment"># init centroids with random samples</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(dataMat, k)</span>:</span></div><div class="line">     numSamples, dim = dataMat.shape</div><div class="line">     centroids = np.zeros((k, dim))</div><div class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">         index = int(np.random.uniform(<span class="number">0</span>, numSamples))</div><div class="line">         centroids[i, :] = dataMat[index, :]</div><div class="line">     <span class="keyword">return</span> centroids</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussianCluster</span><span class="params">(dataMat)</span>:</span></div><div class="line">     m, n = np.shape(dataMat)</div><div class="line">     centroids = initCentroids(dataMat, m)  <span class="comment">## step 1: init centroids</span></div><div class="line">     clusterAssign = np.mat(np.zeros((m, <span class="number">2</span>)))</div><div class="line">     gamma = EM(dataMat)</div><div class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">         <span class="comment"># amx返回矩阵最大值，argmax返回矩阵最大值所在下标</span></div><div class="line">         clusterAssign[i, :] = np.argmax(gamma[i, :]), np.amax(gamma[i, :])  <span class="comment"># 15.确定x的簇标记lambda</span></div><div class="line">         <span class="comment">## step 4: update centroids</span></div><div class="line">     <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</div><div class="line">         pointsInCluster = dataMat[np.nonzero(clusterAssign[:, <span class="number">0</span>].A == j)[<span class="number">0</span>]]</div><div class="line">         centroids[j, :] = np.mean(pointsInCluster, axis=<span class="number">0</span>)  <span class="comment"># 计算出均值向量</span></div><div class="line">     <span class="keyword">return</span> centroids, clusterAssign</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(dataMat, k, centroids, clusterAssment)</span>:</span></div><div class="line">     numSamples, dim = dataMat.shape</div><div class="line">     <span class="keyword">if</span> dim != <span class="number">2</span>:</div><div class="line">         print(<span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span>)</div><div class="line">         <span class="keyword">return</span> <span class="number">1</span></div><div class="line"></div><div class="line">     mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</div><div class="line">     <span class="keyword">if</span> k &gt; len(mark):</div><div class="line">         print(<span class="string">"Sorry! Your k is too large!"</span>)</div><div class="line">         <span class="keyword">return</span> <span class="number">1</span></div><div class="line"></div><div class="line">         <span class="comment"># draw all samples</span></div><div class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(numSamples):</div><div class="line">         markIndex = int(clusterAssment[i, <span class="number">0</span>])</div><div class="line">         plt.plot(dataMat[i, <span class="number">0</span>], dataMat[i, <span class="number">1</span>], mark[markIndex])</div><div class="line"></div><div class="line">     mark = [<span class="string">'Dr'</span>, <span class="string">'Db'</span>, <span class="string">'Dg'</span>, <span class="string">'Dk'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]</div><div class="line">     <span class="comment"># draw the centroids</span></div><div class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">         plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize=<span class="number">12</span>)</div><div class="line"></div><div class="line">     plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">     dataMat = np.mat(loadData(<span class="string">'watermelon4.txt'</span>))</div><div class="line">     centroids, clusterAssign = gaussianCluster(dataMat)</div><div class="line">     print(clusterAssign)</div><div class="line">     showCluster(dataMat, <span class="number">3</span>, centroids, clusterAssign)</div></pre></td></tr></table></figure></div></div><h4 id="GMM-in-Sklearn"><a href="#GMM-in-Sklearn" class="headerlink" title="GMM in Sklearn"></a>GMM in Sklearn</h4><p>sklearn内置GMM算法，为了熟悉该方法，我们生成一组400个样本数据，隶属于4个簇：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gmm</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</div><div class="line">    x,y = make_blobs(n_samples=<span class="number">400</span>,centers=<span class="number">4</span>,cluster_std=<span class="number">0.4</span>,random_state=<span class="number">0</span>)</div><div class="line">    <span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GMM</div><div class="line">    gmm = GMM(n_components=<span class="number">4</span>).fit(x)</div><div class="line">    lables = gmm.predict(x)</div><div class="line">    plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=lables,s=<span class="number">40</span>,cmap=<span class="string">'jet'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810171341383.png" alt="mage-20181017134138"></p><p>GMM给出了分类的置信度，<code>predict_proba</code>返回一个大小为<code>[n_samples,n_clusters]</code>的矩阵，每个元素值反应了样本点属于某个簇的概率大小。这样我们就可以将分类的不确定度可视化，通过使每个点的大小与分类确定度成正比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gmm</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</div><div class="line">    x,y = make_blobs(n_samples=<span class="number">400</span>,centers=<span class="number">4</span>,cluster_std=<span class="number">0.4</span>,random_state=<span class="number">0</span>)</div><div class="line">    <span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GMM</div><div class="line">    gmm = GMM(n_components=<span class="number">4</span>).fit(x)</div><div class="line">    lables = gmm.predict(x)</div><div class="line">    prob = gmm.predict_proba(x)</div><div class="line">    size = <span class="number">50</span>*(prob.max(<span class="number">1</span>))**<span class="number">2</span></div><div class="line">    plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=lables,s=size,cmap=<span class="string">'jet'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810171354542.png" alt="mage-20181017135454"></p><h4 id="GMM-as-Density-Estimation"><a href="#GMM-as-Density-Estimation" class="headerlink" title="GMM as Density Estimation"></a>GMM as Density Estimation</h4><p>除了聚类，高斯混合模型更常用于密度估计，即根据已知的数据样本去拟合该数据的分布，从而生成新的数据，这也使高斯混合模型成为一个生成模型。接下来我们生成一些数据样本，然后利用GMM去拟合该分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">density_estimation</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</div><div class="line">    x,y = make_moons(n_samples=<span class="number">200</span>,noise=<span class="number">.05</span>,random_state=<span class="number">0</span>)</div><div class="line">    plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y,cmap=<span class="string">'jet'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810182213007.png" alt="mage-20181018221300"></p><p>即我们生成的数据是2维度的，如果我们使用2成分的高斯混合模型去聚类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">density_estimation</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</div><div class="line">    x,y = make_moons(n_samples=<span class="number">200</span>,noise=<span class="number">.05</span>,random_state=<span class="number">0</span>)</div><div class="line">    <span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GMM</div><div class="line">    gmm = GMM(n_components=<span class="number">2</span>).fit(x)</div><div class="line">    labels = gmm.predict(x)</div><div class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=labels, cmap=<span class="string">'jet'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810182215480.png" alt="mage-20181018221548"></p><p>可以看到基于最近聚类的方法在这个例子中是不准确的，但是如果我们多增加几个成分，即成分个数变成16，<code>gmm = GMM(n_components=16).fit(x)</code></p><p><img src="/2018/10/12/Clustering/image-201810182219585.png" alt="mage-20181018221958"></p><p>实际上，这16个高斯成分并不是用来数据划分，而是去建模数据整体分布。这是数据的生成模型，即GMM给了如何生成样本数据的模型，比如我们使用该模型去采样500个数据点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">density_estimation</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</div><div class="line">    x,y = make_moons(n_samples=<span class="number">200</span>,noise=<span class="number">.05</span>,random_state=<span class="number">0</span>)</div><div class="line">    <span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GMM</div><div class="line">    gmm = GMM(n_components=<span class="number">16</span>).fit(x)</div><div class="line">    new_samples = gmm.sample(n_samples=<span class="number">400</span>)</div><div class="line">    plt.scatter(new_samples[:,<span class="number">0</span>],new_samples[:,<span class="number">1</span>])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810182225050.png" alt="mage-20181018222505"></p><h4 id="GMM-in-Data-Generation"><a href="#GMM-in-Data-Generation" class="headerlink" title="GMM in Data Generation"></a>GMM in Data Generation</h4><p>上面的例子简单地介绍了GMM在数据生成的应用，接下来我们介绍如何利用GMM 生成手写体数字。首先，我们先载入手写体数字：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritten_estimation</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">    (digits,target) = load_digits(return_X_y=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>我们选取前100个数据进行可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritten_estimation</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">    (digits,target) = load_digits(return_X_y=<span class="keyword">True</span>)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></div><div class="line">        fig,ax = plt.subplots(nrows=<span class="number">10</span>,ncols=<span class="number">10</span>,figsize=(<span class="number">8</span>, <span class="number">8</span>),</div><div class="line">                              subplot_kw=dict(xticks=[], yticks=[]))</div><div class="line">        fig.subplots_adjust(hspace=<span class="number">0.05</span>, wspace=<span class="number">0.05</span>)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">                img = digits[i*<span class="number">10</span>+j].reshape(<span class="number">8</span>,<span class="number">8</span>)</div><div class="line">                im = ax[i,j].imshow(img,cmap=<span class="string">'binary'</span>) <span class="comment">## 图片可视化 imshow()</span></div><div class="line">        plt.show()</div><div class="line">    plot()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810182244258.png" alt="mage-20181018224425"></p><p>现在我们要使用GMM去拟合手写体，但是因为每一张图片的维度是64，对GMM而言，数据维度太高使模型不容易收敛，所以我们使用PCA进行降维。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">pca = PCA(<span class="number">0.99</span>, whiten=<span class="keyword">True</span>)</div><div class="line">data = pca.fit_transform(digits) <span class="comment">#data.shape = (1797, 41)</span></div></pre></td></tr></table></figure><p>最终降维到41，几乎$\frac{1}{3}$无用的信息被去除。接下来我们使用<code>AIC</code>指标来指导我们如何选取最优的成分数目。</p><p><img src="/2018/10/12/Clustering/image-201810182254181.png" alt="mage-20181018225418"></p><p>从图像中可以看到高斯成分数目为110是最佳的，所以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">gmm = GMM(<span class="number">110</span>, covariance_type=<span class="string">'full'</span>, random_state=<span class="number">0</span>)</div><div class="line">gmm.fit(data)</div><div class="line">print(gmm.converged_) <span class="comment">#true</span></div><div class="line">data_new = gmm.sample(<span class="number">100</span>, random_state=<span class="number">0</span>) <span class="comment">#data_new.shape=(100,41)</span></div><div class="line">digits_new = pca.inverse_transform(data_new)</div><div class="line">plot_digits(digits_new)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/10/12/Clustering/image-201810182300164.png" alt="mage-20181018230016"></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Clustering </tag>
            
            <tag> Gaussian Mixture Model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Array</title>
      <link href="/2018/10/04/Array/"/>
      <url>/2018/10/04/Array/</url>
      <content type="html"><![CDATA[<h3 id="简单"><a href="#简单" class="headerlink" title="简单"></a>简单</h3><h4 id="Move-Zeros"><a href="#Move-Zeros" class="headerlink" title="Move Zeros"></a>Move Zeros</h4><h5 id="题目-类似"><a href="#题目-类似" class="headerlink" title="题目 类似"></a><a href="https://leetcode.com/problems/move-zeroes/description/" target="_blank" rel="noopener">题目</a> <a href="https://leetcode.com/problems/remove-element/description/" target="_blank" rel="noopener">类似</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array <code>nums</code>, write a function to move all <code>0</code>‘s to the end of it while maintaining the relative order of the non-zero elements.</p><p><strong>Example:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">12</span>]</div><div class="line">Output: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">12</span>,<span class="number">0</span>,<span class="number">0</span>]</div></pre></td></tr></table></figure><p><strong>Note</strong>:</p><ol><li>You must do this <strong>in-place</strong> without making a copy of the array.</li><li>Minimize the total number of operations.</li></ol></div></div><h5 id="思路"><a href="#思路" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/4822732.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：让我们将一个给定数组中所有的0都移到后面，把非零数前移，要求不能改变非零数的相对应的位置关系，而且不能拷贝额外的数组，那么只能用替换法in-place来做，需要用两个指针，一个不停的向后扫，找到非零位置，然后和前面那个指针交换位置即可</p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">class Solution &#123;</div><div class="line">public:</div><div class="line">    void moveZeroes(vector&lt;int&gt;&amp; nums) &#123;</div><div class="line">        int l = nums.size();</div><div class="line">        for (int i=0,j=0;i&lt;l;i++)&#123;</div><div class="line">            if(nums[i]!=0)&#123;</div><div class="line">                swap(nums[i],nums[j++]);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Rotate-Array"><a href="#Rotate-Array" class="headerlink" title="Rotate Array"></a>Rotate Array</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/rotate-array/description/" target="_blank" rel="noopener">题目</a></h5><p>Given an array, rotate the array to the right by <em>k</em> steps, where <em>k</em> is non-negative.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>] and k = <span class="number">3</span></div><div class="line">Output: [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line">Explanation:</div><div class="line">rotate <span class="number">1</span> steps to the right: [<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</div><div class="line">rotate <span class="number">2</span> steps to the right: [<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</div><div class="line">rotate <span class="number">3</span> steps to the right: [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: [-<span class="number">1</span>,-<span class="number">100</span>,<span class="number">3</span>,<span class="number">99</span>] and k = <span class="number">2</span></div><div class="line">Output: [<span class="number">3</span>,<span class="number">99</span>,-<span class="number">1</span>,-<span class="number">100</span>]</div><div class="line">Explanation: </div><div class="line">rotate <span class="number">1</span> steps to the right: [<span class="number">99</span>,-<span class="number">1</span>,-<span class="number">100</span>,<span class="number">3</span>]</div><div class="line">rotate <span class="number">2</span> steps to the right: [<span class="number">3</span>,<span class="number">99</span>,-<span class="number">1</span>,-<span class="number">100</span>]</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ul><li>Try to come up as many solutions as you can, there are at least 3 different ways to solve this problem.</li><li>Could you do it in-place with O(1) extra space?</li></ul><h5 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/4298711.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：以k为旋转点旋转数组。</p><p>为了防止k大于数组的长度，我们需要对k做取余操作。</p><p>法一：由于旋转数组的操作也可以看做从数组的末尾取k个数组放入数组的开头，所以我们用STL的push_back和erase可以很容易的实现这些操作。</p><p>法二：可以利用三次翻转操作，第一次全翻：<code>7,6,5,4,3,2,1</code>；第二次翻转k之前部分<code>5,6,7||4,3,2,1</code>；最后翻转k之后<code>5,6,7||1,2,3,4</code></p><h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//法一</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">rotate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        k = k%l;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l-k;i++)&#123;</div><div class="line">            nums.push_back(nums[<span class="number">0</span>]);</div><div class="line">            nums.erase(nums.begin());</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//法二</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">rotate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</div><div class="line">        k = k%nums.size();</div><div class="line">        nums = helper(nums,<span class="number">0</span>,nums.size()<span class="number">-1</span>);</div><div class="line">        nums = helper(nums,<span class="number">0</span>,k<span class="number">-1</span>);</div><div class="line">        nums = helper(nums,k,nums.size()<span class="number">-1</span>);        </div><div class="line">        </div><div class="line">    &#125;</div><div class="line">    <span class="keyword">public</span>: <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; helper(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> s, <span class="keyword">int</span> e)&#123;</div><div class="line">        <span class="keyword">int</span> mid = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>(s&lt;e)&#123;</div><div class="line">            mid = nums[s];</div><div class="line">            nums[s] = nums[e];</div><div class="line">            nums[e] = mid;</div><div class="line">            s++;</div><div class="line">            e--;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> nums;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><h4 id="Pascal’s-Triangle-II"><a href="#Pascal’s-Triangle-II" class="headerlink" title="Pascal’s Triangle II"></a>Pascal’s Triangle II</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/pascals-triangle-ii/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a non-negative index <em>k</em> where <em>k</em> ≤ 33, return the <em>k</em>th index row of the Pascal’s triangle.Note that the row index starts from 0.</p><p><img src="/2018/10/04/Array/PascalTriangleAnimated2.gif" alt="ascalTriangleAnimated"></p><p><strong>Example:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: <span class="number">3</span></div><div class="line">Output: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>]</div></pre></td></tr></table></figure><p><strong>Follow up:</strong></p><p>Could you optimize your algorithm to use only <em>O</em>(<em>k</em>) extra space?</p></div></div><h5 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a><a href="https://www.jianshu.com/p/4b40cb8e4ca3" target="_blank" rel="noopener">思路</a></h5><p>题目大意：返回Pascal三角的第<code>k</code>行。</p><p>第一种想法：借助一个二维数组，用来存储Pascal数列，这样空间复杂度是<code>O(N*N)</code></p><p>第二种想法：借助一个一维动态数组，使用双重循环，外循环处理每一行，每次都插入一个1，内循环处理每一行的元素，则当前元素更新：<code>nums[i]+=nums[i+1]</code></p><h5 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; getRow(<span class="keyword">int</span> rowIndex) &#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; pascal(rowIndex+<span class="number">1</span>);</div><div class="line">        <span class="keyword">if</span> (rowIndex&gt;=<span class="number">0</span>)&#123;</div><div class="line">            pascal[<span class="number">0</span>] = <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(<span class="number">1</span>,<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=rowIndex;i++)&#123;</div><div class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; temp(i+<span class="number">1</span>,<span class="number">1</span>);</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;i;j++)&#123;</div><div class="line">                temp[j]=pascal[i<span class="number">-1</span>][j<span class="number">-1</span>]+pascal[i<span class="number">-1</span>][j];</div><div class="line">            &#125;</div><div class="line">            pascal[i]=temp;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> pascal[rowIndex];</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; getRow(<span class="keyword">int</span> rowIndex) &#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;rowIndex+<span class="number">1</span>;i++)&#123;</div><div class="line">            res.insert(begin(res),<span class="number">1</span>);</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;res.size()<span class="number">-1</span>;j++)&#123;</div><div class="line">                res[j]+=res[j+<span class="number">1</span>];</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Plus-One"><a href="#Plus-One" class="headerlink" title="Plus One"></a>Plus One</h4><h5 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/plus-one/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a <strong>non-empty</strong> array of digits representing a non-negative integer, plus one to the integer.</p><p>The digits are stored such that the most significant digit is at the head of the list, and each element in the array contain a single digit.</p><p>You may assume the integer does not contain any leading zero, except the number 0 itself.</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">Output: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>]</div><div class="line">Explanation: The <span class="built_in">array</span> represents the integer <span class="number">123.</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]</div><div class="line">Output: [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">Explanation: The <span class="built_in">array</span> represents the integer <span class="number">4321.</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h5><p>题目大意：一个整数的每一位被存在一个数组里，现在求该整数+1的结果。</p><p>主要的问题就是进位问题：如果该位小于9，那么该位+1，返回数组；否则，该位置空，处理下一位。最极端情况就是每一位都是9，那么数组全置空，增加一位，置1.</p><h5 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; plusOne(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; digits) &#123;</div><div class="line">        <span class="keyword">int</span> l = digits.size();</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=l<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</div><div class="line">            <span class="keyword">if</span>(digits[i]&lt;<span class="number">9</span>)&#123;</div><div class="line">                digits[i]++;</div><div class="line">                <span class="keyword">return</span> digits;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                digits[i]=<span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(l+<span class="number">1</span>);</div><div class="line">        res[<span class="number">0</span>]=<span class="number">1</span>;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Remove-Duplicates-from-Sorted-Array"><a href="#Remove-Duplicates-from-Sorted-Array" class="headerlink" title="Remove Duplicates from Sorted Array"></a>Remove Duplicates from Sorted Array</h4><h5 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-array/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a sorted array <em>nums</em>, remove the duplicates <a href="https://en.wikipedia.org/wiki/In-place_algorithm" target="_blank" rel="noopener"><strong>in-place</strong></a> such that each element appear only <em>once</em> and return the new length.</p><p>Do not allocate extra space for another array, you must do this by <strong>modifying the input array in-place</strong> with O(1) extra memory.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Given nums = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line"></div><div class="line">Your function should <span class="keyword">return</span> length = <span class="number">2</span>, with the first two elements of nums being <span class="number">1</span> and <span class="number">2</span> respectively.</div><div class="line"></div><div class="line">It doesn<span class="string">'t matter what you leave beyond the returned length.</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Given nums = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>],</div><div class="line"></div><div class="line">Your function should <span class="keyword">return</span> length = <span class="number">5</span>, with the first five elements of nums being modified to <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, and <span class="number">4</span> respectively.</div><div class="line"></div><div class="line">It doesn<span class="string">'t matter what values are set beyond the returned length.</span></div></pre></td></tr></table></figure><p><strong>Clarification:</strong></p><p>Confused why the returned value is an integer but your answer is an array?</p><p>Note that the input array is passed in by <strong>reference</strong>, which means modification to the input array will be known to the caller as well.</p><p>Internally you can think of this:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// nums is passed in by reference. (i.e., without making a copy)</span></div><div class="line"><span class="keyword">int</span> len = removeDuplicates(nums);</div><div class="line"></div><div class="line"><span class="comment">// any modification to nums in your function would be known by the caller.</span></div><div class="line"><span class="comment">// using the length returned by your function, it prints the first len elements.</span></div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</div><div class="line">    print(nums[i]);</div><div class="line">&#125;</div></pre></td></tr></table></figure></div></div><h5 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h5><p>题目大意：将数组中存在的重复元素剔除，使每个元素只出现一次。要求直接在原数组上修改</p><h5 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">removeDuplicates</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">if</span> (l&lt;<span class="number">1</span>) <span class="keyword">return</span> l;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">1</span>;</div><div class="line">        <span class="keyword">int</span> ele = nums[<span class="number">0</span>];</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;l;i++)&#123;</div><div class="line">            <span class="keyword">if</span>(ele!=nums[i])&#123;</div><div class="line">                ele=nums[i];</div><div class="line">                nums[count++]=nums[i];</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> count;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Merge-Sorted-Array"><a href="#Merge-Sorted-Array" class="headerlink" title="Merge Sorted Array"></a>Merge Sorted Array</h4><h5 id="题目-4"><a href="#题目-4" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/merge-sorted-array/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given two sorted integer arrays <em>nums1</em> and <em>nums2</em>, merge <em>nums2</em> into <em>nums1</em> as one sorted array.</p><p><strong>Note:</strong></p><ul><li>The number of elements initialized in <em>nums1</em> and <em>nums2</em> are <em>m</em> and <em>n</em> respectively.</li><li>You may assume that <em>nums1</em> has enough space (size that is greater or equal to <em>m</em> + <em>n</em>) to hold additional elements from <em>nums2</em>.</li></ul><p><strong>Example:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">nums1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>], m = <span class="number">3</span></div><div class="line">nums2 = [<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>],       n = <span class="number">3</span></div><div class="line"></div><div class="line">Output: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>]</div></pre></td></tr></table></figure></div></div><h5 id="思路-5"><a href="#思路-5" class="headerlink" title="思路"></a>思路</h5><p>题目大意：将两个有序的数字合并成有序的数组，要求把合并的数组存在<code>nums1</code>中</p><p>最初的想法是设置一个辅助数组，两个指针分别指向两个数组，同时扫描比较，选择小的元素存在辅助数组中。但是这样空间复杂度是<code>O(m+n)</code>。</p><p>一个比较新颖的思路是：从后往前扫描，每次比较选择大的元素，这样直接存储在<code>nums1</code>中，根本不需要辅助数组。</p><h5 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="keyword">int</span> m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2, <span class="keyword">int</span> n)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> cur = m+n<span class="number">-1</span>;</div><div class="line">        <span class="keyword">int</span> pos1 = m<span class="number">-1</span>;</div><div class="line">        <span class="keyword">int</span> pos2 = n<span class="number">-1</span>;</div><div class="line">        <span class="keyword">while</span>(pos1&gt;=<span class="number">0</span> &amp;&amp; pos2&gt;=<span class="number">0</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(nums1[pos1]&gt;nums2[pos2])&#123;</div><div class="line">                nums1[cur--] = nums1[pos1--];</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                nums1[cur--] = nums2[pos2--];</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">while</span>(pos2&gt;=<span class="number">0</span>)&#123;</div><div class="line">            nums1[cur--] = nums2[pos2--];</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Missing-Number"><a href="#Missing-Number" class="headerlink" title="Missing Number"></a>Missing Number</h4><h5 id="题目-5"><a href="#题目-5" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/missing-number/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array containing <em>n</em> distinct numbers taken from <code>0, 1, 2, ..., n</code>, find the one that is missing from the array.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="number">2</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">9</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">0</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="number">8</span></div></pre></td></tr></table></figure><p><strong>Note</strong>:<br>Your algorithm should run in linear runtime complexity. Could you implement it using only constant extra space complexity?</p></div></div><h5 id="思路-6"><a href="#思路-6" class="headerlink" title="思路"></a><a href="https://zxi.mytechroad.com/blog/math/leetcode-268-missing-number/" target="_blank" rel="noopener">思路</a></h5><p>题目大意：找出<code>0,1,2,...,n</code>序列中缺失的一个数。</p><p>最开始的思路是借助一个map，数组中的值作为map的key，遍历完数组之后，遍历<code>0-n</code>，看看是否在map中存在。但是这样的空间复杂度是<code>O(N)</code>。</p><p>其实有两种做法：</p><p><img src="/2018/10/04/Array/268-ep44.png" alt="68-ep4"></p><h5 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">missingNumber</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">int</span> sum_ = (<span class="number">1</span>+l)*l/<span class="number">2</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            sum_ -= nums[i];</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> sum_;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">missingNumber</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            res = res^i^nums[i];</div><div class="line">        &#125;</div><div class="line">        res = res^l;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Majority-Elements"><a href="#Majority-Elements" class="headerlink" title="Majority Elements"></a>Majority Elements</h4><h5 id="题目-6"><a href="#题目-6" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/majority-element/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of size <em>n</em>, find the majority element. The majority element is the element that appears <strong>more than</strong> <code>⌊ n/2 ⌋</code> times.</p><p>You may assume that the array is non-empty and the majority element always exist in the array.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">Output: <span class="number">3</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">Output: <span class="number">2</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-7"><a href="#思路-7" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/4233501.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：数组中存在一个数，其出现的次数大于数组长度的一半，找出这个数。</p><p>用一种叫摩尔投票法 Moore Voting，需要O(n)的时间和O(1)的空间，比前一种方法更好。这种投票法先将第一个数字假设为众数，然后把计数器设为1，比较下一个数和此数是否相等，若相等则计数器加一，反之减一。然后看此时计数器的值，若为零，则将下一个值设为候选众数。以此类推直到遍历完整个数组，当前候选众数即为该数组的众数。不仔细弄懂摩尔投票法的精髓的话，过一阵子还是会忘记的，首先要明确的是这个叼炸天的方法是有前提的，就是数组中一定要有众数的存在才能使用，下面我们来看本算法的思路，这是一种先假设候选者，然后再进行验证的算法。我们现将数组中的第一个数假设为众数，然后进行统计其出现的次数，如果遇到同样的数，则计数器自增1，否则计数器自减1，如果计数器减到了0，则更换下一个数字为候选者。这是一个很巧妙的设定，也是本算法的精髓所在，为啥遇到不同的要计数器减1呢，为啥减到0了又要更换候选者呢？首先是有那个强大的前提存在，一定会有一个出现超过半数的数字存在，那么如果计数器减到0了话，说明目前不是候选者数字的个数已经跟候选者的出现个数相同了，那么这个候选者已经很weak，不一定能出现超过半数，我们选择更换当前的候选者。那有可能你会有疑问，那万一后面又大量的出现了之前的候选者怎么办，不需要担心，如果之前的候选者在后面大量出现的话，其又会重新变为候选者，直到最终验证成为正确的众数.</p><h5 id="代码-7"><a href="#代码-7" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> ele : nums)&#123;</div><div class="line">            <span class="keyword">if</span> (count==<span class="number">0</span>)&#123;</div><div class="line">                count++;</div><div class="line">                res = ele;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(ele==res)&#123;</div><div class="line">                count++;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                count--;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Two-Sum"><a href="#Two-Sum" class="headerlink" title="Two Sum"></a>Two Sum</h4><p>类似题目：<a href="https://leetcode.com/problems/contains-duplicate/description/" target="_blank" rel="noopener">Contains Duplicate</a> <a href="https://leetcode.com/problems/contains-duplicate-ii/description/" target="_blank" rel="noopener">Contains Duplicate II</a> </p><p>都是利用一个hash_table使在线性时间内完成。 </p><h5 id="题目-7"><a href="#题目-7" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/two-sum/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of integers, return <strong>indices</strong> of the two numbers such that they add up to a specific target.</p><p>You may assume that each input would have <strong>exactly</strong> one solution, and you may not use the <em>same</em> element twice.</p><p><strong>Example:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Given nums = [<span class="number">2</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>], target = <span class="number">9</span>,</div><div class="line"></div><div class="line">Because nums[<span class="number">0</span>] + nums[<span class="number">1</span>] = <span class="number">2</span> + <span class="number">7</span> = <span class="number">9</span>,</div><div class="line"><span class="keyword">return</span> [<span class="number">0</span>, <span class="number">1</span>].</div></pre></td></tr></table></figure></div></div><h5 id="思路-8"><a href="#思路-8" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/4130379.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：一个数组，还有一个目标数target，让我们找到两个数字，使其和为target。</p><p>为了提高时间的复杂度，需要用空间来换，这算是一个trade off吧，我们只想用线性的时间复杂度来解决问题，那么就是说只能遍历一个数字，那么另一个数字呢，我们可以事先将其存储起来，使用一个HashMap，来建立数字和其坐标位置之间的映射，我们都知道HashMap是常数级的查找效率，这样，我们在遍历数组的时候，用target减去遍历到的数字，就是另一个需要的数字了，直接在HashMap中查找其是否存在即可。</p><h5 id="代码-8"><a href="#代码-8" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; twoSum(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target) &#123;</div><div class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt; mymap;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</div><div class="line">        <span class="keyword">int</span> the_other = <span class="number">-1</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            the_other = target - nums[i];</div><div class="line">            <span class="keyword">if</span> (mymap.find(the_other) != mymap.end())&#123;</div><div class="line">                res.push_back(mymap[the_other]);</div><div class="line">                res.push_back(i);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                mymap[nums[i]] = i;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Third-Maximum-Number"><a href="#Third-Maximum-Number" class="headerlink" title="Third Maximum Number"></a>Third Maximum Number</h4><h5 id="题目-8"><a href="#题目-8" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/third-maximum-number/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a <strong>non-empty</strong> array of integers, return the <strong>third</strong> maximum number in this array. If it does not exist, return the maximum number. The time complexity must be in O(n).</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</div><div class="line"></div><div class="line">Output: <span class="number">1</span></div><div class="line"></div><div class="line">Explanation: The third maximum is <span class="number">1</span>.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>, <span class="number">2</span>]</div><div class="line"></div><div class="line">Output: <span class="number">2</span></div><div class="line"></div><div class="line">Explanation: The third maximum does not exist, <span class="function">so the <span class="title">maximum</span> <span class="params">(<span class="number">2</span>)</span> is returned instead.</span></div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>]</div><div class="line"></div><div class="line">Output: <span class="number">1</span></div><div class="line"></div><div class="line">Explanation: Note that the third maximum here means the third maximum distinct number.</div><div class="line">Both numbers with value <span class="number">2</span> are both considered as second maximum.</div></pre></td></tr></table></figure></div></div><h5 id="思路-9"><a href="#思路-9" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/5983113.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：求数组中第三大的数，如果不存在的话那么就返回最大的数。</p><p>用三个变量first, second, third来分别保存第一大，第二大，和第三大的数，然后我们遍历数组，如果遍历到的数字大于当前第一大的数first，那么三个变量各自错位赋值，如果当前数字大于second，小于first，那么就更新second和third，如果当前数字大于third，小于second，那就只更新third。</p><p>虽然这道题不难，但是实现的时候有诸多陷阱：初始化要用长整型long的最小值，否则当数组中有INT_MIN存在时，程序就不知道该返回INT_MIN还是最大值first了。其次，第三大不能和第二大相同，必须是严格的小于，而并非小于等于。即欲更新second，num[i]必须比first小。</p><h5 id="代码-9"><a href="#代码-9" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:<span class="function"><span class="keyword">int</span> <span class="title">thirdMax</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">long</span> firstMax = LONG_MIN;</div><div class="line">        <span class="keyword">long</span> secondMax = LONG_MIN;</div><div class="line">        <span class="keyword">long</span> thirdMax = LONG_MIN;</div><div class="line">        <span class="keyword">int</span> cur = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            cur = nums[i];</div><div class="line">            <span class="keyword">if</span>(cur&gt;firstMax)&#123;</div><div class="line">                thirdMax = secondMax;</div><div class="line">                secondMax = firstMax;</div><div class="line">                firstMax = cur;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(cur&gt;secondMax&amp;&amp;cur!=firstMax)&#123;</div><div class="line">                thirdMax = secondMax;</div><div class="line">                secondMax = cur;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(cur&gt;thirdMax&amp;&amp;cur!=firstMax&amp;&amp;cur!=secondMax)&#123;</div><div class="line">                thirdMax = cur;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> thirdMax==LONG_MIN?firstMax:thirdMax;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Max-Consecutive-Ones"><a href="#Max-Consecutive-Ones" class="headerlink" title="Max Consecutive Ones"></a>Max Consecutive Ones</h4><h5 id="题目-9"><a href="#题目-9" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/max-consecutive-ones/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary array, find the maximum number of consecutive 1s in this array.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation: The first two digits or the last three digits are consecutive <span class="number">1</span>s.</div><div class="line">    The maximum number of consecutive <span class="number">1</span>s is <span class="number">3</span>.</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ul><li>The input array will only contain <code>0</code> and <code>1</code>.</li><li>The length of input array is a positive integer and will not exceed 10,000</li></ul></div></div><h5 id="思路-10"><a href="#思路-10" class="headerlink" title="思路"></a>思路</h5><p>题目大意：返回最长的连续1的长度。</p><p>两种思路：</p><p>一是遍历，记住连续1的起始index，每次遇到0，则更新。</p><p>二是遍历，使用count遍历记住连续1的个数，每次遇到0，则清空count。</p><h5 id="代码-10"><a href="#代码-10" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findMaxConsecutiveOnes</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        nums.push_back(<span class="number">0</span>);</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::iterator it;</div><div class="line">        it = nums.begin();</div><div class="line">        it = nums.insert ( it , <span class="number">0</span> );</div><div class="line">        <span class="keyword">int</span> start = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            <span class="keyword">if</span> (nums[i]==<span class="number">0</span>)&#123;</div><div class="line">                res = max(res,i-start<span class="number">-1</span>);</div><div class="line">                start = i;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findMaxConsecutiveOnes</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            <span class="keyword">if</span> (nums[i]==<span class="number">0</span>)&#123;</div><div class="line">                res = max(res,count);</div><div class="line">                count = <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                count++;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> max(res,count); <span class="comment">// for special input [1]</span></div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h3 id="中等"><a href="#中等" class="headerlink" title="中等"></a>中等</h3><h4 id="Number-of-Subarrays-with-Bounded-Maximum"><a href="#Number-of-Subarrays-with-Bounded-Maximum" class="headerlink" title="Number of Subarrays with Bounded Maximum"></a>Number of Subarrays with Bounded Maximum</h4><h5 id="题目-10"><a href="#题目-10" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/number-of-subarrays-with-bounded-maximum/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>We are given an array <code>A</code> of positive integers, and two positive integers <code>L</code> and <code>R</code> (<code>L &lt;= R</code>).</p><p>Return the number of (contiguous, non-empty) subarrays such that the value of the maximum array element in that subarray is at least <code>L</code> and at most <code>R</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Example :</div><div class="line">Input: </div><div class="line">A = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>]</div><div class="line">L = <span class="number">2</span></div><div class="line">R = <span class="number">3</span></div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation: There are three subarrays that meet the requirements: [<span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>].</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ul><li>L, R  and <code>A[i]</code> will be an integer in the range <code>[0, 10^9]</code>.</li><li>The length of <code>A</code> will be in the range of <code>[1, 50000]</code>.</li></ul></div></div><h5 id="思路-11"><a href="#思路-11" class="headerlink" title="思路"></a><a href="https://www.cnblogs.com/AlvinZH/p/8527214.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：找出所有可能的连续子数组，使该数组中的最大值在一个范围里面。</p><p>扫描一遍数组，当前的元素<code>ele</code>可能有三种情况：</p><ol><li>$ele &gt; R$: 那么该元素不能不能在子数组中，而且需要开始一个新的子数组。</li><li>$ele &lt; L$: 那么该元素可以在子数组中，但是该元素与它前面的最近合法数字之间的元素不能单独成为子数组。</li><li>否则，该元素是合法的。</li></ol><p>实现的时候，我们使用两个指针：left=right=-1，left指针指向合法子数组的开始，right指向合法数组中最新的合法元素，即$L&lt;ele&lt;R$, 那么：</p><ol><li>$ele &gt; R$: 连续子数组被割断，则同时更新两个指针，left=right=idx</li><li>$ele &lt; L$: 当前元素加入子数组，但是res += right-left</li><li>否则：right=idx，res += right-left</li></ol><h5 id="代码-11"><a href="#代码-11" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numSubarrayBoundedMax</span><span class="params">(self, A, L, R)</span>:</span></div><div class="line">        res = <span class="number">0</span></div><div class="line">        left = <span class="number">-1</span></div><div class="line">        right = <span class="number">-1</span></div><div class="line">        <span class="keyword">for</span> idx,ele <span class="keyword">in</span> enumerate(A):</div><div class="line">            <span class="keyword">if</span> ele&gt;R:</div><div class="line">                left = idx</div><div class="line">                right = idx</div><div class="line">            <span class="keyword">elif</span> ele&lt;L:</div><div class="line">                res += right-left <span class="comment">#little number appended to the legitimate array</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                right = idx</div><div class="line">                res += right-left</div><div class="line">        <span class="keyword">return</span> res</div></pre></td></tr></table></figure></div></div><h4 id="Friends-Of-Appropriate-Ages"><a href="#Friends-Of-Appropriate-Ages" class="headerlink" title="Friends Of Appropriate Ages"></a>Friends Of Appropriate Ages</h4><h5 id="题目-11"><a href="#题目-11" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/friends-of-appropriate-ages/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Some people will make friend requests. The list of their ages is given and <code>ages[i]</code> is the age of the ith person. </p><p>Person A will NOT friend request person B (B != A) if any of the following conditions are true:</p><ul><li><code>age[B] &lt;= 0.5 * age[A] + 7</code></li><li><code>age[B] &gt; age[A]</code></li><li><code>age[B] &gt; 100 &amp;&amp; age[A] &lt; 100</code></li></ul><p>Otherwise, A will friend request B.</p><p>Note that if A requests B, B does not necessarily request A.  Also, people will not friend request themselves.</p><p>How many total friend requests are made?</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">16</span>,<span class="number">16</span>]</div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: <span class="number">2</span> people friend request each other.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>]</div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: Friend requests are made <span class="number">17</span> -&gt; <span class="number">16</span>, <span class="number">18</span> -&gt; <span class="number">17</span>.</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">20</span>,<span class="number">30</span>,<span class="number">100</span>,<span class="number">110</span>,<span class="number">120</span>]</div><div class="line">Output: </div><div class="line">Explanation: Friend requests are made <span class="number">110</span> -&gt; <span class="number">100</span>, <span class="number">120</span> -&gt; <span class="number">110</span>, <span class="number">120</span> -&gt; <span class="number">100</span>.</div></pre></td></tr></table></figure><p>Notes:</p><ul><li><code>1 &lt;= ages.length &lt;= 20000</code>.</li><li><code>1 &lt;= ages[i] &lt;= 120</code>.</li></ul></div></div><h5 id="思路-12"><a href="#思路-12" class="headerlink" title="思路"></a><a href="http://bookshadow.com/weblog/2018/04/29/leetcode-friends-of-appropriate-ages/" target="_blank" rel="noopener">思路</a></h5><p>题目大意：给出一个年龄列表，在满足条件下，求出最多的好友请求。</p><p>那就遍历一遍好友，对于每一个年龄，求出其可以发出的好友请求的年龄范围，然后加起来。</p><h5 id="代码-12"><a href="#代码-12" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numFriendRequests</span><span class="params">(self, ages)</span>:</span></div><div class="line">        recorder = Counter(ages)</div><div class="line">        res = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> age <span class="keyword">in</span> ages:</div><div class="line">            recorder[age]-=<span class="number">1</span></div><div class="line">            left,right = <span class="number">8</span>+age//<span class="number">2</span>,age</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(left,right+<span class="number">1</span>):</div><div class="line">                res += recorder[i]</div><div class="line">            recorder[age]+=<span class="number">1</span></div><div class="line">        <span class="keyword">return</span> res</div></pre></td></tr></table></figure></div></div><h4 id="Image-Overlap"><a href="#Image-Overlap" class="headerlink" title="Image Overlap"></a>Image Overlap</h4><h5 id="题目-12"><a href="#题目-12" class="headerlink" title="题目"></a><a href="">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Two images <code>A</code> and <code>B</code> are given, represented as binary, square matrices of the same size.  (A binary matrix has only 0s and 1s as values.)</p><p>We translate one image however we choose (sliding it left, right, up, or down any number of units), and place it on top of the other image.  After, the <em>overlap</em> of this translation is the number of positions that have a 1 in both images.</p><p>(Note also that a translation does <strong>not</strong> include any kind of rotation.)</p><p>What is the largest possible overlap?</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Input: A = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</div><div class="line">            [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</div><div class="line">            [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]</div><div class="line">       B = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</div><div class="line">            [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</div><div class="line">            [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]]</div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation: We slide A to right by <span class="number">1</span> unit <span class="keyword">and</span> down by <span class="number">1</span> unit.</div></pre></td></tr></table></figure><p><strong>Notes:</strong> </p><ol><li><code>1 &lt;= A.length = A[0].length = B.length = B[0].length &lt;= 30</code></li><li><code>0 &lt;= A[i][j], B[i][j] &lt;= 1</code></li></ol></div></div><h5 id="思路-13"><a href="#思路-13" class="headerlink" title="思路"></a>思路</h5><p>题目大意：两张图片，使用01矩阵表示，通过移动一个矩阵(即上移，下移，左移和右移)，使最大化两个矩阵相应位置都是1，求出最大重合1的个数。</p><p>每一个矩阵的元素都与另一个矩阵中每一个元素求距离，即$(\Delta x,\Delta y)$，出现最多的$(\Delta x,\Delta y)$的个数就是结果。</p><h5 id="代码-13"><a href="#代码-13" class="headerlink" title="代码"></a><a href="https://blog.csdn.net/fuxuemingzhu/article/details/82597238" target="_blank" rel="noopener">代码</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">largestOverlap</span><span class="params">(self, A, B)</span>:</span></div><div class="line">        row = len(A)</div><div class="line">        col = len(A[<span class="number">0</span>])</div><div class="line">        mapping = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(col):</div><div class="line">                <span class="keyword">if</span> A[i][j]:</div><div class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> range(row):</div><div class="line">                        <span class="keyword">for</span> m <span class="keyword">in</span> range(col):</div><div class="line">                            <span class="keyword">if</span> B[k][m]:</div><div class="line">                                mapping.append((i-k,j-m))</div><div class="line">        res = <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> mapping:</div><div class="line">            c = Counter(mapping)</div><div class="line">            res = c.most_common()[<span class="number">0</span>][<span class="number">1</span>]</div><div class="line">        <span class="keyword">return</span> res</div></pre></td></tr></table></figure></div></div> <h4 id="Length-of-Longest-Fibonacci-Subsequence"><a href="#Length-of-Longest-Fibonacci-Subsequence" class="headerlink" title="Length of Longest Fibonacci Subsequence"></a>Length of Longest Fibonacci Subsequence</h4><h5 id="题目-13"><a href="#题目-13" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/length-of-longest-fibonacci-subsequence/description/" target="_blank" rel="noopener">题目</a></h5><p>A sequence <code>X_1, X_2, ..., X_n</code> is <em>fibonacci-like</em> if:</p><ul><li><code>n &gt;= 3</code></li><li><code>X_i + X_{i+1} = X_{i+2}</code> for all <code>i + 2 &lt;= n</code></li></ul><p>Given a <strong>strictly increasing</strong> array <code>A</code> of positive integers forming a sequence, find the <strong>length</strong> of the longest fibonacci-like subsequence of <code>A</code>.  If one does not exist, return 0.</p><p>(<em>Recall that a subsequence is derived from another sequence A by deleting any number of elements (including none) from A, without changing the order of the remaining elements.  For example, [3, 5, 8] is a subsequence of [3, 4, 5, 6, 7, 8].</em>)</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</div><div class="line">Output: <span class="number">5</span></div><div class="line">Explanation:</div><div class="line">The longest subsequence that is fibonacci-like: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>].</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">18</span>]</div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation:</div><div class="line">The longest subsequence that is fibonacci-like:</div><div class="line">[<span class="number">1</span>,<span class="number">11</span>,<span class="number">12</span>], [<span class="number">3</span>,<span class="number">11</span>,<span class="number">14</span>] <span class="keyword">or</span> [<span class="number">7</span>,<span class="number">11</span>,<span class="number">18</span>].</div></pre></td></tr></table></figure><h5 id="思路-14"><a href="#思路-14" class="headerlink" title="思路"></a><a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-873-length-of-longest-fibonacci-subsequence/" target="_blank" rel="noopener">思路</a></h5><h5 id="代码-14"><a href="#代码-14" class="headerlink" title="代码"></a>代码</h5><h4 id="Best-Time-to-Buy-and-Sell-Stock"><a href="#Best-Time-to-Buy-and-Sell-Stock" class="headerlink" title="Best Time to Buy and Sell Stock"></a>Best Time to Buy and Sell Stock</h4><h5 id="题目-14"><a href="#题目-14" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Say you have an array for which the <em>i</em>th element is the price of a given stock on day <em>i</em>.</p><p>If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</p><p>Note that you cannot sell a stock before you buy one.</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">7</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]</div><div class="line">Output: <span class="number">5</span></div><div class="line">Explanation: Buy on day <span class="number">2</span> (price = <span class="number">1</span>) <span class="keyword">and</span> sell on day <span class="number">5</span> (price = <span class="number">6</span>), profit = <span class="number">6</span><span class="number">-1</span> = <span class="number">5.</span></div><div class="line">             Not <span class="number">7</span><span class="number">-1</span> = <span class="number">6</span>, as selling price needs to be larger than buying price.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">7</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="number">0</span></div><div class="line">Explanation: In <span class="keyword">this</span> <span class="keyword">case</span>, no transaction is done, i.e. max profit = <span class="number">0.</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-15"><a href="#思路-15" class="headerlink" title="思路"></a>思路</h5><h5 id="代码-15"><a href="#代码-15" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; prices)</span> </span>&#123;</div><div class="line">        </div><div class="line">        <span class="keyword">int</span> l = prices.size();</div><div class="line">        <span class="keyword">if</span> (l&lt;<span class="number">1</span>) <span class="keyword">return</span> l;</div><div class="line">        <span class="keyword">int</span> min_ = prices[<span class="number">0</span>];</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> ele : prices)&#123;</div><div class="line">            res = max(res,ele-min_);</div><div class="line">            min_ = min(min_,ele);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Maximum-Subarray"><a href="#Maximum-Subarray" class="headerlink" title="Maximum Subarray"></a>Maximum Subarray</h4><h5 id="题目-15"><a href="#题目-15" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/maximum-subarray/description/" target="_blank" rel="noopener">题目</a></h5><p>Given an integer array <code>nums</code>, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum.</p><p><strong>Example:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [-<span class="number">2</span>,<span class="number">1</span>,-<span class="number">3</span>,<span class="number">4</span>,-<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,-<span class="number">5</span>,<span class="number">4</span>],</div><div class="line">Output: <span class="number">6</span></div><div class="line">Explanation: [<span class="number">4</span>,-<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>] has the largest sum = <span class="number">6</span>.</div></pre></td></tr></table></figure><h5 id="思路-16"><a href="#思路-16" class="headerlink" title="思路"></a>思路</h5><p>题目大意：找出连续和最大的子数组。</p><p>最初的思路是设置一个数组，存储每个元素可以累积的最大值，其值更新是<code>max(nums[i],nums[i]+nums[i-1])</code>。但是，这样空间复杂度是<code>O(N)</code>。</p><p>改进的想法是：</p><p>不要辅助数组，直接使用一个<code>temp</code>变量记录当前位置可以累积的最大值，<code>temp=max(temp+nums[i],nums[i])</code>；之所以可以这样做，是因为在辅助数组中，在更新的时候，我们只利用到当前元素的前一个元素。</p><h5 id="代码-16"><a href="#代码-16" class="headerlink" title="代码"></a>代码</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> temp = nums[<span class="number">0</span>];</div><div class="line">        <span class="keyword">int</span> sum_ = nums[<span class="number">0</span>];</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;l;i++)&#123;</div><div class="line">            temp = max(nums[i],temp+nums[i]);</div><div class="line">            sum_ = max(sum_,temp);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> sum_;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><h4 id="K-diff-Pairs-in-an-Array"><a href="#K-diff-Pairs-in-an-Array" class="headerlink" title="K-diff Pairs in an Array"></a>K-diff Pairs in an Array</h4><h5 id="题目-16"><a href="#题目-16" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/k-diff-pairs-in-an-array/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of integers and an integer <strong>k</strong>, you need to find the number of <strong>unique</strong> k-diff pairs in the array. Here a <strong>k-diff</strong> pair is defined as an integer pair (i, j), where <strong>i</strong> and <strong>j</strong> are both numbers in the array and their <a href="https://en.wikipedia.org/wiki/Absolute_difference" target="_blank" rel="noopener">absolute difference</a> is <strong>k</strong>.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>], k = <span class="number">2</span></div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: There are two <span class="number">2</span>-diff pairs in the array, (<span class="number">1</span>, <span class="number">3</span>) and (<span class="number">3</span>, <span class="number">5</span>).</div><div class="line">Although we have two <span class="number">1</span>s in the input, we should only <span class="keyword">return</span> the number of unique pairs.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], k = <span class="number">1</span></div><div class="line">Output: <span class="number">4</span></div><div class="line">Explanation: There are four <span class="number">1</span>-diff pairs in the array, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">4</span>) and (<span class="number">4</span>, <span class="number">5</span>).</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">4</span>], k = <span class="number">0</span></div><div class="line">Output: <span class="number">1</span></div><div class="line">Explanation: There is one <span class="number">0</span>-diff pair in the array, (<span class="number">1</span>, <span class="number">1</span>).</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ol><li>The pairs (i, j) and (j, i) count as the same pair.</li><li>The length of the array won’t exceed 10,000.</li><li>All the integers in the given input belong to the range: [-1e7, 1e7].</li></ol></div></div><h5 id="思路-17"><a href="#思路-17" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/6545075.html" target="_blank" rel="noopener">思路</a></h5><p>题目大意：一个含有重复数字的无序数组，还有一个整数k，让我们找出有多少对不重复的数对(i, j)使得i和j的差刚好为k。</p><p>由于k有可能为0，而只有含有至少两个相同的数字才能形成数对，那么就是说我们需要统计数组中每个数字的个数。我们可以建立每个数字和其出现次数之间的映射，然后遍历哈希表中的数字，如果k为0且该数字出现的次数大于1，则结果res自增1；如果k不为0，且用当前数字加上k后得到的新数字也在数组中存在，则结果res自增1。需要注意的k不可能为负值。</p><h5 id="代码-17"><a href="#代码-17" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findPairs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</div><div class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt; mymap;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> ele : nums)&#123;</div><div class="line">            mymap[ele]++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">if</span> (k&lt;<span class="number">0</span>) <span class="keyword">return</span> res;</div><div class="line">        <span class="keyword">if</span> (k==<span class="number">0</span>)&#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> ele : mymap)&#123;</div><div class="line">                <span class="keyword">if</span> (ele.second&gt;<span class="number">1</span>)</div><div class="line">                    res++;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">else</span>&#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> ele : mymap)&#123;</div><div class="line">                <span class="keyword">if</span>(mymap.find(ele.first+k) != mymap.end())&#123;</div><div class="line">                    res++;</div><div class="line">                &#125;</div><div class="line">                    </div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Array-Partition-I"><a href="#Array-Partition-I" class="headerlink" title="Array Partition I"></a>Array Partition I</h4><h5 id="题目-17"><a href="#题目-17" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/array-partition-i/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of <strong>2n</strong> integers, your task is to group these integers into <strong>n</strong> pairs of integer, say (a1, b1), (a2, b2), …, (an, bn) which makes sum of min(ai, bi) for all i from 1 to n as large as possible.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]</div><div class="line"></div><div class="line">Output: <span class="number">4</span></div><div class="line">Explanation: n <span class="keyword">is</span> <span class="number">2</span>, <span class="keyword">and</span> the maximum sum of pairs <span class="keyword">is</span> <span class="number">4</span> = min(<span class="number">1</span>, <span class="number">2</span>) + min(<span class="number">3</span>, <span class="number">4</span>).</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ol><li><strong>n</strong> is a positive integer, which is in the range of [1, 10000].</li><li>All the integers in the array will be in the range of [-10000, 10000].</li></ol></div></div><h5 id="思路-18"><a href="#思路-18" class="headerlink" title="思路"></a><a href="https://zxi.mytechroad.com/blog/hashtable/leetcode-561-array-partition-i/" target="_blank" rel="noopener">思路</a></h5><p>题目大意：将长度为<code>2n</code>的数组分成<code>n</code>个长度为2的元组，使<code>n</code>个元组小值的和最大。</p><p>一个很直接的思路就是排序，然后遍历一遍，每次将偶数index上的值相加，这样的时间复杂度由排序控制是<code>O(nlogn)</code>。</p><p>但是，我们可以丢弃排序，增加一个数组，以空间换时间，将时间复杂度降维<code>O(n)</code>，空间复杂度为<code>O(n)</code>。</p><p>由题目看到，元素值范围是<code>[-10000,10000]</code>，这样我们设置一个长度为<code>2*10000</code>的数组，元素值为新数组的下标，新数组的值为下标在已知数组中出现的次数。第一次遍历<code>nums</code>，得到每个元素的频数，而且，隐含了排序。第二次遍历，如果新数组的值为0，则跳过；否则，判断是否为一对元素值得第一个；</p><h5 id="代码-18"><a href="#代码-18" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">arrayPairSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> max_val = <span class="number">10000</span>;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; val_idx_map(<span class="number">2</span>*max_val);</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> ele : nums)&#123;</div><div class="line">            val_idx_map[ele+max_val]++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">bool</span> first = <span class="literal">true</span>;</div><div class="line">        <span class="keyword">int</span> n = -max_val;</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>(n&lt;max_val)&#123;</div><div class="line">            <span class="keyword">if</span> (!val_idx_map[n+max_val])&#123;</div><div class="line">                n++;</div><div class="line">                <span class="keyword">continue</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span> (first)&#123;</div><div class="line">                res += n;</div><div class="line">                first = <span class="literal">false</span>;</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                first = <span class="literal">true</span>;</div><div class="line">            &#125;</div><div class="line">            val_idx_map[n+max_val]--;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> res;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Can-Place-Flowers"><a href="#Can-Place-Flowers" class="headerlink" title="Can Place Flowers"></a>Can Place Flowers</h4><h5 id="题目-18"><a href="#题目-18" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/can-place-flowers/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Suppose you have a long flowerbed in which some of the plots are planted and some are not. However, flowers cannot be planted in adjacent plots - they would compete for water and both would die.</p><p>Given a flowerbed (represented as an array containing 0 and 1, where 0 means empty and 1 means not empty), and a number <strong>n</strong>, return if <strong>n</strong> new flowers can be planted in it without violating the no-adjacent-flowers rule.</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: flowerbed = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], n = <span class="number">1</span></div><div class="line">Output: True</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: flowerbed = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>], n = <span class="number">2</span></div><div class="line">Output: False</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ol><li>The input array won’t violate no-adjacent-flowers rule.</li><li>The input array size is in the range of [1, 20000].</li><li><strong>n</strong> is a non-negative integer which won’t exceed the input array size.</li></ol></div></div><h5 id="思路-19"><a href="#思路-19" class="headerlink" title="思路"></a><a href="http://bookshadow.com/weblog/2017/06/04/leetcode-can-place-flowers/" target="_blank" rel="noopener">思路</a></h5><p>题目大意：给定数组flowerbed表示一个花床，0表示位置为空，1表示位置非空。花不能相邻种植，即不能出现相邻的1。想要种植的花朵数目n，判断是否可以满足要求。</p><p>从左向右遍历flowerbed，将满足要求的0设为1。计数与n比较即可。</p><h5 id="代码-19"><a href="#代码-19" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canPlaceFlowers</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; flowerbed, <span class="keyword">int</span> n)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> l = flowerbed.size();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l;i++)&#123;</div><div class="line">            <span class="keyword">if</span> (flowerbed[i]==<span class="number">1</span>) <span class="keyword">continue</span>;</div><div class="line">            <span class="keyword">if</span> (i&gt;<span class="number">0</span> &amp;&amp; flowerbed[i<span class="number">-1</span>]==<span class="number">1</span>) <span class="keyword">continue</span>;</div><div class="line">            <span class="keyword">if</span> (i&lt;l<span class="number">-1</span> &amp;&amp; flowerbed[i+<span class="number">1</span>]==<span class="number">1</span>) <span class="keyword">continue</span>;</div><div class="line">            res ++;</div><div class="line">            flowerbed[i] = <span class="number">1</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span> (res&gt;=n)</div><div class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">        <span class="keyword">else</span></div><div class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Non-decreasing-Array"><a href="#Non-decreasing-Array" class="headerlink" title="Non-decreasing Array"></a>Non-decreasing Array</h4><h5 id="题目-19"><a href="#题目-19" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/non-decreasing-array/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array with <code>n</code> integers, your task is to check if it could become non-decreasing by modifying <strong>at most</strong> <code>1</code> element.</p><p>We define an array is non-decreasing if <code>array[i] &lt;= array[i + 1]</code> holds for every <code>i</code> (1 &lt;= i &lt; n).</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">4</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">Output: True</div><div class="line">Explanation: You could modify the first <span class="number">4</span> to <span class="number">1</span> to get a non-decreasing <span class="built_in">array</span>.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>]</div><div class="line">Output: False</div><div class="line">Explanation: You can't get a non-decreasing <span class="built_in">array</span> by modify at most one element.</div></pre></td></tr></table></figure><p><strong>Note:</strong> The <code>n</code> belongs to [1, 10,000].</p></div></div><h5 id="思路-20"><a href="#思路-20" class="headerlink" title="思路"></a><a href="http://www.cnblogs.com/grandyang/p/7565424.html" target="_blank" rel="noopener">思路</a></h5><h5 id="代码-20"><a href="#代码-20" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">checkPossibility</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">int</span> l = nums.size();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;l<span class="number">-1</span>;i++)&#123;</div><div class="line">            <span class="keyword">if</span> (nums[i]&gt;nums[i+<span class="number">1</span>])&#123;</div><div class="line">                count+=<span class="number">1</span>;</div><div class="line">                <span class="keyword">if</span>(i==<span class="number">0</span>)&#123;</div><div class="line">                    nums[i]=nums[i+<span class="number">1</span>]<span class="number">-1</span>;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span> <span class="keyword">if</span>(nums[i<span class="number">-1</span>]&lt;nums[i+<span class="number">1</span>])&#123; </div><div class="line">                    nums[i]=nums[i+<span class="number">1</span>]<span class="number">-1</span>;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span>&#123;</div><div class="line">                    nums[i+<span class="number">1</span>]=nums[i]+<span class="number">1</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> (count&gt;<span class="number">1</span>)</div><div class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="1-bit-and-2-bit-Characters"><a href="#1-bit-and-2-bit-Characters" class="headerlink" title="1-bit and 2-bit Characters"></a>1-bit and 2-bit Characters</h4><p><a href="https://leetcode.com/problems/1-bit-and-2-bit-characters/description/" target="_blank" rel="noopener">题目</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>We have two special characters. The first character can be represented by one bit <code>0</code>. The second character can be represented by two bits (<code>10</code> or <code>11</code>).</p><p>Now given a string represented by several bits. Return whether the last character must be a one-bit character or not. The given string will always end with a zero.</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">bits = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line">Output: True</div><div class="line">Explanation: </div><div class="line">The only way to decode it is two-bit character <span class="keyword">and</span> one-bit character. So the last character is one-bit character.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">bits = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</div><div class="line">Output: False</div><div class="line">Explanation: </div><div class="line">The only way to decode it is two-bit character <span class="keyword">and</span> two-bit character. So the last character is NOT one-bit character.</div></pre></td></tr></table></figure></div></div><h5 id="思路-21"><a href="#思路-21" class="headerlink" title="思路"></a>思路</h5><p>题目大意：判断字符串是否只有<code>10,11,0</code>三个子串组成，而且以<code>0</code>结尾。</p><p>仔细分析之后，会发现如果字符以<code>1</code>开头，那么它只能是<code>10</code>或者<code>11</code>，如果是<code>0</code>开头，那么只能是<code>0</code>。基于此，我们扫描一遍字符串，如果当前字符是<code>1</code>，那么跳过下一个字符，直接扫描<code>index = i+2</code>的字符；如果当前字符是<code>0</code>，那么扫描下一个字符，判断是否能到达最后一个字符且为<code>0</code>。</p><h5 id="代码-21"><a href="#代码-21" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isOneBitCharacter</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; bits)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> l=bits.size();</div><div class="line">        <span class="keyword">int</span> i=<span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>(i&lt;l<span class="number">-1</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(bits[i]==<span class="number">0</span>)</div><div class="line">                i++;</div><div class="line">            <span class="keyword">else</span></div><div class="line">                i+=<span class="number">2</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span> (i==(l<span class="number">-1</span>) &amp;&amp; bits[i]==<span class="number">0</span>)</div><div class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">        <span class="keyword">else</span></div><div class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Find-Pivot-Index"><a href="#Find-Pivot-Index" class="headerlink" title="Find Pivot Index"></a>Find Pivot Index</h4><p><a href="https://leetcode.com/problems/find-pivot-index/description/" target="_blank" rel="noopener">题目</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of integers <code>nums</code>, write a method that returns the “pivot” index of this array.</p><p>We define the pivot index as the index where the sum of the numbers to the left of the index is equal to the sum of the numbers to the right of the index.</p><p>If no such index exists, we should return -1. If there are multiple pivot indexes, you should return the left-most pivot index.</p><p><strong>Example 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">nums = [1, 7, 3, 6, 5, 6]</div><div class="line">Output: 3</div><div class="line">Explanation: </div><div class="line">The sum of the numbers to the left of index 3 (nums[3] = 6) is equal to the sum of numbers to the right of index 3.</div><div class="line">Also, 3 is the first index where this occurs.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">Output: -<span class="number">1</span></div><div class="line">Explanation: </div><div class="line">There is no index that satisfies the conditions in the problem statement.</div></pre></td></tr></table></figure></div></div><h5 id="思路-22"><a href="#思路-22" class="headerlink" title="思路"></a>思路</h5><p>题目大意：在一数组中寻找一个点，使该点左边的元素和等于该点的右边元素和。</p><p>第一种方法是遍历每个点，然后分别计算点左边和，点右边和。但是这样的复杂度是$O(n)$.</p><p>第二中方法是同样遍历每个点，但是并不是每次都去计算左边和，右边和。实际上，每一次遍历的时候，左边和、右边和的差异就是当前点：即左边和=上一次的和+当前点，右边和=上一次的和-当前点。</p><h5 id="代码-22"><a href="#代码-22" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pivotIndex</span><span class="params">(self, nums)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums : <span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        l = len(nums)</div><div class="line">        lsum = <span class="number">0</span></div><div class="line">        rsum = sum(nums)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">            <span class="keyword">if</span> (lsum==(rsum-nums[i])):</div><div class="line">                <span class="keyword">return</span> i</div><div class="line">            lsum += nums[i]</div><div class="line">            rsum -= nums[i]</div><div class="line">        <span class="keyword">return</span> <span class="number">-1</span></div></pre></td></tr></table></figure></div></div><h4 id="Largest-Number-At-Least-Twice-of-Others"><a href="#Largest-Number-At-Least-Twice-of-Others" class="headerlink" title="Largest Number At Least Twice of Others"></a>Largest Number At Least Twice of Others</h4><p><a href="https://leetcode.com/problems/largest-number-at-least-twice-of-others/description/" target="_blank" rel="noopener">题目</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>In a given integer array <code>nums</code>, there is always exactly one largest element.</p><p>Find whether the largest element in the array is at least twice as much as every other number in the array.</p><p>If it is, return the <strong>index</strong> of the largest element, otherwise return -1.</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: nums = [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>]</div><div class="line">Output: <span class="number">1</span></div><div class="line">Explanation: <span class="number">6</span> is the largest integer, <span class="keyword">and</span> <span class="keyword">for</span> every other number in the <span class="built_in">array</span> x,</div><div class="line"><span class="number">6</span> is more than twice as big as x.  The index of value <span class="number">6</span> is <span class="number">1</span>, so we <span class="keyword">return</span> <span class="number">1.</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div><div class="line">Output: <span class="number">-1</span></div><div class="line">Explanation: <span class="number">4</span> isn't at least as big as twice the value of <span class="number">3</span>, so we <span class="keyword">return</span> <span class="number">-1.</span></div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ol><li><code>nums</code> will have a length in the range <code>[1, 50]</code>.</li><li>Every <code>nums[i]</code> will be an integer in the range <code>[0, 99]</code>.</li></ol></div></div><h5 id="思路-23"><a href="#思路-23" class="headerlink" title="思路"></a>思路</h5><p>题目大意：判断数组中最大数是否比剩下每个元素的两倍之大。</p><p>遍历一次，找出最大和次大的元素。重点是在次大元素的更新，每次最大元素更新，次大元素继承上一次最大元素值。</p><h5 id="代码-23"><a href="#代码-23" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dominantIndex</span><span class="params">(self, nums)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:<span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        res_index = <span class="number">0</span></div><div class="line">        highest = <span class="number">0</span></div><div class="line">        secondHighest = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> idx,v <span class="keyword">in</span> enumerate(nums):</div><div class="line">            <span class="keyword">if</span> highest&lt;v:</div><div class="line">                secondHighest = highest</div><div class="line">                highest=v</div><div class="line">                res_index = idx</div><div class="line">            <span class="keyword">elif</span> secondHighest&lt;v:</div><div class="line">                secondHighest = v</div><div class="line">        <span class="keyword">if</span> highest&gt;=<span class="number">2</span>*secondHighest:</div><div class="line">            <span class="keyword">return</span> res_index</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">-1</span></div></pre></td></tr></table></figure></div></div><h4 id="Toeplitz-Matrix"><a href="#Toeplitz-Matrix" class="headerlink" title="Toeplitz Matrix"></a>Toeplitz Matrix</h4><p><a href="https://leetcode.com/problems/toeplitz-matrix/description/" target="_blank" rel="noopener">题目</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>A matrix is <em>Toeplitz</em> if every diagonal from top-left to bottom-right has the same element.</p><p>Now given an <code>M x N</code> matrix, return <code>True</code> if and only if the matrix is <em>Toeplitz</em>.</p><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">matrix = [</div><div class="line">  [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</div><div class="line">  [<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</div><div class="line">  [<span class="number">9</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">]</div><div class="line">Output: True</div><div class="line">Explanation:</div><div class="line">In the above grid, the diagonals are:</div><div class="line"><span class="string">"[9]"</span>, <span class="string">"[5, 5]"</span>, <span class="string">"[1, 1, 1]"</span>, <span class="string">"[2, 2, 2]"</span>, <span class="string">"[3, 3]"</span>, <span class="string">"[4]"</span>.</div><div class="line">In each diagonal all elements are the same, so the answer is True.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">matrix = [</div><div class="line">  [<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">  [<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">]</div><div class="line">Output: False</div><div class="line">Explanation:</div><div class="line">The diagonal <span class="string">"[1, 2]"</span> has different elements.</div><div class="line">**Note:**</div></pre></td></tr></table></figure><ol><li><code>matrix</code> will be a 2D array of integers.</li><li><code>matrix</code> will have a number of rows and columns in range <code>[1, 20]</code>.</li><li><code>matrix[i][j]</code> will be integers in range <code>[0, 99]</code>.</li></ol></div></div><h5 id="思路-24"><a href="#思路-24" class="headerlink" title="思路"></a>思路</h5><p>题目大意：判断一个矩阵所有对角线上的元素是否相等。</p><p>感觉挺巧的想法，遍历一遍矩阵，每次判断当前元素与其紧邻的对角线元素是否相等。</p><h5 id="代码-24"><a href="#代码-24" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isToeplitzMatrix</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; matrix)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> row = matrix.size();</div><div class="line">        <span class="keyword">int</span> col = matrix[<span class="number">0</span>].size();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;row<span class="number">-1</span>;i++)&#123;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;col<span class="number">-1</span>;j++)&#123;</div><div class="line">                <span class="keyword">if</span> (matrix[i][j]!=matrix[i+<span class="number">1</span>][j+<span class="number">1</span>])</div><div class="line">                    <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="X-of-a-Kind-in-a-Deck-of-Cards"><a href="#X-of-a-Kind-in-a-Deck-of-Cards" class="headerlink" title="X of a Kind in a Deck of Cards"></a>X of a Kind in a Deck of Cards</h4><h5 id="题目-20"><a href="#题目-20" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/x-of-a-kind-in-a-deck-of-cards/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>In a deck of cards, each card has an integer written on it.</p><p>Return <code>true</code> if and only if you can choose <code>X &gt;= 2</code> such that it is possible to split the entire deck into 1 or more groups of cards, where:</p><ul><li>Each group has exactly <code>X</code> cards.</li><li>All the cards in each group have the same integer.</li></ul><p><strong>Example 1:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="literal">true</span></div><div class="line">Explanation: Possible partition [<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>]</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>]</div><div class="line">Output: <span class="literal">false</span></div><div class="line">Explanation: No possible partition.</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>]</div><div class="line">Output: <span class="literal">false</span></div><div class="line">Explanation: No possible partition.</div></pre></td></tr></table></figure><p><strong>Example 4:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="literal">true</span></div><div class="line">Explanation: Possible partition [<span class="number">1</span>,<span class="number">1</span>]</div></pre></td></tr></table></figure><p><strong>Example 5:</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">Output: <span class="literal">true</span></div><div class="line">Explanation: Possible partition [<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>]</div></pre></td></tr></table></figure></div></div><h5 id="思路-25"><a href="#思路-25" class="headerlink" title="思路"></a><a href="https://leetcode.com/problems/x-of-a-kind-in-a-deck-of-cards/discuss/175845/C++JavaPython-Greatest-Common-Divisor" target="_blank" rel="noopener">思路</a></h5><p>题目大意：将数组划分成大小相同的组，每组元素相同且大小不小于2。</p><p>思路很直接，遍历数组，统计相同元素个数。只要最小组的长度大于2且是其他组的除数，就可以。但是，这个对例子<code>[1,1,1,1,2,2,2,2,2,2]</code>是不行的。实际上，只要各组长度之间存在大于1的最大公约数，该最大公倍数即最终划分的长度，则满足题意。</p><h5 id="代码-25"><a href="#代码-25" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">hasGroupsSizeX</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; deck)</span> </span>&#123;</div><div class="line">        <span class="built_in">map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt; count;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i:deck)&#123;</div><div class="line">            count[i] ++;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">int</span> min_count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;v:count)&#123;</div><div class="line">            <span class="keyword">if</span> (min_count&gt;v.second)&#123;</div><div class="line">                min_count = v.second;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;v:count)&#123;</div><div class="line">            min_count = __gcd(min_count,v.second);</div><div class="line">            <span class="keyword">if</span> (min_count&lt;<span class="number">2</span>)&#123;</div><div class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div><h4 id="Maximize-Distance-to-Closest-Person"><a href="#Maximize-Distance-to-Closest-Person" class="headerlink" title="Maximize Distance to Closest Person"></a>Maximize Distance to Closest Person</h4><h5 id="题目-21"><a href="#题目-21" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/maximize-distance-to-closest-person/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>In a row of <code>seats</code>, <code>1</code> represents a person sitting in that seat, and <code>0</code> represents that the seat is empty. </p><p>There is at least one empty seat, and at least one person sitting.</p><p>Alex wants to sit in the seat such that the distance between him and the closest person to him is maximized. </p><p>Return that maximum distance to closest person.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: </div><div class="line"><span class="function">If Alex sits in the second open <span class="title">seat</span> <span class="params">(seats[<span class="number">2</span>])</span>, then the closest person has distance 2.</span></div><div class="line"><span class="function">If Alex sits in any other open seat, the closest person has distance 1.</span></div><div class="line"><span class="function">Thus, the maximum distance to the closest person is 2.</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation: </div><div class="line">If Alex sits in the last seat, the closest person is <span class="number">3</span> seats away.</div><div class="line">This is the maximum distance possible, so the answer is <span class="number">3</span>.</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ol><li><code>1 &lt;= seats.length &lt;= 20000</code></li><li><code>seats</code> contains only 0s or 1s, at least one <code>0</code>, and at least one <code>1</code>.</li></ol></div></div><h5 id="思路-26"><a href="#思路-26" class="headerlink" title="思路"></a><a href="https://blog.csdn.net/fuxuemingzhu/article/details/80643250" target="_blank" rel="noopener">思路</a></h5><p>题目大意：给定一个0/1表示的座位数组，0表示空座位，1表示该位置有人。现要求找一个位置，使最大与旁边人的距离。</p><p>即对于每一个空座位，需要考虑左边最近人的距离和右边最近人的距离，那么该座位与旁人的最大距离为左右两边距离的较小一个。这样每一个空座位的最大距离求出之后，选择最大距离的座位。</p><p>为了求出每个座位与左右两边人的最近距离，我们使用遍历两次座位数组，第一次遍历求出空座位与周边人的距离；第二次遍历求出与右边人的距离。</p><h5 id="代码-26"><a href="#代码-26" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxDistToClosest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; seats)</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> pos = INT_MAX;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dis(seats.size(),INT_MAX);</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;seats.size();i++)&#123;</div><div class="line">            <span class="keyword">if</span> (seats[i]==<span class="number">1</span>)&#123;</div><div class="line">                pos = i;</div><div class="line">                dis[i] = <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                dis[i] = <span class="built_in">abs</span>(pos-i);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        pos = INT_MAX;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=seats.size()<span class="number">-1</span>;i&gt;<span class="number">-1</span>;i--)&#123;</div><div class="line">            <span class="keyword">if</span> (seats[i]==<span class="number">1</span>)&#123;</div><div class="line">                pos = i;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">else</span>&#123;</div><div class="line">                dis[i] = min(<span class="built_in">abs</span>(i-pos),dis[i]);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">auto</span> maxPosition = max_element(dis.begin(), dis.end());</div><div class="line">        <span class="keyword">return</span> dis[maxPosition - dis.begin()];</div><div class="line">        </div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></div></div>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> LeetCode </tag>
            
            <tag> Array </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Naive Bayes</title>
      <link href="/2018/09/20/Naive-Bayes/"/>
      <url>/2018/09/20/Naive-Bayes/</url>
      <content type="html"><![CDATA[<p>机器学习中的分类方法，通常是基于有限的训练样本尽可能准确地估计后验概率$P(c|x)$。大体上，对后验概率的估计有两种策略：给定$x$，直接建模$P(c|x)$来预测$c$，这样得到的模型是“判别式模型”（discriminative models）；也可以先对联合概率分布$P(x,c)$建模，然后再由此获得$P(c|x)$，这样得到的是“生成式模型”（generative models）。决策树，神经网络，支持向量机都归入判别式模型；而朴素贝叶斯法则属于生成式模型。</p><p>朴素贝叶斯法是基于<strong>贝叶斯定理</strong>与<strong>特征条件独立</strong>假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入|输出的联合概率分布；然后基于此模型，对于给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。朴素贝叶斯法广泛应用于文本分类以及垃圾邮件检测。</p><p>本章叙述朴素贝叶斯法，包括朴素贝叶斯的学习与分类、朴素贝叶斯法的参数估计算法。</p><a id="more"></a><h1 id="贝叶斯理论"><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h1><p><a href="http://www.statisticshowto.com/posterior-distribution-probability/" target="_blank" rel="noopener">ref1</a> <a href="https://stats.stackexchange.com/questions/58564/help-me-understand-bayesian-prior-and-posterior-distributions" target="_blank" rel="noopener">ref2</a> <a href="https://www.youtube.com/watch?v=XcwH9JGfZOU" target="_blank" rel="noopener">How Naive Bayes Classifier Works[1]</a> <a href="https://www.youtube.com/watch?v=k2diLn5Nqbs" target="_blank" rel="noopener">How Naive Bayes Classifier Works[2]</a> </p><p><strong>似然函数</strong>：</p><blockquote><p><a href="https://www.zhihu.com/question/54082000/answer/138115757" target="_blank" rel="noopener">似然函数 VS 概率</a> </p><p>统计学的观点始终认为样本的出现是基于一个分布的，我们假设这个分布为$f$，该分布由参数$\theta$决定，比如我们认为$f$为高斯分布，那么它由参数$u$(均值)和$\sigma^2$(方差)唯一决定，故而不同的参数决定了不同的分布。</p><p>概率$f(x|\theta)$表示的是给定参数$\theta$的情况下，事件$x$出现的可能性；而似然函数$L(\theta|x)$表示我观测到事件$x$已经发生了，选择何种参数$\theta$能最大化$x$出现的可能性。</p><p>即概率是已知参数$\theta$对于事件$x$的函数，而似然函数是已知事件$x$关于$\theta$的函数。</p></blockquote><p><strong>先验概率</strong>：事件发生前的根据以往的经验推测的与该事件相关的概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如$P(X)，P(Y)$。</p><p><strong>后验概率</strong>：条件概率$P(B|A)$是后验概率当事件$B$先于事件$A$发生。</p><p>在事件(试验)真正发生后，通过事件(试验)的结果可以修正先验概率</p><p>比如：你来到一个山洞,这个山洞里可能有熊也可能没有熊, 记你觉得山洞有熊的为事件 $Y$. 然后,你也许听到山洞里传来熊的吼声, 记听到熊吼声为事件 $X$. 你一开始认为山洞有熊的概率是 $P(Y)$; 听到熊的吼声之后,你认为有熊的概率是 $P(Y|X)$。在这里，$P(Y)$就是先验概率,$P(Y|X)$是后验概率.</p><script type="math/tex; mode=display">\mathrm{posterior} \propto \mathrm{prior} \times \mathrm{likelihood}</script><p>注意这里是正比于而不是等于，这个是理解似然函数的一个关键，右侧直接的乘积其实是不满足概率分布归一化的条件的（就是右侧的积分最后不会等于1）那么这个正比符号怎样才能变成等号呢？其实只要再除以一个系数进行归一化就可以了：</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>已知训练数据$T=\{(X_1,Y_1),(X_2,Y_2),…,(X_N,Y_N)\}$，其中$X_i=(X_i^{(1)},X_i^{(2)},…,X_i^{(M)})$，$X_i^{(j)}$是第$i$个样本的第$j$个特征。给定一个实例$x$，预测该实例的分类。</p><p>朴素贝叶斯法基于贝叶斯定理和特征条件独立假设，我们分开介绍。</p><p><strong>贝叶斯定理</strong></p><script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><p>在分类问题背景下，我们已知某个实例$x$，预测$x$隶属的类，即：$P(Y=c_k|X=x)$。则贝叶斯定理可以为：</p><script type="math/tex; mode=display">P(Y=c_k|X=x)=\frac{P(Y=c_k)P(X=x|Y=c_x))}{P(X=x)}</script><p><strong>特征条件独立</strong></p><p>因为每个样本都有多个特征，特征条件独立假设每个特征发生都是彼此独立。比如上式中的$P(X=x|Y=c_x)$，在特征独立假设下：</p><script type="math/tex; mode=display">P(X=x|Y=c_x)=P(x^{(1)}|Y=c_x)\cdot P(x^{(2)}|Y=c_x)\cdot...\cdot P(x^{(M)}|Y=c_x)</script><p>即样本$x$发生的概率是其每个特征发生的概率乘积。</p><p>朴素贝叶斯方法实际上学习到生成数据的机制，属于生成模型，分类时，通过学习到的模型计算后验概率分布$P(y_x|x)$，将后验概率最大的类作为$x$的类输出：</p><script type="math/tex; mode=display">P(Y=c_k|X=x)=\frac{P(Y=c_x)\prod_iP(x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_iP(x^{(i)}|Y=c_k)}</script><p>这是朴素贝叶斯法分类的基本公式，于是贝叶斯分类器可表示为：</p><script type="math/tex; mode=display">y=f(x)=argmax_{c_k}\frac{P(Y=c_x)\prod_iP(x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_iP(x^{(i)}|Y=c_k)}</script><p>由于对每个$c_k$，分母都是相同的，所以</p><script type="math/tex; mode=display">y=f(x)=argmax_{c_k}{P(Y=c_x)\prod_iP(x^{(i)}|Y=c_k)}</script><h1 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h1><p><img src="/2018/09/20/Naive-Bayes/Screen Shot 2018-10-03 at 9.55.16 PM.png" alt="Screen Shot 2018-10-03 at 9.55.16 PM"></p><p><img src="/2018/09/20/Naive-Bayes/Screen Shot 2018-10-03 at 9.55.26 PM.png" alt="Screen Shot 2018-10-03 at 9.55.26 PM"></p><h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><h2 id="极大似然法"><a href="#极大似然法" class="headerlink" title="极大似然法"></a>极大似然法</h2><p>在朴素贝叶斯法中，学习模型意味着估计$P(X=x|Y=c_x)$，故可以利用极大似然法估计相应的概率，即在给定的训练样本中统计计数，计算概率。 </p><p>对于$P(X=x|Y=c_x)$的计算，在条件独立假设下：</p><script type="math/tex; mode=display">P(X=x|Y=c_x)=P(x^{(1)}|Y=c_x)\cdot P(x^{(2)}|Y=c_x)\cdot...\cdot P(x^{(M)}|Y=c_x)</script><p>分解为计算每个特征的概率$P(x^{(i)})$。对于每个特征，我们可以使用一个概率表：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">$x^{(i)}=v1$</th><th style="text-align:center">$x^{(i)}=v2$</th><th style="text-align:center">…</th><th style="text-align:center">$x^{(i)}=vl$</th></tr></thead><tbody><tr><td style="text-align:center">$c_1$</td><td style="text-align:center">$P(x^{(i)}=v1</td><td style="text-align:center">c_1)$</td><td style="text-align:center"></td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">$c_2$</td><td style="text-align:center">$P(x^{(i)}=v1</td><td style="text-align:center">c_2$</td><td style="text-align:center"></td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">…</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$c_n$</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table></div><p><img src="/2018/09/20/Naive-Bayes/Screen Shot 2018-10-01 at 11.23.00 PM.png" alt="Screen Shot 2018-10-01 at 11.23.00 PM"></p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>我们有以下15个观测样本记录着天气情况以及类标签：是否可以打高尔夫球。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Outlook</th><th style="text-align:center">Temperature</th><th style="text-align:center">Humidity</th><th style="text-align:center">Windy</th><th style="text-align:center">Play Golf</th></tr></thead><tbody><tr><td style="text-align:center">rainy</td><td style="text-align:center">hot</td><td style="text-align:center">high</td><td style="text-align:center">false</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">rainy</td><td style="text-align:center">hot</td><td style="text-align:center">high</td><td style="text-align:center">true</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">overcast</td><td style="text-align:center">hot</td><td style="text-align:center">high</td><td style="text-align:center">false</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">sunny</td><td style="text-align:center">mild</td><td style="text-align:center">high</td><td style="text-align:center">false</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">sunny</td><td style="text-align:center">cool</td><td style="text-align:center">normal</td><td style="text-align:center">false</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">sunny</td><td style="text-align:center">cool</td><td style="text-align:center">normal</td><td style="text-align:center">true</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">overcast</td><td style="text-align:center">cool</td><td style="text-align:center">normal</td><td style="text-align:center">true</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">rainy</td><td style="text-align:center">mild</td><td style="text-align:center">high</td><td style="text-align:center">false</td><td style="text-align:center">no</td></tr><tr><td style="text-align:center">rainy</td><td style="text-align:center">cool</td><td style="text-align:center">normal</td><td style="text-align:center">false</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">sunny</td><td style="text-align:center">mild</td><td style="text-align:center">normal</td><td style="text-align:center">false</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">rainy</td><td style="text-align:center">mild</td><td style="text-align:center">normal</td><td style="text-align:center">true</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">overcast</td><td style="text-align:center">mild</td><td style="text-align:center">high</td><td style="text-align:center">true</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">overcast</td><td style="text-align:center">hot</td><td style="text-align:center">normal</td><td style="text-align:center">false</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">sunny</td><td style="text-align:center">mild</td><td style="text-align:center">high</td><td style="text-align:center">true</td><td style="text-align:center">no</td></tr></tbody></table></div><p>现在给出某一天的天气如下，请问这一天是否适合打高尔夫球？</p><div class="table-container"><table><thead><tr><th style="text-align:center">outlook</th><th style="text-align:center">temperature</th><th style="text-align:center">humidity</th><th style="text-align:center">windy</th><th style="text-align:center">play golf</th></tr></thead><tbody><tr><td style="text-align:center">rainy</td><td style="text-align:center">mild</td><td style="text-align:center">normal</td><td style="text-align:center">true</td><td style="text-align:center">?</td></tr></tbody></table></div><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol><li><p>先验概率</p><p>我们基于最大似然法，利用观测样本算出先验概率。因为类标签只有两种：yes和no。</p><p>$P(yes)=\frac{9}{14}$，  $P(no)=\frac{5}{14}$ </p></li><li><p>条件概率</p><p>对于每一种特征，我们求出该特征的条件概率表。</p><p>| Outlook | rainy | overcast | sunny |<br>| :——-: | :—-: | :———: | :—-: |<br>|   yes   |  2/9  |   4/9    |  3/9  |<br>|   no    |  3/5  |   2/5    |   0   |</p><p>| Temperature | hot  | mild | cool |<br>| :————-: | :—: | :—: | :—: |<br>|     yes     | 2/9  | 4/9  | 3/9  |<br>|     no      | 2/5  | 2/5  | 1/5  |</p><p>| Humidity | high | normal |<br>| :———: | :—: | :——: |<br>|   yes    | 3/9  |  6/9   |<br>|    no    | 4/5  |  1/5   |</p><p>| windy | true | false |<br>| :—-: | :—: | :—-: |<br>|  yes  | 3/9  |  6/9  |<br>|  no   | 3/5  |  2/5  |</p></li><li><p>预测</p><p>利用预测公式</p><script type="math/tex; mode=display">P(Y=y_i|X)=P(Y=y_i)P(X^{(1)}|Y=yes)P(X^{(2)}|Y=yes)...P(X^{(m)}|Y=yes)</script><p>其中，$m$是特征数。</p><p>所以，对于每个类标签$Yes$和$No$，我们可以计算出概率为：</p><script type="math/tex; mode=display">P(Yes|X)=P(rainy|Yes)P(mild|Yes)P(normal|Yes)P(windy|Yes)P(Yes)\\=\frac{2}{9}\frac{4}{9}\frac{6}{9}\frac{3}{9}\frac{9}{14}=0.0141099347</script><script type="math/tex; mode=display">P(No|X)=P(rainy|No)P(mild|No)P(normal|No)P(windy|No)P(No)=0.010285714</script><p>将上式正则化：</p><script type="math/tex; mode=display">P(Yes)=\frac{0.0141099347}{0.0141099347+0.010285714}=0.578368999\\P(No)=\frac{0.010285714}{0.0141099347+0.010285714}=0.421631001</script><p>所以，我们预测可以出行打高尔夫球。</p></li></ol><h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>使用极大似然估计可能会出现某种特征的概率为0的情况，如上例中，$P(Outlook=sunny|Y=no)=0$。这样会导致$P(Outlook=sunny, Temperature=,Humidity=,Windy=|Y=no)=0$，无论其他特征出现的概率多大，只要事件中Outlook特征为sunny，那么该事件的概率是0。这样显然是不合理的，所以为防止0概率出现，我们在统计计数时，对最后的计数结果加上一个正数$\lambda$，当$\lambda=0$时，就是极大似然估计，常取$\lambda=1$，这时称为拉普拉斯平滑。</p><p>以上面的例子为例：</p><ol><li><p>先验概率</p><p>我们基于最大似然法，利用观测样本算出先验概率。因为类标签只有两种：yes和no。</p><p>$P(yes)=\frac{9+1}{14+2}=\frac{10}{16}$，  $P(no)=\frac{5+1}{14+2}=\frac{6}{16}$ </p></li><li><p>条件概率</p><p>对于每一种特征，我们求出该特征的条件概率表。</p><p>| Outlook |    rainy    |  overcast   |    sunny    |<br>| :——-: | :————-: | :————-: | :————-: |<br>|   yes   | (2+1)/(9+3) | (4+1)/(9+3) | (3+1)/(9+3) |<br>|   no    | (3+1)/(5+3) | (2+1)/(5+3) | (0+1)/(5+3) |</p><p>| Temperature | hot  | mild | cool |<br>| :————-: | :—: | :—: | :—: |<br>|     yes     | 3/12 | 5/12 | 4/12 |<br>|     no      | 3/8  | 3/8  | 2/8  |</p><p>| Humidity | high | normal |<br>| :———: | :—: | :——: |<br>|   yes    | 4/11 |  7/11  |<br>|    no    | 5/7  |  2/7   |</p><p>| windy | true | false |<br>| :—-: | :—: | :—-: |<br>|  yes  | 4/11 | 7/11  |<br>|  no   | 4/7  |  3/7  |</p></li><li><p>预测</p><p>利用预测公式</p><script type="math/tex; mode=display">P(Y=y_i|X)=P(Y=y_i)P(X^{(1)}|Y=yes)P(X^{(2)}|Y=yes)...P(X^{(m)}|Y=yes)</script><p>其中，$m$是特征数。</p><p>所以，对于每个类标签$Yes$和$No$，我们可以计算出概率为：</p><script type="math/tex; mode=display">P(Yes|X)=P(rainy|Yes)P(mild|Yes)P(normal|Yes)P(windy|Yes)P(Yes)\\=\frac{3}{12}\frac{5}{12}\frac{7}{11}\frac{4}{11}\frac{10}{16}=0.01506</script><script type="math/tex; mode=display">P(No|X)=P(rainy|No)P(mild|No)P(normal|No)P(windy|No)P(No)=0.01147</script><p>将上式正则化：</p><script type="math/tex; mode=display">P(Yes)=\frac{0.01506}{0.01506+0.01147}=0.567659\\P(No)=\frac{0.01147}{0.01506+0.01147}=0.43234</script><p>所以，我们预测可以出行打高尔夫球。</p></li></ol><h1 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h1><p>朴素贝叶斯采用了属性条件独立性假设，但是在现实中这个假设往往很难成立，于是人们对该假设进行一定程度的放松，即“半朴素贝叶斯分类器”学习方法。“独依赖估计“（One-Dependent Estimator, ODE）是其中一种常用方法，顾名思义，该方法假设每个属性最多依赖一个其他属性，即：</p><script type="math/tex; mode=display">P(c|x)\propto P(c)\prod_{i=1}^{d}P(x_i|c,pa_i)</script><p>其中$pa_i$为属性$x_i$的依赖属性，称为$x_i$的父属性，如果对每个属性的父属性已知，则可采用上面类似方法来估计概率$P(x_i|c,pa_i)$。</p><h1 id="应用：邮件分类"><a href="#应用：邮件分类" class="headerlink" title="应用：邮件分类"></a>应用：邮件分类</h1><p><a href="https://github.com/Wellat/MLaction/tree/master/Ch04_NaiveBayes" target="_blank" rel="noopener">data</a> </p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>C++ Programing Language</title>
      <link href="/2018/09/12/C-Programing-Language/"/>
      <url>/2018/09/12/C-Programing-Language/</url>
      <content type="html"><![CDATA[<h1 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h1><p><code>vector&lt;vector&lt;int&gt;&gt;</code> 初始化 <a href="https://blog.csdn.net/yooliee/article/details/71498321" target="_blank" rel="noopener">ref</a> </p><p>采用vector模板中的方法<code>push_back()</code> </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt; </span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="comment">//array用来保存一个3*3的二维数组，array的每个元素都是vector&lt;int&gt;类型</span></div><div class="line">    <span class="built_in">vector</span> &lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;<span class="built_in">array</span>;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="number">3</span>; i++)&#123;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;<span class="number">3</span>; j++)&#123;</div><div class="line">            <span class="keyword">int</span> value;</div><div class="line">            <span class="built_in">cin</span> &gt;&gt; value;</div><div class="line">            v.push_back(value);</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">array</span>.push_back(v); <span class="comment">//保存array的每个元素</span></div><div class="line">        v.clear();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="built_in">array</span>.size(); i++)</div><div class="line">    &#123;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;<span class="number">3</span>; j++)</div><div class="line">            <span class="built_in">cout</span> &lt;&lt;<span class="built_in">array</span>[i][j];</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h1 id="Unsorted-set"><a href="#Unsorted-set" class="headerlink" title="Unsorted_set"></a>Unsorted_set</h1><p><a href="http://classfoo.com/ccby/article/qNNOJ#sec_4PWie3" target="_blank" rel="noopener">代码示例</a> </p><p><code>unsorted_set</code>是无序容器，基于哈希表实现。</p>]]></content>
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="/2018/09/11/Support-Vector-Machine/"/>
      <url>/2018/09/11/Support-Vector-Machine/</url>
      <content type="html"><![CDATA[<p>支持向量机(SVM)是一个用于分类和回归的线性模型，它可以解决线性和非线性问题。</p><p>支持向量机的思想很简单：我们尽量找到一条直线或者一个超平面将数据集分成不同的类。</p><p>在逻辑斯蒂回归中，我们用了sigmoid激活函数将$y=wx+b$的计算结果压缩到$[0,1]$范围内，这样，如果我们的压缩值大于临界值0.5，我们就将该样例分类到正例；否则我们将之分类到负例。在SVM中，我们将临界值设置为-1和+1，即如果$y=wx+b$的计算结果大于1，我们分类至正例；如果$y=wx+b$的计算结果小于-1，我们分类至负例，这样我们就得到了一个区间$[-1,1]$，称之为间隔。</p><a id="more"></a><p><a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" target="_blank" rel="noopener">ref1</a> <a href="https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989" target="_blank" rel="noopener">ref2</a> <a href="https://blog.csdn.net/v_july_v/article/details/7624837" target="_blank" rel="noopener">支持向量机通俗导论（理解SVM的三层境界</a> </p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>给定数据集$D=\{(x_1,y_1), (x_2,y_2),…,(x_m,y_m)\}, y_i\in{\{-1,+1\}}$, 欲找到一个划分超平面, 将不同类别的样本分开. </p><p><img src="/2018/09/11/Support-Vector-Machine/1_VDATmWG1E1ZNg7hdasOh5g.png" alt="1_VDATmWG1E1ZNg7hdasOh5g"></p><p>但是能将训练样本分开的划分超平面有多个, 直观上, 去找位于两类样本”正中间”的划分超平面,  即下图中的黄线是比较好的分割线。可用线性方程描述:</p><blockquote><script type="math/tex; mode=display">w^Tx+b=0</script></blockquote><p><img src="/2018/09/11/Support-Vector-Machine/1_AMR3v-jCvUMXPUtQskzxmQ.png" alt="1_AMR3v-jCvUMXPUtQskzxmQ"></p><p>那么如何用数学方法描述黄色的那条直线是比较好的分割线呢？使用点到直线的距离，我们欲使直线两边的点离直线的距离越远越好，这样不同类的点也距离越远，说明我们的分割线比较明确地分类出正类和负类。</p><p>我们介绍两种衡量距离的方法：函数间隔和几何间隔。</p><p><strong>函数间隔</strong>      对于给定的数据集和超平面$(w,b)$, 定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为:</p><script type="math/tex; mode=display">functional\_margin_i=y_i(w\dot{}x_i+b)</script><p>定义样本点关于超平面最小的函数间隔为:</p><script type="math/tex; mode=display">functianal\_margin(min)=\min\limits_{i=1,2,...,m}functional\_margin_i</script><blockquote><p><a href="https://www.zhihu.com/question/20466147/answer/28469993" target="_blank" rel="noopener">ref</a> </p><p>在超平面确定的情况下，即$w\cdot x+b=0$，对于二分类问题，如果$w\cdot x_i+b&gt;0$，则$x_i$的类别被判别为1；否则判定为-1。所以，$y_i(w\cdot x_i+b)&gt;0$意味着$x_i$的分类结果是正确的，而且$y_i(w\cdot x_i+b)$的值越大，分类结果的确信度越大，反之亦然。</p></blockquote><p><strong>几何间隔（点到平面距离）</strong>    对于给定的数据集和超平面$(w,b)$, 定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为:</p><script type="math/tex; mode=display">geometric\_margin=y_i\frac{w\dot{}x_i+b}{||w||}</script><p>定义样本点关于超平面最小的几何间隔为:</p><script type="math/tex; mode=display">geometric\_margin(min)=\min\limits_{i=1,2,...,m}geometric\_margin_i</script><blockquote><p>但是如果成比例的改变$w$和$b$, 超平面并没有改变, 但是函数间隔随之改变, 故对超平面的法向量进行规范化，使得间隔是确定的，这时函数间隔成为了几何间隔。</p></blockquote><p><strong>间隔最大化</strong>      </p><p>间隔最大化的直观解释: 对训练数据找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类.  即不仅将正负实例点分开, 而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开. 这样, 对未知的新实例有很好的分类预测能力. 具体地, 这个问题可以表示为以下的约束最优化问题:</p><script type="math/tex; mode=display">\begin{align*}&\max\limits_{w,b} geometric\_margin\\&s.t. y_i(\frac{w\dot{}x_i+b}{||w||})\ge{geometric\_margin, i=1,2,..,N} \end{align*}</script><p>即希望最大化超平面$(w,b)$关于数据集的几个间隔$geometric_margin$, 约束条件表示的是超平面$(w,b)$关于每个样本点的几个间隔至少是$geometric_margin$. 超平面两边分别是正例和负例, 故会存在正例对超平面的最小几何间隔$geometric_margin+$, 负例对超平面的最小几何间隔$geometric_margin-$, 选其中最小的, 如果$geometric_margin+$&lt;$geometric_margin-$, 则一定有正例样本点落在间隔面上; $geometric_margin+$&gt;$geometric_margin-$, 则一定有负例点落在间隔面上; $geometric_margin+$=$geometric_margin-$, 则间隔面上有正负实例样本点.</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>SVM的目标就是寻找一个几何间隔最大化的分离超平面，该问题可以表示为下面的约束问题：</p><script type="math/tex; mode=display">\begin{align*}&\max\limits_{w,b} geometric\_margin\\&s.t. y_i(\frac{w\dot{}x_i+b}{||w||})\ge{geometric\_margin, i=1,2,..,N} \end{align*}</script><p>即我们希望最大化超平面$(w,b)$关于训练集数据的几何间隔，约束条件表示每个样本点关于超平面的几何间隔至少是$geometric_margin$。</p><p>考虑几何间隔和函数间隔的关系, 即$\frac{1}{||w||}$大小，则</p><script type="math/tex; mode=display">\begin{align*}&\max\limits_{w,b} \frac{functional\_margin}{||w||}\\&s.t. y_i({w\dot{}x_i+b})\ge{functional\_margin, i=1,2,..,N} \end{align*}</script><p>仍然最大化最小几何间隔, 即间隔临界样本点到超平面的距离, 约束变成了所有的样本点的函数间隔都要大于最小的函数间隔.</p><p>考虑到函数间隔$functional_margin$的取值并不影响最优化问题的解. 因为上面我们介绍过了，之所以采样几何间隔，就是因为成比例扩大/缩小$(w,b)$，函数间隔的值会改变，即使超平面不变。所以，对于同一个超平面，一个样本关于该平面的函数间隔有无穷多个，可以是1，可以是10等等，只需要成比例扩大/缩小$(w,b)$。故，为简化问题，我们领函数间隔为1，代入上式：</p><script type="math/tex; mode=display">\max \limits_{w,b}\frac{1}{||w||}\\s.t. y_i(w^Tx_i+b)\ge1, i=1,2,...,m</script><p>又因为最大化$\frac{1}{||w||}$，等价于最小化$||w||$，等价于最小化$\frac{1}{2}||w||^2$，故上式等价于:</p><script type="math/tex; mode=display">\min \limits_{w,b}\frac{1}{2}{||w||^2}=\frac{1}{2}\sum_{j=1}w_i^2\\s.t. y_i(w^Tx_i+b)\ge1, i=1,2,...,m</script><h1 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h1><p>比较下面两张图：我们发现图二的决策边界更具有一般性，但是我们的模型得到的是图一中的结果，图一中蓝色的决策边界有点过拟合，因为它过于在意完全分类正确，对于左上角的一个异常样本，为了将它分类正确，导致最终的分割线严重偏斜。所以，我们希望允许SVM有错误分类，这样才能提高模型的泛化能力。</p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-13 at 6.07.12 PM.png" alt="Screen Shot 2018-09-13 at 6.07.12 PM"></p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-13 at 6.07.01 PM.png" alt="Screen Shot 2018-09-13 at 6.07.01 PM"></p><p>在上述SVM的定义中，因为我们要求所有样本点的函数间隔都要大于等于1，即每个样本点都分类正确；那么允许分类错误意味着某些样本点不满足函数间隔大于等于1的约束条件, 故可以针对每个样本点引进一个松弛变量$\xi_{i}\ge0$, 使函数间隔加上松弛变量大于等于1. 这样, 约束条件变为:</p><script type="math/tex; mode=display">y_i(w\dot{x_i}+b)+\xi_{i}\ge1</script><p>同时, 对每个松弛变量$\xi_{i}$, 支付一个代价$\xi_{i}$, 则目标函数由原来的$\frac{1}{2}||w||^2$变成</p><script type="math/tex; mode=display">\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i</script><p>$C\ge0$称为惩罚参数. 最小化目标函数包含两层含义: 使$\frac{1}{2}||w||^2$尽量小即间隔尽量打, 同时使误分类点的个数尽量小, C是调和二者的系数. C越大，则允许的分类错误越少；C越小，则允许多的分类误差。</p><p>令$z_i=y_i(w\dot{x_i}+b)-1$时，</p><script type="math/tex; mode=display">l_{0/1}(z)=\left\{\begin{align} 1, & \text{if z<0}\\0, &\text{otherwise}\end{align}\right.</script><p>代入得：</p><script type="math/tex; mode=display">\frac{1}{2}||w||^2+C\sum_{i=1}^{N}l_{0/1}(y_i(w\dot{x_i}+b)-1)</script><p>但是$l_{0/1}$是阶跃函数，非凸，非连续，所以我们采用其他函数来代替，替代损失函数一般选取凸的连续函数且是$l_{0/1}$的上界，常用的：</p><script type="math/tex; mode=display">hinge损失：l_{hinge}(z)=max(0,1-z)\\指数损失：l_{exp}(z)=e^{-z}\\对率损失：l_{log}=log(1+e^{-z})</script><p><img src="/2018/09/11/Support-Vector-Machine/image-20180918214002918.png" alt="image-20180918214002918"></p><blockquote><p>以上函数的x正半轴几乎为0，与我们的事实符合：$z_i=y_i(w\dot{x_i}+b)-1&gt;0$，则没有任何损失。</p></blockquote><p>则线性支持向量机的学习问题变成了如下凸二次规划问题:</p><script type="math/tex; mode=display">\min\limits_{w, b,\xi}\frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_i\\\begin{align*}s.t. &y_i(w\dot{x_i}+b)\ge1-\xi_{i}, i=1,2,...,N\\&\xi_i\ge0, i=1,2,...,m\end{align*}</script><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>Say given an example $(x_i,y_i)$, and use the shorthand for the scores vector: $s=f(x_i,W)$, then the SVM loss has the following form:</p><script type="math/tex; mode=display">L_i = \sum_{j\ne y_i}\left\{\begin{align} 0& & \text{if   }  s_{y_i}\ge s_j+1 \\s_j-s_{y_i}+1& &\text{if otherwise}\end{align}=\sum_{j\ne y_i}max(0,s_j-s_{y_i}+1)\right.</script><p>其中，$y_i$是groundtruth label，$s_{y_i}$是分类器给真实分类的分数，$s_j$是分类器给其他类的分数，该损失函数鼓励真实分类的分数大于其他分类的分数+1。</p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2019-06-03 at 4.04.02 PM.png" alt="creen Shot 2019-06-03 at 4.04.02 P"></p><p>The two given figures are plots of function $f(x)=max(0,1-x)$ .We can see from the second figure that when x is between 0 and 1, the loss is in range $[0,1]$. </p><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>Say we have an image classification and we use it to classify three images, like the following:</p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2019-06-03 at 4.27.50 PM.png" alt="creen Shot 2019-06-03 at 4.27.50 P"></p><p>The output of classification is a 3d vector, each element of the vector is the probability of image classification. For example, for the cat image, the vector is $[3.2, 5.1, -1.7]$, meaning this picture is cat with probability 3.2, is car with prob 5.1 and frog with prob -1.7.</p><p> Then, the $s_{y_i}=3.2$, $s_1=5.1$ and $s_2=-1.7$，so the loss is:</p><script type="math/tex; mode=display">L_{cat}=max(0,5.1+1-3.2)+max(0,-1.7+1-3.2)=max(0,2.9)+max(0,-3.9)=2.9</script><p>Similarily, $L_{car}=0$, $L_{frog}=12.9$.</p><p>So the overall loss is:</p><script type="math/tex; mode=display">L=\frac{1}{N}\sum_{i=1}^{N}L_i\\L=\frac{(2.9+0+12.9)}{3}=5.27</script><blockquote><ol><li><p>What happens to loss if car scores change a bit?</p><p>The answer is the loss will not change. The SVM loss only cares about getting the correct score to be greater than one more than the incorrect scores. But in this case, the car score is already quite a bit large than the others. So if the scores for this class changes, this margin of one will still be retained and the loss will not change.</p></li><li><p>What is the min/max possible loss?</p><p>The min loss is 0 because across all the classes, if our correct score was much larger then we will incur zero loss across all the classes. </p><p>The max loss is infinite. According to the hinge loss figure, if correct score goes very very negative, then we could incur potentially infinite loss.</p></li><li><p>At initialization W is small so all $s\approx 0$. What is the loss?</p><p>The answer is the number of class minus one.</p></li><li><p>What if the sum was over all classes?(including $j=y_j$).</p><p>The loss increases by one.</p></li><li><p>What if we used mean instead of sum?</p><p>The answer is that it doesn’t change. We just rescale the whole loss function by a constant value and we don’t care about the true value of the loss.</p></li><li><p>What if we used $L_i=\sum_{j\ne y_i}max(0,s_j-s_{y_i}+1)^2$?</p><p>The answer is it is different. We are kind of changing the trade-offs between good and badness in kind of nonlinear way, and this would end up actually computing a different loss function.</p></li><li><p>Suppose we found a W such that $L=0$. Is this W unique?</p><p>The answer is no. $2W$ is also has $L=0$.</p></li></ol></blockquote><h1 id="Derivative-ref"><a href="#Derivative-ref" class="headerlink" title="Derivative ref"></a>Derivative <a href="https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html" target="_blank" rel="noopener">ref</a></h1><p>Say for a single datapoint $(x_i,y_i)$, we have the following hinge loss:</p><script type="math/tex; mode=display">L_i=\sum_{j\ne y_i}^{c} max(0, s_j+1-s_i)</script><p>where $c$ is the class number and $s_j=w_j^T x_i$ is the score for the $j{th}$ class. What we do here is to iterate scores for all classes and compare them with the score of truth class.</p><p>To spread out, </p><script type="math/tex; mode=display">\begin{align*}L_i = &\max(0,1+w_1x_i-w_{y_i}x_i) + \\ &\max(0, 1+w_2x_i-w_{y_i}x_i) + \\& \quad \quad \quad \quad \quad \quad \vdots \\&\max(0, 1+w_cx_i-w_{y_i}x_i)\end{align*}</script><p>If $(w_jx_i+1-w_{y_i}x_i)&gt;0$, $\frac{dL_i}{dw_j}=x_i$.</p><p>But for $w_{y_i}$,</p><script type="math/tex; mode=display">\begin{align*}\frac{dL_i}{dw_{y_i}} &= - \sum_{j\neq y_i} \mathbb{1}(x_iw_j - x_iw_{y_i} + \Delta > 0) x_i \tag{3}\end{align*}</script><h1 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h1><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.W = <span class="keyword">None</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(self,W, X, y, reg)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line"><span class="string">        :param W:A numpy array of shape (D, C) containing weights.</span></div><div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing a minibatch of data.</span></div><div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c means</span></div><div class="line"><span class="string">        that X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">        :param reg:(float) regularization strength</span></div><div class="line"><span class="string">        :return:a tuple of:</span></div><div class="line"><span class="string">                 - loss as single float</span></div><div class="line"><span class="string">                 - gradient with respect to weights W; an array of same shape as W</span></div><div class="line"><span class="string">        '''</span></div><div class="line">        dW = np.zeros(W.shape)</div><div class="line">        num_classes = W.shape[<span class="number">1</span>]</div><div class="line">        num_train = X.shape[<span class="number">0</span>]</div><div class="line">        loss = <span class="number">0.0</span></div><div class="line">        scores = np.dot(X, W)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</div><div class="line">            target = y[i]</div><div class="line">            score = scores[i]</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</div><div class="line">                <span class="keyword">if</span> target == j:</div><div class="line">                    <span class="keyword">continue</span></div><div class="line">                <span class="keyword">if</span> score[j] - score[target] + <span class="number">1</span> &gt; <span class="number">0</span>:</div><div class="line">                    loss += score[j] - score[target] + <span class="number">1</span></div><div class="line">                    dW[:, j] += X[i]</div><div class="line">                    dW[:, target] -= X[i]</div><div class="line">        loss /= num_train</div><div class="line">        loss += reg * np.sum(W ** <span class="number">2</span>)</div><div class="line">        dW = dW / num_train + <span class="number">2</span> * reg * W</div><div class="line">        <span class="keyword">return</span> loss, dW</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(self,W, X, y, reg)</span>:</span></div><div class="line">        N = np.shape(X)[<span class="number">0</span>]</div><div class="line">        C = np.shape(W)[<span class="number">1</span>]</div><div class="line">        loss = <span class="number">0.0</span></div><div class="line">        dW = np.zeros(W.shape)</div><div class="line"></div><div class="line">        scores = np.dot(X, W)</div><div class="line">        target_scores = scores[np.arange(N), y].reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line">        scores_pos = scores - target_scores + <span class="number">1</span></div><div class="line">        scores_pos[np.arange(N), y] = <span class="number">0</span></div><div class="line">        scores_pos = np.maximum(scores_pos, <span class="number">0</span>)</div><div class="line">        loss = np.sum(scores_pos) / N</div><div class="line"></div><div class="line">        mask = np.zeros(scores_pos.shape)</div><div class="line">        mask[scores_pos &gt; <span class="number">0</span>] = <span class="number">1</span></div><div class="line">        mask[np.arange(N), y] -= np.sum(mask, axis=<span class="number">1</span>)  <span class="comment"># attention:"-"</span></div><div class="line">        dW = np.dot(X.T, mask)</div><div class="line">        dW = dW / N + <span class="number">2</span> * reg * W</div><div class="line"></div><div class="line">        <span class="keyword">return</span> loss, dW</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.svm_loss_vectorized(self.W, X_batch, y_batch, reg)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></div><div class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing training data; there are N</span></div><div class="line"><span class="string">          training samples each of dimension D.</span></div><div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c</span></div><div class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></div><div class="line"><span class="string">        :param learning_rate:(float) learning rate for optimization.</span></div><div class="line"><span class="string">        :param reg:(float) regularization strength.</span></div><div class="line"><span class="string">        :param num_iters:(integer) number of steps to take when optimizing</span></div><div class="line"><span class="string">        :param batch_size:(integer) number of training examples to use at each step.</span></div><div class="line"><span class="string">        :param verbose:(boolean) If true, print progress during optimization.</span></div><div class="line"><span class="string">        :return:A list containing the value of the loss function at each training iteration.</span></div><div class="line"><span class="string">        '''</span></div><div class="line">        num_train, dim = X.shape</div><div class="line">        num_classes = np.max(y) + <span class="number">1</span>  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></div><div class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># lazily initialize W</span></div><div class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</div><div class="line"></div><div class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></div><div class="line">        loss_history = []</div><div class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</div><div class="line">            mask = np.random.choice(num_train, batch_size)</div><div class="line">            X_batch = X[mask]</div><div class="line">            y_batch = y[mask]</div><div class="line"></div><div class="line">            <span class="comment"># evaluate loss and gradient</span></div><div class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</div><div class="line">            loss_history.append(loss)</div><div class="line">            self.W -= learning_rate * grad</div><div class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</div><div class="line">        <span class="keyword">return</span> loss_history</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div><div class="line">        y_pred = np.zeros(X.shape[<span class="number">0</span>])</div><div class="line">        scores = X.dot(self.W)</div><div class="line">        y_pred = np.argmax(scores,axis=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> y_pred</div></pre></td></tr></table></figure></div></div><h1 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>通俗说，只要涉及空间的变换和內积的运算，我们就可以使用Kernel trick来简化运算。</p><p>一般的，在高维空间计算內积，我们需要分两步：</p><ol><li><p>将原始低维数据空间$X$映射到更高维的$Z$空间：</p><script type="math/tex; mode=display">\phi(x):X\to Z</script></li><li><p>在$Z$空间里计算內积：</p><script type="math/tex; mode=display">z_i = \phi(x_i)\\z_j = \phi(x_j)\\k = z_i^Tz_j</script></li></ol><p>而核方法：</p><script type="math/tex; mode=display">K(x_i,x_j)=z_i^Tz_j=\phi(x_i)^T\phi(x_j)</script><p>即，$K(x_i,x_j)$计算得到的结果就是原始数据空间中的两点先升维$\phi(x)$再內积$\phi(x_i)^T\phi(x_j)$的结果，不必进行显示的升维操作。</p><h2 id="Kernel-Trick-in-SVM"><a href="#Kernel-Trick-in-SVM" class="headerlink" title="Kernel Trick in SVM"></a>Kernel Trick in SVM</h2><p><a href="https://www.youtube.com/watch?v=vMmG_7JcfIc" target="_blank" rel="noopener">kernel visualiztion</a> </p><p>那么SVM中如何利用核方法呢？且看下面的这个例子。</p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-15 at 10.42.27 PM (2" alt="Screen Shot 2018-09-15 at 10.42.27 PM (2)">.png)</p><p>在一维空间（直线上）我们有一系列样本点，蓝色为正例，红色为负例，显然我们不能找到一个线性的分割来分类它们。那么，如果将一维的数据点映射到二维平面呢？即对每个样本点，我们先进行映射：$(x)\to (x,y)$，其中$y=x^2$，这样就将蓝色和红色的样本点分开了，于是，我们就可以使用一条直线取划分样本点了。</p><p>再看一个二维空间的例子，如下图所示：</p><p><img src="/2018/09/11/Support-Vector-Machine/1260px-Kernel_trick_idea.svg.png" alt="1260px-Kernel_trick_idea.svg"></p><p>显然我们无法使用线性的分割去分类上述点，但是如果我们将二维平面上的点增加一个维度$z$，使平面上点映射到三维空间上，$(x,y)\to (x,y,z)$，如右边图所示，这样我们就可以使用一个平面进行划分了。</p><h2 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h2><p>通过上面的讨论，我们希望样本在新的特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。但是在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间，故选择合适的核函数很重要。</p><ul><li><p>线性核：$k(x_i,x_j)=x_i^Tx_j$</p></li><li><p>多项式核：$k(x_i,x_j)=(\gamma x_i^Tx_j+r)^d$</p><blockquote><p>当$d&gt;1$</p></blockquote></li><li><p>高斯核：$k(x_i,x_j)=e^{(-\frac{||x_i-x_j||^2}{2\sigma^2})}$</p><blockquote><p>亦称RBF核，$\sigma&gt;0$为高斯核的带宽</p><p>如果$\sigma$设的太小，方差会很小，方差很小的高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，(如果让方差无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)而测试准确率不高的可能，就是通常说的过训练；而如果设的过大，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率。<a href="https://xijunlee.github.io/2017/03/29/sklearn%E4%B8%ADSVM%E8%B0%83%E5%8F%82%E8%AF%B4%E6%98%8E%E5%8F%8A%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">ref</a> </p></blockquote></li><li><p>拉普拉斯核：$k(x_i,x_j)=e^{(-\frac{||x_i-x_j||^2}{\sigma^2})}$</p><blockquote><p>$\sigma&gt;0$</p></blockquote></li><li><p>Sigmoid核：$k(x_i,x_j)=tanh(\beta x_i^Tx_j+\theta)$</p><blockquote><p>$\beta&gt;0$，$\theta&lt;0$</p></blockquote></li></ul><h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">X = np.array([[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>]])</div><div class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line">clf = SVC(kernel=<span class="string">'linear'</span>)</div><div class="line">clf = SVC.fit(X, y)</div><div class="line">prediction = clf.predict([[<span class="number">0</span>,<span class="number">6</span>]])</div></pre></td></tr></table></figure><h2 id="Exercise1"><a href="#Exercise1" class="headerlink" title="Exercise1"></a>Exercise1</h2><p><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/" target="_blank" rel="noopener">Machine Learning Exercises In Python, Part 6 - SVM</a> </p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-11 at 10.16.55 AM.png" alt="Screen Shot 2018-09-11 at 10.16.55 AM"></p><p><img src="/2018/09/11/Support-Vector-Machine/Screen Shot 2018-09-11 at 9.44.08 AM.png" alt="Screen Shot 2018-09-11 at 9.44.08 AM"></p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Support Vector Machine </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Grid Search</title>
      <link href="/2018/09/03/Grid-Search/"/>
      <url>/2018/09/03/Grid-Search/</url>
      <content type="html"><![CDATA[<p>模型参数(Model Parameters)</p><p>模型参数是根据训练集数据而定义的，故它们是利用训练集数据训练得到的，它们往往不能手动设置，常见的模型参数包括：</p><ul><li>线性模型、非线性模型的系数</li><li>神经网络的权重，隐藏层的层数，每一层的神经元个数等</li><li>随机森林中决策树的个数</li></ul><p>模型超参数(Model Hyper-Parameters)</p><p>模型超参数往往独立于训练集而被定义，所以它们不能从训练集中学习得到。常见的超参数包括：</p><ul><li>模型的学习速率</li><li>k折交叉验证的k值</li></ul><p>Grid Search</p><p>每一个模型几乎都有许多超参数，所以寻找超参数的一个直观的方法是尝试这些超参数的不能组合，然后比较结果。</p><p>Python实现</p><p>下面我们以寻找逻辑斯蒂回归模型最佳正则函数和学习速率为例，来感受一个Grid Search。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create logistic regression object</span></div><div class="line">logistic = linear_model.LogisticRegression()</div><div class="line"><span class="comment"># Create a list of all of the different penalty values that you want to test and save them to a variable called 'penalty'</span></div><div class="line">penalty = [<span class="string">'l1'</span>, <span class="string">'l2'</span>]</div><div class="line"><span class="comment"># Create a list of all of the different C values that you want to test and save them to a variable called 'C'</span></div><div class="line">C = [<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">1</span>, <span class="number">100</span>]</div><div class="line"><span class="comment"># Now that you have two lists each holding the different values that you want test, use the dict() function to combine them into a dictionary.</span></div><div class="line"><span class="comment"># Save your new dictionary to the variable 'hyperparameters'</span></div><div class="line">hyperparameters = dict(C=C, penalty=penalty)</div><div class="line"><span class="comment"># Fit your model using gridsearch</span></div><div class="line">clf = GridSearchCV(logistic, hyperparameters, cv=<span class="number">5</span>, verbose=<span class="number">0</span>)</div><div class="line">best_model = clf.fit(X, Y)</div><div class="line"><span class="comment">#Print all the Parameters that gave the best results:</span></div><div class="line">print(<span class="string">'Best Parameters'</span>,clf.best_params_)</div><div class="line"><span class="comment"># You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()</span></div><div class="line">print(<span class="string">'Best Penalty:'</span>, best_model.best_estimator_.get_params()[<span class="string">'penalty'</span>])</div><div class="line">print(<span class="string">'Best C:'</span>, best_model.best_estimator_.get_params()[<span class="string">'C'</span>])</div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Grid Search </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tree</title>
      <link href="/2018/08/23/Tree/"/>
      <url>/2018/08/23/Tree/</url>
      <content type="html"><![CDATA[<h5 id="树的定义"><a href="#树的定义" class="headerlink" title="树的定义"></a>树的定义</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Definition for a binary tree node.</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></div><div class="line">        self.val = x</div><div class="line">        self.left = <span class="keyword">None</span></div><div class="line">        self.right = <span class="keyword">None</span></div></pre></td></tr></table></figure><h5 id="树的遍历"><a href="#树的遍历" class="headerlink" title="树的遍历"></a>树的遍历</h5><p>Pre-order ：root， (left)，(right)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> </div><div class="line">        self.res.append(root.val)</div><div class="line">        self.helper(root.left)</div><div class="line">        self.helper(root.right)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preorderTraversal</span><span class="params">(self, root)</span>:</span></div><div class="line">        self.res = []</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> self.res</div><div class="line">        self.helper(root)</div><div class="line">        <span class="keyword">return</span> self.res</div></pre></td></tr></table></figure><p>In-order： (left)，root，(right)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> </div><div class="line">        self.helper(root.left)</div><div class="line">        self.res.append(root.val)</div><div class="line">        self.helper(root.right)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></div><div class="line">        self.res = []</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> self.res</div><div class="line">        self.helper(root)</div><div class="line">        <span class="keyword">return</span> self.res</div></pre></td></tr></table></figure><p>Post-order： (left)，(right)，root</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> </div><div class="line">        self.helper(root.left)</div><div class="line">        self.helper(root.right)</div><div class="line">        self.res.append(root.val)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">postorderTraversal</span><span class="params">(self, root)</span>:</span></div><div class="line">        self.res = []</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> self.res</div><div class="line">        self.helper(root)</div><div class="line">        <span class="keyword">return</span> self.res</div></pre></td></tr></table></figure><h5 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">import</span> Queue <span class="keyword">as</span> queue</div><div class="line"><span class="keyword">except</span> ImportError:</div><div class="line">    <span class="keyword">import</span> queue <span class="comment">#python2.+</span></div><div class="line"><span class="comment">#定义一个队列</span></div><div class="line"><span class="keyword">import</span> queue <span class="comment">#python3.+</span></div><div class="line">qu = queue.Queue()</div><div class="line"><span class="comment">#入队列</span></div><div class="line">qu.put(xx)</div><div class="line"><span class="comment">#出队列</span></div><div class="line">qu.get()</div><div class="line"><span class="comment">#队列大小</span></div><div class="line">qu.qsize()</div><div class="line"><span class="comment">#队列空</span></div><div class="line">qu.empty()</div></pre></td></tr></table></figure><a id="more"></a><h2 id="简单"><a href="#简单" class="headerlink" title="简单"></a>简单</h2><h4 id="Merge-Two-Binary-Trees"><a href="#Merge-Two-Binary-Trees" class="headerlink" title="Merge Two Binary Trees"></a>Merge Two Binary Trees</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目 )"></a><a href="(https://leetcode.com/problems/merge-two-binary-trees/description/">题目</a> )</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given two binary trees and imagine that when you put one of them to cover the other, some nodes of the two trees are overlapped while the others are not.</p><p>You need to merge them into a new binary tree. The merge rule is that if two nodes overlap, then sum node values up as the new value of the merged node. Otherwise, the NOT null node will be used as the node of new tree.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">Tree <span class="number">1</span>                     Tree <span class="number">2</span>                  </div><div class="line">          <span class="number">1</span>                         <span class="number">2</span>                             </div><div class="line">         / \                       / \                            </div><div class="line">        <span class="number">3</span>   <span class="number">2</span>                     <span class="number">1</span>   <span class="number">3</span>                        </div><div class="line">       /                           \   \                      </div><div class="line">      <span class="number">5</span>                             <span class="number">4</span>   <span class="number">7</span>                  </div><div class="line">Output: </div><div class="line">Merged tree:</div><div class="line">     <span class="number">3</span></div><div class="line">    / \</div><div class="line">   <span class="number">4</span>   <span class="number">5</span></div><div class="line">  / \   \ </div><div class="line"> <span class="number">5</span>   <span class="number">4</span>   <span class="number">7</span></div></pre></td></tr></table></figure></div></div><h5 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h5><p>题目大意：将两棵树合并，合并规则：对应位置的节点值相加成为新的节点；如果两个节点中有一个空节点，则另一个非空节点成为新的节点。</p><p>显然是递归求解啊，使用递归时候，考虑一个节点的情况，剩下的节点就递归调用。考虑两棵树的根节点，如果有一棵树是空，那么就不要合并了，直接返回另一棵树；如果都不空，则将两个根节点相加成为新节点值，该新节点的左子树就使用两棵树的左子树作为参数递归调用，同理右子树。</p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergeTrees</span><span class="params">(self, t1, t2)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> t1:<span class="keyword">return</span> t2</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> t2:<span class="keyword">return</span> t1</div><div class="line">        t1.val += t2.val</div><div class="line">        t1.left = self.mergeTrees(t1.left,t2.left)</div><div class="line">        t1.right = self.mergeTrees(t1.right,t2.right)</div><div class="line">        <span class="keyword">return</span> t1</div></pre></td></tr></table></figure></div></div><h4 id="Find-Mode-in-Binary-Search-Tree"><a href="#Find-Mode-in-Binary-Search-Tree" class="headerlink" title="Find Mode in Binary Search Tree"></a>Find Mode in Binary Search Tree</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/find-mode-in-binary-search-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary search tree (BST) with duplicates, find all the <a href="https://en.wikipedia.org/wiki/Mode_(statistics" target="_blank" rel="noopener">mode(s)</a>) (the most frequently occurred element) in the given BST.</p><p>Assume a BST is defined as follows:</p><ul><li>The left subtree of a node contains only nodes with keys <strong>less than or equal to</strong> the node’s key.</li><li>The right subtree of a node contains only nodes with keys <strong>greater than or equal to</strong> the node’s key.</li><li>Both the left and right subtrees must also be binary search trees.</li></ul><p>For example:<br>Given BST <code>[1,null,2,2]</code>,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span></div><div class="line"> \</div><div class="line">  <span class="number">2</span></div><div class="line"> /</div><div class="line"><span class="number">2</span></div></pre></td></tr></table></figure><p>return <code>[2]</code>.</p></div></div><h5 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h5><p>题目大意：给定一棵平衡搜索树，该树的父节点值可以等于孩子节点值，要求找出该树出现次数最多的节点值。</p><p>最简单的做法就是设置一个字典，记录每个节点出现次数，然后遍历该树。</p><h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self,root,dic)</span>:</span></div><div class="line">        <span class="keyword">if</span> root:</div><div class="line">            dic[root.val]+=<span class="number">1</span></div><div class="line">            dic = self.find(root.left,dic)</div><div class="line">            dic = self.find(root.right,dic)</div><div class="line">        <span class="keyword">return</span> dic</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMode</span><span class="params">(self, root)</span>:</span></div><div class="line">        re = []</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> re</div><div class="line">        dic = collections.defaultdict(int)</div><div class="line">        dic = self.find(root,dic)</div><div class="line">        fre = collections.Counter(dic).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">1</span>]</div><div class="line">        <span class="keyword">for</span> k,v <span class="keyword">in</span> dic.items():</div><div class="line">            <span class="keyword">if</span> v==fre:</div><div class="line">                re.append(k)</div><div class="line">        <span class="keyword">return</span> re</div></pre></td></tr></table></figure></div></div><h4 id="Sum-of-Left-Leaves"><a href="#Sum-of-Left-Leaves" class="headerlink" title="Sum of Left Leaves"></a>Sum of Left Leaves</h4><h5 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/sum-of-left-leaves/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Find the sum of all left leaves in a given binary tree.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">    <span class="number">3</span></div><div class="line">   / \</div><div class="line">  <span class="number">9</span>  <span class="number">20</span></div><div class="line">    /  \</div><div class="line">   <span class="number">15</span>   <span class="number">7</span></div><div class="line"></div><div class="line">There are two left leaves <span class="keyword">in</span> the binary tree, <span class="keyword">with</span> values <span class="number">9</span> <span class="keyword">and</span> <span class="number">15</span> respectively. Return <span class="number">24.</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h5><p>题目大意：求左叶子节点的和；</p><p>关于左叶子，前提它是叶子，同时又位于一个节点的左边。</p><p>递归求解，重点是如何判断一个节点是左叶子？只要该节点是左节点且其左右子树均是空，那么就可以返回改点的值，同时递归该点的右兄弟节点。</p><h5 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sumOfLeftLeaves</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.left.left <span class="keyword">and</span> <span class="keyword">not</span> root.left.right:</div><div class="line">            <span class="keyword">return</span> root.left.val + self.sumOfLeftLeaves(root.right)</div><div class="line">        <span class="keyword">return</span> self.sumOfLeftLeaves(root.left) + self.sumOfLeftLeaves(root.right)</div></pre></td></tr></table></figure></div></div><h4 id="Maximum-Depth-of-Binary-Tree"><a href="#Maximum-Depth-of-Binary-Tree" class="headerlink" title="Maximum Depth of Binary Tree"></a>Maximum Depth of Binary Tree</h4><h5 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/maximum-depth-of-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, find its maximum depth.</p><p>The maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node.</p><p><strong>Note:</strong> A leaf is a node with no children.</p><p><strong>Example:</strong></p><p>Given binary tree <code>[3,9,20,null,null,15,7]</code>,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  <span class="number">3</span></div><div class="line"> / \</div><div class="line"><span class="number">9</span>  <span class="number">20</span></div><div class="line">  /  \</div><div class="line"> <span class="number">15</span>   <span class="number">7</span></div></pre></td></tr></table></figure><p>return its depth = 3.</p></div></div><h5 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h5><p>题目大意：求树的最大高度。</p><p>递归求解，对于一个结点，如果是空，则返回高度0；否则需要递归求解器左右子树的高度，那么该节点的高度是<code>1+max( length_left_tree , length_right_tree )</code>.</p><h5 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.maxDepth(root.left)</div><div class="line">        r = self.maxDepth(root.right)</div><div class="line">        <span class="keyword">return</span> max(l,r)+<span class="number">1</span></div></pre></td></tr></table></figure></div></div><h4 id="Convert-Sorted-Array-to-Binary-Search-Tree"><a href="#Convert-Sorted-Array-to-Binary-Search-Tree" class="headerlink" title="Convert Sorted Array to Binary Search Tree"></a>Convert Sorted Array to Binary Search Tree</h4><h5 id="题目-4"><a href="#题目-4" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/convert-sorted-array-to-binary-search-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array where elements are sorted in ascending order, convert it to a height balanced BST.</p><p>For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of <em>every</em> node never differ by more than 1.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Given the sorted array: [<span class="number">-10</span>,<span class="number">-3</span>,<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],</div><div class="line"></div><div class="line">One possible answer <span class="keyword">is</span>: [<span class="number">0</span>,<span class="number">-3</span>,<span class="number">9</span>,<span class="number">-10</span>,null,<span class="number">5</span>], which represents the following height balanced BST:</div><div class="line"></div><div class="line">      <span class="number">0</span></div><div class="line">     / \</div><div class="line">   <span class="number">-3</span>   <span class="number">9</span></div><div class="line">   /   /</div><div class="line"> <span class="number">-10</span>  <span class="number">5</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h5><p>题目大意：将一个有序上升的数组还原成一棵二分搜索树。</p><p>为了使左右子树的高度差不超过1，则每次选择中点作为根，根左边的数组作为左子树，右边的数组作为右子树，递归建树。</p><h5 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sortedArrayToBST</span><span class="params">(self, nums)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:<span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">        mid = len(nums)//<span class="number">2</span></div><div class="line">        root = TreeNode(nums[mid])</div><div class="line">        l = self.sortedArrayToBST(nums[<span class="number">0</span>:mid])</div><div class="line">        r = self.sortedArrayToBST(nums[mid+<span class="number">1</span>:])</div><div class="line">        root.left = l</div><div class="line">        root.right = r</div><div class="line">        <span class="keyword">return</span> root</div></pre></td></tr></table></figure></div></div><h4 id="Binary-Tree-Paths"><a href="#Binary-Tree-Paths" class="headerlink" title="Binary Tree Paths"></a>Binary Tree Paths</h4><h5 id="题目-5"><a href="#题目-5" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/binary-tree-paths/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, return all root-to-leaf paths.</p><p><strong>Note:</strong> A leaf is a node with no children.</p><p><strong>Example:</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line"></div><div class="line">   <span class="number">1</span></div><div class="line"> /   \</div><div class="line"><span class="number">2</span>     <span class="number">3</span></div><div class="line"> \</div><div class="line">  <span class="number">5</span></div><div class="line"></div><div class="line">Output: [<span class="string">"1-&gt;2-&gt;5"</span>, <span class="string">"1-&gt;3"</span>]</div><div class="line"></div><div class="line">Explanation: All root-to-leaf paths are: 1-&gt;2-&gt;5, 1-&gt;3</div></pre></td></tr></table></figure></div></div><h5 id="思路-5"><a href="#思路-5" class="headerlink" title="思路"></a>思路</h5><p>题目大意：写出树的所有路径。</p><p>递归遍历。</p><h5 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"></div></div><h4 id="Invert-Binary-Tree"><a href="#Invert-Binary-Tree" class="headerlink" title="Invert Binary Tree"></a>Invert Binary Tree</h4><h5 id="题目-6"><a href="#题目-6" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/invert-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Invert a binary tree.</p><p><strong>Example:</strong></p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">     <span class="number">4</span></div><div class="line">   /   \</div><div class="line">  <span class="number">2</span>     <span class="number">7</span></div><div class="line"> / \   / \</div><div class="line"><span class="number">1</span>   <span class="number">3</span> <span class="number">6</span>   <span class="number">9</span></div></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">     <span class="number">4</span></div><div class="line">   /   \</div><div class="line">  <span class="number">7</span>     <span class="number">2</span></div><div class="line"> / \   / \</div><div class="line"><span class="number">9</span>   <span class="number">6</span> <span class="number">3</span>   <span class="number">1</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-6"><a href="#思路-6" class="headerlink" title="思路"></a>思路</h5><p>题目大意：交换二叉树中所有节点的左右子树。</p><p>递归。对于一个节点，判断空，如是返回空；否则，递归invert其左子树和右子树，然后交换。</p><h5 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invertTree</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: TreeNode</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">        l = self.invertTree(root.left)</div><div class="line">        r = self.invertTree(root.right)</div><div class="line">        root.left = r</div><div class="line">        root.right = l</div><div class="line">        <span class="keyword">return</span> root</div></pre></td></tr></table></figure></div></div><h4 id="Same-Tree"><a href="#Same-Tree" class="headerlink" title="Same Tree"></a>Same Tree</h4><h5 id="题目-7"><a href="#题目-7" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/same-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given two binary trees, write a function to check if they are the same or not.</p><p>Two binary trees are considered the same if they are structurally identical and the nodes have the same value.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input:     <span class="number">1</span>         <span class="number">1</span></div><div class="line">          / \       / \</div><div class="line">         <span class="number">2</span>   <span class="number">3</span>     <span class="number">2</span>   <span class="number">3</span></div><div class="line"></div><div class="line">        [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],   [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line"></div><div class="line">Output: <span class="keyword">true</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input:     <span class="number">1</span>         <span class="number">1</span></div><div class="line">          /           \</div><div class="line">         <span class="number">2</span>             <span class="number">2</span></div><div class="line"></div><div class="line">        [<span class="number">1</span>,<span class="number">2</span>],     [<span class="number">1</span>,<span class="keyword">null</span>,<span class="number">2</span>]</div><div class="line"></div><div class="line">Output: <span class="keyword">false</span></div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input:     <span class="number">1</span>         <span class="number">1</span></div><div class="line">          / \       / \</div><div class="line">         <span class="number">2</span>   <span class="number">1</span>     <span class="number">1</span>   <span class="number">2</span></div><div class="line"></div><div class="line">        [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],   [<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line"></div><div class="line">Output: <span class="keyword">false</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-7"><a href="#思路-7" class="headerlink" title="思路"></a>思路</h5><p>题目大意是要判断是否两棵树相同，相同的要求是具有相同的结构且节点值一致。比较简单，使用递归的方法，两棵树的相对节点，判断其值是否相同，如果相同，则递归判断其左子树和右子树。</p><h5 id="代码-7"><a href="#代码-7" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSameTree</span><span class="params">(self, p, q)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type p: TreeNode</span></div><div class="line"><span class="string">        :type q: TreeNode</span></div><div class="line"><span class="string">        :rtype: bool</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">or</span> <span class="keyword">not</span> q:</div><div class="line">            <span class="keyword">return</span> p==q</div><div class="line">        <span class="keyword">elif</span> p.val == q.val:</div><div class="line">            <span class="keyword">return</span> self.isSameTree(p.left,q.left) <span class="keyword">and</span> self.isSameTree(p.right,q.right)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure></div></div><h4 id="Binary-Tree-Level-Order-Traversal-II"><a href="#Binary-Tree-Level-Order-Traversal-II" class="headerlink" title="Binary Tree Level Order Traversal II"></a>Binary Tree Level Order Traversal II</h4><h5 id="题目-8"><a href="#题目-8" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/binary-tree-level-order-traversal-ii/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, return the <em>bottom-up level order</em> traversal of its nodes’ values. (ie, from left to right, level by level from leaf to root).</p><p>For example:<br>Given binary tree <code>[3,9,20,null,null,15,7]</code>,</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  <span class="number">3</span></div><div class="line"> / \</div><div class="line"><span class="number">9</span>  <span class="number">20</span></div><div class="line">  /  \</div><div class="line"> <span class="number">15</span>   <span class="number">7</span></div></pre></td></tr></table></figure><p>return its bottom-up level order traversal as:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[</div><div class="line">  [<span class="number">15</span>,<span class="number">7</span>],</div><div class="line">  [<span class="number">9</span>,<span class="number">20</span>],</div><div class="line">  [<span class="number">3</span>]</div><div class="line">]</div></pre></td></tr></table></figure></div></div><h5 id="思路-8"><a href="#思路-8" class="headerlink" title="思路"></a>思路</h5><p>这道题就是对树进行广度遍历，借助队列。需要注意的是：在判断队列是否为空，需要检查其队列大小。</p><h5 id="代码-8"><a href="#代码-8" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> queue</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">levelOrderBottom</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</div><div class="line">            <span class="keyword">return</span> []</div><div class="line">        nodes = []</div><div class="line">        qu = queue.Queue()</div><div class="line">        qu.put(root)</div><div class="line">        <span class="keyword">while</span> qu.qsize()&gt;<span class="number">0</span>: </div><div class="line">            l = qu.qsize()</div><div class="line">            node = []</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">                front = qu.get()</div><div class="line">                node.append(front.val)</div><div class="line">                <span class="keyword">if</span> front.left:</div><div class="line">                    qu.put(front.left)</div><div class="line">                <span class="keyword">if</span> front.right:</div><div class="line">                    qu.put(front.right)</div><div class="line">            nodes.insert(<span class="number">0</span>,node)</div><div class="line">        <span class="keyword">return</span> nodes</div></pre></td></tr></table></figure></div></div><h4 id="Path-Sum"><a href="#Path-Sum" class="headerlink" title="Path Sum"></a>Path Sum</h4><h5 id="题目-9"><a href="#题目-9" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/path-sum/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree and a sum, determine if the tree has a root-to-leaf path such that adding up all the values along the path equals the given sum.</p><p><strong>Note:</strong> A leaf is a node with no children.</p><p><strong>Example:</strong></p><p>Given the below binary tree and <code>sum = 22</code>,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">      <span class="number">5</span></div><div class="line">     / \</div><div class="line">    <span class="number">4</span>   <span class="number">8</span></div><div class="line">   /   / \</div><div class="line">  <span class="number">11</span>  <span class="number">13</span>  <span class="number">4</span></div><div class="line"> /  \      \</div><div class="line"><span class="number">7</span>    <span class="number">2</span>      <span class="number">1</span></div></pre></td></tr></table></figure><p>return true, as there exist a root-to-leaf path <code>5-&gt;4-&gt;11-&gt;2</code> which sum is 22.</p></div></div><h5 id="思路-9"><a href="#思路-9" class="headerlink" title="思路"></a>思路</h5><p>题目大意：求是否存在一条根到叶子的路径，使路径和等于给定的目标值。</p><p>递归求解，每次判断是否该节点的值等于目标值，且该点是否叶子，如果是，则找到了；否则，递归判断左子树和右子树，但是，此时目标值应该更新为（目标值-节点值）。</p><h5 id="代码-9"><a href="#代码-9" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasPathSum</span><span class="params">(self, root, sum)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root : <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">if</span> root.val == sum <span class="keyword">and</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right: <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        l,r = <span class="keyword">False</span>, <span class="keyword">False</span></div><div class="line">        <span class="keyword">if</span> root.left:</div><div class="line">            l = self.hasPathSum(root.left,sum-root.val)</div><div class="line">        <span class="keyword">if</span> root.right:</div><div class="line">            r = self.hasPathSum(root.right,sum-root.val)</div><div class="line">        <span class="keyword">return</span> l <span class="keyword">or</span> r</div></pre></td></tr></table></figure></div></div><h2 id="中等"><a href="#中等" class="headerlink" title="中等"></a>中等</h2><h4 id="Binary-Tree-Tilt"><a href="#Binary-Tree-Tilt" class="headerlink" title="Binary Tree Tilt"></a>Binary Tree Tilt</h4><h5 id="题目-10"><a href="#题目-10" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/binary-tree-tilt/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, return the tilt of the <strong>whole tree</strong>.</p><p>The tilt of a <strong>tree node</strong> is defined as the <strong>absolute difference</strong> between the sum of all left subtree node values and the sum of all right subtree node values. Null node has tilt 0.</p><p>The tilt of the <strong>whole tree</strong> is defined as the sum of all nodes’ tilt.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">         <span class="number">1</span></div><div class="line">       /   \</div><div class="line">      <span class="number">2</span>     <span class="number">3</span></div><div class="line">Output: <span class="number">1</span></div><div class="line">Explanation: </div><div class="line">Tilt of node <span class="number">2</span> : <span class="number">0</span></div><div class="line">Tilt of node <span class="number">3</span> : <span class="number">0</span></div><div class="line">Tilt of node <span class="number">1</span> : |<span class="number">2</span><span class="number">-3</span>| = <span class="number">1</span></div><div class="line">Tilt of binary tree : <span class="number">0</span> + <span class="number">0</span> + <span class="number">1</span> = <span class="number">1</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-10"><a href="#思路-10" class="headerlink" title="思路"></a>思路</h5><p>题目大意：求解树所有节点倾斜度的和。一个节点倾斜度定义为其左子树节点和与右子树节点和的绝对值差。</p><p>利用后续遍历，那么就会从叶结点开始处理，这样我们就能很方便的计算结点的累加和，同时也可以很容易的根据子树和来计算tilt。</p><h5 id="代码-10"><a href="#代码-10" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.helper(root.left)</div><div class="line">        r = self.helper(root.right)</div><div class="line">        self.tilt += abs(l-r)</div><div class="line">        <span class="keyword">return</span> l+r+root.val</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findTilt</span><span class="params">(self, root)</span>:</span></div><div class="line">        self.tilt = <span class="number">0</span></div><div class="line">        self.helper(root)</div><div class="line">        <span class="keyword">return</span> self.tilt</div></pre></td></tr></table></figure></div></div><h5 id="Subtree-of-Another-Tree"><a href="#Subtree-of-Another-Tree" class="headerlink" title="Subtree of Another Tree"></a>Subtree of Another Tree</h5><h5 id="题目-11"><a href="#题目-11" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/subtree-of-another-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given two non-empty binary trees <strong>s</strong> and <strong>t</strong>, check whether tree <strong>t</strong> has exactly the same structure and node values with a subtree of <strong>s</strong>. A subtree of <strong>s</strong> is a tree consists of a node in <strong>s</strong> and all of this node’s descendants. The tree <strong>s</strong> could also be considered as a subtree of itself.</p><p><strong>Example 1:</strong><br>Given tree s:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    <span class="number">3</span></div><div class="line">   / \</div><div class="line">  <span class="number">4</span>   <span class="number">5</span></div><div class="line"> / \</div><div class="line"><span class="number">1</span>   <span class="number">2</span></div></pre></td></tr></table></figure><p>Given tree t:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">  <span class="number">4</span> </div><div class="line"> / \</div><div class="line"><span class="number">1</span>   <span class="number">2</span></div></pre></td></tr></table></figure><p>Return </p><p>true</p><p>, because t has the same structure and node values with a subtree of s.</p><p><strong>Example 2:</strong><br>Given tree s:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">    <span class="number">3</span></div><div class="line">   / \</div><div class="line">  <span class="number">4</span>   <span class="number">5</span></div><div class="line"> / \</div><div class="line"><span class="number">1</span>   <span class="number">2</span></div><div class="line">   /</div><div class="line">  <span class="number">0</span></div></pre></td></tr></table></figure><p>Given tree t:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">  <span class="number">4</span></div><div class="line"> / \</div><div class="line"><span class="number">1</span>   <span class="number">2</span></div></pre></td></tr></table></figure><p>Return false</p></div></div><h5 id="思路-11"><a href="#思路-11" class="headerlink" title="思路"></a>思路</h5><p>题目大意：判断一棵树是否为另一棵树的子树。子树要求从某一个节点开始之叶子，完全相同。</p><p>递归。以根节点为例，如果两棵树的根节点都是空，则算子树；如果有一个节点是空，则不能是。这样保证了都不空情况下，判断节点值是否相等，相等则递归左右子树。</p><h5 id="代码-11"><a href="#代码-11" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,s,t)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> t <span class="keyword">and</span> <span class="keyword">not</span> s:<span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">or</span> <span class="keyword">not</span> t:<span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">if</span> s.val != t.val:<span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">return</span> self.helper(s.left,t.left) <span class="keyword">and</span> self.helper(s.right,t.right)</div><div class="line">        <span class="keyword">pass</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSubtree</span><span class="params">(self, s, t)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s:<span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">if</span> self.helper(s,t):<span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        lflag = self.isSubtree(s.left,t)</div><div class="line">        rflag = self.isSubtree(s.right,t)</div><div class="line">        <span class="keyword">return</span> lflag <span class="keyword">or</span> rflag</div></pre></td></tr></table></figure></div></div><h4 id="Construct-String-from-Binary-Tree"><a href="#Construct-String-from-Binary-Tree" class="headerlink" title="Construct String from Binary Tree"></a>Construct String from Binary Tree</h4><h5 id="题目-12"><a href="#题目-12" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/construct-string-from-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>You need to construct a string consists of parenthesis and integers from a binary tree with the preorder traversing way.</p><p>The null node needs to be represented by empty parenthesis pair “()”. And you need to omit all the empty parenthesis pairs that don’t affect the one-to-one mapping relationship between the string and the original binary tree.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Input: Binary tree: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line">       <span class="number">1</span></div><div class="line">     /   \</div><div class="line">    <span class="number">2</span>     <span class="number">3</span></div><div class="line">   /    </div><div class="line">  <span class="number">4</span>     </div><div class="line"></div><div class="line">Output: <span class="string">"1(2(4))(3)"</span></div><div class="line"></div><div class="line">Explanation: Originallay it needs to be <span class="string">"1(2(4)())(3()())"</span>, </div><div class="line">but you need to omit all the unnecessary empty parenthesis pairs. </div><div class="line">And it will be <span class="string">"1(2(4))(3)"</span>.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Input: Binary tree: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,null,<span class="number">4</span>]</div><div class="line">       <span class="number">1</span></div><div class="line">     /   \</div><div class="line">    <span class="number">2</span>     <span class="number">3</span></div><div class="line">     \  </div><div class="line">      <span class="number">4</span> </div><div class="line"></div><div class="line">Output: <span class="string">"1(2()(4))(3)"</span></div><div class="line"></div><div class="line">Explanation: Almost the same <span class="keyword">as</span> the first example, </div><div class="line"><span class="keyword">except</span> we can<span class="string">'t omit the first parenthesis pair to break the one-to-one mapping relationship between the input and the output.</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-12"><a href="#思路-12" class="headerlink" title="思路"></a>思路</h5><p>题目大意：按照树的结构将节点值写出一个序列，根节点在前面，使用括弧将左子树包起来，使用另一个括弧将右子树包起来，不允许空的括弧。但是，根据上面的两个例子，在某些情况下又必须保留空括弧，如果将例子2中的括弧删去，则例子1和例子2有相同的序列，但是他们是不同的树。那么，什么情况下需要保留空括弧呢？即左子树空但是右子树不空。递归解决，考虑根节点，先递归求解出左子树序列，右子树序列；然后整合序列：右子树不空（不论左子树是否空），<code>root(left)(right)</code>；否则左子树不空（右子树空），<code>root(left)</code>；左右子树都空时，<code>root</code></p><h5 id="代码-12"><a href="#代码-12" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tree2str</span><span class="params">(self, t)</span>:</span></div><div class="line">        res = <span class="string">''</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> t:<span class="keyword">return</span> res</div><div class="line">        lres = self.tree2str(t.left)</div><div class="line">        rres = self.tree2str(t.right)</div><div class="line">        <span class="keyword">if</span> rres:</div><div class="line">            <span class="keyword">return</span> str(t.val)+<span class="string">'(%s)'</span>%lres+<span class="string">'(%s)'</span>%rres</div><div class="line">        <span class="keyword">elif</span> lres:</div><div class="line">            <span class="keyword">return</span> str(t.val)+<span class="string">'(%s)'</span>%lres</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> str(t.val)</div></pre></td></tr></table></figure></div></div><h4 id="Two-Sum-IV-Input-is-a-BST"><a href="#Two-Sum-IV-Input-is-a-BST" class="headerlink" title="Two Sum IV - Input is a BST"></a>Two Sum IV - Input is a BST</h4><h5 id="题目-13"><a href="#题目-13" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/two-sum-iv-input-is-a-bst/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a Binary Search Tree and a target number, return true if there exist two elements in the BST such that their sum is equal to the given target.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">    <span class="number">5</span></div><div class="line">   / \</div><div class="line">  <span class="number">3</span>   <span class="number">6</span></div><div class="line"> / \   \</div><div class="line"><span class="number">2</span>   <span class="number">4</span>   <span class="number">7</span></div><div class="line"></div><div class="line">Target = <span class="number">9</span></div><div class="line"></div><div class="line">Output: <span class="keyword">True</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">    <span class="number">5</span></div><div class="line">   / \</div><div class="line">  <span class="number">3</span>   <span class="number">6</span></div><div class="line"> / \   \</div><div class="line"><span class="number">2</span>   <span class="number">4</span>   <span class="number">7</span></div><div class="line"></div><div class="line">Target = <span class="number">28</span></div><div class="line"></div><div class="line">Output: <span class="keyword">False</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-13"><a href="#思路-13" class="headerlink" title="思路"></a><a href="https://blog.csdn.net/huanghanqian/article/details/77836875" target="_blank" rel="noopener">思路</a></h5><p>题目大意：在一棵二分搜索树上查找是否存在两个数，使这两个数的和恰为给定的以目标值。</p><p>二分搜索树，中序遍历之后的节点值是一个递增序列，这样我们就可以使用<code>2-sum</code>方法求解。使用两个指针，分别指向序列的第一个元素和最后一个元素，比较两个指针指向值的和与目标值的大小关系。相等，结束；目标值比较大，两个元素的和比较大，则右指针左移，欲尝试更小的和；如果两个元素的和比较小，则左指针右移。</p><h5 id="代码-13"><a href="#代码-13" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorder</span><span class="params">(self,root,node_list)</span>:</span></div><div class="line">        <span class="keyword">if</span> root.left:</div><div class="line">            self.inorder(root.left,node_list)</div><div class="line">        node_list.append(root.val)</div><div class="line">        <span class="keyword">if</span> root.right:</div><div class="line">            self.inorder(root.right,node_list)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findTarget</span><span class="params">(self, root, k)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        node_list = []</div><div class="line">        self.inorder(root,node_list)</div><div class="line">        start = <span class="number">0</span></div><div class="line">        end = len(node_list)<span class="number">-1</span></div><div class="line">        <span class="keyword">while</span> start&lt;end:</div><div class="line">            <span class="keyword">if</span> node_list[start]+node_list[end]==k:</div><div class="line">                <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">            <span class="keyword">elif</span> node_list[start]+node_list[end]&lt;k:</div><div class="line">                start += <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                end -= <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure></div></div><h4 id="Longest-Univalue-Path"><a href="#Longest-Univalue-Path" class="headerlink" title="Longest Univalue Path"></a>Longest Univalue Path</h4><h5 id="题目-14"><a href="#题目-14" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/longest-univalue-path/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, find the length of the longest path where each node in the path has the same value. This path may or may not pass through the root.</p><p><strong>Note:</strong> The length of path between two nodes is represented by the number of edges between them.</p><p><strong>Example 1:</strong></p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    <span class="number">5</span></div><div class="line">   / \</div><div class="line">  <span class="number">4</span>   <span class="number">5</span></div><div class="line"> / \   \</div><div class="line"><span class="number">1</span>   <span class="number">1</span>   <span class="number">5</span></div></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    <span class="number">1</span></div><div class="line">   / \</div><div class="line">  <span class="number">4</span>   <span class="number">5</span></div><div class="line"> / \   \</div><div class="line"><span class="number">4</span>   <span class="number">4</span>   <span class="number">5</span></div></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2</div></pre></td></tr></table></figure></div></div><h5 id="思路-14"><a href="#思路-14" class="headerlink" title="思路"></a>思路</h5><p>题目大意：找出树上最长路径，使该路径上每个节点值都相同。并不要求该路径一定要经过根，路径上边的数量表示路径长度。</p><p>这道题的思路类似<a href="https://leetcode.com/problems/path-sum-iii/description/" target="_blank" rel="noopener">Path Sum III</a>，因为允许不经过root。首先来分析节点值相同的路径的集中情况，题目给出的两个例子包含了所有的两种情况：例子1是路径由父母节点和其某个子树构成；例子2路径存在某个子树中，不经过父亲节点。</p><p>情况一：如果根节点的值与子节点值相同，那么递归求解左右子树下的路径，然后选择长度更长的路径+1。</p><p>情况二：如果根节点的值与子节点值相同，那么递归求解左右子树下的路径，最长路径由左子树-根-右子树组成。</p><h5 id="代码-14"><a href="#代码-14" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self,rootval,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root <span class="keyword">or</span> rootval!=root.val :<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.find(root.val,root.left)</div><div class="line">        r = self.find(root.val,root.right)</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>+max(l,r)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestUnivaluePath</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.longestUnivaluePath(root.left)</div><div class="line">        r = self.longestUnivaluePath(root.right)</div><div class="line">        <span class="keyword">return</span> max( max(l,r), self.find(root.val,root.left)+self.find(root.val,root.right) )</div></pre></td></tr></table></figure></div></div><h4 id="Second-Minimum-Node-In-a-Binary-Tree"><a href="#Second-Minimum-Node-In-a-Binary-Tree" class="headerlink" title="Second Minimum Node In a Binary Tree"></a>Second Minimum Node In a Binary Tree</h4><h5 id="题目-15"><a href="#题目-15" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/second-minimum-node-in-a-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a non-empty special binary tree consisting of nodes with the non-negative value, where each node in this tree has exactly <code>two</code>or <code>zero</code> sub-node. If the node has two sub-nodes, then this node’s value is the smaller value among its two sub-nodes.</p><p>Given such a binary tree, you need to output the <strong>second minimum</strong> value in the set made of all the nodes’ value in the whole tree.</p><p>If no such second minimum value exists, output -1 instead.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">    <span class="number">2</span></div><div class="line">   / \</div><div class="line">  <span class="number">2</span>   <span class="number">5</span></div><div class="line">     / \</div><div class="line">    <span class="number">5</span>   <span class="number">7</span></div><div class="line"></div><div class="line">Output: <span class="number">5</span></div><div class="line">Explanation: The smallest value is <span class="number">2</span>, the second smallest value is <span class="number">5</span>.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">    <span class="number">2</span></div><div class="line">   / \</div><div class="line">  <span class="number">2</span>   <span class="number">2</span></div><div class="line"></div><div class="line">Output: -<span class="number">1</span></div><div class="line">Explanation: The smallest value is <span class="number">2</span>, but there isn<span class="string">'t any second smallest value.</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-15"><a href="#思路-15" class="headerlink" title="思路"></a>思路</h5><p>题目大意：给定一棵树，这棵树上的节点要么没有子树，要么一定有左右子树而且左右子树的节点值一定不小于该节点值。现在求该树上的第二小的值。</p><p>根据该树的描述，根节点一定是最小的，那么就遍历树找比根节点大，但又小于剩下的节点值。那么如何找出满足条件的值呢？我们初始化<code>secondNum=max_value</code>，每次遍历一个节点，只要满足<code>root.val&lt;current_node.val&lt;secondNum</code>，我们就更新：<code>secondNum=current_node.val</code>。使用<code>DFS</code>和<code>BFS</code>两种方法遍历，发现第一种方法更快。<code>dfs:36ms</code> 而<code>bfs:44ms</code></p><h5 id="代码-15"><a href="#代码-15" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self,root,first,second)</span>:</span></div><div class="line">        <span class="keyword">if</span> first&lt;root.val&lt;second:</div><div class="line">            second = root.val</div><div class="line">        <span class="keyword">if</span> root.left:</div><div class="line">            second = self.dfs(root.left,first,second)</div><div class="line">            second = self.dfs(root.right,first,second)</div><div class="line">        <span class="keyword">return</span> second</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSecondMinimumValue</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        second = sys.maxsize</div><div class="line">        second = self.dfs(root,root.val,second)</div><div class="line">        <span class="keyword">if</span> second == sys.maxsize:</div><div class="line">            <span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> second</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> queue</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSecondMinimumValue</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        secondNum = sys.maxsize</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        qu = queue.Queue()</div><div class="line">        qu.put(root)</div><div class="line">        <span class="keyword">while</span> qu.qsize()&gt;<span class="number">0</span>:</div><div class="line">            node = qu.get()</div><div class="line">            <span class="keyword">if</span> node.val&gt;root.val <span class="keyword">and</span> node.val&lt;secondNum:</div><div class="line">                secondNum = node.val</div><div class="line">            <span class="keyword">if</span> node.left:</div><div class="line">                qu.put(node.left)</div><div class="line">                qu.put(node.right)</div><div class="line">        <span class="keyword">if</span> secondNum==sys.maxsize:</div><div class="line">            <span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> secondNum</div></pre></td></tr></table></figure></div></div><h4 id="Binary-Tree-Paths-1"><a href="#Binary-Tree-Paths-1" class="headerlink" title="Binary Tree Paths"></a>Binary Tree Paths</h4><h5 id="题目-16"><a href="#题目-16" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/binary-tree-paths/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, return all root-to-leaf paths.</p><p><strong>Note:</strong> A leaf is a node with no children.</p><p><strong>Example:</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line"></div><div class="line">   <span class="number">1</span></div><div class="line"> /   \</div><div class="line"><span class="number">2</span>     <span class="number">3</span></div><div class="line"> \</div><div class="line">  <span class="number">5</span></div><div class="line"></div><div class="line">Output: [<span class="string">"1-&gt;2-&gt;5"</span>, <span class="string">"1-&gt;3"</span>]</div><div class="line"></div><div class="line">Explanation: All root-to-leaf paths are: 1-&gt;2-&gt;5, 1-&gt;3</div></pre></td></tr></table></figure></div></div><h5 id="思路-16"><a href="#思路-16" class="headerlink" title="思路"></a>思路</h5><p>题目大意：找到一棵树的全部路径。</p><p>递归求解，如果碰到一个节点是叶子，即没有左右子树，则找到了一条路径，返回。否则，将该节点值加入路径，然后递归求解其左右子树。</p><h5 id="代码-16"><a href="#代码-16" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self,result,root, string)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</div><div class="line">            result.append(string)</div><div class="line">            <span class="keyword">return</span> result</div><div class="line">        <span class="keyword">if</span> root.left:</div><div class="line">            result = self.find(result,root.left,string+<span class="string">'-&gt;'</span>+str(root.left.val))</div><div class="line">        <span class="keyword">if</span> root.right:</div><div class="line">            result = self.find(result,root.right,string+<span class="string">'-&gt;'</span>+str(root.right.val))</div><div class="line">        <span class="keyword">return</span> result</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">binaryTreePaths</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: List[str]</span></div><div class="line"><span class="string">        """</span></div><div class="line">        result = []</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> result</div><div class="line">        <span class="keyword">return</span> self.find(result,root,str(root.val))</div></pre></td></tr></table></figure></div></div><h4 id="Lowest-Common-Ancestor-of-a-Binary-Search-Tree"><a href="#Lowest-Common-Ancestor-of-a-Binary-Search-Tree" class="headerlink" title="Lowest Common Ancestor of a Binary Search Tree"></a>Lowest Common Ancestor of a Binary Search Tree</h4><h5 id="题目-17"><a href="#题目-17" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-search-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary search tree (BST), find the lowest common ancestor (LCA) of two given nodes in the BST.</p><p>According to the <a href="https://en.wikipedia.org/wiki/Lowest_common_ancestor" target="_blank" rel="noopener">definition of LCA on Wikipedia</a>: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow <strong>a node to be a descendant of itself</strong>).”</p><p>Given binary search tree:  root = [6,2,8,0,4,7,9,null,null,3,5]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">     _______6______</div><div class="line">    /              \</div><div class="line"> ___2__          ___8__</div><div class="line">/      \        /      \</div><div class="line"><span class="number">0</span>      _4       <span class="number">7</span>       <span class="number">9</span></div><div class="line">      /  \</div><div class="line">      <span class="number">3</span>   <span class="number">5</span></div></pre></td></tr></table></figure><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: root = [<span class="number">6</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,null,null,<span class="number">3</span>,<span class="number">5</span>], p = <span class="number">2</span>, q = <span class="number">8</span></div><div class="line">Output: <span class="number">6</span></div><div class="line">Explanation: The LCA of nodes <span class="number">2</span> <span class="keyword">and</span> <span class="number">8</span> <span class="keyword">is</span> <span class="number">6.</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: root = [<span class="number">6</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>,null,null,<span class="number">3</span>,<span class="number">5</span>], p = <span class="number">2</span>, q = <span class="number">4</span></div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: The LCA of nodes <span class="number">2</span> <span class="keyword">and</span> <span class="number">4</span> <span class="keyword">is</span> <span class="number">2</span>, since a node can be a descendant of itself </div><div class="line">             according to the LCA definition.</div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ul><li>All of the nodes’ values will be unique.</li><li>p and q are different and both values will exist in the BST.</li></ul></div></div><h5 id="思路-17"><a href="#思路-17" class="headerlink" title="思路"></a>思路</h5><p>题目大意：在二分搜索树树上寻找两个节点的最近祖先，节点本身就是该节点的祖先。</p><p>做这道题一定要利用这棵树是二分搜索树的相关性质：即左子树的节点值小于根节点值，而右子树的节点值大于根节点值。那么如果两个节点分落在一个节点的左右子树，那么共同祖先只有一个，便是该点；如果两个节点在一个节点的同一个子树，那么判断是否其中一个节点是另一个节点的祖先；最后递归上述步骤。</p><h5 id="代码-17"><a href="#代码-17" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lowestCommonAncestor</span><span class="params">(self, root, p, q)</span>:</span></div><div class="line">        <span class="keyword">if</span> root.val == p.val <span class="keyword">or</span> root.val == q.val:</div><div class="line">            <span class="keyword">return</span> root</div><div class="line">        <span class="keyword">if</span> (root.val-q.val)*(root.val-p.val) &lt;<span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> root</div><div class="line">        <span class="keyword">if</span> root.val-q.val&gt;<span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.left,p,q)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self.lowestCommonAncestor(root.right,p,q)</div></pre></td></tr></table></figure></div></div><h4 id="Symmetric-Tree"><a href="#Symmetric-Tree" class="headerlink" title="Symmetric Tree"></a>Symmetric Tree</h4><h5 id="题目-18"><a href="#题目-18" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/symmetric-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, check whether it is a mirror of itself (ie, symmetric around its center).</p><p>For example, this binary tree <code>[1,2,2,3,4,4,3]</code> is symmetric:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    <span class="number">1</span></div><div class="line">   / \</div><div class="line">  <span class="number">2</span>   <span class="number">2</span></div><div class="line"> / \ / \</div><div class="line"><span class="number">3</span>  <span class="number">4</span> <span class="number">4</span>  <span class="number">3</span></div></pre></td></tr></table></figure><p>But the following <code>[1,2,2,null,3,null,3]</code> is not:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  <span class="number">1</span></div><div class="line"> / \</div><div class="line"><span class="number">2</span>   <span class="number">2</span></div><div class="line"> \   \</div><div class="line"> <span class="number">3</span>    <span class="number">3</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-18"><a href="#思路-18" class="headerlink" title="思路"></a>思路</h5><p>题目大意：判断一棵树是否对称。</p><p>判断对称，以中间为中心，判断左右是否相同，但是我们判断的是节点值是否相等。可以发现，待判断的节点都是成对的：左子树的左节点 vs 右子树的右节点；左子树的右节点 vs 右子树的左节点。递归求解。</p><h5 id="代码-18"><a href="#代码-18" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self,p,q)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">and</span> <span class="keyword">not</span> q:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">or</span> <span class="keyword">not</span> q:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">if</span> p.val==q.val:</div><div class="line">            <span class="keyword">return</span> self.find(p.left,q.right) <span class="keyword">and</span> self.find(p.right,q.left)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetric</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: bool</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="keyword">return</span> self.find(root.left,root.right)</div></pre></td></tr></table></figure></div></div><h4 id="Balanced-Binary-Tree"><a href="#Balanced-Binary-Tree" class="headerlink" title="Balanced Binary Tree"></a>Balanced Binary Tree</h4><h5 id="题目-19"><a href="#题目-19" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/balanced-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, determine if it is height-balanced.</p><p>For this problem, a height-balanced binary tree is defined as:</p><blockquote><p>a binary tree in which the depth of the two subtrees of <em>every</em> node never differ by more than 1.</p></blockquote><p><strong>Example 1:</strong></p><p>Given the following tree <code>[3,9,20,null,null,15,7]</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  <span class="number">3</span></div><div class="line"> / \</div><div class="line"><span class="number">9</span>  <span class="number">20</span></div><div class="line">  /  \</div><div class="line"> <span class="number">15</span>   <span class="number">7</span></div></pre></td></tr></table></figure><p>Return true.<br><strong>Example 2:</strong></p><p>Given the following tree <code>[1,2,2,3,3,null,null,4,4]</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">      <span class="number">1</span></div><div class="line">     / \</div><div class="line">    <span class="number">2</span>   <span class="number">2</span></div><div class="line">   / \</div><div class="line">  <span class="number">3</span>   <span class="number">3</span></div><div class="line"> / \</div><div class="line"><span class="number">4</span>   <span class="number">4</span></div></pre></td></tr></table></figure><p>Return false.</p></div></div><h5 id="思路-19"><a href="#思路-19" class="headerlink" title="思路"></a><a href="https://zxi.mytechroad.com/blog/leetcode/leetcode-110-balanced-binary-tree/" target="_blank" rel="noopener">思路</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>题目大意：判断一棵树是否为平衡树，所谓平衡树，任意顶点的左右子树的高度相差不超过1。</p><p>那么根据平衡树的定义，首先判断根的左右子树是否满足定义；入室，则递归判断root的左子树和右子树。为了计算顶点的高度，我们需要引进高度计算函数。但是该方法的复杂度：</p><p>为了计算root的高度，我们需要从root遍历到leaf顶点，$O(n)$</p><p>为了计算左子树的高度，我们需要遍历一半的顶点，则计算左子树和右子树的复杂度$2O(\frac{1}{2})$</p><p>那么对于根的左子树，我们需要计算它的左右子树，各计算$\frac{1}{4}$的节点，时间是$O(\frac{1}{4})$，故对根的左子树的左右子树计算得总时间是$2O(\frac{1}{4})$；对于根的右子树，需要计算其左右子树，也需要$2O(\frac{1}{4})$时间；则总时间啊$4O(\frac{1}{4})$。</p><p>这样，算法的总时间是$O(nlogn)$。</p><p><img src="/2018/08/23/Tree/Screen Shot 2018-08-25 at 4.47.56 PM.png" alt="Screen Shot 2018-08-25 at 4.47.56 PM"></p><p>仔细检查可以发现，我们有大量重复计算，左子树高度其实已经在计算root得高度时已经计算过了，实际上我们只需要遍历一次树就可以求出各顶点的高度。具体实现时，由于我们使用递归方法，在计算父亲节点之前，必然先求出孩子节点，所以，叶子节点的高度是最先求出来的，一旦求出后，我们马上比较它的左子树和右子树高度，如果大于1，则必然整棵树不是平衡树，返回-1。否则返回该节点的高度。所以，每次我们计算了节点高度后，先判断是否为-1，如果是，则返回，说明这棵树不是平衡树。</p></div></div><h5 id="代码-19"><a href="#代码-19" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>方法一：</p><p><img src="/2018/08/23/Tree/Screen Shot 2018-08-25 at 4.49.46 PM.png" alt="Screen Shot 2018-08-25 at 4.49.46 PM"></p><p>方法二：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find_height</span><span class="params">(self,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.find_height(root.left)</div><div class="line">        <span class="keyword">if</span> l==<span class="number">-1</span>:<span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        r = self.find_height(root.right)</div><div class="line">        <span class="keyword">if</span> r==<span class="number">-1</span>:<span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        <span class="keyword">if</span> abs(l-r)&gt;<span class="number">1</span>:<span class="keyword">return</span> <span class="number">-1</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>+max(l,r)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isBalanced</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: bool</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        height = self.find_height(root)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span> <span class="keyword">if</span> height==<span class="number">-1</span> <span class="keyword">else</span> <span class="keyword">True</span></div></pre></td></tr></table></figure></div></div><h4 id="Minimum-Depth-of-Binary-Tree"><a href="#Minimum-Depth-of-Binary-Tree" class="headerlink" title="Minimum Depth of Binary Tree"></a>Minimum Depth of Binary Tree</h4><h5 id="题目-20"><a href="#题目-20" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/minimum-depth-of-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, find its minimum depth.</p><p>The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node.</p><p><strong>Note:</strong> A leaf is a node with no children.</p><p><strong>Example:</strong></p><p>Given binary tree <code>[3,9,20,null,null,15,7]</code>,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  <span class="number">3</span></div><div class="line"> / \</div><div class="line"><span class="number">9</span>  <span class="number">20</span></div><div class="line">  /  \</div><div class="line"> <span class="number">15</span>   <span class="number">7</span></div></pre></td></tr></table></figure><p>return its minimum depth = 2.</p></div></div><h5 id="思路-20"><a href="#思路-20" class="headerlink" title="思路"></a>思路</h5><p>题目定义：求根到叶子的最短路径。</p><p>这道题完全是上一道题目Balanced Binary Tree的一部分，实质就是计算高度。求出根的左子树高度和右子树高度，选择小的那个；至于子树的高度，可以使用递归求解。但是有一点需要注意的是，求得是root到leaf的路径，如果给的树是如下，发现左子树高度是1，右子树高度是0，那么是不是应该选择右子树呢，答案为1呢？不，这样就不是一条从根到叶子的路径的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"> <span class="number">3</span></div><div class="line"> / </div><div class="line"><span class="number">9</span></div></pre></td></tr></table></figure><h5 id="代码-20"><a href="#代码-20" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDepth</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type root: TreeNode</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.minDepth(root.left)</div><div class="line">        r = self.minDepth(root.right)</div><div class="line">        <span class="keyword">if</span> min(l,r)==<span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> l+r+<span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>+min(l,r)</div></pre></td></tr></table></figure></div></div><h2 id="困难"><a href="#困难" class="headerlink" title="困难"></a>困难</h2><h4 id="Path-Sum-III"><a href="#Path-Sum-III" class="headerlink" title="Path Sum III"></a>Path Sum III</h4><h5 id="题目-21"><a href="#题目-21" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/path-sum-iii/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>ou are given a binary tree in which each node contains an integer value.</p><p>Find the number of paths that sum to a given value.</p><p>The path does not need to start or end at the root or a leaf, but it must go downwards (traveling only from parent nodes to child nodes).</p><p>The tree has no more than 1,000 nodes and the values are in the range -1,000,000 to 1,000,000.</p><p><strong>Example:</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">root = [<span class="number">10</span>,<span class="number">5</span>,<span class="number">-3</span>,<span class="number">3</span>,<span class="number">2</span>,null,<span class="number">11</span>,<span class="number">3</span>,<span class="number">-2</span>,null,<span class="number">1</span>], sum = <span class="number">8</span></div><div class="line"></div><div class="line">      <span class="number">10</span></div><div class="line">     /  \</div><div class="line">    <span class="number">5</span>   <span class="number">-3</span></div><div class="line">   / \    \</div><div class="line">  <span class="number">3</span>   <span class="number">2</span>   <span class="number">11</span></div><div class="line"> / \   \</div><div class="line"><span class="number">3</span>  <span class="number">-2</span>   <span class="number">1</span></div><div class="line"></div><div class="line">Return <span class="number">3.</span> The paths that sum to <span class="number">8</span> are:</div><div class="line"></div><div class="line">1.  5 -&gt; 3</div><div class="line">2.  5 -&gt; 2 -&gt; 1</div><div class="line">3. -3 -&gt; 11</div></pre></td></tr></table></figure></div></div><h5 id="思路-21"><a href="#思路-21" class="headerlink" title="思路"></a>思路</h5><p>题目大意：在树上找到所有的路径，使每一条路径的和等于目标值，路径不要求从根开始，以叶子节点结束。</p><p>首先，我们使用一个额外函数来找到一条符合要求的路径：如果当前节点值等于目标值，则count+1；递归求解该节点的左右子树；</p><p>难点是如何求不以root开始的符合要求的路径？就是把每一个节点都当作root处理。</p><h5 id="代码-21"><a href="#代码-21" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(self,root,sum)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        count = <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> root.val == sum:</div><div class="line">            count += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> count+self.find(root.left,sum-root.val)+self.find(root.right,sum-root.val)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pathSum</span><span class="params">(self, root, sum)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> self.find(root,sum) + self.pathSum(root.right,sum) + self.pathSum(root.left,sum)</div></pre></td></tr></table></figure></div></div><h4 id="Trim-a-Binary-Search-Tree"><a href="#Trim-a-Binary-Search-Tree" class="headerlink" title="Trim a Binary Search Tree"></a>Trim a Binary Search Tree</h4><h5 id="题目-22"><a href="#题目-22" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/trim-a-binary-search-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary search tree and the lowest and highest boundaries as <code>L</code> and <code>R</code>, trim the tree so that all its elements lies in <code>[L, R]</code> (R &gt;= L). You might need to change the root of the tree, so the result should return the new root of the trimmed binary search tree.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">    <span class="number">1</span></div><div class="line">   / \</div><div class="line">  <span class="number">0</span>   <span class="number">2</span></div><div class="line"></div><div class="line">  L = <span class="number">1</span></div><div class="line">  R = <span class="number">2</span></div><div class="line"></div><div class="line">Output: </div><div class="line">    <span class="number">1</span></div><div class="line">      \</div><div class="line">       <span class="number">2</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">Input: </div><div class="line">    <span class="number">3</span></div><div class="line">   / \</div><div class="line">  <span class="number">0</span>   <span class="number">4</span></div><div class="line">   \</div><div class="line">    <span class="number">2</span></div><div class="line">   /</div><div class="line">  <span class="number">1</span></div><div class="line"></div><div class="line">  L = <span class="number">1</span></div><div class="line">  R = <span class="number">3</span></div><div class="line"></div><div class="line">Output: </div><div class="line">      <span class="number">3</span></div><div class="line">     / </div><div class="line">   <span class="number">2</span>   </div><div class="line">  /</div><div class="line"> <span class="number">1</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-22"><a href="#思路-22" class="headerlink" title="思路"></a><a href="https://zxi.mytechroad.com/blog/leetcode/leetcode-669-trim-a-binary-search-tree/" target="_blank" rel="noopener">思路</a></h5><p>题目大意：给出一个取值区间范围，削减二分搜索树使该树上的节点值均介于该取值范围内。</p><p>首先一定要利用该树是二分搜索树的一些性质，左子树的节点值小于右子树的节点值。递归求解，对于每一个节点，如果该节点值小于<code>L</code>，那么该节点及其左子树便被削减，递归求解其右子树；如果该节点值大于<code>R</code>，则该节点及其右子树被削减，递归求解其左子树；如果该节点值在范围内，则递归求解左子树和右子树。</p><p><img src="/2018/08/23/Tree/Screen Shot 2018-09-17 at 12.28.26 PM.png" alt="Screen Shot 2018-09-17 at 12.28.26 PM"></p><h5 id="代码-22"><a href="#代码-22" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trimBST</span><span class="params">(self, root, L, R)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="keyword">None</span></div><div class="line">        <span class="keyword">if</span> root.val&lt;L:</div><div class="line">            <span class="keyword">return</span> self.trimBST(root.right,L,R)</div><div class="line">        <span class="keyword">if</span> root.val&gt;R:</div><div class="line">            <span class="keyword">return</span> self.trimBST(root.left,L,R)</div><div class="line">        root.left = self.trimBST(root.left,L,R)</div><div class="line">        root.right = self.trimBST(root.right,L,R)</div><div class="line">        <span class="keyword">return</span> root</div></pre></td></tr></table></figure></div></div><h4 id="Convert-BST-to-Greater-Tree"><a href="#Convert-BST-to-Greater-Tree" class="headerlink" title="Convert BST to Greater Tree"></a>Convert BST to Greater Tree</h4><h5 id="题目-23"><a href="#题目-23" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/convert-bst-to-greater-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a Binary Search Tree (BST), convert it to a Greater Tree such that every key of the original BST is changed to the original key plus sum of all keys greater than the original key in BST.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Input: The root of a Binary Search Tree like this:</div><div class="line">              <span class="number">5</span></div><div class="line">            /   \</div><div class="line">           <span class="number">2</span>     <span class="number">13</span></div><div class="line"></div><div class="line">Output: The root of a Greater Tree like this:</div><div class="line">             <span class="number">18</span></div><div class="line">            /   \</div><div class="line">          <span class="number">20</span>     <span class="number">13</span></div></pre></td></tr></table></figure></div></div><h5 id="思路-23"><a href="#思路-23" class="headerlink" title="思路"></a>思路</h5><p>题目大意：对于搜索二叉树上每一个节点，更新其值为该节点与比该节点值大的所有节点值和。如题目给的例子，2是最小值，所以更新其值为<code>2+5+13=20</code>；对于5，比它大的只有13，更新<code>5+13=18</code>；又因为13是最大值，不更新。</p><p>这样最先处理最右的节点，然后<strong>不断累积sum和</strong>，我们使用中序遍历思想，但是先遍历右子树，然后是父节点，最后是左子树。</p><h5 id="代码-23"><a href="#代码-23" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root,tsum)</span>:</span>   </div><div class="line">        <span class="keyword">if</span> root.right:</div><div class="line">            root.right,tsum = self.helper(root.right,tsum)</div><div class="line">        root.val += tsum</div><div class="line">        tsum = root.val</div><div class="line">        <span class="keyword">if</span> root.left:</div><div class="line">            root.left,tsum = self.helper(root.left,tsum)</div><div class="line">        <span class="keyword">return</span> root,tsum</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convertBST</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> root</div><div class="line">        root,tsum = self.helper(root,<span class="number">0</span>)</div><div class="line">        <span class="keyword">return</span> root</div></pre></td></tr></table></figure></div></div><h4 id="Diameter-of-Binary-Tree"><a href="#Diameter-of-Binary-Tree" class="headerlink" title="Diameter of Binary Tree"></a>Diameter of Binary Tree</h4><h5 id="题目-24"><a href="#题目-24" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/diameter-of-binary-tree/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a binary tree, you need to compute the length of the diameter of the tree. The diameter of a binary tree is the length of the <strong>longest</strong> path between any two nodes in a tree. This path may or may not pass through the root.</p><p><strong>Example:</strong><br>Given a binary tree </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    <span class="number">1</span></div><div class="line">   / \</div><div class="line">  <span class="number">2</span>   <span class="number">3</span></div><div class="line"> / \     </div><div class="line"><span class="number">4</span>   <span class="number">5</span></div></pre></td></tr></table></figure><p>Return <strong>3</strong>, which is the length of the path [4,2,1,3] or [5,2,1,3].</p><p><strong>Note:</strong> The length of path between two nodes is represented by the number of edges between them.</p></div></div><h5 id="思路-24"><a href="#思路-24" class="headerlink" title="思路"></a>思路</h5><p>题目大意：求二叉树的直径，直径的定义是树种最长路径。该直径可以不过root。</p><p>首先一看到不过root，就想到需要将左子树、右子树作为参数递归调用函数。那么问题就变成了如何求解一个点的最长路径？最长路径=左子树深度+右子树深度。那么问题就变成了如何求解树的深度。即如果树空，则深度为0；否则 深度=max（左子树，右子树）+1。该种解法重复遍历树多次。</p><p>思路是那样，但是我们想控制时间复杂度在<code>O(N)</code>，我们设置一个全局变量<code>diameter</code>，每次处理一个节点时，将该节点的深度与<code>diameter</code>比较，更新。</p><h5 id="代码-24"><a href="#代码-24" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bfs</span><span class="params">(self,root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = self.bfs(root.left)</div><div class="line">        r = self.bfs(root.right)</div><div class="line">        <span class="keyword">return</span> max(l,r)+<span class="number">1</span></div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameterOfBinaryTree</span><span class="params">(self, root)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        cur = self.bfs(root.left)+self.bfs(root.right)</div><div class="line">        cur_l = self.diameterOfBinaryTree(root.left)</div><div class="line">        cur_r = self.diameterOfBinaryTree(root.right)</div><div class="line">        cur_child = max(cur_l,cur_r)</div><div class="line">        <span class="keyword">return</span> max(cur,cur_child)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(self,root)</span>:</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">            l = self.dfs(root.left)</div><div class="line">            r = self.dfs(root.right)</div><div class="line">            <span class="keyword">if</span> l+r&gt;self.dia:</div><div class="line">                self.dia = l+r</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>+max(l,r)     </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">diameterOfBinaryTree</span><span class="params">(self, root)</span>:</span></div><div class="line">        self.dia = <span class="number">0</span> </div><div class="line">        self.dfs(root)</div><div class="line">        <span class="keyword">return</span> self.dia</div></pre></td></tr></table></figure></div></div><h4 id="Unique-Binary-Search-Trees-II"><a href="#Unique-Binary-Search-Trees-II" class="headerlink" title="Unique Binary Search Trees II"></a>Unique Binary Search Trees II</h4><h5 id="题目-25"><a href="#题目-25" class="headerlink" title="题目"></a><a href="https://leetcode.com/problems/unique-binary-search-trees-ii/description/" target="_blank" rel="noopener">题目</a></h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an integer <em>n</em>, generate all structurally unique <strong>BST’s</strong> (binary search trees) that store values 1 … <em>n</em>.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">Input: <span class="number">3</span></div><div class="line">Output:</div><div class="line">[</div><div class="line">  [<span class="number">1</span>,null,<span class="number">3</span>,<span class="number">2</span>],</div><div class="line">  [<span class="number">3</span>,<span class="number">2</span>,null,<span class="number">1</span>],</div><div class="line">  [<span class="number">3</span>,<span class="number">1</span>,null,null,<span class="number">2</span>],</div><div class="line">  [<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>],</div><div class="line">  [<span class="number">1</span>,null,<span class="number">2</span>,null,<span class="number">3</span>]</div><div class="line">]</div><div class="line">Explanation:</div><div class="line">The above output corresponds to the <span class="number">5</span> unique BST<span class="string">'s shown below:</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">   1         3     3      2      1</span></div><div class="line"><span class="string">    \       /     /      / \      \</span></div><div class="line"><span class="string">     3     2     1      1   3      2</span></div><div class="line"><span class="string">    /     /       \                 \</span></div><div class="line"><span class="string">   2     1         2                 3</span></div></pre></td></tr></table></figure></div></div><p><a href="https://leetcode.com/problems/all-possible-full-binary-trees/description/" target="_blank" rel="noopener">All Possible Full Binary Trees</a> </p><h5 id="思路-25"><a href="#思路-25" class="headerlink" title="思路"></a>思路</h5><h5 id="代码-25"><a href="#代码-25" class="headerlink" title="代码"></a>代码</h5><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,lis)</span>:</span></div><div class="line">        n = len(lis)</div><div class="line">        <span class="keyword">if</span> n==<span class="number">0</span>:<span class="keyword">return</span> [<span class="keyword">None</span>]</div><div class="line">        <span class="keyword">if</span> n==<span class="number">1</span>:<span class="keyword">return</span> [TreeNode(lis[<span class="number">0</span>])]</div><div class="line">        res = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> self.helper(lis[:i]):</div><div class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> self.helper(lis[i+<span class="number">1</span>:]):</div><div class="line">                    root = TreeNode(lis[i])</div><div class="line">                    root.left = j</div><div class="line">                    root.right = k</div><div class="line">                    res.append(root)</div><div class="line">        <span class="keyword">return</span> res</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generateTrees</span><span class="params">(self, n)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type n: int</span></div><div class="line"><span class="string">        :rtype: List[TreeNode]</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> n==<span class="number">0</span>:<span class="keyword">return</span> []</div><div class="line">        lis = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>)]</div><div class="line">        <span class="keyword">return</span> self.helper(lis)</div></pre></td></tr></table></figure></div></div>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Tree </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Algorithm</title>
      <link href="/2018/08/21/Algorithm/"/>
      <url>/2018/08/21/Algorithm/</url>
      <content type="html"><![CDATA[<p>Algorithm is a finite list of instrictions (to people) to accomplish something useful (that solves some problems); properties are following:</p><ul><li>clear(unambiguous), doable(terminate)</li><li>does what it is supposed to do</li><li>does not have any redundancy(several steps together caused redundacy, not caused by a single step)</li></ul><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Move-all-zeros-to-the-leftmost"><a href="#Move-all-zeros-to-the-leftmost" class="headerlink" title="Move all zeros to the leftmost"></a>Move all zeros to the leftmost</h2><p>Now we want to find an efficient algorithm to move all 0’s in a 0/1-array to the leftmost places which minimizes (say) the number of changes in the array-items.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">input : [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</div><div class="line">output : [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</div></pre></td></tr></table></figure><blockquote><p>use two pointers. One points to the rightmost zero and the other one points to the leftmost one and then swap them.</p></blockquote><h2 id="Path-count"><a href="#Path-count" class="headerlink" title="Path count"></a>Path count</h2><p>How to count #(acyclic-paths between two nodes in a complete digraph of N nodes) - contd. </p><blockquote><ul><li>(1$\to$ n with 1 step) : 1</li><li>(1$\to$ n with 2 steps) : n-2</li><li>(1$\to$ n with 3 steps) : (n-2)(n-3)</li><li>….</li><li>(1$\to$ n with k steps) : (n-2)(n-3)…(n-k) </li><li>(1$\to$ n with (n-1) step) : (n-2)(n-3)…2*1</li></ul><p>$#$$  (1,n-paths) =1+(n-2)+(n-2)(n-3)+…+(n-2)(n-3)2\cdot1 \approx e\cdot (n-2)!$</p></blockquote><h1 id="Shortest-Paths"><a href="#Shortest-Paths" class="headerlink" title="Shortest Paths"></a>Shortest Paths</h1><p>complete acyclic digraph</p><p>$#$links $=(n-1)+(n-2)+…+1= $</p><p>$#$$ (1\to n\  paths)=$</p><p>subpath optimality</p><p>if $  x_1\to x_2\to …\to x_m$ is an optimal $x_1 \to x_m$ path, then $x_i \to x_{i+1}\to …\to x_{j-1}\to x_j$ is optimal. </p><h3 id="Dijkstra算法"><a href="#Dijkstra算法" class="headerlink" title="Dijkstra算法"></a><a href="https://blog.csdn.net/qq_35644234/article/details/60870719" target="_blank" rel="noopener">Dijkstra算法</a></h3><p>求从顶点$v_1$到其他各个顶点的最短路径</p><p><img src="/2018/08/21/Algorithm/20170308144724663.png" alt="20170308144724663"></p><ol><li><p>设置dis数组</p><p>数组中的元素值表征$v_1$到其他顶点的距离</p><p>初始化顶点集为：$T=\{ v_1\}$, 路径是：$v_1$</p><p><img src="/2018/08/21/Algorithm/20170308150247263.png" alt="20170308150247263"></p><p>即$dis(v_1,v_1)=0$，$dis(v_1,v_2)=\infin$，$dis(v_1,v_3)=10$，$dis(v_1,v_4)=\infin$，$dis(v_1,v_5)=30$，$dis(v_1,v_6)=100$。</p></li><li><p>选择一个最短距离</p><p>因为$dis(v_1,v_3)=10$是最短的，所以将$v_3$加入到顶点集中：$T=\{ v_1, v_3\}$</p><p>路径是：$v_1 \to v_3$</p><p>一旦有新的点加入，则我们更新dis数组，$v_3$可以到达的点有：$v_4$。</p><p>$dis(v_1 \to v_3 \to v_4)=10+50=60&lt; \infin $，所以更新$dis[3]=60$</p><p><img src="/2018/08/21/Algorithm/20170308150707766.png" alt="20170308150707766"></p></li><li><p>继续选择一个最短距离</p><p>除去顶点集$\{v_1,v_3\}$中的点，在dis中选择一个距离最小的点：$v_5$，加入顶点集$\{v_1,v_3, v_5\}$。</p><p>路径是：$(v_1 \to t_3\to t_5)$</p><p>更新dis数组，$v_5$可以到达的点是：$v_4$和$v_6$。</p><p>$dis(v_1 \to t_3\to t_5\to t_4 )=30+20=50 &lt; 60$，更新$dis(v_4)=50$</p><p>$dis(v_1 \to t_3\to t_5\to t_6 )=30+60=90 &lt; 100$，更新$dis(v_5)=90$</p><p><img src="/2018/08/21/Algorithm/20171205193212203.png" alt="20171205193212203"></p></li><li><p>继续选择一个最短距离</p><p>将$v_4$加入顶点集$\{v_1, v_3, v_5,v_4\}$，路径是：$(v_1 \to t_3\to t_5\to t_4)$</p><p>$t_4$可以到达的点：$v_6$</p><p>$dis(v_4 \to v_6)=50+10=60 &lt; 90$，更新</p><p><img src="/2018/08/21/Algorithm/20170308151732132.png" alt="20170308151732132"></p></li><li><p>选择一个最短距离</p><p>将$v_6$加入顶点集$\{v_1, v_3, v_5,v_4, v_6\}$，路径是：$(v_1 \to t_3\to t_5\to t_4 \to v_6)$。</p><p>$v_6$不能到达任何点。</p><p><img src="/2018/08/21/Algorithm/20170308152038851.png" alt="20170308152038851"></p></li><li><p>结束</p><p>因为$dis(v_2)=\infin $，则没有任何点可以到达它，所以，算法结束。</p></li></ol><p><img src="/2018/08/21/Algorithm/Screen Shot 2018-09-04 at 3.39.07 PM.png" alt="Screen Shot 2018-09-04 at 3.39.07 PM"></p><ul><li><p>the link (x,y) is processed at most once.</p><blockquote><p>because x is closed at most once, (x,y) is processed when x is closed.</p><p>d(y)=d(x)+c(x,y) if y is not open or closed.</p><p>if d(y)&gt;[d(x)+c(c,y)], d(y)=d(x)+c(x,y)</p><p>everytime d(y) is changed, parent(y)=x</p></blockquote></li><li><p>if G has N nodes, max number of links is N(N-1)=O(N^2), so processing all links takes at most O(N^2)</p><p>the max number of acyclic paths from s to other nodes in G is $e(N-1)!$. </p><blockquote><p>When the graph is a complete digraph and because one link is processed at most once.</p></blockquote></li><li><p>Time complexity for dijkstra’s algoritm is $O(N^2)$.</p><blockquote><p>Step 1 takes $O(N)$ time for initialization.</p><p>Step 2 takes $O(1)$ time</p><p>Step 3 and Step 4 has two loops:each loop takes $O(N)$ time, so two steps take $O(N^2)$</p><p>SO the overall time complexity is $O(N)+O(1)+O(N^2)=O(N^2)$</p><p>max number of closed nodes is O(N)</p><p>computation to choose $i_{th}$ node for closing:</p><p>$OPEN_1=\{s\}$</p><p>$|OPEN|_2\le N-1$</p><p>$|OPEN|_3\le N-2$</p><p>….</p><p>$|OPEN|_{N-1}\le N-(N-2)=2$</p><p>$|OPEN|_N\le 1$</p><p>$OPEN_i=\{x_i,y_i,y_2,…,y_{N-i}\}$, because $|OPEN|_i$ is at most $N-(i-1)$, and in order to find a shortest path, we need to compare:</p><p>$d(x_i)\le d(y_1)$</p><p>$d(x_i)\le d(y_2)$</p><p>$d(x_i)\le d(y_{N-i})$</p><p>which requires at most $(N-i)$ comparisons </p><p>so $OPEN_2$ requires at  most $N-2$ comparisons in selecting $x_2$</p><p> $OPEN_3$ requires at  most $N-3$ comparisons in selecting $x_3$</p><p>…</p><p> $OPEN_{N-1}$ requires at  most $1$ comparisons in selecting $x_{N-1}$</p><p>SO THE total computation is $1+2+3+…+(N-2)=\frac{N^2-3N+2}{2}$</p><ul><li>The initialization takes $O(N)$ time, where we make $d(x)=\infin$ for each node $x$.</li><li>The sum of all links weights computation is $O(N^2)$</li><li>The path of $s\to x_i$ is computed at most $i-1$, so computation of all paths is $(1+2+..+(N-1))=O(N^2)$. </li><li>Th total number of paths from starting node $s$ is $O(e\cdot (N-2)!)=O((N-2)!)$, ============================================================</li></ul></blockquote></li><li><p>Dijkstra Algorithm is not suitable for graph with negative path cost.</p></li></ul><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><p>We use following formula to measure how close the set of points to each other:</p><script type="math/tex; mode=display">\sum_{i,j}||x_i-x_j||^2</script>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Variant GAN</title>
      <link href="/2018/08/20/Variant-GAN/"/>
      <url>/2018/08/20/Variant-GAN/</url>
      <content type="html"><![CDATA[<p>我们之前介绍了GAN模型，在本章中，将介绍一些GAN的变体模型。</p><a id="more"></a><p><a href="https://github.com/wiseodd/generative-models" target="_blank" rel="noopener">CODE in TENSORFLOW</a> <a href="https://www.sohu.com/a/143961544_741733" target="_blank" rel="noopener">summary</a> <a href="https://github.com/eriklindernoren/Keras-GAN" target="_blank" rel="noopener">CODE in KERAS</a> </p><p>原始的GAN模型存在着无约束、不可控、噪声信号z很难解释等问，近年来，在原始GAN模型的基础上衍生出了很多种模型，如：条——CGAN、卷积——DCGAN等等</p><h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><p><a href="https://zhuanlan.zhihu.com/p/27012520" target="_blank" rel="noopener">REF1</a> <a href="https://www.leiphone.com/news/201701/yZvIqK8VbxoYejLl.html?viewType=weixin" target="_blank" rel="noopener">GAN vs DCGAN</a> </p><p>DCGAN的原理和GAN是一样的，它只是把上述的G和D换成了两个卷积神经网络（CNN）。但不是直接换就可以了，DCGAN对卷积神经网络的结构做了一些改变，以提高样本的质量和收敛的速度，这些改变有：</p><ul><li>取消所有pooling层。G网络中使用转置卷积（transposed convolutional layer）进行上采样，D网络中用加入stride的卷积代替pooling。</li><li>在D和G中均使用batch normalization</li><li>去掉FC层，使网络变为全卷积网络</li><li>G网络中使用ReLU作为激活函数，最后一层使用tanh</li><li>D网络中使用LeakyReLU作为激活函数</li></ul><p>DCGAN极大的提升了GAN训练的稳定性以及生成结果质量。<a href="https://blog.csdn.net/qq_25737169/article/details/78857788" target="_blank" rel="noopener">Ref</a> </p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/p.jpg" alt="p"></p><p>DCGAN的生成器网络结构如上图所示，相较原始的GAN，DCGAN几乎完全使用了卷积层代替全链接层，判别器几乎是和生成器对称的，从上图中我们可以看到，整个网络没有pooling层和上采样层的存在，实际上是使用了带步长（fractional-strided）的卷积代替了上采样，以增加训练的稳定性。</p><h3 id="Experiment-Settings"><a href="#Experiment-Settings" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%204.33.12%20PM.png" alt="Screen Shot 2018-08-06 at 4.33.12 PM"></p><p><img src="/2018/08/20/Variant-GAN/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%204.36.15%20PM.png" alt="Screen Shot 2018-08-06 at 4.36.15 PM"></p><p><img src="/2018/08/20/Variant-GAN/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%204.36.22%20PM.png" alt="Screen Shot 2018-08-06 at 4.36.22 PM"></p><h2 id="LS-GAN"><a href="#LS-GAN" class="headerlink" title="LS-GAN"></a>LS-GAN</h2><p><a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/lsgan/lsgan.py" target="_blank" rel="noopener">code</a> <a href="http://www.myzaker.com/article/58bbd2171bc8e0892800000f/" target="_blank" rel="noopener">ref1</a> </p><p>最小二乘GAN，在gan的基础上将目标函数变成一个平方误差。实现上直接在gan的基础上修改loss函数</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-08-04%20at%204.19.29%20PM.png" alt="Screen Shot 2018-08-04 at 4.19.29 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">self.discriminator.compile(loss=<span class="string">'mse'</span>,</div><div class="line">            optimizer=optimizer,</div><div class="line">            metrics=[<span class="string">'accuracy'</span>])</div><div class="line">self.combined.compile(loss=<span class="string">'mse'</span>, optimizer=optimizer)</div></pre></td></tr></table></figure><h2 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h2><p><a href="https://blog.csdn.net/wspba/article/details/54666907" target="_blank" rel="noopener">ref1</a> </p><p>生成对抗网络：Generative Adversarial Networks，通过generator和discriminator的对抗学习，最终可以得到一个与real data分布一致的fake data，但是由于generator的输入z是一个连续的噪声信号，并且没有任何约束，我们很难通过控制z中某数的大小变化让生成的图像发生变化。</p><p>GAN中输入是随机的数据，没有太多意义，那么我们很自然的会想到能否用输入改成一个有意义的数据，最简单的就是数字字体生成，能否输入一个数字，然后输出对应的字体。这就是CGAN要做的事。</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%2010.00.41%20AM.png" alt="Screen Shot 2018-07-31 at 10.00.41 AM"></p><p>To include conditional constrain into the cycleGAN network, the adversarial<br>loss is modified to include the conditional feature vector as part of the input of the generator and<br>discriminator</p><p>在生成器模型中，条件变量y实际上是作为一个额外的输入层（additional input layer），它与生成器的噪声输入p(z)组合形成了一个联合的隐层表达；在判别器模型中，y与真实数据x也是作为输入，并输入到一个判别函数当中。实际上就是将z和x分别于y进行concat，分别作为生成器和判别器的输入，再来进行训练。</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%2010.03.51%20AM.png" alt="Screen Shot 2018-07-31 at 10.03.51 AM"></p><p>训练方式几乎就是不变的，但是从GAN的无监督变成了有监督。只是大家可以看到，这里和传统的图像分类这样的任务正好反过来了，图像分类是输入图片，然后对图像进行分类，而这里是输入分类，要反过来输出图像。</p><h3 id="Experiment-Settings-1"><a href="#Experiment-Settings-1" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%204.51.26%20PM.png" alt="Screen Shot 2018-08-06 at 4.51.26 PM"></p><p><img src="/2018/08/20/Variant-GAN/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%204.51.35%20PM.png" alt="Screen Shot 2018-08-06 at 4.51.35 PM"></p><h2 id="Info-GAN"><a href="#Info-GAN" class="headerlink" title="Info-GAN"></a>Info-GAN</h2><p><a href="http://www.360doc.com/content/17/0930/22/99071_691460743.shtml" target="_blank" rel="noopener">REF1</a> <a href="https://blog.csdn.net/wspba/article/details/54808833" target="_blank" rel="noopener">REF2</a> <a href="https://blog.csdn.net/hjimce/article/details/55657325" target="_blank" rel="noopener">GOTTA</a> <a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/infogan/infogan.py" target="_blank" rel="noopener">CODE in KERAS</a> <a href="https://www.inference.vc/infogan-variational-bound-on-mutual-information-twice/" target="_blank" rel="noopener">to-do</a> </p><p>有了CGAN，我们可以有一个单一输入y，然后通过调整z输出不同的图像。但是CGAN是有监督的，我们需要指定y。那么有没有可能实现无监督的CGAN？这个想法本身就比较疯狂，要实现无监督的CGAN，意味着需要让神经网络不但通过学习提取了特征，还需要把特征表达出来。对于MNIST，如何通过无监督学习让神经网络知道你输入y=2时就输出2的字体？或者用一个连续的值来调整字的粗细，方向？</p><p>怎么做呢？作者引入了信息论的知识，也就是mutual information互信息。作者的思路就是G网络的输入除了z之外同样类似CGAN输入一个c变量，这个变量一开始神经网络并不知道是什么含义，但是没关系，我们希望c与G网络输出的x之间的互信息最大化，也就是让神经网络自己去训练c与输出之间的关系。</p><p>为了引入c，作者利用互信息来对c进行约束，这是因为如果c对于生成数据G(z,c)具有可解释性，那么c和G(z,c)应该具有高度相关性，即互信息大，而如果是无约束的话，那么它们之间没有特定的关系，即互信息接近于0。因此我们希望c与G(z,c)的互信息I(c;G(z,c))越大越好，mutual information在文章中定义如下：</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%204.29.24%20PM.png" alt="Screen Shot 2018-07-31 at 4.29.24 PM"></p><p>基于I，整个GAN的训练目标变成：</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%204.29.48%20PM.png" alt="Screen Shot 2018-07-31 at 4.29.48 PM"></p><blockquote><ol><li>超参数$\lambda$设置为1</li><li>相当于在原始gan模型上加了一个互信息的正则化项</li><li>对于c，如果是categorical latent code，可以使用softmax的非线性输出来代表Q(c|x)；如果是continuous latent code，可以使用高斯分布来表示。</li></ol></blockquote><p>在最后实现的时候，infoGAN可以看成三个网络：(1)生成网络$x=G(c,z)$；(2)判别真伪网络$y_1=D_1(x)$；(3)判别类别$c$网络$y_2=D_2(x)$(当$c$代表类别信息的时候，网络最后一层是softmax层，且$D_1,D_2$共享网络参数，除了网络的最后一层外)；</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%203.34.27%20PM.png" alt="Screen Shot 2018-07-31 at 3.34.27 PM"></p><p>相比CGAN，InfoGAN在网络上做了一定改变：</p><p>（1）D网络的输入只有x，不加c。</p><p>（2）Q网络和D网络共享同一个网络，只是到最后一层独立输出。</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%204.44.58%20PM.png" alt="Screen Shot 2018-07-31 at 4.44.58 PM"></p><h3 id="Experiment-Setup"><a href="#Experiment-Setup" class="headerlink" title="Experiment Setup"></a>Experiment Setup</h3><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%205.11.30%20PM.png" alt="Screen Shot 2018-08-06 at 5.11.30 PM"></p><p><img src="/2018/08/20/Variant-GAN/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%205.11.57%20PM.png" alt="Screen Shot 2018-08-06 at 5.11.57 PM"></p><h2 id="AC-GAN"><a href="#AC-GAN" class="headerlink" title="AC-GAN"></a>AC-GAN</h2><p><a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/acgan/acgan.py" target="_blank" rel="noopener">ac-gan in keras</a> </p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-08-02%20at%207.59.25%20PM.png" alt="Screen Shot 2018-08-02 at 7.59.25 PM"></p><p><strong>Generator Input  [ noise , label ]</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">noise = Input(shape=(self.latent_dim,))</div><div class="line">        label = Input(shape=(<span class="number">1</span>,), dtype=<span class="string">'int32'</span>)</div><div class="line">        label_embedding = Flatten()(Embedding(self.num_classes, <span class="number">100</span>)(label))</div><div class="line">        model_input = multiply([noise, label_embedding])</div><div class="line">        img = model(model_input)</div></pre></td></tr></table></figure><p><strong>Discriminator Output [ real/fake , prediction label ]</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">validity = Dense(<span class="number">1</span>, activation=<span class="string">"sigmoid"</span>)(features)</div><div class="line">label = Dense(self.num_classes+<span class="number">1</span>, activation=<span class="string">"softmax"</span>)(features)</div></pre></td></tr></table></figure><h3 id="Experiment-setup"><a href="#Experiment-setup" class="headerlink" title="Experiment setup"></a>Experiment setup</h3><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-08-06%20at%209.46.02%20PM.png" alt="Screen Shot 2018-08-06 at 9.46.02 PM"></p><h2 id="Context-GAN"><a href="#Context-GAN" class="headerlink" title="Context-GAN"></a>Context-GAN</h2><p><a href="https://arxiv.org/pdf/1604.07379.pdf" target="_blank" rel="noopener">PAPER</a> <a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/context_encoder/context_encoder.py" target="_blank" rel="noopener">CODE</a> </p><h2 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h2><p><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">REF1</a> <a href="http://www.sohu.com/a/148114422_500659" target="_blank" rel="noopener">ref2</a> <a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan/wgan.py" target="_blank" rel="noopener">code</a> </p><p>限制discriminator的权重范围$[-0.01,0.01]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Clip critic weights</span></div><div class="line"><span class="keyword">for</span> l <span class="keyword">in</span> self.critic.layers:</div><div class="line">    weights = l.get_weights()</div><div class="line">    weights = [np.clip(w, -self.clip_value,self.clip_value) <span class="keyword">for</span> w <span class="keyword">in</span> weights]</div><div class="line">    l.set_weights(weights)</div></pre></td></tr></table></figure><p>损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wasserstein_loss</span><span class="params">(self, y_true, y_pred)</span>:</span></div><div class="line">    <span class="keyword">return</span> K.mean(y_true * y_pred)</div></pre></td></tr></table></figure><p>ground-truth</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adversarial ground truths</span></div><div class="line">valid = -np.ones((batch_size, <span class="number">1</span>))</div><div class="line">fake = np.ones((batch_size, <span class="number">1</span>))</div></pre></td></tr></table></figure><h2 id="GAN-IN-KERAS"><a href="#GAN-IN-KERAS" class="headerlink" title="GAN_IN_KERAS"></a>GAN_IN_KERAS</h2><p>构造网络</p><p>虽然训练的时候是基于batch大小，但是在设计网络的时候，考虑的是只训练一个样本的网络，Keras有两种形式设置一个神经网络，Model和Sequential。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">input_shape = (img_rows, img_cols, channel)</div><div class="line">x_input = Input(input_shape) <span class="comment">#如果有多个输入，则写多个Input函数</span></div><div class="line">h = Conv2D(<span class="number">256</span>, (k, k), strides=(<span class="number">2</span>, <span class="number">2</span>), border_mode=<span class="string">'same'</span>)(x)</div><div class="line">h = BatchNormalization(momentum=<span class="number">0.8</span>)(h)</div><div class="line">h = LeakyReLU(<span class="number">0.2</span>)(h)</div><div class="line">....</div><div class="line">y_output = Dense(<span class="number">10</span>)(h)</div><div class="line">model = Model([x_input], [y_output]) <span class="comment">#指明网络的输入，输出，需要与train_on_batch相对应</span></div></pre></td></tr></table></figure><p>损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">loss = </div><div class="line">model.add_loss(loss)</div><div class="line">discriminator.compile(optimizer=opt, loss = <span class="keyword">None</span>)</div><div class="line">discriminator.summary()</div></pre></td></tr></table></figure><p>训练</p><p>一般我们先训练discriminator，它的输入正常情况下是真假图片，真图片来自于训练集，假图片来自于generator输出，我们利用generator模型的prediction函数生成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">imgs_batch = x_train[idx,:,:,:]</div><div class="line">generated_imgs = generator.predict(imgs_batch)</div></pre></td></tr></table></figure><p>这样我们就有了真假图片，就可以训练了，函数是model_name.train_on_batch(x,y)，其中$x$就是神经网络的训练集输入，$y$是训练集的ground-truth，看discriminator的输出。</p><p>训练discriminator时，它的输入就是一张图片，输出就是真假概率，所以</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">discriminator.train_on_batch( x = imgs_batch, y = np.ones(shape=(batch_size,<span class="number">1</span>)) ) </div><div class="line"></div><div class="line">discriminator.train_on_batch( x = generated_imgs, y = np.zeros(shape=(batch_size,<span class="number">1</span>)) )</div></pre></td></tr></table></figure><p>训练generate模型，则输入是noise，x = np.random.uniform(0,1,size=(batch_size,100))，它的ground_truth是y=np.ones(shape=(batch_size,1))。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">generator.train_on_batch(x = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,size=(batch_size,<span class="number">100</span>)),</div><div class="line">                         y=np.ones(shape=(batch_size,<span class="number">1</span>)) )</div></pre></td></tr></table></figure><blockquote><p>如果是cgan或者info-gan，那么会有额外的输入和输出，如下图：</p><p><img src="/2018/08/20/Variant-GAN/Generative-Adversarial-Networks/Screen%20Shot%202018-07-31%20at%204.44.58%20PM.png" alt="Screen Shot 2018-07-31 at 4.44.58 PM"></p><p>这是info-gan，可以看到，在gan基础上有了一个额外输入$c(latent)$，有了一个额外的输出$c$。所以generator的input是x = np.concatenate( z , c )，输出是y = ( f_r_label , c ) ，每一个输出都需要一个ground-truth，所以generator的训练代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; generator.train_on_batch(x = np.concatenate( z , c ),y = [np.ones(batch_size,<span class="number">1</span>),c])</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><h2 id="DRAW-A-Recurrent-Neural-Network-For-Image-Generation"><a href="#DRAW-A-Recurrent-Neural-Network-For-Image-Generation" class="headerlink" title="DRAW: A Recurrent Neural Network For Image Generation"></a>DRAW: A Recurrent Neural Network For Image Generation</h2><p> <a href="https://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/ericjang/draw" target="_blank" rel="noopener">tensorflow</a>  <a href="">2015 - 800citation</a> <a href="https://blog.evjang.com/2016/06/understanding-and-implementing.html" target="_blank" rel="noopener">ref</a> </p><p>the DRAW network is a generative model of<br>images in the variational auto-encoder framework that decomposes image formation into multiple<br>stages of additions to a canvas matrix. The DRAW paper assumes an LSTM based generative model<br>of these sequential drawing actions which is more general than our model. In practice, these drawing<br>actions seem to progressively refine an initially blurry region of an image to be sharper. </p><h2 id="Pix2pix"><a href="#Pix2pix" class="headerlink" title="Pix2pix"></a>Pix2pix</h2><p><a href="https://arxiv.org/pdf/1611.07004.pdf" target="_blank" rel="noopener">paper</a>  <a href="http://www.sohu.com/a/134110704_741733" target="_blank" rel="noopener">ref1</a> <a href="https://github.com/affinelayer/pix2pix-tensorflow" target="_blank" rel="noopener">tensorflow</a> </p><p>pix2pix是基于条件对抗生成网络来学习从输入图像到输出图像的映射。</p><h3 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h3><p>生成器的目标就是将输入的图像转换为目标图像，利用编码-解码器的架构来实现。</p><p><img src="/2018/08/20/Variant-GAN/9dd87e799b054c538ead26df0d7394fb_th.png" alt="9dd87e799b054c538ead26df0d7394fb_th"></p><p>输入图像是256*256大小3种颜色通道的图像（红、绿、蓝通道，对于黑白图，这三个通道的数值相等）。生成器接收输入，并将它用一系列编码器（卷积＋激活函数）简化成更小的表示（representation）。这样做的基本想法是我们用最终的编码层可以得到一种对数据更高层次（抽象）的表达。解码器层呢主要做了相反的操作（反卷积deconvolution＋激活函数），并将编码层的动作反过来。</p><p><img src="/2018/08/20/Variant-GAN/ac5b965024b647a8954ade2c6bdf22d3_th.png" alt="ac5b965024b647a8954ade2c6bdf22d3_th"></p><p>为了提高这种图像到图像转换的效率，作者用了一种称为U－Net的方法而不是编码－解码器方法。它们本来是一回事，但是U－Net引入了所谓的“跳跃链接”，以便将编码器层和解码器层直接相连，这种跳跃链接可以为网络提供一种省略编码／解码部分的选择，从而当网络真的用不到它们的时候，处理效率就会很快了。</p><p><img src="/2018/08/20/Variant-GAN/84bbfddafa9c4d9cb3c2b5aa2ec2e3f2_th.png" alt="84bbfddafa9c4d9cb3c2b5aa2ec2e3f2_th"></p><blockquote><p>U-Net网络结构</p><p><img src="/2018/08/20/Variant-GAN/u_net.png" alt="u_net"></p></blockquote><h3 id="辨别器"><a href="#辨别器" class="headerlink" title="辨别器"></a>辨别器</h3><p>对比发现，生成器和辨别器的输入是不同的：前者只有一张图片作为输入，后者有两张图片作为输入，辨别器的输出是一个30*30的图像，每一个像素都是0～1的数值，代表了这一部分与目标图像比起来有多像。在pix2pix的执行中，这30*30图像中的每一个像素都对应了输入图像的70*70个块（patch，这些patch会有很多重叠，因为输入图像大小是256*256）。这种架构称为“PatchGAN”。</p><p><img src="/2018/08/20/Variant-GAN/86c9fccb701d4bc8b4011fa4bc259f6a_th.png" alt="86c9fccb701d4bc8b4011fa4bc259f6a_th"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h4 id="生成器-1"><a href="#生成器-1" class="headerlink" title="生成器"></a>生成器</h4><p>除了辨别器判断生成图片的真假损失</p><p><img src="/2018/08/20/Variant-GAN/4.png" alt="4"></p><p>还有$L_1$损失来衡量生成图片的优劣。</p><p><img src="/2018/08/20/Variant-GAN/6.png" alt="6"></p><h4 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h4><p>在损失函数中，L1被添加进来来保证输入和输出的共性。这就启发出了一个观点，那就是图像的变形分为两种，局部的和全局的。既然L1可以防止全局的变形。那么只要让D去保证局部能够精准即可。于是，Pix2Pix中的D被实现为Patch-D，D区分图像中的每个$N*N$大小的补丁是真是假。</p><p><img src="/2018/08/20/Variant-GAN/4.png" alt="4"></p><h2 id="Pix2PixHD"><a href="#Pix2PixHD" class="headerlink" title="Pix2PixHD"></a>Pix2PixHD</h2><p><a href="">paper</a>  <a href="https://blog.csdn.net/linmingan/article/details/79941645" target="_blank" rel="noopener">ref1</a> <a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/78740321" target="_blank" rel="noopener">ref2</a> </p><h2 id="Coupled-GAN"><a href="#Coupled-GAN" class="headerlink" title="Coupled-GAN"></a>Coupled-GAN</h2><p><a href="https://arxiv.org/pdf/1606.07536.pdf" target="_blank" rel="noopener">paper</a> <a href="https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/" target="_blank" rel="noopener">ref</a> <a href="https://github.com/eriklindernoren/Keras-GAN/blob/master/cogan/cogan.py" target="_blank" rel="noopener">keras</a> </p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>AAE是一种学习数据分布$P(X)$的方法，Conditional GAN则学习数据的条件分布$P(X|c)$，而Coupled-GAN意在学习数据的联合分布$P(X_1,X_2)$，其中$X_1, X_2$是来自不同域的，比如同一个场景的图片但是一个是彩色图片，一个是深度图片；又或者同一张脸的不同表情（笑和不笑）。</p><p>Coupled-GAN是从数据的边缘分布$x_1 \sim P(X_1)$和$x_2 \sim P(X_2)$采样，来学习数据的联合分布，而实现这个的技巧是权值共享。我们认为数据的高层级表征是共享的，故我们强制网络共享部分层的权重，这样CoGAN会趋近学习到两种域数据的共同表征，也就是它们的联合分布。</p><p>但是，网络共享哪些层的权重呢？我们知道，神经网络在进行分类任务时，是从下到上的方式学习数据的特征，也即是说从低层级特征到高层级特征。低层级特征过于细致，不够泛化，它们往往捕捉图片的厚度，色彩饱和度等特征；而高层级特征能够学习到数据更为抽象的特征，比如“鸟”，“狗”，忽略图片色彩厚度等细节，所以我们强制神经网络的处理高层级的层共享参数。</p><p>对discriminator来说，它的共享层应为最后几层；而对generator来说，它的共享层应该是前面几层，因为生成器的工作方式是从抽象特征生成具体图像。</p><p><img src="/2018/08/20/Variant-GAN/schematic.png" alt="schematic"></p><h3 id="LOSS-FUNC"><a href="#LOSS-FUNC" class="headerlink" title="LOSS FUNC"></a>LOSS FUNC</h3><p>只使用了GAN损失。</p><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><h4 id="Digit-Rotation"><a href="#Digit-Rotation" class="headerlink" title="Digit Rotation"></a>Digit Rotation</h4><p>生成MNIST手写体数字图片相对应的90度旋转图片。两个数据域分别是MNIST图片和被旋转90的手写数字图片。</p><p><img src="/2018/08/20/Variant-GAN/Screen Shot 2018-09-03 at 9.51.47 AM.png" alt="Screen Shot 2018-09-03 at 9.51.47 AM"></p><h4 id="Digit-Edge"><a href="#Digit-Edge" class="headerlink" title="Digit Edge"></a>Digit Edge</h4><p>生成手写体数字相应的边缘图片。</p><p><img src="/2018/08/20/Variant-GAN/Screen Shot 2018-09-03 at 10.09.37 AM.png" alt="Screen Shot 2018-09-03 at 10.09.37 AM"></p><h4 id="Negative-Image"><a href="#Negative-Image" class="headerlink" title="Negative Image"></a>Negative Image</h4><p>生成图像的反图像。<a href="https://blog.csdn.net/JohinieLi/article/details/77098915" target="_blank" rel="noopener">PIL-Negative Img</a> </p><p><img src="/2018/08/20/Variant-GAN/Screen Shot 2018-09-03 at 10.10.36 AM.png" alt="Screen Shot 2018-09-03 at 10.10.36 AM"></p><h4 id="Color-amp-Depth-Images"><a href="#Color-amp-Depth-Images" class="headerlink" title="Color &amp; Depth Images"></a>Color &amp; Depth Images</h4><h2 id="Domain数据迁移"><a href="#Domain数据迁移" class="headerlink" title="Domain数据迁移"></a>Domain数据迁移</h2><p><strong>CycleGAN</strong>，<strong>DualGAN</strong>，<strong>DiscoGAN</strong></p><h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><p><a href="https://zhuanlan.zhihu.com/p/26995910" target="_blank" rel="noopener">ref1</a> <a href="https://blog.csdn.net/qq_21190081/article/details/78807931" target="_blank" rel="noopener">ref2</a> <a href="https://blog.csdn.net/on2way/article/details/78768221" target="_blank" rel="noopener">ref3</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Variant GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2018/08/18/Logistic-Regression/"/>
      <url>/2018/08/18/Logistic-Regression/</url>
      <content type="html"><![CDATA[<p>用于分类任务的逻辑斯蒂回归。</p><a id="more"></a><h1 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>线性回归试图用一个线性函数的预测值取逼近真实值，那么是否能用一个线性函数的预测值去逼近真实值的衍生值呢？假设我们使用预测值去逼近一个对数函数$g(y)=In(y)$，那么$g(y)=f(y)$，</p><script type="math/tex; mode=display">In(y)=w^Tx+b</script><p>这样我们就得到了对数线性回归，它实际上是试图让$e^{(w^Tx+b)}$去逼近$y$，本质上仍然是线性回归，但已是在求输入空间到输出空间的<strong>非线性</strong>函数的映射。如下图所示，指数空间的真实值通过对数函数映射到一个线性空间：</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-06-05 at 11.53.07 AM.png" alt="creen Shot 2019-06-05 at 11.53.07 A"></p><p>线性模型可以用于回归学习，但是如果要做的是分类任务该如何？启发自对数线性回归，我们可以找到一个类似对数函数的一个单调可微函数将分类任务的真实标记$y$映射到线性回归模型的预测值。</p><p>考虑二分类问题，其输出标记是$y\in\{0,1\}$，而线性回归模型的预测值$z=w^Tx+b$是实值，故使用一个阶跃函数，进行实值与离散值的映射：</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-06-05 at 11.53.42 AM.png" alt="creen Shot 2019-06-05 at 11.53.42 A"></p><p>若预测值$z$大于零就判为正例，小于零就判为负例，预测值为临界值零则任意判断，</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-06-05 at 11.53.57 AM.png" alt="creen Shot 2019-06-05 at 11.53.57 A"></p><p>因为阶跃函数是不连续的，所以我们使用了一个连续可微的函数替代函数：</p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(w^Tx+b)}}</script><p>上式可变化为：</p><script type="math/tex; mode=display">In\frac{y}{1-y}=w^Tx+b</script><p>如果将$y$看作样本$x$作为正例的概率，则$1-y$表示反例的概率，两者的比值反映了$x$作为正例的相对可能性，称为几率，对几率取对数称为对数几率。可以看出上式用线性回归模型的预测值去逼近真实标记的对数几率，因为对应模型称为逻辑斯蒂回归。需要注意的是虽然名字是”回归”，但是它是分类方法。</p><h2 id="算法求解"><a href="#算法求解" class="headerlink" title="算法求解"></a>算法求解</h2><p>如果将预测值$y$看作类后验概率估计$p(y=1|x)$，则上式：</p><script type="math/tex; mode=display">In(\frac{p(y=1|x)}{p(y=0|x)})=w^Tx+b</script><p>又因为$p(y=1|x)+p(y=0|x)=1$，所以联合求解得到：</p><script type="math/tex; mode=display">p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\p(y=1|x)=\frac{1}{1+e^{w^Tx+b}}</script><p>可以通过极大似然法求解：</p><script type="math/tex; mode=display">max \ P=\prod_{i=1}^{m}[p(y=1|x_i)^{y_i}p(y=0|x_i)^{(1-y_i)}]</script><p>即令每个样本属于其真实标记的概率越大越好，两边取对数：</p><script type="math/tex; mode=display">max\ In(P)=\sum_{i=1}^{m}y_iIn[p(y=1|x_i)]+(1-y_i)In[p(y=0|x_i)]</script><p>如果真实标记$y_i=1$，则$y_iIn[p(y=1|x_i)]+(1-y_i)In[p(y=0|x_i)]=p(y=1|x_i)$；</p><p>如果真实标记$y_i=0$，则$y_iIn[p(y=1|x_i)]+(1-y_i)In[p(y=0|x_i)]=p(y=0|x_i)$；</p><p>即：</p><script type="math/tex; mode=display">max \ In(Loss)=\sum_{i=1}^{m}y_i(w^Tx+b)+In(\frac{1}{e^{w^Tx+b}+1})</script><p>max上式等价于：</p><script type="math/tex; mode=display">min \ In(Loss)=\sum_{i=1}^{m}-y_i(w^Tx+b)+In({1+e^{w^Tx+b}})</script><p>可以使用梯度下降法求解最佳的$w$和$b$：</p><script type="math/tex; mode=display">w_j=w_j-\alpha\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_{j}^{(i)}</script><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>样本集$D\{(x_i,y_i)\}^{n}_{i=1}$, 利用最大似然法, 即令每个样本属于其真实标签的概率越大越好，故似然函数为</p><script type="math/tex; mode=display">\prod_{i=1}^{n}p^{y_i}(1-p)^{(1-y_i)}</script><p>对数似然函数:</p><script type="math/tex; mode=display">L(p)=\sum_{i=1}^{n}y_iInP+\sum_{i=1}^{n}(1-y_i)In(1-p)</script><p>对$P$求$w$导数:</p><script type="math/tex; mode=display">P=\frac{1}{1+e^{-(wx+b)}}\\\frac{\partial{P}}{\partial{w}}=\frac{-1}{(1+e^{-(wx+b)})^2}*e^{-(wx+b)}*-x=P(1-P)</script><p>$L(P)$对$w$求导数:</p><script type="math/tex; mode=display">\frac{\partial{L(P)}}{\partial{w}}=\sum_{i=1}^{n}y_i\frac{1}{P}\frac{\partial{P}}{\partial{w}}+\sum_{i=1}^{n}(1-y_i)\frac{-1}{1-p}\frac{\partial{P}}{\partial{w}}=XY(1-P)+(-1+Y)XP=(Y-P)X</script></div></div><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><script type="math/tex; mode=display">E(w,b)=-\frac{1}{n}[\sum_{i=1}^{n}y^{(i)}logh_w(x^{(i)})+(1-y^{(i)})log(1-h_w(x^{(i)}))]+\lambda\sum_{i=1}^{n}w_{i}^{2}</script><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>$w_0=w_0-\alpha\frac{1}{n}\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_0^{(i)}$</p><p>$w_j=w_j-\alpha[\frac{1}{n}\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}-\frac{\lambda}{n}w_j]$</p><h2 id="Softmax-Loss-Multinomial-Logistic-Regression"><a href="#Softmax-Loss-Multinomial-Logistic-Regression" class="headerlink" title="Softmax Loss - Multinomial Logistic Regression"></a>Softmax Loss - Multinomial Logistic Regression</h2><p>对于多分类任务，模型的输出是一个多维向量，向量的每一位表示该输出属于某一类的概率。softmax则将输出建模成一个分布：</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.21.08 PM.png" alt="creen Shot 2019-03-08 at 2.21.08 P"></p><p>其中，$x_i$是第$i$个输入，$k$表示某一个类，则上式计算该输入属于某一个类的概率。</p><p>那么我们的损失函数就变成了：</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.24.14 PM.png" alt="creen Shot 2019-03-08 at 2.24.14 P"></p><p>对于每一个样本，我们最大化它属于正确类的概率。因为$ 0\le P(Y=y_i|X=x_i)\le 1$,所以$logP&lt;0$；我们追求的是$P()$越大，$L_i$越小，但是对于$logP$, 恰恰相反，所以对损失函数取负。</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.29.26 PM.png" alt="creen Shot 2019-03-08 at 2.29.26 P"></p><p>对于上面的例子，分类器的输出是$(3.2,5.1,-1.7)$,经过softmax函数计算，我们得到$(0.13,0.87,0.0)$，那么该样本的损失为$L_i=-log(0.13)$.</p><blockquote><ol><li><p>What is the min/max possible loss $L_i$?</p><p>The min loss is 0 when we classify the input correctly with a extremely high score. The max loss is $\infin$ when we classify the input to the correct category with a extremely low score. And according to the figure, it is infinite.</p></li><li><p>Usually at initialization $W$ is small so all $s\approx 0$. What is the loss?</p><p>The answer is $-log(\frac{1}{C})$, where $C$ is the class number.</p></li></ol></blockquote><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2019-03-08 at 2.34.05 PM.png" alt="creen Shot 2019-03-08 at 2.34.05 P"></p><h3 id="Softmax-Forward"><a href="#Softmax-Forward" class="headerlink" title="Softmax Forward"></a>Softmax Forward</h3><p><a href="https://deepnotes.io/softmax-crossentropy" target="_blank" rel="noopener">ref1</a>  <a href="http://bigstuffgoingon.com/blog/posts/softmax-loss-gradient/#The-gradient-of-the-softmax-classifier-loss-function" target="_blank" rel="noopener">ref2</a></p><p>Softmax function takes an N-dimensional vector of real number and transforms it into a vector of real number in range $(0,1)$ which add up to 1. For each output element $a_j$ in the last layer:</p><script type="math/tex; mode=display">f(a_j)=\frac{e^{a_j}}{\sum_{i=1}^{k}e^{a_i}}</script><p>In python, we have the code for softmax function as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div><div class="line">    exps = np.exp(x)</div><div class="line">    <span class="keyword">return</span> exps/np.sum(exps)</div></pre></td></tr></table></figure><p>We have to note that the numerical range of float number in numpy is limited. For <code>float64</code> the upper bound is $10^{308}$. For exponential, it is not difficult to overshoot that limit, in which case python returns <code>nan</code>.</p><p>To make our softmax function numerically stable, we simply normalize the values in the vector, by multiplying the numerator and denominator with a constant $C$.</p><script type="math/tex; mode=display">\begin{align}p_i &= \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}} \\&= \frac{Ce^{a_i}}{C\sum_{k=1}^N e^{a_k}} \\&= \frac{e^{a_i + \log(C)}}{\sum_{k=1}^N e^{a_k + \log(C)}} \\\end{align}</script><p>We can choose an arbitrary value for $log(C)$ term, but generally $log(C)=−max(a)$ is chosen, as it shifts all of elements in the vector to negative to zero, and negatives with large exponents saturate to zero rather than the infinity, avoiding overflowing and resulting in <code>nan</code>.</p><p>The code for our stable softmax is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></div><div class="line">    exps = np.exp(x-np.max(x))</div><div class="line">    <span class="keyword">return</span> exps/np.sum(exps)</div></pre></td></tr></table></figure><h3 id="Derivative"><a href="#Derivative" class="headerlink" title="Derivative"></a>Derivative</h3><p>Due to the desirable property of softmax function outputting a probability distribution, we use it as the final layer in neural networks. For this we need to calculate the derivative or gradient and pass it back to the previous layer during backpropagation.</p><p>For each node $a_j$ in the output layer, we want to calculate:</p><script type="math/tex; mode=display">\frac{\partial p_i}{\partial a_j} = \frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}</script><p>If $i=j$:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}&= \frac{e^{a_i} \sum_{k=1}^N e^{a_k} - e^{a_j}e^{a_i}}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\&= \frac{e^{a_i} \left( \sum_{k=1}^N e^{a_k} - e^{a_j}\right )}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\&= \frac{ e^{a_j} }{\sum_{k=1}^N e^{a_k} } \times \frac{\left( \sum_{k=1}^N e^{a_k} - e^{a_j}\right ) }{\sum_{k=1}^N e^{a_k} } \\&= p_i(1-p_j)\end{align}</script><p>For $i\ne j$:</p><script type="math/tex; mode=display">\begin{align}\frac{\partial  \frac{e^{a_i}}{\sum_{k=1}^N e^{a_k}}}{\partial a_j}&= \frac{0 - e^{a_j}e^{a_i}}{\left( \sum_{k=1}^N e^{a_k}\right)^2} \\&= \frac{- e^{a_j} }{\sum_{k=1}^N e^{a_k} } \times \frac{e^{a_i} }{\sum_{k=1}^N e^{a_k} } \\&= - p_j.p_i\end{align}</script><p>So the derivative of the softmax function is given as,</p><script type="math/tex; mode=display">\frac{\partial p_i}{\partial a_j} = \begin{cases}p_i(1-p_j) &  if & i=j \\-p_j.p_i & if & i \neq j\end{cases}</script><p>Or using Kronecker delta $\delta{ij} = \begin{cases} 1 &amp; if &amp; i=j \\ 0 &amp; if &amp; i\neq j \end{cases}$ </p><script type="math/tex; mode=display">\frac{\partial p_i}{\partial a_j} =  p_i(\delta_{ij}-p_j)</script><h3 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h3><p>Corss entropy loss indicates the distance between what the model belives the output distribution should be, and what the original distribution really is. It is defined as:</p><script type="math/tex; mode=display">H(y,p) = - \sum_i y_i log(p_i)</script><p>It is widely used as an alternative of squared error. It is used when node activations can be understand as representing the probability that each hypothesis might be true, i.e. when the output is a probability is a probability distribution. Thus it is used as a loss function in neural networks which have softmax activations in the output layer:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(x,y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    X is the output from fully connected layer (num_examples x num_classes)</span></div><div class="line"><span class="string">    y is labels (num_examples x 1)</span></div><div class="line"><span class="string">    Note that y is not one-hot encoded vector. </span></div><div class="line"><span class="string">    It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    l = y.shape[<span class="number">0</span>]</div><div class="line">    prob = softmax(x)</div><div class="line">    prob_of_groundtruth = [row[y[i]] <span class="keyword">for</span> i,row <span class="keyword">in</span> enumerate(prob)]</div><div class="line">    log_likelihood = -np.log(prob_of_groundtruth)</div><div class="line">    loss = np.sum(log_likelihood)</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><h3 id="Derivative-of-Cross-Entropy-Loss-with-Softmax"><a href="#Derivative-of-Cross-Entropy-Loss-with-Softmax" class="headerlink" title="Derivative of Cross Entropy Loss with Softmax"></a>Derivative of Cross Entropy Loss with Softmax</h3><p>Cross Entropy Loss with Softmax function are used as the output layer extensively. Now we use the derivative of softmax that we derived earlier to derive the derivative of the cross entropy loss function.</p><p>For each sample $(x_i,y_i)$, there are $c$ classes</p><script type="math/tex; mode=display">\begin{align}L_i &= - \sum_i^c y_i log(p_i) \\\end{align}</script><p>Then for each output node $o_i$, i.e., score of the class $i$ :</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L_i}{\partial o_i} &= - \sum_k y_k \frac{\partial log(p_k)}{\partial o_i } \\&= - \sum_k y_k \frac{\partial log(p_k)}{\partial p_k} \times \frac{\partial p_k}{ \partial o_i} \\&= - \sum y_k \frac{1}{p_k} \times \frac{\partial p_k}{\partial o_i} \\\end{align}</script><p>From derivative of softmax we derived earlier that $\frac{\partial p_i}{\partial a_j} = \begin{cases}p_i(1-p_j)&amp;  if &amp; i=j \\ -p_j.p_i &amp; if &amp; i \neq j\end{cases}$,</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L}{\partial o_i}  &= -y_i(1-p_i) - \sum_{k\neq i} y_k \frac{1}{p_k}(-p_k.p_i) \\&= -y_i(1-p_i) + \sum_{k \neq 1} y_k.p_i \\&= - y_i + y_ip_i + \sum_{k \neq 1} y_k.p_i \\&= p_i\left( y_i +  \sum_{k \neq 1} y_k\right) - y_i \\&= p_i\left( y_i +  \sum_{k \neq 1} y_k\right)  - y_i\end{align}</script><p>$y$ is a one hot encoded vector for the label, so $\sum_{k}y_k =1$, and $y_i + \sum_{k\ne i}y_k =1$. So we have,</p><script type="math/tex; mode=display">\frac{\partial L}{\partial o_i} = p_i - y_i</script><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">delta_cross_entropy</span><span class="params">(x,y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    X is the output from fully connected layer (num_examples x num_classes)</span></div><div class="line"><span class="string">    y is labels (num_examples x 1)</span></div><div class="line"><span class="string">    Note that y is not one-hot encoded vector. </span></div><div class="line"><span class="string">    It can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    m = y.shape[<span class="number">0</span>]</div><div class="line">    prob = softmax(x)</div><div class="line">    prob_of_groundtruth = [row[y[i]] <span class="keyword">for</span> i,row <span class="keyword">in</span> enumerate(prob)]</div><div class="line">    prob_of_groundtruth -= <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> prob_of_groundtruth/m</div></pre></td></tr></table></figure><p>In summary, the overall loss of cross entropy loss with sigmoid function is as following:</p><p>for each sample $(x_i,y_i)$, </p><script type="math/tex; mode=display">Loss=-\frac{1}{n}\sum_{i}^{n}y_ilog(p_i) + \frac{1}{2}\sum_{m}\sum_{n}w_{m,n}^{2}\\p_i=\frac{e^{o_i}}{\sum_{k}^{c}e^{o_k}}</script><p>where $o_i$ is the non-logit output, $p_i$ is the probability of being class $i$, $y_i$ is the groundtruth class. The second part of $Loss$ is the regularization.</p><h3 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h3><p>The plaint implementation is as following:</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Softmax</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.W = <span class="keyword">None</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(self,W, X, y, reg)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line"><span class="string">        :param W:A numpy array of shape (D, C) containing weights.</span></div><div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing a minibatch of data.</span></div><div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c means</span></div><div class="line"><span class="string">        that X[i] has label c, where 0 &lt;= c &lt; C.</span></div><div class="line"><span class="string">        :param reg:(float) regularization strength</span></div><div class="line"><span class="string">        :return:a tuple of:</span></div><div class="line"><span class="string">                 - loss as single float</span></div><div class="line"><span class="string">                 - gradient with respect to weights W; an array of same shape as W</span></div><div class="line"><span class="string">        '''</span></div><div class="line">        loss = <span class="number">0.0</span></div><div class="line">        dW = np.zeros_like(W)</div><div class="line">        D, C = W.shape</div><div class="line">        N, D = X.shape</div><div class="line">        scores = X.dot(W)</div><div class="line">        norm_scores = scores - np.max(scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line">        prob_scores = np.exp(norm_scores)</div><div class="line">        prob_scores /= np.sum(prob_scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div><div class="line">            loss -= np.log(prob_scores[i][y[i]])</div><div class="line">        loss = loss / N + reg * <span class="number">0.5</span> * np.sum(W ** <span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div><div class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(C):</div><div class="line">                <span class="keyword">if</span> y[i] == c:</div><div class="line">                    dW[:, c] += X[i] * (prob_scores[i][c] - <span class="number">1</span>)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    dW[:, c] += X[i] * prob_scores[i][c]</div><div class="line">        dW = dW / N + reg * W</div><div class="line">        <span class="keyword">return</span> loss, dW</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(self,W, X, y, reg)</span>:</span></div><div class="line"></div><div class="line">        loss = <span class="number">0.0</span></div><div class="line">        dW = np.zeros(W.shape)</div><div class="line"></div><div class="line">        D, C = W.shape</div><div class="line">        N, D = X.shape</div><div class="line">        scores = X.dot(W)</div><div class="line">        norm_scores = scores - np.max(scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line">        prob_scores = np.exp(norm_scores)</div><div class="line">        prob_scores /= np.sum(prob_scores, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># LOSS Calculation</span></div><div class="line">        loss = <span class="number">-1</span> * np.sum(np.log(prob_scores[np.arange(N), y]))</div><div class="line">        loss = loss / N + reg * <span class="number">0.5</span> * np.sum(W ** <span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Gradient Calculation</span></div><div class="line">        prob_scores[np.arange(N), y] -= <span class="number">1</span></div><div class="line">        dW = X.T.dot(prob_scores)</div><div class="line">        dW = dW / N + reg * W</div><div class="line"></div><div class="line">        <span class="keyword">return</span> loss, dW</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.softmax_loss_vectorized(self.W, X_batch, y_batch, reg)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></div><div class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">        :param X:A numpy array of shape (N, D) containing training data; there are N</span></div><div class="line"><span class="string">          training samples each of dimension D.</span></div><div class="line"><span class="string">        :param y:A numpy array of shape (N,) containing training labels; y[i] = c</span></div><div class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></div><div class="line"><span class="string">        :param learning_rate:(float) learning rate for optimization.</span></div><div class="line"><span class="string">        :param reg:(float) regularization strength.</span></div><div class="line"><span class="string">        :param num_iters:(integer) number of steps to take when optimizing</span></div><div class="line"><span class="string">        :param batch_size:(integer) number of training examples to use at each step.</span></div><div class="line"><span class="string">        :param verbose:(boolean) If true, print progress during optimization.</span></div><div class="line"><span class="string">        :return:A list containing the value of the loss function at each training iteration.</span></div><div class="line"><span class="string">        '''</span></div><div class="line">        num_train, dim = X.shape</div><div class="line">        num_classes = np.max(y) + <span class="number">1</span>  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></div><div class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># lazily initialize W</span></div><div class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</div><div class="line"></div><div class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></div><div class="line">        loss_history = []</div><div class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</div><div class="line">            mask = np.random.choice(num_train, batch_size)</div><div class="line">            X_batch = X[mask]</div><div class="line">            y_batch = y[mask]</div><div class="line"></div><div class="line">            <span class="comment"># evaluate loss and gradient</span></div><div class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</div><div class="line">            loss_history.append(loss)</div><div class="line">            self.W -= learning_rate * grad</div><div class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</div><div class="line">        <span class="keyword">return</span> loss_history</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></div><div class="line">        scores = X.dot(self.W)</div><div class="line">        y_pred = np.argmax(scores,axis=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> y_pred</div></pre></td></tr></table></figure></div></div><h2 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h2><p>目前我们接触的问题是二分类问题，即每一个样本的label只有两种可能性(0|1)。但是，有时候会碰到多分类问题，即每一个样本label有两种以上的选择，比如对水果图片进行分类的任务就是多分类任务，因为一张图片可能是苹果，香蕉，橘子或者草莓等。</p><p>多分类的学习思路是将多分类任务拆分成若干个二分类任务求解，经典的拆分方法有三种：“一对一”(One vs One，简称OvO)，“一对余”(One vs Rest，简称OvR)和“多对多”(Many vs Many，简称MvM)。</p><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-14%20at%205.48.16%20PM.png" alt="Screen Shot 2018-08-14 at 5.48.16 PM"></p><h3 id="一对一（OvO）"><a href="#一对一（OvO）" class="headerlink" title="一对一（OvO）"></a>一对一（OvO）</h3><p><strong>思想</strong>：给定具有$N$个类别的数据集，分为$N$个类别的堆，两两一组，一正一反，训练出$\frac{N(N-1)}{2}$个分类器。测试的时候，将测试样本送给所有的分类器，故产生$\frac{N(N-1)}{2}$个结果，投票选出最终的类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line"></div><div class="line">iris = datasets.load_iris()</div><div class="line">x,y = iris.data, iris.target</div><div class="line">model = OneVsOneClassifier(LinearSVC(random_state=<span class="number">0</span>))</div><div class="line">model.fit(x,y)</div><div class="line">re = model.predict(x)</div><div class="line">print(re)</div></pre></td></tr></table></figure><h3 id="一对余-OvR"><a href="#一对余-OvR" class="headerlink" title="一对余(OvR)"></a>一对余(OvR)</h3><p><strong>思想</strong>：每次将一个类的样例作为正类，剩下的其他类的样例作为反例来训练$N$个分类器。测试时，若仅有一个分类器预测为正类，则对应的类别作为最终的分类结果。若有多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记为分类结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line">iris = datasets.load_iris()</div><div class="line">X, y = iris.data, iris.target</div><div class="line">OneVsRestClassifier(LinearSVC(random_state=<span class="number">0</span>)).fit(X, y).predict(X)</div></pre></td></tr></table></figure><p><strong>比较</strong>：<img src="/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-14%20at%205.55.35%20PM.png" alt="Screen Shot 2018-08-14 at 5.55.35 PM"></p><h3 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h3><p><a href="https://zhuanlan.zhihu.com/p/32940093" target="_blank" rel="noopener">ref1</a> </p><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>机器学习中常常会遇到数据的<strong>类别不平衡（class imbalance）</strong>，也叫数据偏斜（class skew）。以常见的二分类问题为例，我们希望预测病人是否得了某种罕见疾病。但在历史数据中，阳性的比例可能很低（如百分之0.1）。在这种情况下，学习出好的分类器是很难的，而且在这种情况下得到结论往往也是很具迷惑性的。</p><p>以上面提到的场景来说，如果我们的分类器<strong>总是</strong>预测一个人未患病，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结果是没有意义的。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ul><li>试用其他评价指标</li><li>对数据集进行重采样</li><li>​</li></ul><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>需要一个标准对数据进行约束：</p><ol><li><p>x增加全1行</p></li><li><p>shape</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x.shape = (m,n) <span class="comment"># m:样本数；n：特征数</span></div><div class="line">y.shape = (m,) <span class="comment"># 一维行向量</span></div><div class="line">thetas = (n,<span class="number">1</span>) <span class="comment"># 一维列向量</span></div></pre></td></tr></table></figure></li></ol><h2 id="预测是否录取学生-ref"><a href="#预测是否录取学生-ref" class="headerlink" title="预测是否录取学生 ref"></a>预测是否录取学生 <a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/" target="_blank" rel="noopener">ref</a></h2><p>数据集包含学生的两次考试成绩以及其是否被录取的信息。</p><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>数据载入的方式千千万万种方式，但是一旦载入之后，需要做的操作包括：</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">data = np.loadtxt(<span class="string">'./dataset/ex2data1.txt'</span>,delimiter=<span class="string">','</span>)</div><div class="line">x = np.array(data[:,:<span class="number">-1</span>])</div><div class="line">y = np.array(data[:,<span class="number">-1</span>])</div><div class="line"></div><div class="line">row,_ = np.shape(x)</div><div class="line">ones = np.ones((row,<span class="number">1</span>))</div><div class="line">x_new = np.concatenate((ones,x),axis=<span class="number">1</span>)</div><div class="line">_,col = np.shape(x_new)</div><div class="line">thetas = np.zeros([col,<span class="number">1</span>])</div></pre></td></tr></table></figure></div></div><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(x,y)</span>:</span></div><div class="line">    plt.scatter(x[y==<span class="number">0</span>,<span class="number">0</span>],x[y==<span class="number">0</span>,<span class="number">1</span>],marker=<span class="string">'o'</span>,label=<span class="string">'Not admitted'</span>,c = <span class="string">'y'</span>)</div><div class="line">    plt.scatter(x[y == <span class="number">1</span>, <span class="number">0</span>], x[y == <span class="number">1</span>, <span class="number">1</span>], marker=<span class="string">'+'</span>, label=<span class="string">'Admitted'</span>,c=<span class="string">'black'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Exam1 score'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Exam2 score'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p><img src="/2018/08/18/Logistic-Regression/image-20180818165938933.png" alt="image-20180818165938933"></p></div></div><h3 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a>Sigmoid激活函数</h3><p>我们的sigmoid函数为：</p><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2018-08-18 at 5.02.58 PM.png" alt="Screen Shot 2018-08-18 at 5.02.58 PM"></p><p>在实现激活函数之后，我们使用几个值取测试是否写对了：如果激活函数的输入是很大的值，那么我们的输出应该接近1；如果是很小的数，应该接近0；如果输入0，则输出0.5。而且，输入不应区分向量，矩阵的。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></div><div class="line">    gz = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</div><div class="line">    <span class="keyword">return</span> gz</div></pre></td></tr></table></figure></div></div><h3 id="代价函数和梯度下降法"><a href="#代价函数和梯度下降法" class="headerlink" title="代价函数和梯度下降法"></a>代价函数和梯度下降法</h3><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2018-08-19 at 8.23.02 PM.png" alt="Screen Shot 2018-08-19 at 8.23.02 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_func</span><span class="params">(thetas,x,y)</span>:</span></div><div class="line">    z = x.dot(thetas)</div><div class="line">    y_pred = sigmoid(z)</div><div class="line">    loss = y.T.dot(np.log(y_pred)) + (<span class="number">1</span>-y).T.dot(np.log(<span class="number">1</span>-y_pred))</div><div class="line">    <span class="keyword">return</span> -loss/row</div></pre></td></tr></table></figure><blockquote><p>初次调用时，loss=0.693</p></blockquote><p><img src="/2018/08/18/Logistic-Regression/Screen Shot 2018-08-19 at 8.23.25 PM.png" alt="Screen Shot 2018-08-19 at 8.23.25 PM"></p><p>这次我们的梯度下降法返回梯度量，不直接返回参数更新</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(thetas,x,y)</span>:</span></div><div class="line">    z = x.dot(thetas)</div><div class="line">    y_pred = sigmoid(z)</div><div class="line">    gd = (y_pred-y).T.dot(x) / row</div><div class="line">    <span class="keyword">return</span> gd.T</div></pre></td></tr></table></figure></div></div><h3 id="损失函数优化"><a href="#损失函数优化" class="headerlink" title="损失函数优化"></a>损失函数优化</h3><p>这次，我们使用<code>scipy.optimize</code>寻找目标函数的最优值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scipy.optimize.fmin_tnc(func, x0, fprime=<span class="keyword">None</span>, args=())</div></pre></td></tr></table></figure><ul><li><p>func：目标函数，它必须满足以下条件之一：</p><p>— 返回函数值<code>f</code>和梯度值<code>g</code></p><p>— 返回函数值<code>f</code>，但是必须提供梯度函数作为<code>fprime</code>的参数</p><p>—返回函数值<code>f</code>，设置 <code>approx_grad=True</code></p></li><li><p>x0：待优化参数的初始值</p></li><li><p>fprime：梯度函数</p></li><li><p>args：<code>func</code>函数的参数</p></li><li><p>该函数的返回值：x—solution；nfeval—函数优化次数；</p></li></ul><p>最优化之后的损失函数值是0.203左右。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">scipy_gd</span><span class="params">(thetas,x,y)</span>:</span></div><div class="line">    result = opt.fmin_tnc(</div><div class="line">        func=cost_func,x0=thetas,fprime=gradient_descent,args=(x,y))</div><div class="line">    <span class="keyword">return</span> result[<span class="number">0</span>]</div></pre></td></tr></table></figure></div></div><h3 id="预测及评价"><a href="#预测及评价" class="headerlink" title="预测及评价"></a>预测及评价</h3><p>我们定义一个预测函数，输入未知值，预测其分类</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(thetas,x)</span>:</span></div><div class="line">    z = x.dot(thetas)</div><div class="line">    prob = sigmoid(z)</div><div class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> prob]</div></pre></td></tr></table></figure></div></div><p>使用准确率来评估预测结果</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    thetas = scipy_gd(thetas,x_new,y)</div><div class="line">    predictions = predict(thetas,x_new)</div><div class="line">    correct = [<span class="number">1</span> <span class="keyword">if</span> ((a == <span class="number">1</span> <span class="keyword">and</span> b == <span class="number">1</span>) <span class="keyword">or</span> (a == <span class="number">0</span> <span class="keyword">and</span> b == <span class="number">0</span>)) <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, y)]</div><div class="line">    accuracy = (sum(map(int, correct)) % len(correct))</div><div class="line">    plot_boundary(x,y,thetas)</div></pre></td></tr></table></figure></div></div><h3 id="画出分类边界"><a href="#画出分类边界" class="headerlink" title="画出分类边界"></a>画出分类边界</h3><p>需要注意的是，此时的x并不需要额外的全1行。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> pl</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_boundary</span><span class="params">(x,y,thetas)</span>:</span></div><div class="line">    plot_data(x,y)</div><div class="line">    h = <span class="number">0.1</span></div><div class="line">    x0_min, x0_max = x[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, x[:, <span class="number">0</span>].max() + <span class="number">0.1</span></div><div class="line">    x1_min, x1_max = x[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, x[:, <span class="number">1</span>].max() + <span class="number">0.1</span></div><div class="line">    x0, x1 = np.meshgrid(np.arange(x0_min, x0_max, h),</div><div class="line">                         np.arange(x1_min, x1_max, h))</div><div class="line">    z = np.c_[x0.ravel(), x1.ravel()].dot(thetas[<span class="number">1</span>:]) + thetas[<span class="number">0</span>]</div><div class="line">    z = sigmoid(z)</div><div class="line">    z = z.reshape(x0.shape)</div><div class="line">    plt.contour(x0, x1, z, cmap=pl.cm.Paired)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/08/18/Logistic-Regression/image-20180822205326946.png" alt="image-20180822205326946"></p></div></div><h2 id="正则化的回归"><a href="#正则化的回归" class="headerlink" title="正则化的回归"></a>正则化的回归</h2><p><a href="https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/" target="_blank" rel="noopener">ref</a> </p><p>本题我们使用正则化的逻辑斯蒂回归来分类。给定的数据集是关于微芯片的两次测试数据，label是合格与不合格。</p><ol><li><p>数据可视化</p><p>直接利用上面的画图函数，可以看到我们需要使用一个非线性曲线去分类，类似椭圆。显然这不能是线性分类，对于非线性分类问题，逻辑回归会使用多项式扩展特征。这带来的弊端就是引入巨大特征。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p><img src="/2018/08/18/Logistic-Regression/image-20180822210526246.png" alt="image-20180822210526246"></p></div></div></li><li><p>​</p></li></ol><h2 id="逻辑斯蒂回归预测西瓜"><a href="#逻辑斯蒂回归预测西瓜" class="headerlink" title="逻辑斯蒂回归预测西瓜"></a>逻辑斯蒂回归预测西瓜</h2><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/Screen%20Shot%202018-08-08%20at%2011.32.08%20AM.png" alt="Screen Shot 2018-08-08 at 11.32.08 AM"></p><ol><li><p>数据载入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">dataset = np.loadtxt(<span class="string">'watermelon_3a.csv'</span>, delimiter=<span class="string">","</span>)</div><div class="line">X = dataset[:, <span class="number">1</span>:<span class="number">3</span>]</div><div class="line">y = dataset[:, <span class="number">3</span>]</div><div class="line">m, n = np.shape(X)</div></pre></td></tr></table></figure></li><li><p>数据可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># draw scatter diagram to show the raw data</span></div><div class="line">f1 = plt.figure(<span class="number">1</span>)</div><div class="line">plt.title(<span class="string">'watermelon_3a'</span>)</div><div class="line">plt.xlabel(<span class="string">'density'</span>)</div><div class="line">plt.ylabel(<span class="string">'ratio_sugar'</span>)</div><div class="line">plt.scatter(X[y == <span class="number">0</span>, <span class="number">0</span>], X[y == <span class="number">0</span>, <span class="number">1</span>], marker=<span class="string">'o'</span>, color=<span class="string">'k'</span>, s=<span class="number">100</span>, label=<span class="string">'bad'</span>)</div><div class="line">plt.scatter(X[y == <span class="number">1</span>, <span class="number">0</span>], X[y == <span class="number">1</span>, <span class="number">1</span>], marker=<span class="string">'o'</span>, color=<span class="string">'g'</span>, s=<span class="number">100</span>, label=<span class="string">'good'</span>)</div><div class="line">plt.legend(loc=<span class="string">'upper right'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808113730536.png" alt="image-20180808113730536"></p></li><li><p>利用sklearn求解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> pl</div><div class="line">X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">0</span>)</div><div class="line">log_model = LogisticRegression()  </div><div class="line">log_model.fit(X_train, y_train) </div><div class="line">weights , b = log_model.coef_, log_model.intercept_</div><div class="line"><span class="comment"># weights = [[-0.0865987   0.39410864]] </span></div><div class="line"><span class="comment"># b = [-0.02152586]</span></div></pre></td></tr></table></figure></li><li><p>结果可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">###计算模型各种评价标准</span></div><div class="line">print(metrics.confusion_matrix(y_test, y_pred))</div><div class="line">print(metrics.classification_report(y_test, y_pred))</div><div class="line">y_pred = log_model.predict(X_test)</div><div class="line">precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">h = <span class="number">0.001</span></div><div class="line">x0_min, x0_max = X[:, <span class="number">0</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">0</span>].max() + <span class="number">0.1</span></div><div class="line">x1_min, x1_max = X[:, <span class="number">1</span>].min() - <span class="number">0.1</span>, X[:, <span class="number">1</span>].max() + <span class="number">0.1</span></div><div class="line">x0, x1 = np.meshgrid(np.arange(x0_min, x0_max, h),</div><div class="line">                     np.arange(x1_min, x1_max, h))</div><div class="line">z = log_model.predict(np.c_[x0.ravel(), x1.ravel()])</div><div class="line">z = z.reshape(x0.shape)</div><div class="line">plt.contourf(x0, x1, z, cmap=pl.cm.Paired)</div><div class="line">plt.contour(x0, x1, z, cmap=pl.cm.Paired)</div></pre></td></tr></table></figure><blockquote><ul><li><a href="https://www.cnblogs.com/lemonbit/p/7593898.html" target="_blank" rel="noopener">meshgrid</a> 通俗说，输入是两个一维向量x和y，对于y中的每一个值$y_i$，都为之复制一个x，相当于从$y_i$出发画一条平行x轴的直线，有多少个y点，就有多少条直线；同理，对于x中的灭一个$x_i$，为之复制一个y，等价于从该$x_i$出发画一条平行于$y$轴的直线。这样很多条直线相交就会产生$len(y) \ \cdot \ len(x)$形状的矩阵。</li><li><code>np.c_[a,b]</code>将两数组在column方向合并,等价于<code>np.concatenate((a,b),axis=1)</code></li><li><code>np.ravel()</code>将数组展开，多维降为一维，类似<code>np.flatten()</code>，只不过<code>np.flatten()</code>返回拷贝，而<code>ravel()</code>返回视图，会对原始数组产生影响。</li></ul></blockquote><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808114529415.png" alt="image-20180808114529415"></p><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180808123047797.png" alt="image-20180808123047797"></p></li></ol><h2 id="交叉验证法-vs-留一法"><a href="#交叉验证法-vs-留一法" class="headerlink" title="交叉验证法 vs 留一法"></a>交叉验证法 vs 留一法</h2><p>基于UCI数据集，比较10折交叉验证和留一法所估计出的对率回归的准确率。</p><p>我们选择<a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener">Iris</a>数据和<a href="http://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center" target="_blank" rel="noopener">blood transfusion</a>数据</p><p><strong>Iris数据</strong>共有4个特征个一个类别label。</p><p>第一个是sepal length，第二个是sepal width，第三个是petal length，第四个特征是petal width。类别有三类：— Iris Setosa  — Iris Versicolour  — Iris Virginica</p><p><strong>blood transfusion</strong>数据总共有4个特征和一个label。</p><p>R(Recency):距离上次献血时间(month)<br>F(Frequency):总献血次数<br>M(Monetary):总献血量<br>T(Time):距离第一次献血(month)<br>Y:2007的三月是否献血;1-献血，0-没有献血</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(data,y)</span>:</span></div><div class="line">    sns.set(style=<span class="string">'white'</span>,color_codes=<span class="keyword">True</span>)</div><div class="line">    sns.pairplot(data,hue=y)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    iris = sns.load_dataset(<span class="string">'iris'</span>)</div><div class="line">    plot_data(iris,<span class="string">'species'</span>)</div><div class="line"></div><div class="line">    data = pd.read_csv(<span class="string">'blood_dataset.txt'</span>,sep=<span class="string">','</span>,header=<span class="keyword">None</span>)</div><div class="line">    data.columns = [<span class="string">'R'</span>,<span class="string">'F'</span>,<span class="string">'M'</span>,<span class="string">'T'</span>,<span class="string">'Y'</span>]</div><div class="line">    plot_data(data,<span class="string">'Y'</span>)</div></pre></td></tr></table></figure><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180813110624641.png" alt="image-20180813110624641"></p><p><img src="/2018/08/18/Logistic-Regression/Linear-Regression/image-20180813111620624.png" alt="image-20180813111620624"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict, LeaveOneOut</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv</span><span class="params">(x,y)</span>:</span></div><div class="line">    log_regression = LogisticRegression()</div><div class="line">    pre = cross_val_predict(log_regression,x,y,cv=<span class="number">5</span>)</div><div class="line">    print(metrics.accuracy_score(y,pre))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loo</span><span class="params">(x,y)</span>:</span></div><div class="line">    loo = LeaveOneOut()</div><div class="line">    accuracy = <span class="number">0</span></div><div class="line">    log_regression = LogisticRegression()</div><div class="line">    <span class="keyword">for</span> train,test <span class="keyword">in</span> loo.split(x):</div><div class="line">        log_regression.fit(x[train],y[train])</div><div class="line">        y_p = log_regression.predict(x[test])</div><div class="line">        <span class="keyword">if</span> y_p == y[test] : accuracy += <span class="number">1</span></div><div class="line">    print(accuracy/np.shape(x)[<span class="number">0</span>]) <span class="comment"># 0.9533333333333334</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_blood</span><span class="params">()</span>:</span></div><div class="line">    data = np.loadtxt(<span class="string">'blood_dataset.txt'</span>,delimiter=<span class="string">','</span>) <span class="comment">#shape = (748,5)</span></div><div class="line">    x = data[:,:<span class="number">-1</span>] <span class="comment"># shape = (748, 4)</span></div><div class="line">    y = data[:,<span class="number">-1</span>] <span class="comment"># shape = (748,)</span></div><div class="line">    <span class="keyword">return</span> x,y</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    iris = load_iris()</div><div class="line">    x = iris.data  <span class="comment"># x.shape = (150,4)</span></div><div class="line">    y = iris.target  <span class="comment"># x.shape = (150,)</span></div><div class="line">    cv(x,y) <span class="comment"># 0.96</span></div><div class="line">    loo(x,y) <span class="comment"># 0.9533333333333334</span></div><div class="line"></div><div class="line">    x,y=load_blood()</div><div class="line">    cv(x,y) <span class="comment"># 0.7807486631016043</span></div><div class="line">    loo(x,y) <span class="comment"># 0.7700534759358288</span></div></pre></td></tr></table></figure><blockquote><p>也可以看到，两种交叉验证的结果相近，但是由于此数据集的类分性不如iris明显，所得结果也要差一些。同时由程序运行可以看出，<strong>LOOCV的运行时间相对较长</strong>，这一点随着数据量的增大而愈发明显。</p><p>所以，一般情况下选择K-折交叉验证即可满足精度要求，同时运算量相对小。</p></blockquote>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Regression </tag>
            
            <tag> Logistic Regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python Sth</title>
      <link href="/2018/08/09/Python-Sth/"/>
      <url>/2018/08/09/Python-Sth/</url>
      <content type="html"><![CDATA[<h2 id="EasyDict"><a href="#EasyDict" class="headerlink" title="EasyDict"></a>EasyDict</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> easydict <span class="keyword">import</span> EasyDict <span class="keyword">as</span> edict</div><div class="line">mydict = edict(&#123;&#125;)</div><div class="line">mydict.foo = <span class="number">3</span></div><div class="line">print(mydict.foo) <span class="comment"># 3</span></div></pre></td></tr></table></figure><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>检查目录，并生成一级目录</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">path = <span class="string">'./xx/'</span></div><div class="line"><span class="keyword">if</span> os.path.isdir(path) == <span class="keyword">False</span>:</div><div class="line">    os.mkdir(path)</div></pre></td></tr></table></figure><p>多级目录生成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">path = <span class="string">'./xx/yy/zz/'</span></div><div class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(path):</div><div class="line">    os.makedirs(path)</div></pre></td></tr></table></figure><h2 id="R-amp-W-in-TXT"><a href="#R-amp-W-in-TXT" class="headerlink" title="R &amp; W in TXT"></a>R &amp; W in TXT</h2><p> <a href="https://blog.csdn.net/heyijia0327/article/details/42506063" target="_blank" rel="noopener">ref</a> </p><p><code>np.loadtxt()</code>用于从文本加载数据。 文本文件中的每一行必须含有相同的数据。</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loadtxt(fname, dtype=&lt;class 'float'&gt;, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0)</div></pre></td></tr></table></figure><ul><li>fname：文件名</li><li>dtype：数据类型，默认float</li><li>comments：注释</li><li>delimiter：分隔符，默认是空格</li><li>skiprows：跳过前几行，0是第一行，必须int</li><li><code>usecols</code>：要读取哪些列，0是第一列。例如，usecols = （1,4,5）将提取第2，第5和第6列。默认读取所有列。</li><li><code>unpack</code>如果为<code>True</code>，将分列读取。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'odom.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">    data = f.readlines()  <span class="comment">#txt中所有字符串读入data</span></div><div class="line"> </div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> data:</div><div class="line">        odom = line.split()        <span class="comment">#将单个数据分隔开存好</span></div><div class="line">        numbers_float = map(float, odom) <span class="comment">#转化为浮点数</span></div></pre></td></tr></table></figure><p>TXT追加内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">my_open = open(file_name, <span class="string">'a'</span>)</div><div class="line"><span class="comment">#打开fie_name路径下的my_infor.txt文件,采用追加模式</span></div><div class="line"><span class="comment">#若文件不存在,创建，若存在，追加</span></div><div class="line">my_open.write(<span class="string">'xx\n'</span>)</div><div class="line">my_open.close()</div></pre></td></tr></table></figure><h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><h3 id="文件移动"><a href="#文件移动" class="headerlink" title="文件移动"></a>文件移动</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> shutil</div><div class="line">shutil.move(src,des)</div></pre></td></tr></table></figure><h1 id="Print"><a href="#Print" class="headerlink" title="Print"></a>Print</h1><h2 id="pprint"><a href="#pprint" class="headerlink" title="pprint"></a>pprint</h2><p>it is used for pretty-print data structures.</p><h2 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h2><h3 id="np-random-rand"><a href="#np-random-rand" class="headerlink" title="np.random.rand()"></a>np.random.rand()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.random.rand(d0,d1,…,dn)</div></pre></td></tr></table></figure><ul><li>rand函数根据给定维度生成[0,1)之间的数据，包含0，不包含1</li><li>dn为每个维度</li><li>返回值为指定维度的数组</li></ul><h3 id="np-random-randn"><a href="#np-random-randn" class="headerlink" title="np.random.randn()"></a>np.random.randn()</h3><ul><li>randn函数返回具有标准正态分布的数据。</li><li>dn为每个维度</li><li>返回值为指定维度的数组</li></ul><h3 id="np-random-randint"><a href="#np-random-randint" class="headerlink" title="np.random.randint()"></a>np.random.randint()</h3><p><code>numpy.random.randint(low, high, size)</code></p><ul><li>返回开区间 <strong>[low, high)</strong>范围内的整数值</li><li>默认high是None,如果只有low，那范围就是[0,low)。如果有high，范围就是[low,high)</li><li>size是输出数组的维度（形状），可以是列表，或者元组</li></ul><h3 id="np-random-choice"><a href="#np-random-choice" class="headerlink" title="np.random.choice()"></a>np.random.choice()</h3><p><code>numpy.random.choice(a, size)</code></p><ul><li>a可以是一个数或者一个array，如果是一个数则sample范围是【0，a);如果是array，则从中sample</li><li>size为sample大小；如果size=(m,n,k)，则采样$m<em>n</em>k$个。</li></ul><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><h3 id="数字补0"><a href="#数字补0" class="headerlink" title="数字补0"></a>数字补0</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">n = <span class="number">123</span></div><div class="line">number = <span class="string">"%05d"</span>%n <span class="comment">### number = 00123</span></div></pre></td></tr></table></figure><h2 id="Plt"><a href="#Plt" class="headerlink" title="Plt"></a>Plt</h2><p>图片保存去除周边空白<a href="https://www.jianshu.com/p/5f4c71b3024d" target="_blank" rel="noopener">ref</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>)) </div><div class="line">p = plt.subplot(<span class="number">111</span>)</div><div class="line">plt.savefig(<span class="string">'image_name'</span>, ,bbox_inches=<span class="string">'tight'</span>)</div></pre></td></tr></table></figure><blockquote><p>bbox_inches =’tight’ 只能用于保存图片，不能用于显示。</p></blockquote><h2 id="Time"><a href="#Time" class="headerlink" title="Time"></a>Time</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> time</div><div class="line">start_time = time.time()</div></pre></td></tr></table></figure><h2 id="Zip"><a href="#Zip" class="headerlink" title="Zip"></a>Zip</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">name = [ <span class="string">"Manjeet"</span>, <span class="string">"Nikhil"</span>, <span class="string">"Shambhavi"</span>, <span class="string">"Astha"</span> ] </div><div class="line">roll_no = [ <span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span> ] </div><div class="line">marks = [ <span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span> ] </div><div class="line"><span class="comment"># using zip() to map values </span></div><div class="line">mapped = zip(name, roll_no, marks) </div><div class="line"><span class="comment"># converting values to print as set </span></div><div class="line">mapped = set(mapped)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The zipped result <span class="keyword">is</span> : &#123;(<span class="string">'Shambhavi'</span>, <span class="number">3</span>, <span class="number">60</span>), (<span class="string">'Astha'</span>, <span class="number">2</span>, <span class="number">70</span>), (<span class="string">'Manjeet'</span>, <span class="number">4</span>, <span class="number">40</span>), (<span class="string">'Nikhil'</span>, <span class="number">1</span>, <span class="number">50</span>)&#125;</div></pre></td></tr></table></figure><p><strong>How to unzip?</strong><br>Unzipping means converting the zipped values back to the individual self as they were. This is done with the help of “<strong>*</strong>” operator.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">name = [ <span class="string">"Manjeet"</span>, <span class="string">"Nikhil"</span>, <span class="string">"Shambhavi"</span>, <span class="string">"Astha"</span> ] </div><div class="line">roll_no = [ <span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span> ] </div><div class="line">marks = [ <span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span> ] </div><div class="line"><span class="comment"># using zip() to map values </span></div><div class="line">mapped = zip(name, roll_no, marks) </div><div class="line"><span class="comment"># converting values to print as list </span></div><div class="line">mapped = list(mapped) </div><div class="line"><span class="comment"># printing resultant values  </span></div><div class="line"><span class="keyword">print</span> (<span class="string">"The zipped result is : "</span>,end=<span class="string">""</span>) </div><div class="line"><span class="keyword">print</span> (mapped) </div><div class="line">print(<span class="string">"\n"</span>) </div><div class="line"><span class="comment"># unzipping values </span></div><div class="line">namz, roll_noz, marksz = zip(*mapped) </div><div class="line"><span class="keyword">print</span> (<span class="string">"The unzipped result: \n"</span>,end=<span class="string">""</span>) </div><div class="line"><span class="comment"># printing initial lists </span></div><div class="line"><span class="keyword">print</span> (<span class="string">"The name list is : "</span>,end=<span class="string">""</span>) </div><div class="line"><span class="keyword">print</span> (namz) </div><div class="line"><span class="keyword">print</span> (<span class="string">"The roll_no list is : "</span>,end=<span class="string">""</span>) </div><div class="line"><span class="keyword">print</span> (roll_noz) </div><div class="line"><span class="keyword">print</span> (<span class="string">"The marks list is : "</span>,end=<span class="string">""</span>) </div><div class="line"><span class="keyword">print</span> (marksz)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">The zipped result <span class="keyword">is</span> : [(<span class="string">'Manjeet'</span>, <span class="number">4</span>, <span class="number">40</span>), (<span class="string">'Nikhil'</span>, <span class="number">1</span>, <span class="number">50</span>), </div><div class="line">(<span class="string">'Shambhavi'</span>, <span class="number">3</span>, <span class="number">60</span>), (<span class="string">'Astha'</span>, <span class="number">2</span>, <span class="number">70</span>)]</div><div class="line">The unzipped result: </div><div class="line">The name list <span class="keyword">is</span> : (<span class="string">'Manjeet'</span>, <span class="string">'Nikhil'</span>, <span class="string">'Shambhavi'</span>, <span class="string">'Astha'</span>)</div><div class="line">The roll_no list <span class="keyword">is</span> : (<span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</div><div class="line">The marks list <span class="keyword">is</span> : (<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>, <span class="number">70</span>)</div></pre></td></tr></table></figure><h2 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> argparse</div><div class="line">    parser = argparse.ArgumentParser(description=<span class="string">"Process some parameters."</span>)</div><div class="line">    parser.add_argument(<span class="string">'--trainFeature'</span>, <span class="string">'-tf'</span>, type=int, required=<span class="keyword">True</span>, default=<span class="number">1</span>, help=<span class="string">'train feature'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--trainInitial'</span>, <span class="string">'-ti'</span>, type=int, required=<span class="keyword">True</span>, default=<span class="number">1</span>,help=<span class="string">'train initial'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--learningRate'</span>, <span class="string">'-lr'</span>, type=float, required=<span class="keyword">True</span>, default=<span class="number">0.00001</span>,help=<span class="string">'learning rate'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--criticNum'</span>, <span class="string">'-cn'</span>, type=int, required=<span class="keyword">True</span>, default=<span class="number">5</span>,help=<span class="string">'critic number'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--clipRange'</span>, <span class="string">'-cr'</span>, type=float, required=<span class="keyword">True</span>,default=<span class="number">0.01</span>, help=<span class="string">'clip range'</span>)</div><div class="line"></div><div class="line">    args = parser.parse_args()</div><div class="line">    opts = &#123;k:v <span class="keyword">for</span> k,v <span class="keyword">in</span> args._get_kwagrs()&#125;</div><div class="line">    n_critic = args.criticNum</div><div class="line">    train_initial = args.trainInitial</div><div class="line">    train_feature = args.trainFeature</div><div class="line">    lr = args.learningRate</div><div class="line">    clip = args.clipRange</div><div class="line"></div><div class="line">    dgan = StageTwo(n_critic,train_initial,train_feature,lr,clip)</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python xx.py -trainFeature 1 -trainInitial 1 -learningRate 0.001 -criticNum 5 -clipRange 0.01</div></pre></td></tr></table></figure><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p><a href="https://towardsdatascience.com/5-quick-and-easy-data-visualizations-in-python-with-code-a2284bae952f" target="_blank" rel="noopener">5 Quick and Easy Data Visualizations in Python with Code</a> </p><p><a href="https://mail.google.com/mail/u/0/#inbox/FMfcgxwBVWMvVdMrPPqPgrmqWvZFZxvh" target="_blank" rel="noopener">8 of the best articles on visualizing data</a> </p><h2 id="ElementTree"><a href="#ElementTree" class="headerlink" title="ElementTree"></a>ElementTree</h2><p>！！！！！所有的输入都必须是<strong>字符串</strong>！！！！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">indent</span><span class="params">(elem, level=<span class="number">0</span>)</span>:</span></div><div class="line">  i = <span class="string">"\n"</span> + level*<span class="string">"  "</span></div><div class="line">  <span class="keyword">if</span> len(elem):</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> elem.text <span class="keyword">or</span> <span class="keyword">not</span> elem.text.strip():</div><div class="line">      elem.text = i + <span class="string">"  "</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> elem.tail <span class="keyword">or</span> <span class="keyword">not</span> elem.tail.strip():</div><div class="line">      elem.tail = i</div><div class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> elem:</div><div class="line">      indent(elem, level+<span class="number">1</span>)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> elem.tail <span class="keyword">or</span> <span class="keyword">not</span> elem.tail.strip():</div><div class="line">      elem.tail = i</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    <span class="keyword">if</span> level <span class="keyword">and</span> (<span class="keyword">not</span> elem.tail <span class="keyword">or</span> <span class="keyword">not</span> elem.tail.strip()):</div><div class="line">      elem.tail = i</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createXML</span><span class="params">()</span>:</span></div><div class="line">    root =ET.Element(<span class="string">'annotation'</span>)</div><div class="line">    filename = ET.Element(<span class="string">'filename'</span>)</div><div class="line">    filename.text = <span class="string">'xx'</span></div><div class="line">    root.append(filename)</div><div class="line"></div><div class="line">    folder = ET.Element(<span class="string">'folder'</span>)</div><div class="line">    folder.text = <span class="string">'cad60/'</span></div><div class="line">    root.append(folder)</div><div class="line"></div><div class="line">    object = ET.Element(<span class="string">'object'</span>)</div><div class="line">    root.append(object)</div><div class="line"></div><div class="line">    name = ET.SubElement(object,<span class="string">'name'</span>)</div><div class="line">    name.text = <span class="string">'person'</span></div><div class="line">    action = ET.SubElement(object, <span class="string">'action'</span>)</div><div class="line">    action.text = <span class="string">'12'</span></div><div class="line">    bndbox = ET.SubElement(object,<span class="string">'bndbox'</span>)</div><div class="line">    xmax = ET.SubElement(bndbox,<span class="string">'xmax'</span>)</div><div class="line">    xmax.text = <span class="string">'1'</span></div><div class="line">    xmin = ET.SubElement(bndbox, <span class="string">'xmin'</span>)</div><div class="line">    xmin.text = <span class="string">'2'</span></div><div class="line">    ymax = ET.SubElement(bndbox, <span class="string">'ymax'</span>)</div><div class="line">    ymax.text = <span class="string">'3'</span></div><div class="line">    ymin = ET.SubElement(bndbox,<span class="string">'ymin'</span>)</div><div class="line">    ymin.text = <span class="string">'4'</span></div><div class="line"></div><div class="line">    size = ET.Element(<span class="string">'size'</span>)</div><div class="line">    root.append(size)</div><div class="line">    depth = ET.SubElement(size,<span class="string">'depth'</span>)</div><div class="line">    depth.text = <span class="string">'1'</span></div><div class="line">    height = ET.SubElement(size, <span class="string">'height'</span>)</div><div class="line">    height.text = <span class="string">'2'</span></div><div class="line">    width = ET.SubElement(size, <span class="string">'width'</span>)</div><div class="line">    width.text = <span class="string">'3'</span></div><div class="line"></div><div class="line">    indent(root)</div><div class="line"></div><div class="line">    tree = ET.ElementTree(root)</div><div class="line">    tree.write(<span class="string">'wq.xml'</span>,<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure><p><img src="/2018/08/09/Python-Sth/Screen Shot 2019-07-27 at 8.41.15 PM.png" alt="creen Shot 2019-07-27 at 8.41.15 P"></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Adversarial AutoEncoder</title>
      <link href="/2018/08/08/Adversarial-AutoEncoder/"/>
      <url>/2018/08/08/Adversarial-AutoEncoder/</url>
      <content type="html"><![CDATA[<h1 id="Adversarial-Autoencoder"><a href="#Adversarial-Autoencoder" class="headerlink" title="Adversarial Autoencoder"></a>Adversarial Autoencoder</h1><p><a href="https://duvenaud.github.io/learn-discrete/slides/AdversarialAutoencoders.pdf" target="_blank" rel="noopener">ref1</a> <a href="http://closure11.com/%E5%AF%B9%E6%8A%97%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%9Aadversarial-autoencoders/" target="_blank" rel="noopener">gocha</a> <a href="https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9" target="_blank" rel="noopener">good one</a>  </p><h2 id="无监督AAE"><a href="#无监督AAE" class="headerlink" title="无监督AAE"></a>无监督AAE</h2><p>autoencoder中的encoder的输出(潜在码)并不能在某一特定空间均匀分布，而且AE 的 G 只能保证将由 x 生成的 z 还原为 x。如果我们随机生成 1 个 z，经过 AE 的 G 后往往不会得到有效的图像。</p><p>所以我们希望encoder的输出可以服从某一分布，这个分布可以是正态分布，均匀分布等，这样就会使encoder的输出均匀分布在给定的先验分布，使decoder学习到一个先验分布到输入数据的映射(本例中就是学习到MNIST手写体数据的分布)，那么此时只需从这个先验分布采样出 z，就能通过 G 得到有效的图像。</p><p>假设你正在学习一门课程，如果你的老师并没有提供任何资料，你又会如何学习这门课呢？但是考试怎么办呢，难道要自己瞎答吗？这种情况就是类似我们的encoder的输出并不服从某种特定分布，这样decoder就无法学习到一个从任意数字到图片的映射。</p><p>但是如果你有了一个课程指导书，你就可以在考试之前复习这本书，这样就知道了可能的考试内容。类似的，如果我们的encoder输出服从一个已知分布，那么encoder就会将潜在码覆盖整个先验分布。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><img src="/2018/08/08/Adversarial-AutoEncoder/Screen-Shot-2016-10-13-at-11.17.21-AM-690x332.png" alt="Screen-Shot-2016-10-13-at-11.17.21-AM-690x332"></p><p><img src="/2018/08/08/Adversarial-AutoEncoder/Screen Shot 2018-08-08 at 9.33.36 PM.png" alt="Screen Shot 2018-08-08 at 9.33.36 PM"></p><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_nnf4UUq9Uuf2l19iCYaNfg.png" alt="1_nnf4UUq9Uuf2l19iCYaNfg"></p><ul><li>$x$是输入</li><li>$q(z|x)$是encoder基于输入$x$的输出</li><li>$z$是潜在码，同时也是一个假输入，从$q(z|x)$中采样得到</li><li>$z’$是采样自想要的分布，作为真实输入</li><li>$p(x|z)$是基于$z$的decoder输入</li><li>$x_$是重构图像</li></ul><p>我们的主要目的是迫使encoder的输出逼近一个先验分布（比如正态分布，gamma分布等）。我们将使用encoder$(q(z|x))$作为生成器，而判别器辨别它的输入是来自于一个先验分布$p(z)$，亦或是来自于encoder的输出$z$，decoder仍然进行图片重构的工作。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>AAE的训练分成两个部分：重构阶段和正则化阶段。</p><p>训练上述模型，分成两个阶段：一个是对辨别器的训练；另一个是对GAN模型的训练。</p><p>对于分辨器，其输入就是真假latent code，输出real概率</p><p>对于GAN模型，它需要配合分辨器来完成训练。encoder-decoder产生两个输出：一个是latent code，一个是image，但是我们只需要latent code作为分辨器的输入，从而完成GAN模型的训练。</p><h4 id="重构阶段"><a href="#重构阶段" class="headerlink" title="重构阶段"></a>重构阶段</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_DKPl7YOnX-8FJQuHAZop-g.png" alt="1_DKPl7YOnX-8FJQuHAZop-g"></p><p>在该阶段，我们需要训练encoder和decoder来最小化重构误差（输入图片与重构图片间的均方误差）。我们将输入传递给encoder，encoder输出一个潜在码；随后，我们将该潜在码送入decoder从而得到一张重构图像。</p><h4 id="正则化阶段"><a href="#正则化阶段" class="headerlink" title="正则化阶段"></a>正则化阶段</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1__pIXKcCCqJRNmIWTRymJzA.png" alt="1__pIXKcCCqJRNmIWTRymJzA"></p><p>在该阶段，我们训练生成器和辨别器，我们将encod的输出$z$和随机采样$z’$（来自于想要的分布）作为输入来训练辨别器，这样辨别器就会返回1如果输入是$z’$，而返回0如果输入是$z$。接下来，为了迫使encoder的输出$z$逼近我们想要的分布，我们将encoder的输出作为辨别器的输入，连接encoder和辨别器。</p><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_DoJESN2LvxpxNVYADRJXWw.png" alt="1_DoJESN2LvxpxNVYADRJXWw"></p><p>我们固定辨别器的权重参数，固定输入的目标标签为1，然后我们输入一些图像到encoder，并计算辨别器的输出与目标标签间的差异（使用交叉熵损失函数），这个阶段我们只更新encoder的权重参数，这样促使encoder去学习我们想要的分布，使输出服从这个分布。</p><h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><h4 id="Encoder构造"><a href="#Encoder构造" class="headerlink" title="Encoder构造"></a>Encoder构造</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_Hud7t2vLY2JIP3SXn4WTDA.png" alt="1_Hud7t2vLY2JIP3SXn4WTDA"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_encoder</span><span class="params">(self)</span>:</span></div><div class="line">    input_img = Input(self.input)</div><div class="line">    h = Flatten()(input_img)</div><div class="line">    h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">    h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">    mean = Dense(<span class="number">2</span>)(h)</div><div class="line">    logvar = Dense(<span class="number">2</span>)(h)</div><div class="line">    z = Lambda(self.sampling, output_shape=(self.latent_dim,))([mean, logvar])</div><div class="line">    encoder = Model(input_img,z)</div><div class="line">    encoder.summary()</div><div class="line">    <span class="keyword">return</span> encoder</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">(self, args)</span>:</span></div><div class="line">    z_mean, z_log_sigma = args</div><div class="line">    epsilon = K.random_normal(shape=(K.shape(z_mean)[<span class="number">0</span>], self.latent_dim), mean=<span class="number">0.</span>, stddev=<span class="number">1.0</span>)</div><div class="line">    <span class="keyword">return</span> z_mean + K.exp(z_log_sigma / <span class="number">2</span>) * epsilon</div></pre></td></tr></table></figure></div></div><h4 id="Decoder构造"><a href="#Decoder构造" class="headerlink" title="Decoder构造"></a>Decoder构造</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_0t7JrvUqyzg7AdQGDjZkRw.png" alt="1_0t7JrvUqyzg7AdQGDjZkRw"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_decoder</span><span class="params">(self)</span>:</span> <span class="comment"># ok</span></div><div class="line">    input_code = Input((self.latent_dim,))</div><div class="line">    h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(input_code)</div><div class="line">    h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">    h = Dense(<span class="number">784</span>,activation=<span class="string">'sigmoid'</span>)(h)</div><div class="line">    recon_img = Reshape(self.input)(h)</div><div class="line">    decoder = Model(input_code,recon_img)</div><div class="line">    decoder.summary()</div><div class="line">    <span class="keyword">return</span> decoder</div></pre></td></tr></table></figure></div></div><h4 id="Discriminator构造"><a href="#Discriminator构造" class="headerlink" title="Discriminator构造"></a>Discriminator构造</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_Df3_l66beZqsqRe5i6lZRw.png" alt="1_Df3_l66beZqsqRe5i6lZRw"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_discriminator</span><span class="params">(self)</span>:</span> <span class="comment"># ok</span></div><div class="line">    input_code = Input((self.latent_dim,))</div><div class="line">    h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(input_code)</div><div class="line">    h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">    valid = Dense(<span class="number">1</span>)(h)</div><div class="line">    discriminator = Model(input_code,valid)</div><div class="line">    discriminator.summary()</div><div class="line">    <span class="keyword">return</span> discriminator</div></pre></td></tr></table></figure></div></div><h4 id="GAN的构造与编译"><a href="#GAN的构造与编译" class="headerlink" title="GAN的构造与编译"></a>GAN的构造与编译</h4><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">aae</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.input = (<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</div><div class="line">        self.latent_dim = <span class="number">2</span></div><div class="line">        optimizer = Adam(lr=<span class="number">0.0002</span>,beta_1=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Build and compile the discriminator</span></div><div class="line">        self.discriminator = self.make_discriminator()</div><div class="line">        self.discriminator.compile(optimizer=optimizer,</div><div class="line">                                   loss=[<span class="string">'binary_crossentropy'</span>],</div><div class="line">                                   metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">        <span class="comment"># Build the encoder / decoder</span></div><div class="line">        self.encoder = self.make_encoder()</div><div class="line">        self.decoder = self.make_decoder()</div><div class="line">        image = Input(self.input)</div><div class="line">        latent_code = self.encoder(image)</div><div class="line">        recon_img = self.decoder(latent_code)</div><div class="line"></div><div class="line">        <span class="comment"># for the adversarial_autoencoder model, we only train the generator</span></div><div class="line">        self.discriminator.trainable = <span class="keyword">False</span></div><div class="line">        valid = self.discriminator(latent_code)</div><div class="line"></div><div class="line">        <span class="comment"># The adversarial_autoencoder model  (stacked generator and discriminator)</span></div><div class="line"></div><div class="line">        self.adversarial_autoencoder = Model(image,[recon_img,valid])</div><div class="line">        self.adversarial_autoencoder.compile(loss=[<span class="string">'mae'</span>,<span class="string">'binary_crossentropy'</span>],</div><div class="line">                                             loss_weights=[<span class="number">0.999</span>,<span class="number">0.001</span>],</div><div class="line">                                             optimizer=optimizer)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,epoches=<span class="number">1000</span>,batch_size=<span class="number">100</span>)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># Load the dataset</span></div><div class="line">    (X_train, _), (_, _) = mnist.load_data()</div><div class="line">    <span class="comment"># Configure input</span></div><div class="line">    X_train = (X_train.astype(np.float32) - <span class="number">127.5</span>) / <span class="number">127.5</span> <span class="comment"># pixel between [-1,1]</span></div><div class="line">    X_train = np.expand_dims(X_train, axis=<span class="number">3</span>) <span class="comment"># change shape from (60000,28,28) to (60000,28,28,1)</span></div><div class="line">    <span class="comment"># Adversarial ground truths</span></div><div class="line">    valid = np.ones((batch_size, <span class="number">1</span>))</div><div class="line">    fake = np.zeros((batch_size, <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,epoches+<span class="number">1</span>):</div><div class="line">        <span class="comment"># Select a random half batch of images</span></div><div class="line">        idx = np.random.randint(<span class="number">0</span>, X_train.shape[<span class="number">0</span>], batch_size)</div><div class="line">        imgs = X_train[idx]</div><div class="line"></div><div class="line">        latent_fake = self.encoder.predict(imgs)</div><div class="line">        latent_real = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (batch_size, <span class="number">2</span>))</div><div class="line"></div><div class="line">        <span class="comment"># train the discriminator</span></div><div class="line">        d_loss_real = self.discriminator.train_on_batch(latent_real, valid)</div><div class="line">        d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)</div><div class="line">        d_loss = <span class="number">0.5</span> * np.add(d_loss_real, d_loss_fake)</div><div class="line"></div><div class="line">        <span class="comment"># train the generator</span></div><div class="line">        g_loss = self.adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])</div><div class="line">        print(<span class="string">"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]"</span> % (</div><div class="line">        epoch, d_loss[<span class="number">0</span>], <span class="number">100</span> * d_loss[<span class="number">1</span>], g_loss[<span class="number">0</span>], g_loss[<span class="number">1</span>]))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    model = aae()</div><div class="line">    model.train()</div></pre></td></tr></table></figure></div></div><h2 id="监督AAE"><a href="#监督AAE" class="headerlink" title="监督AAE"></a>监督AAE</h2><p><a href="https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7" target="_blank" rel="noopener">Disentanglement of style and content</a> </p><p>对于一个写作主题，每一个人写出来的文章都有不同的内容（content）和字体（style）。对于MNIST字体，可以发现它的所有图像都有一样的style，所以我们想要从这个数据集中学习MNIST字体的style。为了更明确content和style的区别，我们有如下图：</p><p><img src="/2018/08/08/Adversarial-AutoEncoder/Screen Shot 2018-08-28 at 11.39.06 AM.png" alt="Screen Shot 2018-08-28 at 11.39.06 AM"></p><p>每个文本都有相同的content “Autoencoder”，但是它们的字体是不一样的，现在我们想要从图片中去区分style（Myriad Pro, MV Boil,…）和content，特征分离是表征学习(<a href="http://www.cl.uni-heidelberg.de/courses/ws14/deepl/BengioETAL12.pdf" target="_blank" rel="noopener">representation learning</a>)的一个重要内容。</p><p>Autoencoder和Adversarial Autoencoder都是无监督学习，因为在训练过程中我们并没有世人任何label信息，但是如果利用图片的label信息则会帮助AAE去提取style信息而不受图片content的影响，这样我们的AAE就变成了一个监督模型。</p><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_vGU0REkvre1DI7sFLU97_g.png" alt="1_vGU0REkvre1DI7sFLU97_g"></p><p>除了利用latent code作为decoder的输入，我们同时把标签<code>y</code>信息作为另一个输入，decoder利用这两个输入来生成图片。encoder学习图片的style，decoder利用该学习到的style和额外的内容信息<code>y</code>来重构输入图片</p><p>相比较于无监督AAE，唯一的区别就是decoder的输入：</p><ul><li>来自encoder的latent code</li><li>图像标签的独热表示</li></ul><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><h4 id="重构阶段-1"><a href="#重构阶段-1" class="headerlink" title="重构阶段"></a>重构阶段</h4><p>我们将输入图像送入encoder得到输出latent code<code>z</code>，然后将<code>z</code>和图像标签<code>y</code>串接起来成为一个更大的输入送入decoder。我们训练AE使最小化图片重构损失</p><h4 id="正则化阶段-1"><a href="#正则化阶段-1" class="headerlink" title="正则化阶段"></a>正则化阶段</h4><p>与无监督    AAE一样。</p><h3 id="Python实现-1"><a href="#Python实现-1" class="headerlink" title="Python实现"></a>Python实现</h3><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_lzIl05QPdy-aEtVvh-y1LQ.png" alt="1_lzIl05QPdy-aEtVvh-y1LQ"></p><p>MNIST的图像总共有10类，则<code>y</code>的独热向量长度就是10，laten code的长度是2，则decoder的输入长度是（10+2）</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Reshape, Flatten, Dropout, multiply</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> BatchNormalization, Embedding, Lambda,Concatenate</div><div class="line"><span class="keyword">from</span> keras.layers.advanced_activations <span class="keyword">import</span> LeakyReLU</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, Model</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">import</span> keras</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">aae</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.input = (<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</div><div class="line">        self.latent_dim = <span class="number">15</span></div><div class="line">        self.classes = <span class="number">10</span></div><div class="line">        optimizer = Adam(lr=<span class="number">0.0002</span>,beta_1=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Build and compile the discriminator</span></div><div class="line">        self.discriminator = self.make_discriminator()</div><div class="line">        self.discriminator.compile(optimizer=optimizer,</div><div class="line">                                   loss=[<span class="string">'binary_crossentropy'</span>],</div><div class="line">                                   metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">        <span class="comment"># Build the encoder / decoder</span></div><div class="line">        self.encoder = self.make_encoder()</div><div class="line">        self.decoder = self.make_decoder()</div><div class="line">        image = Input(self.input)</div><div class="line">        label = Input((self.classes,))</div><div class="line">        latent_code = self.encoder(image)</div><div class="line">        recon_img = self.decoder([label,latent_code])</div><div class="line"></div><div class="line">        <span class="comment"># for the adversarial_autoencoder model, we only train the generator</span></div><div class="line">        self.discriminator.trainable = <span class="keyword">False</span></div><div class="line">        valid = self.discriminator(latent_code)</div><div class="line"></div><div class="line">        <span class="comment"># The adversarial_autoencoder model  (stacked generator and discriminator)</span></div><div class="line"></div><div class="line">        self.adversarial_autoencoder = Model([image,label],[recon_img,valid])</div><div class="line">        self.adversarial_autoencoder.compile(loss=[<span class="string">'mse'</span>,<span class="string">'binary_crossentropy'</span>],</div><div class="line">                                             loss_weights=[<span class="number">0.999</span>,<span class="number">0.001</span>],</div><div class="line">                                             optimizer=optimizer)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">(self, args)</span>:</span></div><div class="line">        z_mean, z_log_sigma = args</div><div class="line">        epsilon = K.random_normal(shape=(K.shape(z_mean)[<span class="number">0</span>], self.latent_dim), mean=<span class="number">0.</span>, stddev=<span class="number">1.0</span>)</div><div class="line">        <span class="keyword">return</span> z_mean + K.exp(z_log_sigma / <span class="number">2</span>) * epsilon</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_encoder</span><span class="params">(self)</span>:</span></div><div class="line">        input_img = Input(self.input)</div><div class="line">        h = Flatten()(input_img)</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">        mean = Dense(self.latent_dim)(h)</div><div class="line">        logvar = Dense(self.latent_dim)(h)</div><div class="line">        z = Lambda(self.sampling, output_shape=(self.latent_dim,))([mean, logvar])</div><div class="line">        encoder = Model(input_img,z)</div><div class="line">        encoder.summary()</div><div class="line">        <span class="keyword">return</span> encoder</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_decoder</span><span class="params">(self)</span>:</span> <span class="comment"># ok</span></div><div class="line">        input_code = Input((self.latent_dim,))</div><div class="line">        input_label = Input((self.classes,))</div><div class="line">        combine_input = Concatenate(axis=<span class="number">-1</span>)([input_label, input_code])</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(combine_input)</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">        h = Dense(<span class="number">784</span>,activation=<span class="string">'sigmoid'</span>)(h)</div><div class="line">        recon_img = Reshape(self.input)(h)</div><div class="line">        decoder = Model([input_label,input_code],recon_img)</div><div class="line">        decoder.summary()</div><div class="line">        <span class="keyword">return</span> decoder</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_discriminator</span><span class="params">(self)</span>:</span> <span class="comment"># ok</span></div><div class="line">        input_code = Input((self.latent_dim,))</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(input_code)</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">        valid = Dense(<span class="number">1</span>)(h)</div><div class="line">        discriminator = Model(input_code,valid)</div><div class="line">        discriminator.summary()</div><div class="line">        <span class="keyword">return</span> discriminator</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,epoches=<span class="number">1000</span>,batch_size=<span class="number">100</span>)</span>:</span></div><div class="line"></div><div class="line">        <span class="comment"># Load the dataset</span></div><div class="line">        (X_train, y_train), (_, _) = mnist.load_data()</div><div class="line">        <span class="comment"># Configure input</span></div><div class="line">        X_train = (X_train.astype(np.float32) - <span class="number">127.5</span>) / <span class="number">127.5</span> <span class="comment"># pixel between [-1,1]</span></div><div class="line">        X_train = np.expand_dims(X_train, axis=<span class="number">3</span>) <span class="comment"># change shape from (60000,28,28) to (60000,28,28,1)</span></div><div class="line">        y_train = y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line">        <span class="comment"># Adversarial ground truths</span></div><div class="line">        valid = np.ones((batch_size, <span class="number">1</span>))</div><div class="line">        fake = np.zeros((batch_size, <span class="number">1</span>))</div><div class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>,epoches+<span class="number">1</span>):</div><div class="line">            <span class="comment"># Select a random half batch of images</span></div><div class="line">            idx = np.random.randint(<span class="number">0</span>, X_train.shape[<span class="number">0</span>], batch_size)</div><div class="line">            imgs = X_train[idx]</div><div class="line">            labels = y_train[idx]</div><div class="line">            labels = keras.utils.to_categorical(labels,self.classes)</div><div class="line"></div><div class="line">            latent_fake = self.encoder.predict(imgs)</div><div class="line">            latent_real = np.random.normal(<span class="number">0</span>, <span class="number">5.</span>, (batch_size, self.latent_dim))</div><div class="line"></div><div class="line">            <span class="comment"># train the discriminator</span></div><div class="line">            d_loss_real = self.discriminator.train_on_batch(latent_real, valid)</div><div class="line">            d_loss_fake = self.discriminator.train_on_batch(latent_fake, fake)</div><div class="line">            d_loss = <span class="number">0.5</span> * np.add(d_loss_real, d_loss_fake)</div><div class="line"></div><div class="line">            <span class="comment"># train the generator</span></div><div class="line">            g_loss = self.adversarial_autoencoder.train_on_batch([imgs,labels], [imgs, valid])</div><div class="line">            print(<span class="string">"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]"</span> % (</div><div class="line">            epoch, d_loss[<span class="number">0</span>], <span class="number">100</span> * d_loss[<span class="number">1</span>], g_loss[<span class="number">0</span>], g_loss[<span class="number">1</span>]))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    model = aae()</div><div class="line">    model.train()</div></pre></td></tr></table></figure></div></div><h2 id="GAN分类器"><a href="#GAN分类器" class="headerlink" title="GAN分类器"></a>GAN分类器</h2><p><a href="https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95" target="_blank" rel="noopener">ref</a> </p><p>本节我们将介绍如何利用encoder对MNIST手写体进行分类，并与传统的神经网络分类器(NN)比较，为保证实验公平性，encoder和NN使用相同的结构。</p><p>我们首先介绍传统的NN分类器，如下图所示，</p><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_OPtU8py5KBpbVUZKylGDPA.png" alt="1_OPtU8py5KBpbVUZKylGDPA"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"></div></div><p>那么我们如何将encoder改造成为一个分类器呢？实际上，encoder分类器不仅能够提升分类准确率，还可以减少数据维度，从图片中分离内容和风格，我们的模型如下：</p><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_8RuZ8kguLuosOGoDpiSgUQ.png" alt="1_8RuZ8kguLuosOGoDpiSgUQ"></p><p>可以看到，我们增加了额外的discriminator（$D_{cat}$），该分类器以对抗的方式与encoder一起训练，从而迫使encoder产生10维的独热分类向量</p><p>在AAE的基础上对encoder作了修改，此时encoder有两个输出：latent code（z）和classification（y），由于有10个类，则y为10维向量，而z的维度由用户决定。</p><h4 id="图片重构阶段"><a href="#图片重构阶段" class="headerlink" title="图片重构阶段"></a>图片重构阶段</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_onLoFTa8qcFMdgm9ILKDaw.png" alt="1_onLoFTa8qcFMdgm9ILKDaw"></p><p>该阶段我们欲使生成的图片逼近我们的真实图片，所以我们使用MSE（mean squared error）来衡量输入图片与输出图片间的差异。</p><h4 id="正则化阶段-2"><a href="#正则化阶段-2" class="headerlink" title="正则化阶段"></a>正则化阶段</h4><p>该阶段由两个部分组成：$D_{cat}$和$D_{gauss}$的训练</p><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_HKvMbwaXmDAg12GjgOC39Q.png" alt="1_HKvMbwaXmDAg12GjgOC39Q"></p><p>我们首先训练discriminator <strong>D_cat</strong>来辨别真实的分类标签$y^{‘}$和encoder生成的分类标签$y$。为此，我们将图片作为encoder的输入取产生$y$和$z$，然后将生成的$y$和真实的$y^{‘}$用于discriminator的训练。最后，我们固定分辨器的参数，并设置目标为1，训练encoder来欺骗分辨器。</p><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_qMGBvI2q14lNKGMrx_gMnw.png" alt="1_qMGBvI2q14lNKGMrx_gMnw"></p><p>同样的，为了生成具有高斯分布的latent code（z），我们还需要训练分辨器$D_{gauss}$。</p><h4 id="半监督分类器阶段"><a href="#半监督分类器阶段" class="headerlink" title="半监督分类器阶段"></a>半监督分类器阶段</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_ZS7wa7H8tHUDWkuqGZJT2Q.png" alt="1_ZS7wa7H8tHUDWkuqGZJT2Q"></p><p>最终我们训练encoder来对手写体数字进行分类，目的是最小化生成的分类标签与真实的标签的交叉熵。</p><h3 id="Python实现-2"><a href="#Python实现-2" class="headerlink" title="Python实现"></a>Python实现</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_qufKKgKUPUGvYKWsmqiv5w.png" alt="1_qufKKgKUPUGvYKWsmqiv5w"></p><p>在原始encoder的基础上，只需要需改encoder的输出维度，增加对分类标签的输出</p><h4 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_nRF013A75pCdlgOT48w0MA.png" alt="1_nRF013A75pCdlgOT48w0MA"></p><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p><img src="/2018/08/08/Adversarial-AutoEncoder/1_jzDB_IJhVL74Zb0NU03Uew.png" alt="1_jzDB_IJhVL74Zb0NU03Uew"></p><p>我们需要两个分辨器，它们除了输入维度不同，其他是一样的</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"></div></div><h2 id="CVAE-GAN"><a href="#CVAE-GAN" class="headerlink" title="CVAE-GAN"></a>CVAE-GAN</h2><p><a href="https://www.jiqizhixin.com/articles/2018-06-12-5" target="_blank" rel="noopener">REF1</a> </p><p>《 CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training》</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"></div></div>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2018/08/06/Linear-Regression/"/>
      <url>/2018/08/06/Linear-Regression/</url>
      <content type="html"><![CDATA[<a id="more"></a><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>线性回归模型，顾名思义就是线性模型求解回归问题。</p><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><p>【<strong>定义</strong>】给定具有$d$个属性的数据样本$x=\{x_1,x_2,…,x_d\}$，$x_i$是$x$在第$i$个属性上的取值，线性模型通过对属性的线性组合来进行预测的函数：</p><script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+...+w_dx_d+b</script><p>向量形式表示为：</p><script type="math/tex; mode=display">f(x)=w^Tx+b</script><p>其中$w=(w_1;w_2;…;w_d)$，只要确定了$w$和$b$，模型就得以确定。</p><blockquote><p>向量都表示成竖直的一条直线。</p></blockquote><h3 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h3><p>回归属于监督学习的范畴，用于预测输入变量与输出变量的关系。其本质就是数据拟合，选择一条函数曲线使其很好地拟合已知的数据，同时能够预测未知的数据。</p><p>按照输入变量的属性个数，分为一元回归和多元回归；按照输入变量和输出变量之间的映射关系，分为线性回归和非线性回归。</p><h3 id="线性回归-1"><a href="#线性回归-1" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性模型描述的是属性间的组合关系，而回归问题求解的是输入与输出的关系。</p><p>即使用一个线性函数去建模输入变量属性间的线性关系，从而发现输入变量与输出变量的关系。给定数据集$D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}$，其中$x_i=(x_i1;x_i2;…;x_id)$。</p><p><strong>问题描述</strong>：线性回归试图学习一个线性模型$f(x_i)=w^Tx_i+b$，使$f(x_i)\approx y_i$。</p><blockquote><p>又称为多元线性回归，或者多变量线性回归。</p></blockquote><h2 id="算法求解"><a href="#算法求解" class="headerlink" title="算法求解"></a>算法求解</h2><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>只要确定了权重$w$和偏差$b$的值，那么我们就可以得到模型了，而我们想让预测值$f(x_i)$无限接近真实值$y_i$，所以使用均方误差作为性能度量，即我们试图让均方差最小：</p><script type="math/tex; mode=display">Loss(w^*,b^*)=\sum_{i=1}^{m}(f(x_i)-y_i)^2</script><p>均方误差对应了常用的欧几里得距离，基于均方误差最小化进行模型求解的方法称为“最小二乘法”，实际上，最小二乘法就是试图找到一条直线，使样本到直线的欧氏距离之和最小。</p><blockquote><ul><li>关于为什么使用均方误差作为损失度量，一个是因为其本质是最大似然法，<a href="https://blog.csdn.net/saltriver/article/details/57544704" target="_blank" rel="noopener">ref</a> </li><li>常用损失函数对比<a href="https://www.jiqizhixin.com/articles/2018-06-21-3" target="_blank" rel="noopener">ref2</a> </li></ul></blockquote><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><h4 id="Loss函数求导取极值"><a href="#Loss函数求导取极值" class="headerlink" title="Loss函数求导取极值"></a>Loss函数求导取极值</h4><p>我们可以对损失函数$Loss$求导，并令导数为零来求得最优参数：</p><script type="math/tex; mode=display">\begin{cases} w=\frac{\sum_{i=1}^{n}y_i(x_i-\bar{x})}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}(\sum_{i=1}^{n}x_i)^2}\\b=\frac{1}{m}\sum_{i=1}^{n}(y_i-wx_i)    \end{cases}</script><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p><strong>公式推导</strong></p><ol><li><p>偏导</p><script type="math/tex; mode=display">\begin{cases} \frac{\partial{}E}{\partial{w}}=2[w\sum_{i=1}^{n}x_{i}^{2}-\sum_{i=1}^{n}(y_i-b)x_i]\\\frac{\partial{}E}{\partial{b}}=2[mb-\sum^{n}_{i=1}(y_i-wx_i)]    \end{cases}</script></li><li><p>求解b，$\frac{\partial{}E}{\partial{b}}=0$</p><script type="math/tex; mode=display">\begin{align*}mb&=\sum^{n}_{i=1}(y_i-wx_i)\\&=(y_1-wx_1)+(y_2-wx_2)+...+(y_n-wx_n)=(y_1+y_2+...+y_n)-w(x_1+x_2+...+x_n)\\&=\sum{y_i}-w\sum{x_i}\end{align*}</script><p>则$b=\frac{1}{n}\sum_{i=1}^{n}(y_i-wx_i)=\bar{y}-w\bar{x}$</p></li><li><p>求解w，$\frac{\partial{}E}{\partial{w}}=0$</p><script type="math/tex; mode=display">\begin{align*}w\sum_{i=1}^{n}x_{i}^{2}&=\sum_{i=1}^{n}(y_i-\bar{y}+w\bar{x})x_i\\&=(y_1-\bar{y}+w\bar{x})x_1+(y_2-\bar{y}+w\bar{x})x_2+...+(x_n-\bar{y}+w\bar{x})x_n\\&=(y_1x_1+y_2x_2+...+y_nx_n)-\bar{y}(x_1+x_2+...+x_n)+w\bar{x}(x_1+x_2+...+x_n)\\&=\sum_{i=1}^{n}x_iy_i-\bar{y}\sum_{i=1}^{n}x_i+w\bar{x}\sum_{i=1}^{n}x_i\\&=\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}+w\bar{x}n\bar{x}\end{align*}</script><p>则$w(\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^2)=\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}$，故</p><script type="math/tex; mode=display">w=\frac{\sum_{i=1}^{n}x_iy_i-n\bar{y}\bar{x}}{\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^2}</script></li></ol></div></div><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>损失函数是：</p><script type="math/tex; mode=display">\begin{align*}Loss(w,b)&=\sum_{i=1}^{n}(f(x_i)-y_i)^2\\&=\sum_{i=1}^{n}(wx_i+b-y_i)^2\end{align*}</script><p>参数更新：</p><script type="math/tex; mode=display">\begin{cases} w=w-\alpha(f(x)-y){x}\\b=b-\alpha(f(x)-y)    \end{cases}</script><blockquote><script type="math/tex; mode=display">\begin{align*}\frac{\partial{E}}{\partial{w}}&=\frac{\partial{}}{\partial{w}}{\frac{1}{2}(f(x)-y)^2}\\&=(f(x)-y)\frac{\partial}{\partial{w}}(wx+b-y)\\&=(f(x)-y)x\end{align*}</script></blockquote><p>即：</p><script type="math/tex; mode=display">w_i=w_i-\alpha\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)\cdot x_i</script><p>此时，$b=w_0$，$x_0=1$。</p><h4 id="正则化的损失函数"><a href="#正则化的损失函数" class="headerlink" title="正则化的损失函数"></a>正则化的损失函数</h4><p>此时目标函数为：</p><script type="math/tex; mode=display">\begin{align*}E(w,b)&=\sum_{i=1}^{n}(wx_i+b-y_i)^2+\lambda\sum_{i=1}^{n}w_{i}^{2}\end{align*}</script><blockquote><p>$w_0$不需要正则化</p></blockquote><p>此时梯度下降法为：</p><script type="math/tex; mode=display">w_0=w_0-\alpha\frac{1}{n}\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_0^{(i)}\\w_j=w_j-\alpha[\frac{1}{n}\sum_{i=1}^{n}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}-\frac{\lambda}{n}w_j]</script><h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p><strong>Feature Scaling</strong></p><p>It involves <strong>dividing the input values by the range</strong> (i.e. the maximum value minus the minimum value) of the input variable, resulting in <strong>a new range of just 1</strong>.</p><p><strong>Mean normalization</strong> </p><p>This involves <strong>subtracting the average value</strong> for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. </p><p>By combining the two techniques and adjust the input values as shown in the following formula:</p><script type="math/tex; mode=display">x_i=\frac{x_i-\mu_i}{s_i}</script><p>where $\mu_i$ is the average of all the values for feature $(i)$ and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.</p><h1 id="Convergence-Figure"><a href="#Convergence-Figure" class="headerlink" title="Convergence Figure"></a>Convergence Figure</h1><p>In order to make sure that our algorithm runs correctly, we need to debug gradient descent. Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent. If $J(\theta)$ ever increases, then you probably need to decrease learning rate $\alpha$.</p><p><strong>Summary</strong></p><ul><li><p>If $\alpha$ is too small, it could result in slow convergence.</p></li><li><p>If $\alpha$ is too large, it may not decrease on every iteration and thus may not converge, like the following pic:</p></li></ul><p><img src="/2018/08/06/Linear-Regression/Screen Shot 2018-08-06 at 12.58.12 PM.png" alt="Screen Shot 2018-08-06 at 12.58.12 PM"></p><p><strong>Example</strong></p><p><img src="/2018/08/06/Linear-Regression/Screen Shot 2018-08-06 at 12.54.04 PM.png" alt="Screen Shot 2018-08-06 at 12.54.04 PM"></p><blockquote><p>A is $\alpha=0.1$, B is $\alpha=0.01$, A is $\alpha=1$.</p><p>In graph C, the cost function is increasing, so the learning rate is set too high. Both graphs A and B converge to an optimum of the cost function, but graph B does so very slowly, so its learning rate is set too low. Graph A lies between the two.</p></blockquote><h1 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h1><p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p><p> We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p><p><img src="/2018/08/06/Linear-Regression/Screen Shot 2018-08-07 at 12.18.01 AM.png" alt="Screen Shot 2018-08-07 at 12.18.01 AM"></p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><h2 id="线性回归预测一维数据"><a href="#线性回归预测一维数据" class="headerlink" title="线性回归预测一维数据"></a>线性回归预测一维数据</h2><p>来自Andrew Ng第二周的课程练习，给出城市人口及该市的商店利润，预测如何进行店铺扩展，即选择哪座城市开分店？给定的数据只有一个人口特征及利润目标值。</p><p>第一步就是进行<strong>数据的可视化</strong>。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">()</span>:</span></div><div class="line">    data = np.loadtxt(<span class="string">'./dataset/ex1data1.txt'</span>, delimiter=<span class="string">','</span>)</div><div class="line">    x = data[:, <span class="number">0</span>]</div><div class="line">    y = data[:, <span class="number">1</span>]</div><div class="line">    plt.scatter(x, y, marker=<span class="string">'x'</span>)</div><div class="line">    plt.xlabel(<span class="string">"Population of City in 10,000s"</span>)</div><div class="line">    plt.ylabel(<span class="string">"Profit in $10,000s"</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/08/06/Linear-Regression/image-20180815174124083.png" alt="image-20180815174124083"></p></div></div><p>第二步：数据处理及参数初始化</p><p>我们为数据增加一列全一</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">length = x.shape[<span class="number">0</span>]</div><div class="line">z = np.ones(length)</div><div class="line">x = np.array(list(zip(z,x)))  <span class="comment"># add a column of ones to x</span></div><div class="line">theta = np.zeros([x.shape[<span class="number">1</span>],<span class="number">1</span>]) <span class="comment"># initialize fitting parameters</span></div><div class="line">alpha = <span class="number">0.01</span></div><div class="line">iters = <span class="number">1500</span></div></pre></td></tr></table></figure></div></div><p>第三步：损失函数</p><p>我们使用公式</p><p><img src="/2018/08/06/Linear-Regression/Screen Shot 2018-08-15 at 6.19.39 PM.png" alt="Screen Shot 2018-08-15 at 6.19.39 PM"></p><p>初次调用，返回值是32.07。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span><span class="params">(x,y,theta)</span>:</span></div><div class="line">    y_ = x.dot(theta)</div><div class="line">    loss = (y-y_)**<span class="number">2</span></div><div class="line">    cost = sum(loss) / (<span class="number">2</span>*length)</div><div class="line">    <span class="keyword">return</span> cost</div></pre></td></tr></table></figure></div></div><p>第四步：梯度下降法</p><p>检查梯度下降法是否正确的一个手段：查看损失函数是否在逐步减小。我们使用梯度更新公式：</p><p><img src="/2018/08/06/Linear-Regression/Screen Shot 2018-08-15 at 6.26.20 PM.png" alt="Screen Shot 2018-08-15 at 6.26.20 PM"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(x,y,theta,alpha,iters)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</div><div class="line">        <span class="comment"># cost_function(x,y,theta)</span></div><div class="line">        y_ = x.dot(theta)</div><div class="line">        loss = (y_ - y)</div><div class="line">        gd = loss.T.dot(x) <span class="comment"># shape = (1,2)</span></div><div class="line">        gd = alpha * gd.T / length</div><div class="line">        theta = theta - gd</div><div class="line">    <span class="keyword">return</span> theta</div></pre></td></tr></table></figure><blockquote><p>需要注意的一个点是：loss = ( y_pred - y )。反过来的话，会出现loss趋于无穷大。这个是对loss函数求导得到的，无论$(h_\theta(x)-y )^2$还是$(y-h_\theta(x) )^2$，都是一样的。</p></blockquote></div></div><p>第五步：可视化拟合曲线</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span><span class="params">(x,y,theta)</span>:</span></div><div class="line">    plt.scatter(x, y, marker=<span class="string">'x'</span>,label=<span class="string">'Training data'</span>)</div><div class="line">    plt.xlabel(<span class="string">"Population of City in 10,000s"</span>)</div><div class="line">    plt.ylabel(<span class="string">"Profit in $10,000s"</span>)</div><div class="line">    x_ = np.linspace(min(x),max(x),<span class="number">1000</span>)</div><div class="line">    y_ = x_*theta[<span class="number">1</span>] + theta[<span class="number">0</span>]</div><div class="line">    plt.plot(x_,y_,label=<span class="string">'Linear regression'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/08/06/Linear-Regression/image-20180816163328931.png" alt="image-20180816163328931"></p></div></div><h2 id="线性回归预测多特征数据"><a href="#线性回归预测多特征数据" class="headerlink" title="线性回归预测多特征数据"></a>线性回归预测多特征数据</h2><p>我们使用线性回归预测房价，数据集有两个特征：第一列是房屋面积(单位：$feet^2$)，第二列是房间数，最后一列是房价。</p><h3 id="特征正则化"><a href="#特征正则化" class="headerlink" title="特征正则化"></a>特征正则化</h3><p>发现两个特征数据范围相差特别大，所以需要对数据进行正则化，这样可以加快梯度下降法的收敛。</p><ul><li>减去均值</li><li>除以标准差</li></ul><p>正则化之后，别忘记加一个全一列。</p><p><strong>注意：</strong>记得存储mean和std的值，这样我们在预测位置数据的时候，第一步就是使用mean和std对未知数据做同样的处理。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_norm</span><span class="params">(x)</span>:</span></div><div class="line">    mean = np.mean(x,axis=<span class="number">0</span>)</div><div class="line">    std = np.std(x,axis=<span class="number">0</span>)</div><div class="line">    x = (x-mean)/std</div><div class="line">    ones = np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>))</div><div class="line">    x = np.concatenate([ones, x], axis=<span class="number">1</span>)</div><div class="line">    <span class="keyword">return</span> x,mean,std</div></pre></td></tr></table></figure></div></div><h3 id="梯度下降法-1"><a href="#梯度下降法-1" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>先实现损失函数</p><p><img src="/2018/08/06/Linear-Regression/Screen Shot 2018-08-15 at 6.19.39 PM.png" alt="Screen Shot 2018-08-15 at 6.19.39 PM"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_func</span><span class="params">(x,y,thetas)</span>:</span></div><div class="line">    y_pred = x.dot(thetas)</div><div class="line">    loss = sum((y_pred-y)**<span class="number">2</span>)/(<span class="number">2</span>*x.shape[<span class="number">0</span>])</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure></div></div><p>再实现梯度下降法，损失函数都是10次方。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_dec</span><span class="params">(x,y,thetas,lr,iter)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iter):</div><div class="line">        y_pred = x.dot(thetas)</div><div class="line">        gd = (y_pred - y).T.dot(x)</div><div class="line">        thetas -= lr * gd.T / x.shape[<span class="number">0</span>]</div><div class="line">        loss = cost_func(x, y, thetas)</div><div class="line">        print(loss)</div><div class="line">    <span class="keyword">return</span> thetas</div></pre></td></tr></table></figure></div></div>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Regression </tag>
            
            <tag> Linear Regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux命令</title>
      <link href="/2018/08/05/Linux%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/08/05/Linux%E5%91%BD%E4%BB%A4/</url>
      <content type="html"><![CDATA[<h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><p>退出保存</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">:wq</div></pre></td></tr></table></figure><p>利用scp远程上传下载文件/文件夹<a href="https://www.cnblogs.com/zhaofeng555/p/8075279.html" target="_blank" rel="noopener">ref1</a> </p><p>统计文件个数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ls -l |grep "^-"|wc -l #统计当前目录下文件数量</div><div class="line">ls -l |grep "^ｄ"|wc -l #统计当前目录下文件夹数量</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp -r dir1/. /dir2/</div></pre></td></tr></table></figure><p>设置屏幕分辨率</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xrandr -s 1366x768</div></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><ul><li><p>删除目录下所有文件</p><p>用通配符*英文星号可以表示“所有文件”这个概念，所以删除文件夹下所有文件的方法就是，先用cd命令切换到这个文件夹下，然后执行rm ./*命令表示删除当前目录下所有的文件，但是注意，如果文件夹下有子目录，这条命令就无法生效了，因为它无法删除子目录（删除子目录要加上-r选项）。</p></li></ul>]]></content>
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Training Techiniques</title>
      <link href="/2018/08/03/Training-Techiniques/"/>
      <url>/2018/08/03/Training-Techiniques/</url>
      <content type="html"><![CDATA[<p>Techniques for training a Deep Learning model.</p><a id="more"></a><h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-17 at 3.37.19 PM.png" alt="creen Shot 2019-02-17 at 3.37.19 P"></p><p>Say we have a binary classification problem where we want to draw a line to separate these red points from these blue points, like the following picture. On the left, thses data points are not normalized and not centered and far away from the origion, although we can still use a line to seperate them, if that line wiggles just a little bit, then our classification is going to get totally destroyed. That kind of means that in the example on the left, the loss function is now extremely sensitive to small perturbations in that linear classifier in our weight matrix. We can still represent the same functions, but that might make learning quite difficult.</p><p>On the right situation, if you take the data cloud and move it into the origin and you make it unit variance, then now again, we can still classfiy that data quite well, but now as we wiggle that line a little bit, our loss function is less sensitive to small perturbations in the parameter values. That maybe makes optimization a little bit easier.</p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-23 at 3.01.12 PM.png" alt="creen Shot 2019-02-23 at 3.01.12 P"></p><h1 id="Soft-labels"><a href="#Soft-labels" class="headerlink" title="Soft labels"></a>Soft labels</h1><p>This is extremely important when training the discriminator. Having hard labels (1 or 0) nearly killed all learning early on, leading the discriminator to approach 0 loss very rapidly. I ended up using a random number between 0 and 0.1 to represent 0 labels (real images) and a random number between 0.9 and 1.0 to represent 1 labels (generated images). This is not required when training the generator. <a href="https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9" target="_blank" rel="noopener">resource</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">valid = np.random.uniform(<span class="number">0.9</span>,<span class="number">1.0</span>,size=(batch_size,<span class="number">1</span>))</div><div class="line">fake = np.random.uniform(<span class="number">0</span>,<span class="number">0.1</span>,size=(batch_size,<span class="number">1</span>))</div></pre></td></tr></table></figure><h1 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h1><p>Loss not going down: learning rate is too low</p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 9.52.33 AM.png" alt="creen Shot 2019-02-21 at 9.52.33 A"></p><p>Loss exploding: learning rate too high</p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 9.53.03 AM.png" alt="creen Shot 2019-02-21 at 9.53.03 A"></p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 10.01.01 AM.png" alt="creen Shot 2019-02-21 at 10.01.01 A"></p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 10.01.40 AM.png" alt="creen Shot 2019-02-21 at 10.01.40 A"></p><h1 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h1><h2 id="Problem-with-initializing-all-weights-to-0"><a href="#Problem-with-initializing-all-weights-to-0" class="headerlink" title="Problem with initializing all weights to 0"></a>Problem with initializing all weights to 0</h2><p>In this case, the equations of the learning algorithm would fail to make any changes to the network weights, and the model will be stuck. It is important to note that the bias weight in each neuron is set to zero by default, not a small random value. <a href="https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/" target="_blank" rel="noopener">ref</a> </p><p>During forward propagation each unit in hidden layer gets signal:</p><p><img src="/2018/08/03/Training-Techiniques/gif.gif" alt="i"></p><p>That is, each hidden unit gets sum of inputs multiplied by the corresponding weight.</p><p>Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, <strong>each hidden unit will get exactly the same signal</strong>. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs <code>sigmoid(sum(inputs))</code>). If all weights are zeros, which is even worse, every hidden unit will get zero signal. <strong>No matter what was the input - if all weights are the same, all units in hidden layer will be the same too</strong>.</p><p>This is the main issue with symmetry and reason why you should initialize weights randomly (or, at least, with different values). Note, that this issue affects all architectures that use each-to-each connections. <a href="https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers" target="_blank" rel="noopener">ref</a></p><h2 id="Problems-with-initializing-weights-randomly-ref"><a href="#Problems-with-initializing-weights-randomly-ref" class="headerlink" title="Problems with initializing weights randomly ref"></a>Problems with initializing weights randomly <a href="https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" target="_blank" rel="noopener">ref</a></h2><h3 id="Vanishing-gradients"><a href="#Vanishing-gradients" class="headerlink" title="Vanishing gradients"></a>Vanishing gradients</h3><p>If weights are initialized with low values it gets mapped to 0, then the activation value would be small, say, almost 0. When backpropogating gradients, samll gradients times small weights, it will result in smaller gradients, meaning the earlier layers, the samller gradients, resulting vanishing gradients.</p><h3 id="Exploding-gradients"><a href="#Exploding-gradients" class="headerlink" title="Exploding gradients"></a>Exploding gradients</h3><p>Consider you have non-negative and large weights and small activations A (as can be the case for sigmoid(z)). When these weights are multiplied along the layers, they cause a large change in the cost. Thus, the gradients are also going to be large. This means that the changes in W, by <code>W — ⍺ * dW,</code> will be in huge steps, the downward moment will increase.</p><blockquote><p>This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn!</p></blockquote><p>Another impact of exploding gradients is that huge values of the gradients may cause number overflow resulting in incorrect computations or introductions of NaN’s. This might also lead to the loss taking the value NaN.</p><h2 id="New-Initialization-techniques-ref"><a href="#New-Initialization-techniques-ref" class="headerlink" title="New Initialization techniques ref"></a>New Initialization techniques <a href="https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78" target="_blank" rel="noopener">ref</a></h2><h3 id="Xavier-initialization"><a href="#Xavier-initialization" class="headerlink" title="Xavier initialization"></a>Xavier initialization</h3><p>It is used when we use tanh as our activation function.</p><p><img src="/2018/08/03/Training-Techiniques/1_Lv9TNpAXffRnO0p0WMGJwQ.png" alt="_Lv9TNpAXffRnO0p0WMGJw"></p><p>Some also use the following as initialization:</p><p><img src="/2018/08/03/Training-Techiniques/1_QIzXjH8uefVbcaycsjfdmw.png" alt="_QIzXjH8uefVbcaycsjfdm"></p><h3 id="He-initialization"><a href="#He-initialization" class="headerlink" title="He initialization"></a>He initialization</h3><p>When we use relu as activation, we just simply multiply random initialization with:</p><p><img src="/2018/08/03/Training-Techiniques/1_zxD6Nr6TyAb8JEG6oXAjkg.png" alt="_zxD6Nr6TyAb8JEG6oXAjk"></p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-21 at 10.12.09 AM-0765570.png" alt="creen Shot 2019-02-21 at 10.12.09 AM-076557"></p><h1 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p><p>The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.</p><p>It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension. [cs231n]</p><p>Usually, in order to train a neural network, we do some preprocessing to the input data. For example, we could normalize all data (whitening) so that it resembles a normal distribution (that means, zero mean and a unitary variance). Why do we do this preprocessing? Well, there are many reasons for that, some of them being: preventing the early saturation of non-linear activation functions like the sigmoid function, assuring that all input data is in the same range of values, etc.</p><p>But the problem appears in the intermediate layers because the distribution of the activations is constantly changing during training. This slows down the training process because each layer must learn to adapt themselves to a new distribution in every training step. This problem is known as <strong>internal covariate shift</strong>.</p><p>So… what happens if we force the input of every layer to have approximately the same distribution in every training step? <a href="https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad" target="_blank" rel="noopener">ref</a> </p><p>BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong> <a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">ref</a> </p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>Batch normalization is a method we can use to normalize the inputs of each layer, in order to fight the internal covariate shift problem.</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180405225246905-37854887.png" alt="192699-20180405225246905-3785488"></p><p>假设某个隐层神经元原先的激活输入x取值符合正态分布，正态分布均值是-2，方差是0.5，对应上图中最左端的浅蓝色曲线，通过BN后转换为均值为0，方差是1的正态分布（对应上图中的深蓝色图形），意味着什么，意味着输入x的取值正态分布整体右移2（均值的变化），图形曲线更平缓了（方差增大的变化）。这个图的意思是，BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差为1的正态分布通过平移均值压缩或者扩大曲线尖锐程度，调整为均值为0方差为1的正态分布。</p><p>　　那么把激活输入x调整到这个正态分布有什么用？首先我们看下均值为0，方差为1的标准正态分布代表什么含义：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180405225314624-527885612.png" alt="192699-20180405225314624-52788561"></p><p>这意味着在一个标准差范围内，也就是说64%的概率x其值落在[-1,1]的范围内，在两个标准差范围内，也就是说95%的概率x其值落在了[-2,2]的范围内。那么这又意味着什么？我们知道，激活值x=WU+B,U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid，那么看下sigmoid(x)其图形：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143109455-1460017374.png" alt="192699-20180407143109455-146001737"></p><p>及sigmoid(x)的导数为：G’=f(x)*(1-f(x))，因为f(x)=sigmoid(x)在0到1之间，所以G’在0到0.25之间，其对应的图如下：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142351924-124461667.png" alt="192699-20180407142351924-12446166"></p><p>假设没有经过BN调整前x的原先正态分布均值是-6，方差是1，那么意味着95%的值落在了[-8,-4]之间，那么对应的Sigmoid（x）函数的值明显接近于0，这是典型的梯度饱和区，在这个区域里梯度变化很慢，为什么是梯度饱和区？请看下sigmoid(x)如果取值接近0或者接近于1的时候对应导数函数取值，接近于0，意味着梯度变化很小甚至消失。而假设经过BN后，均值是0，方差是1，那么意味着95%的x值落在了[-2,2]区间内，很明显这一段是sigmoid(x)函数接近于线性变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也即是梯度变化较大，对应导数函数图中明显大于0的区域，就是梯度非饱和区。</p><p>　　从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？就是说<strong>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p><p>　　但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的<strong>表达能力</strong>下降了，这也意味着深度的意义就没有了。<strong>所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)</strong>，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。</p><p><strong>训练阶段如何做BatchNorm</strong></p><p>上面是对BN的抽象分析和解释，具体在Mini-Batch SGD下做BN怎么做？其实论文里面这块写得很清楚也容易理解。为了保证这篇文章完整性，这里简单说明下。</p><p>　　假设对于一个深层神经网络来说，其中两层结构如下：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180405213859690-1933561230.png" alt="192699-20180405213859690-193356123"></p><p>要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180405213955224-1791925244.png" alt="192699-20180405213955224-179192524"></p><p>对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142802238-1209499294.png" alt="192699-20180407142802238-120949929"></p><p>要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p><p>　　上文说过经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。</strong>但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142923190-79595046.png" alt="192699-20180407142923190-7959504"></p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407142956288-903484055.png" alt="192699-20180407142956288-90348405"></p><p>Look at the last line of the algorithm. After normalizing the input <code>x</code> the result is squashed through a linear function with parameters <code>gamma</code> and <code>beta</code>. These are learnable parameters of the BatchNorm Layer and make it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.” If <code>gamma = sqrt(var(x))</code> and <code>beta = mean(x)</code>, the original activation is restored. This is, what makes BatchNorm really powerful. We initialize the BatchNorm Parameters to transform the input to zero mean/unit variance distributions but during training they can learn that any other distribution might be better. </p><p>Btw: it’s called “Batch” Normalization because we perform this transformation and calculate the statistics only for a subpart (a batch) of the entire training set. <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">source</a> </p><p><strong>BatchNorm的推理(Inference)过程</strong></p><p>BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p><p>　　既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p><p>　　决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143405654-1995556833.png" alt="192699-20180407143405654-199555683"></p><p>有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143658338-63450857.png" alt="192699-20180407143658338-6345085"></p><p>这个公式其实和训练时</p><p><img src="/2018/08/03/Training-Techiniques/1192699-20180407143807788-1841864822.png" alt="192699-20180407143807788-184186482"></p><p>是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？我猜作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：</p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-20 at 4.09.16 PM.png" alt="creen Shot 2019-02-20 at 4.09.16 P"></p><p>都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p><p><strong>BatchNorm的好处</strong></p><p>BatchNorm为什么NB呢，关键还是效果好。<strong>①**</strong>不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。**总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h3><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Forward pass for batch normalization.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></div><div class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></div><div class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></div><div class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></div><div class="line"><span class="string">    data at test-time.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></div><div class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></div><div class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></div><div class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></div><div class="line"><span class="string">    large number of training images rather than using a running average. For</span></div><div class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></div><div class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></div><div class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Input:</span></div><div class="line"><span class="string">    - x: Data of shape (N, D)</span></div><div class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></div><div class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></div><div class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></div><div class="line"><span class="string">      - mode: 'train' or 'test'; required</span></div><div class="line"><span class="string">      - eps: Constant for numeric stability</span></div><div class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></div><div class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></div><div class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - out: of shape (N, D)</span></div><div class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></div><div class="line"><span class="string">    """</span></div><div class="line">    mode = bn_param[<span class="string">'mode'</span>]</div><div class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</div><div class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</div><div class="line"></div><div class="line">    N, D = x.shape</div><div class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</div><div class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</div><div class="line"></div><div class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></div><div class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></div><div class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></div><div class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></div><div class="line">        <span class="comment">#                                                                     #</span></div><div class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></div><div class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></div><div class="line">        <span class="comment"># variable.                                                           #</span></div><div class="line">        <span class="comment">#                                                                     #</span></div><div class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></div><div class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></div><div class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></div><div class="line">        <span class="comment"># variables.                                                          #</span></div><div class="line">        <span class="comment">#                                                                     #</span></div><div class="line">        <span class="comment"># Note that though you should be keeping track of the running         #</span></div><div class="line">        <span class="comment"># variance, you should normalize the data based on the standard       #</span></div><div class="line">        <span class="comment"># deviation (square root of variance) instead!                        # </span></div><div class="line">        <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)   #</span></div><div class="line">        <span class="comment"># might prove to be helpful.                                          #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">        sample_mean = np.mean(x,axis=<span class="number">0</span>)</div><div class="line">        sample_var = np.var(x,axis=<span class="number">0</span>)</div><div class="line">        x_hat = (x-sample_mean)/np.sqrt(sample_var+eps)</div><div class="line">        out = gamma*x_hat + beta</div><div class="line">        </div><div class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</div><div class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</div><div class="line">        </div><div class="line">        cache = (x,sample_mean,sample_var,x_hat,out,gamma,beta,eps)</div><div class="line"></div><div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></div><div class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></div><div class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></div><div class="line">        <span class="comment"># Store the result in the out variable.                               #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">        out = gamma*(x-running_mean)/np.sqrt(running_var+eps) + beta</div><div class="line"></div><div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</div><div class="line"></div><div class="line">    <span class="comment"># Store the updated running means back into bn_param</span></div><div class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</div><div class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</div><div class="line"></div><div class="line">    <span class="keyword">return</span> out, cache</div></pre></td></tr></table></figure></div></div><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2020-01-23 at 6.32.07 PM.png" alt="creen Shot 2020-01-23 at 6.32.07 P"></p><h3 id="Navie-Backward"><a href="#Navie-Backward" class="headerlink" title="Navie Backward"></a><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">Navie Backward</a></h3><p><img src="/2018/08/03/Training-Techiniques/BNcircuit.png" alt="Ncircui"></p><h4 id="Step-9"><a href="#Step-9" class="headerlink" title="Step 9"></a>Step 9</h4><p><img src="/2018/08/03/Training-Techiniques/step9.png" alt="tep"></p><h4 id="Step-8"><a href="#Step-8" class="headerlink" title="Step 8"></a>Step 8</h4><p><img src="/2018/08/03/Training-Techiniques/step8.png" alt="tep"></p><h4 id="Step-7"><a href="#Step-7" class="headerlink" title="Step 7"></a>Step 7</h4><p><img src="/2018/08/03/Training-Techiniques/step7.png" alt="tep"></p><h4 id="Step-6"><a href="#Step-6" class="headerlink" title="Step 6"></a>Step 6</h4><p><img src="/2018/08/03/Training-Techiniques/step6.png" alt="tep"></p><h4 id="Step-5"><a href="#Step-5" class="headerlink" title="Step 5"></a>Step 5</h4><p><img src="/2018/08/03/Training-Techiniques/step5.png" alt="tep"></p><h4 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h4><p><img src="/2018/08/03/Training-Techiniques/step4-4426132.png" alt="tep4-442613"></p><h4 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h4><p><img src="/2018/08/03/Training-Techiniques/step3.png" alt="tep"></p><h4 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h4><p><img src="/2018/08/03/Training-Techiniques/step2.png" alt="tep"></p><h4 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h4><p><img src="/2018/08/03/Training-Techiniques/step1.png" alt="tep"></p><h4 id="Step-0"><a href="#Step-0" class="headerlink" title="Step 0"></a>Step 0</h4><p><img src="/2018/08/03/Training-Techiniques/step0.png" alt="tep"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Backward pass for batch normalization.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></div><div class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></div><div class="line"><span class="string">    intermediate nodes.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></div><div class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Returns a tuple of:</span></div><div class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></div><div class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></div><div class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></div><div class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></div><div class="line">    <span class="comment"># Referencing the original paper (https://arxiv.org/abs/1502.03167)       #</span></div><div class="line">    <span class="comment"># might prove to be helpful.                                              #</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    N,D = dout.shape</div><div class="line">    (x,sample_mean,sample_var,x_hat,out,gamma,beta,eps) = cache</div><div class="line">    </div><div class="line">    dbeta = np.sum(dout,axis=<span class="number">0</span>)</div><div class="line">    dgamma = np.sum(x_hat*dout,axis=<span class="number">0</span>)</div><div class="line">    dx_hat = dout*gamma</div><div class="line">    </div><div class="line">    ivar = <span class="number">1</span>/np.sqrt(sample_var+eps)</div><div class="line">    xmu = x-sample_mean</div><div class="line">    </div><div class="line">    dx_xmu1 = dx_hat*ivar </div><div class="line">    divar = np.sum(dx_hat*xmu,axis=<span class="number">0</span>)</div><div class="line">    </div><div class="line">    dsqrt_var = <span class="number">-1</span>*divar/(sample_var+eps)</div><div class="line">    </div><div class="line">    dvar = <span class="number">0.5</span>*dsqrt_var/np.sqrt(sample_var+eps)</div><div class="line">    </div><div class="line">    dsq = np.ones((N,D))*dvar/N</div><div class="line">    </div><div class="line">    dx_xmu2 = <span class="number">2</span>*xmu*dsq</div><div class="line">    </div><div class="line">    dx1 = dx_xmu1 + dx_xmu2</div><div class="line">    dmu = <span class="number">-1</span>*np.sum(dx_xmu1 + dx_xmu2,axis=<span class="number">0</span>)</div><div class="line">    </div><div class="line">    dx2 = np.ones((N,D))*dmu/N</div><div class="line">    </div><div class="line">    dx = dx1+dx2</div><div class="line">    </div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</div></pre></td></tr></table></figure></div></div><h3 id="Alternative-Backward"><a href="#Alternative-Backward" class="headerlink" title="Alternative Backward"></a><a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="noopener">Alternative Backward</a></h3><h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p><a href="https://blog.csdn.net/Jaster_wisdom/article/details/78380839" target="_blank" rel="noopener">ref1</a> <a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">zhihu</a> <a href="http://www.sohu.com/a/200918239_206784" target="_blank" rel="noopener">intuition</a> <a href="https://www.cnblogs.com/rgvb178/p/6055213.html" target="_blank" rel="noopener">details</a> <a href="https://towardsdatascience.com/deep-learning-concepts-part-1-ea0b14b234c8" target="_blank" rel="noopener">to do</a> </p><p>根据是否饱和，激活函数可以分类为“饱和激活函数”和“非饱和激活函数”。</p><p><strong>sigmoid和tanh</strong>是“饱和激活函数”，而ReLU及其变体则是“非饱和激活函数”，但是他们都属于非线性激活函数。使用“非饱和激活函数”的优势在于两点：     </p><p>1.首先，“非饱和激活函数”能解决所谓的“梯度消失”问题。 vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一   </p><p> 2.其次，它能加快收敛速度。    </p><p><img src="/2018/08/03/Training-Techiniques/692825-20180328173642905-311674055.png" alt="692825-20180328173642905-311674055"></p><h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>将实数压缩到$(0,1)$，用来二分类。</p><script type="math/tex; mode=display">f(x) = \frac{1}{1+e^{-x}}</script><p><img src="/2018/08/03/Training-Techiniques/20171028233231084.png" alt="20171028233231084"></p><p>sigmoid的优缺点<a href="https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/" target="_blank" rel="noopener">source</a> </p><ul><li><p><strong>Vanishing gradients</strong>: Notice, the sigmoid function is flat near 0 and 1. In other words, the gradient of the sigmoid is near 0 and 1. During backpropagation through the network with sigmoid activation, the gradients in neurons whose output is near 0 or 1 are nearly 0. These neurons are called saturated neurons. Thus, the weights in these neurons do not update. Not only that, the weights of neurons connected to such neurons are also slowly updated. This problem is also known as vanishing gradient. So, imagine if there was a large network comprising of sigmoid neurons in which many of them are in a saturated regime, then the network will not be able to backpropagate.</p><blockquote><p><a href="http://www.sohu.com/a/148114422_500659" target="_blank" rel="noopener">ref1</a> 在GAN中， 给 D 最后的输出加个 Sigmoid 激活函数，让它取值在 0 到 1 之间？事实上这个方案在理论上是没有问题的，然而这会造成训练的困难。因为 Sigmoid 函数具有饱和区，一旦 D 进入了饱和区，就很难传回梯度来更新 G 了。</p></blockquote></li><li><p><strong>Not zero centered</strong>: Sigmoid outputs are not zero-centered. The output is always between 0 and 1, that means that the output after applying sigmoid is always positive hence, i.e. $x_i &gt; 0$.</p><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-13%20at%204.07.13%20PM.png" alt="creen Shot 2019-02-13 at 4.07.13 P"></p><p>which means the gradients on the weights $w$ during backpropagation become either all positive or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. Say we </p><blockquote><ul><li>Tanh, Rectified Linear Unit (ReLU), Leaky ReLU and Parametric ReLU are all zero-centered activation functions.</li><li>This is also why you want zero-mean data</li></ul></blockquote></li><li><p><strong>Computationally expensive</strong>: The exp() function is computationally expensive compared with the other non-linear activation functions.</p></li></ul><h2 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h2><p>将实数压缩到$[-1,1]$</p><script type="math/tex; mode=display">f(x) = tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\ f'(x)=1-f^2(x)</script><p><img src="/2018/08/03/Training-Techiniques/20171028235024692.png" alt="20171028235024692"></p><blockquote><p>ZERO-centered but still kills gradients when saturated</p></blockquote><h2 id="ReLu函数"><a href="#ReLu函数" class="headerlink" title="ReLu函数"></a>ReLu函数</h2><script type="math/tex; mode=display">f(x)=max(0,x)</script><p><img src="/2018/08/03/Training-Techiniques/20171028235337038.png" alt="20171028235337038"></p><blockquote><p><strong>Advantages:</strong> Does not saturate (in +region); Very computationally efficient; Converges much faster than sigmoid/tanh in practice (e.g. 6x); </p><p><strong>Disadvantages:</strong> Not zero-centered output; It kills the gradient when $x \le 0$  </p><p><a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks" target="_blank" rel="noopener">dead unit problem</a> <a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" target="_blank" rel="noopener">dying unit</a> </p></blockquote><h2 id="LeakyReLu"><a href="#LeakyReLu" class="headerlink" title="LeakyReLu"></a>LeakyReLu</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.54.22%20PM.png" alt="creen Shot 2019-02-17 at 2.54.22 P"></p><h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.55.56%20PM.png" alt="creen Shot 2019-02-17 at 2.55.56 P"></p><h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><p><img src="/2018/08/03/Training-Techiniques/Screen%20Shot%202019-02-17%20at%202.58.24%20PM.png" alt="creen Shot 2019-02-17 at 2.58.24 P"></p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-17 at 2.59.26 PM.png" alt="creen Shot 2019-02-17 at 2.59.26 P"></p><h1 id="Optimizer-ref"><a href="#Optimizer-ref" class="headerlink" title="Optimizer ref"></a>Optimizer <a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">ref</a></h1><p><a href="https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/" target="_blank" rel="noopener">ref</a> </p><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>The simplest form of update is to change the parameters along the negative gradient direction (<strong>since the gradient indicates the direction of increase</strong>, but we usually wish to minimize a loss function). Assuming a vector of parameters <code>x</code> and the gradient <code>dx</code>, the simplest update has the form:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Vanilla update</span></div><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure><p>where <code>learning_rate</code> is a hyperparameter - a fixed constant. When evaluated on the full dataset, and when the learning rate is low enough, this is guaranteed to make non-negative progress on the loss function.</p><p><a href="https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7" target="_blank" rel="noopener">problem in sgd</a> </p><p><a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" target="_blank" rel="noopener">Stochastic Gradient Descent with momentum</a> </p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-23 at 3.14.40 PM.png" alt="creen Shot 2019-02-23 at 3.14.40 P"></p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-02-23 at 3.18.02 PM.png" alt="creen Shot 2019-02-23 at 3.18.02 P"></p><p>Our gradients come from minibatches so they can be noisy!</p><h2 id="SGD-Momentum-ref"><a href="#SGD-Momentum-ref" class="headerlink" title="SGD+Momentum ref"></a>SGD+Momentum <a href="https://deepnotes.io/sgd-momentum-adaptive" target="_blank" rel="noopener">ref</a></h2><p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations.</p><script type="math/tex; mode=display">v_t = \gamma v_{t-1}+\eta \Delta_{\theta}J(\theta)\\\theta=\theta-v_t</script><p>The momentum term $\gamma$ tends to be 0.9/0.99.</p><p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. $\gamma &lt;1$). The same thing happens to our parameter updates: <strong>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.</strong> As a result, we gain faster convergence and reduced oscillation.</p><blockquote><p>By considering the previous gradient when computing the current gradient, there are two functions.</p><p>First of all, if the previous gradient and current gradient share the same direction, it will result in big steps to update the parameters.</p><p>Secondly, if the two gradients don’t share the same direction, we can prevent the updating step from overshooting. In other words, such low variance can help optimiization reduce the vibration on the vetical direction while moving toward to the convergence point.</p></blockquote><h2 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h2><p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p><p>Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term $ \gamma v_{t-1}$ to move the parameters $\theta$. <strong>Computing $\theta- \gamma v_{t-1}$ thus gives us an approximation of the next position of the parameters</strong> (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters $\theta$ but w.r.t. the approximate future position of our parameters:</p><script type="math/tex; mode=display">\begin{aligned} v_{t} &=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\ \theta &=\theta-v_{t} \end{aligned}</script><p>Again, we set the momentum term $\gamma$ to a value of around 0.9. While Momentum first computes the current gradient (small blue vector in the following image) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks.</p><p><img src="/2018/08/03/Training-Techiniques/nesterov_update_vector.png" alt="esterov_update_vecto"></p><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates<br>(i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data.</p><p>Previously, we performed an update for all parameters $\theta$ at once as every parameters $\theta_i$ used the same learning rate $\eta$. As Adagrad uses a different learning rate for every parameter $\theta_i$ at every time step $t$, we first show Adagrad’s per-parameter update, which we then vectorize. For brevity, we use $g_t$ to denote the gradient at time step $t$. $g_{t,i}$ is then the partial derivative of the objective function w.r.t. to the parameter $\theta_i$ at time step $t$:</p><script type="math/tex; mode=display">g_{t, i}=\nabla_{\theta} J\left(\theta_{t, i}\right)</script><p>The SGD update for every parameter $\theta_i$ at each time step $t$ then becomes:</p><script type="math/tex; mode=display">\theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i}</script><p>In its update rule, Adagrad modifies the general learning rate $\eta$ at each time step $t$ for every parameter $\theta_i$ based on the past gradients that have been computed for $\theta_i$:</p><script type="math/tex; mode=display">\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}</script><p>$G_{t} \in \mathbb{R}^{d \times d}$ here is a diagonal matrix where each diagonal element $[i,i]$ is the sum of the squares of the gradients w.r.t. $\theta_i$ up to time step $t$, while $\epsilon$ is a smoothing term that avoids division by zero (usually on the order of $1e^{-8}$).</p><p>As $G_t$ contains the sum of the squares of the past gradients w.r.t. to all parameters $\theta$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product $\oplus$ between $G_t$ and $g_t$: </p><script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t}</script><p>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. </p><p>Instead of inefficiently storing $w$ previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $E[g^2]_t$ at time step $t$ then depends (as a fraction $\gamma$ similarly to the Momentum term) only on the previous average and the current gradient:</p><script type="math/tex; mode=display">E\left[g^{2}\right]_{t}=\gamma E\left[g^{2}\right]_{t-1}+(1-\gamma) g_{t}^{2} \\\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} g_{t}</script><p>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests $\gamma$ to be set to 0.9, while a good default value for the learning rate is 0.001.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(w, dw, config=None)</span>:</span>  </div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Uses the RMSProp update rule, which uses a moving average of squared</span></div><div class="line"><span class="string">    gradient values to set adaptive per-parameter learning rates.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    config format:</span></div><div class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></div><div class="line"><span class="string">    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared</span></div><div class="line"><span class="string">      gradient cache.</span></div><div class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></div><div class="line"><span class="string">    - cache: Moving average of second moments of gradients.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>: config = &#123;&#125;</div><div class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-2</span>)</div><div class="line">    config.setdefault(<span class="string">'decay_rate'</span>, <span class="number">0.99</span>)</div><div class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</div><div class="line">    config.setdefault(<span class="string">'cache'</span>, np.zeros_like(w))</div><div class="line"></div><div class="line">    next_w = <span class="keyword">None</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the RMSprop update formula, storing the next value of w #</span></div><div class="line">    <span class="comment"># in the next_w variable. Don't forget to update cache value stored in    #</span></div><div class="line">    <span class="comment"># config['cache'].                                                        #</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">    next_cache = config[<span class="string">'decay_rate'</span>] * config[<span class="string">'cache'</span>] + (<span class="number">1</span> - config[<span class="string">'decay_rate'</span>]) * dw ** <span class="number">2</span></div><div class="line">    next_w = w - config[<span class="string">'learning_rate'</span>] * dw / np.sqrt(next_cache + config[<span class="string">'epsilon'</span>])</div><div class="line">    config[<span class="string">'cache'</span>] = next_cache</div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> next_w, config</div></pre></td></tr></table></figure><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>In addition to storing an exponentially decaying average of past squared gradients like RMSprop, Adam also keeps an exponentially decaying average of past gradients. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows:</p><script type="math/tex; mode=display">\begin{aligned} m_{t} &=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\ v_{t} &=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \end{aligned}</script><p>$m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. $\beta_1$ and $\beta_2$ are close to 1).</p><p>They counteract these biases by computing bias-corrected first and second moment estimates:</p><script type="math/tex; mode=display">\begin{aligned} \hat{m}_{t} &=\frac{m_{t}}{1-\beta_{1}^{t}} \\ \hat{v}_{t} &=\frac{v_{t}}{1-\beta_{2}^{t}} \end{aligned}</script><p><strong>Note that $t$ is the iteration number.</strong></p><p>They then use these to update the parameters, which yields the Adam update rule:</p><script type="math/tex; mode=display">\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon} \hat{m}_{t}</script><p>The authors propose default values of 0.9 for $\beta_1$, 0.999 for $\beta_2$ and $10^{-8}$ for $\epsilon$. </p><p>It is worth noting that the next new <code>m</code> should be <code>mt</code> instead of $\hat m_t$.  </p><blockquote><p>Since Adam divide the update by $\sqrt v$, the smallest average gradients will get the larger updates. We can view the coeeficient $\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}$ of $\hat m_t$ as controlling, where for all gradients they share the same learning rate $ \eta $ but greater gradients have smaller $\frac{\eta}{\sqrt{\hat{v}_{t}}+\epsilon}$, which means compared to parameters with smaller gradient, the parameters with bigger gradients have samll updates.</p><p>The parameters with small gradients are in the plateau area of the cost function, and thus the moving process is slow and the training process will be slow. If we could make those parameters in plateau area get larger updates, it will help them move out of plateau quickly.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(w, dw, config=None)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Uses the Adam update rule, which incorporates moving averages of both the</span></div><div class="line"><span class="string">    gradient and its square and a bias correction term.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    config format:</span></div><div class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></div><div class="line"><span class="string">    - beta1: Decay rate for moving average of first moment of gradient.</span></div><div class="line"><span class="string">    - beta2: Decay rate for moving average of second moment of gradient.</span></div><div class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></div><div class="line"><span class="string">    - m: Moving average of gradient.</span></div><div class="line"><span class="string">    - v: Moving average of squared gradient.</span></div><div class="line"><span class="string">    - t: Iteration number.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>: config = &#123;&#125;</div><div class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-3</span>)</div><div class="line">    config.setdefault(<span class="string">'beta1'</span>, <span class="number">0.9</span>)</div><div class="line">    config.setdefault(<span class="string">'beta2'</span>, <span class="number">0.999</span>)</div><div class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</div><div class="line">    config.setdefault(<span class="string">'m'</span>, np.zeros_like(w))</div><div class="line">    config.setdefault(<span class="string">'v'</span>, np.zeros_like(w))</div><div class="line">    config.setdefault(<span class="string">'t'</span>, <span class="number">0</span>)</div><div class="line"></div><div class="line">    next_w = <span class="keyword">None</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the Adam update formula, storing the next value of w in #</span></div><div class="line">    <span class="comment"># the next_w variable. Don't forget to update the m, v, and t variables   #</span></div><div class="line">    <span class="comment"># stored in config.                                                       #</span></div><div class="line">    <span class="comment">#                                                                         #</span></div><div class="line">    <span class="comment"># <span class="doctag">NOTE:</span> In order to match the reference output, please modify t _before_  #</span></div><div class="line">    <span class="comment"># using it in any calculations.                                           #</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">    mt = config[<span class="string">'beta1'</span>] * config[<span class="string">'m'</span>] + (<span class="number">1</span> - config[<span class="string">'beta1'</span>]) * dw</div><div class="line">    vt = config[<span class="string">'beta2'</span>] * config[<span class="string">'v'</span>] + (<span class="number">1</span> - config[<span class="string">'beta2'</span>]) * dw ** <span class="number">2</span></div><div class="line"></div><div class="line">    t = config[<span class="string">'t'</span>] + <span class="number">1</span></div><div class="line"></div><div class="line">    mu_mt = mt / (<span class="number">1</span> - config[<span class="string">'beta1'</span>] ** t)</div><div class="line">    mu_vt = vt / (<span class="number">1</span> - config[<span class="string">'beta2'</span>] ** t)</div><div class="line"></div><div class="line">    next_w = w - config[<span class="string">'learning_rate'</span>] * mu_mt / (np.sqrt(mu_vt) + config[<span class="string">'epsilon'</span>])</div><div class="line">    config[<span class="string">'m'</span>] = mt</div><div class="line">    config[<span class="string">'v'</span>] = vt</div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></div><div class="line">    <span class="comment">###########################################################################</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> next_w, config</div></pre></td></tr></table></figure><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><a href="https://towardsdatascience.com/coding-neural-network-dropout-3095632d25ce" target="_blank" rel="noopener">ref</a> <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">ref2</a> </p><p><strong>Dropout</strong> is a regularization technique. On each iteration, we randomly shut down some neurons (units) on each layer and don’t use those neurons in both forward propagation and back-propagation. <strong>Since the units that will be dropped out on each iteration will be random, the learning algorithm will have no idea which neurons will be shut down on every iteration; therefore, force the learning algorithm to spread out the weights and not focus on some specific feattures (units).</strong> Moreover, dropout help improving generalization error by:</p><ul><li><strong>Since we drop some units on each iteration, this will lead to smaller network which in turns means simpler network (regularization).</strong></li><li>Can be seen as an approximation to bagging techniques. Each iteration can be viewed as different model since we’re dropping randomly different units on each layer. This means that the error would be the average of errors from all different models (iterations). Therefore, averaging errors from different models especially if those errors are uncorrelated would reduce the overall errors. In the worst case where errors are perfectly correlated, averaging among all models won’t help at all; however, we know that in practice errors have some degree of uncorrelation. As result, it will always improve generalization error.</li></ul><p>We can use different probabilities on each layer; however, the output layer would always have <code>keep_prob = 1</code> and the input layer has high <code>keep_prob</code>such as 0.9 or 1. If a hidden layer has <code>keep_prob = 0.8</code>, this means that; on each iteration, each unit has 80% probablitity of being included and 20% probability of being dropped out.</p><p>It’s easy to remember things when the network has a lot of parameters (overfit), but it’s hard to remember things when effectively the network only has so many parameters to work with. Hence, the network must learn to generalize more to get the same performance as remembering things. So, that’s why Dropout will increase the test time performance: it improves generalization and reduce the risk of overfitting. <a href="https://wiseodd.github.io/techblog/2016/06/25/dropout/" target="_blank" rel="noopener">ref</a> </p><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2019-06-18 at 5.12.22 PM.png" alt="creen Shot 2019-06-18 at 5.12.22 P"></p><p>Therefore, at test time we multiply by dropout probability; Or, at training time, we divide by dropout probability.</p><p>The reason why we don’t apply dropout during testing is that it will make the prediction random and the evaluation will not be accurate.</p><h2 id="Numpy-Implementation"><a href="#Numpy-Implementation" class="headerlink" title="Numpy Implementation"></a>Numpy Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Performs the forward pass for (inverted) dropout.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - x: Input data, of any shape</span></div><div class="line"><span class="string">    - dropout_param: A dictionary with the following keys:</span></div><div class="line"><span class="string">      - p: Dropout parameter. We keep each neuron output with probability p.</span></div><div class="line"><span class="string">      - mode: 'test' or 'train'. If the mode is train, then perform dropout;</span></div><div class="line"><span class="string">        if the mode is test, then just return the input.</span></div><div class="line"><span class="string">      - seed: Seed for the random number generator. Passing seed makes this</span></div><div class="line"><span class="string">        function deterministic, which is needed for gradient checking but not</span></div><div class="line"><span class="string">        in real networks.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Outputs:</span></div><div class="line"><span class="string">    - out: Array of the same shape as x.</span></div><div class="line"><span class="string">    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout</span></div><div class="line"><span class="string">      mask that was used to multiply the input; in test mode, mask is None.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.</span></div><div class="line"><span class="string">    See http://cs231n.github.io/neural-networks-2/#reg for more details.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    NOTE 2: Keep in mind that p is the probability of **keep** a neuron</span></div><div class="line"><span class="string">    output; this might be contrary to some sources, where it is referred to</span></div><div class="line"><span class="string">    as the probability of dropping a neuron output.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    p, mode = dropout_param[<span class="string">'p'</span>], dropout_param[<span class="string">'mode'</span>]</div><div class="line">    <span class="keyword">if</span> <span class="string">'seed'</span> <span class="keyword">in</span> dropout_param:</div><div class="line">        np.random.seed(dropout_param[<span class="string">'seed'</span>])</div><div class="line"></div><div class="line">    mask = <span class="keyword">None</span></div><div class="line">    out = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase forward pass for inverted dropout.   #</span></div><div class="line">        <span class="comment"># Store the dropout mask in the mask variable.                        #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">        mask = np.random.rand(*x.shape)&gt;p <span class="comment">#unpack your shape tuple using * </span></div><div class="line">        out = x*mask/p</div><div class="line"></div><div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test phase forward pass for inverted dropout.   #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">        out = x</div><div class="line"></div><div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment">#                            END OF YOUR CODE                         #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line"></div><div class="line">    cache = (dropout_param, mask)</div><div class="line">    out = out.astype(x.dtype, copy=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> out, cache</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Perform the backward pass for (inverted) dropout.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">    Inputs:</span></div><div class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></div><div class="line"><span class="string">    - cache: (dropout_param, mask) from dropout_forward.</span></div><div class="line"><span class="string">    """</span></div><div class="line">    dropout_param, mask = cache</div><div class="line">    mode = dropout_param[<span class="string">'mode'</span>]</div><div class="line"></div><div class="line">    dx = <span class="keyword">None</span></div><div class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase backward pass for inverted dropout   #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">        dx = mask*dout/dropout_param[<span class="string">'p'</span>]</div><div class="line"></div><div class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></div><div class="line">        <span class="comment">#######################################################################</span></div><div class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</div><div class="line">        dx = dout</div><div class="line">    <span class="keyword">return</span> dx</div></pre></td></tr></table></figure><h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><p><a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">paper1</a> <a href="https://arxiv.org/pdf/1603.05027v2.pdf" target="_blank" rel="noopener">paper2</a> </p><h2 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h2><p>simplely stack layers exhibit higher training error when the depth increases.</p><h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p><img src="/2018/08/03/Training-Techiniques/Screen Shot 2018-11-16 at 2.33.45 PM.png" alt="creen Shot 2018-11-16 at 2.33.45 P"></p><p><img src="/2018/08/03/Training-Techiniques/residualnet_34.png" alt="esidualnet_3"></p><p>Figure from <a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/residual_net.html" target="_blank" rel="noopener">ref</a> </p><p><img src="/2018/08/03/Training-Techiniques/2228224-1a6202911b46d1dc.png" alt="228224-1a6202911b46d1d"></p><p><a href="https://www.jianshu.com/p/e502e4b43e6d" target="_blank" rel="noopener">pic</a> </p><h1 id="PatchGan"><a href="#PatchGan" class="headerlink" title="PatchGan"></a>PatchGan</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate output shape of D (PatchGAN)</span></div><div class="line">patch = int(self.img_rows / <span class="number">2</span>**<span class="number">4</span>)</div><div class="line">self.disc_patch = (patch, patch, <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># Build and compile the discriminator</span></div><div class="line">self.discriminator = self.build_discriminator()</div><div class="line">self.discriminator.compile(loss=<span class="string">'mse'</span>,optimizer=optimizer,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_discriminator</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">d_layer</span><span class="params">(layer_input, filters, f_size=<span class="number">4</span>, bn=True)</span>:</span></div><div class="line">        d = Conv2D(filters, kernel_size=f_size, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)(layer_input)</div><div class="line">        d = LeakyReLU(alpha=<span class="number">0.2</span>)(d)</div><div class="line">        <span class="keyword">if</span> bn:</div><div class="line">            d = BatchNormalization(momentum=<span class="number">0.8</span>)(d)</div><div class="line">        <span class="keyword">return</span> d</div><div class="line">    img_A = Input(shape=self.img_shape)</div><div class="line">    img_B = Input(shape=self.img_shape)</div><div class="line">    combined_imgs = Concatenate(axis=<span class="number">-1</span>)([img_A, img_B])</div><div class="line">    d1 = d_layer(combined_imgs, self.df, bn=<span class="keyword">False</span>)</div><div class="line">    d2 = d_layer(d1, self.df*<span class="number">2</span>)</div><div class="line">    d3 = d_layer(d2, self.df*<span class="number">4</span>)</div><div class="line">    d4 = d_layer(d3, self.df*<span class="number">8</span>)</div><div class="line">    validity = Conv2D(<span class="number">1</span>, kernel_size=<span class="number">4</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>)(d4)</div><div class="line">    <span class="keyword">return</span> Model([img_A, img_B], validity)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></div><div class="line">    valid = np.ones((batch_size,) + self.disc_patch)</div><div class="line">    fake = np.zeros((batch_size,) + self.disc_patch)</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        fake_A = self.generator.predict(imgs_B)</div><div class="line">        d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)</div><div class="line">        d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)</div><div class="line">        d_loss = <span class="number">0.5</span> * np.add(d_loss_real, d_loss_fake)</div><div class="line">        </div><div class="line">        g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])</div></pre></td></tr></table></figure><blockquote><ul><li><p>The output of discriminator is (None, patch, patch, 1), which can be seen using model.summary(). While the traditional discriminator’s output is (None, 1), distinguishing each image.</p></li><li><p>If traditional discriminator, the last two line codes would be:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   d5 = Flatten()(d4)</div><div class="line">&gt;   validity = layers.Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>)(d5)</div><div class="line">&gt;   <span class="keyword">return</span> Model([img_A, img_B], validity)</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  ​</p></blockquote>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Keras</title>
      <link href="/2018/07/23/Keras/"/>
      <url>/2018/07/23/Keras/</url>
      <content type="html"><![CDATA[<p>Keras使用技巧。</p><p>TODO</p><ul><li>[ ] LSTM</li><li>[ ] CONV1D</li><li>[ ] TIMEDISTRIBUTED-VIDEOS</li><li>[ ] FIT_GENERATOR</li><li>[ ] STATEFUL_RNN</li></ul><a id="more"></a><h1 id="Keras模块结构"><a href="#Keras模块结构" class="headerlink" title="Keras模块结构"></a>Keras模块结构</h1><p><img src="/2018/07/23/Keras/1119747-20170707133635659-888158147.png" alt="1119747-20170707133635659-888158147"></p><h1 id="Keras构建神经网络"><a href="#Keras构建神经网络" class="headerlink" title="Keras构建神经网络"></a>Keras构建神经网络</h1><p><img src="/2018/07/23/Keras/1119747-20170707133932722-715494711.png" alt="1119747-20170707133932722-715494711"></p><h1 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h1><p>There are two ways to implement a keras model, Sequential and Model. We will use an image classification example to see how these ways work and their corresponding properties.</p><p>We use MNIST hand-written recognition as the example. We will implement a three-layers convolution network, i.e., <code>input_layer -&gt; conv2d -&gt; conv2d -&gt; dense -&gt; output_layer</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Conv2D, Flatten</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</div><div class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class="line">X_train = X_train.reshape(<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</div><div class="line">X_test = X_test.reshape(<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</div><div class="line">y_train = to_categorical(y_train)</div><div class="line">y_test = to_categorical(y_test)</div><div class="line">model = Sequential()</div><div class="line"><span class="comment">#add model layers</span></div><div class="line">model.add(Conv2D(<span class="number">64</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>),name=<span class="string">'conv1'</span>))</div><div class="line">model.add(Conv2D(<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>,name=<span class="string">'conv2'</span>))</div><div class="line">model.add(Flatten(name=<span class="string">'flatten'</span>))</div><div class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>,name=<span class="string">'dense'</span>))</div><div class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</div></pre></td></tr></table></figure><h2 id="Attributes"><a href="#Attributes" class="headerlink" title="Attributes"></a>Attributes</h2><p><code>model.layers</code>: return a flattened list of the layers comprising the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(model.layers)</div><div class="line"><span class="comment">##[&lt;keras.layers.convolutional.Conv2D object at 0x11b62a780&gt;, &lt;keras.layers.convolutional.Conv2D object at 0x11b63df98&gt;, &lt;keras.layers.core.Flatten object at 0x11eb17588&gt;, &lt;keras.layers.core.Dense object at 0x11eb170f0&gt;]</span></div></pre></td></tr></table></figure><p><code>model.inputs</code>: return the list of input tensors of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(model.inputs)</div><div class="line"><span class="comment">##[&lt;tf.Tensor 'conv1_input:0' shape=(?, 28, 28, 1) dtype=float32&gt;]</span></div></pre></td></tr></table></figure><p><code>model.outputs</code>: return the list of output tensors of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(model.outputs)</div><div class="line"><span class="comment">##[&lt;tf.Tensor 'dense/Softmax:0' shape=(?, 10) dtype=float32&gt;]</span></div></pre></td></tr></table></figure><p><code>model.summary</code>: prints a summary representation of the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">_________________________________________________________________</div><div class="line">Layer (type)                 Output Shape              Param <span class="comment">#   </span></div><div class="line">=================================================================</div><div class="line">conv1 (Conv2D)               (<span class="keyword">None</span>, <span class="number">26</span>, <span class="number">26</span>, <span class="number">64</span>)        <span class="number">640</span>       </div><div class="line">_________________________________________________________________</div><div class="line">conv2 (Conv2D)               (<span class="keyword">None</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">32</span>)        <span class="number">18464</span>     </div><div class="line">_________________________________________________________________</div><div class="line">flatten (Flatten)            (<span class="keyword">None</span>, <span class="number">18432</span>)             <span class="number">0</span>         </div><div class="line">_________________________________________________________________</div><div class="line">dense (Dense)                (<span class="keyword">None</span>, <span class="number">10</span>)                <span class="number">184330</span>    </div><div class="line">=================================================================</div><div class="line">Total params: <span class="number">203</span>,<span class="number">434</span></div><div class="line">Trainable params: <span class="number">203</span>,<span class="number">434</span></div><div class="line">Non-trainable params: <span class="number">0</span></div><div class="line">_________________________________________________________________</div></pre></td></tr></table></figure><p><code>model.get_weights()</code>: return a list of all weights tensors as Numpy arrays, where weights are stored in model.get_weights()[0] and bias are stored in model.get_weights()[2].</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(model.get_weights())</div></pre></td></tr></table></figure><h2 id="Model-subclassing"><a href="#Model-subclassing" class="headerlink" title="Model subclassing"></a>Model subclassing</h2><p>In addition to these two types of models, you may create your own fully-customizable models by subclassing the <code>Model</code> class and implementing your own forward pass in the <code>call</code> method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> keras</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleMLP</span><span class="params">(keras.Model)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_bn=False, use_dp=False, num_classes=<span class="number">10</span>)</span>:</span></div><div class="line">        super(SimpleMLP, self).__init__(name=<span class="string">'mlp'</span>)</div><div class="line">        self.use_bn = use_bn</div><div class="line">        self.use_dp = use_dp</div><div class="line">        self.num_classes = num_classes</div><div class="line"></div><div class="line">        self.dense1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</div><div class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</div><div class="line">        <span class="keyword">if</span> self.use_dp:</div><div class="line">            self.dp = keras.layers.Dropout(<span class="number">0.5</span>)</div><div class="line">        <span class="keyword">if</span> self.use_bn:</div><div class="line">            self.bn = keras.layers.BatchNormalization(axis=<span class="number">-1</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></div><div class="line">        x = self.dense1(inputs)</div><div class="line">        <span class="keyword">if</span> self.use_dp:</div><div class="line">            x = self.dp(x)</div><div class="line">        <span class="keyword">if</span> self.use_bn:</div><div class="line">            x = self.bn(x)</div><div class="line">        <span class="keyword">return</span> self.dense2(x)</div><div class="line"></div><div class="line">model = SimpleMLP()</div><div class="line">model.compile(...)</div><div class="line">model.fit(...)</div></pre></td></tr></table></figure><p>In <code>call</code>, you may specify custom losses by calling <code>self.add_loss(loss_tensor)</code> (like you would in a custom layer).</p><p>But in subclassed models, the following methods and attributes are not available:</p><p><img src="/2018/07/23/Keras/Screen Shot 2019-03-29 at 1.06.19 PM.png" alt="creen Shot 2019-03-29 at 1.06.19 P"></p><h2 id="Model-class-API"><a href="#Model-class-API" class="headerlink" title="Model class API"></a>Model class API</h2><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><h4 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">compile(optimizer, loss=<span class="keyword">None</span>, metrics=<span class="keyword">None</span>, loss_weights=<span class="keyword">None</span>, sample_weight_mode=<span class="keyword">None</span>, weighted_metrics=<span class="keyword">None</span>, target_tensors=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><h4 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fit(x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, batch_size=<span class="keyword">None</span>, epochs=<span class="number">1</span>, verbose=<span class="number">1</span>, callbacks=<span class="keyword">None</span>, validation_split=<span class="number">0.0</span>, validation_data=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>, class_weight=<span class="keyword">None</span>, sample_weight=<span class="keyword">None</span>, initial_epoch=<span class="number">0</span>, steps_per_epoch=<span class="keyword">None</span>, validation_steps=<span class="keyword">None</span>, validation_freq=<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><ul><li><strong>verbose</strong>: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.</li><li>A <code>History</code> object. Its <code>History.history</code> attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).</li></ul></blockquote><h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">evaluate(x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, batch_size=<span class="keyword">None</span>, verbose=<span class="number">1</span>, sample_weight=<span class="keyword">None</span>, steps=<span class="keyword">None</span>, callbacks=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><blockquote><p>Returns scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute <code>model.metrics_names</code> will give you the display labels for the scalar outputs.</p><p>Computation is done in batches.</p><ul><li><strong>verbose</strong>: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.</li></ul></blockquote><h4 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">predict(x, batch_size=<span class="keyword">None</span>, verbose=<span class="number">0</span>, steps=<span class="keyword">None</span>, callbacks=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><blockquote><ul><li><strong>verbose</strong>: Verbosity mode, 0 or 1.</li></ul></blockquote><h4 id="train-on-batch"><a href="#train-on-batch" class="headerlink" title="train_on_batch"></a>train_on_batch</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_on_batch(x, y, sample_weight=<span class="keyword">None</span>, class_weight=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><blockquote><ul><li>Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute <code>model.metrics_names</code> will give you the display labels for the scalar outputs.</li><li>Runs a single gradient update on a single batch of data.</li></ul></blockquote><h4 id="test-on-batch"><a href="#test-on-batch" class="headerlink" title="test_on_batch"></a>test_on_batch</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test_on_batch(x, y, sample_weight=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><blockquote><ul><li>Test the model on a single batch of samples.</li><li>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute <code>model.metrics_names</code> will give you the display labels for the scalar outputs.</li></ul></blockquote><h4 id="predict-on-batch"><a href="#predict-on-batch" class="headerlink" title="predict_on_batch"></a>predict_on_batch</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">predict_on_batch(x)</div></pre></td></tr></table></figure><h4 id="fit-generator"><a href="#fit-generator" class="headerlink" title="fit_generator"></a>fit_generator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fit_generator(generator, steps_per_epoch=<span class="keyword">None</span>, epochs=<span class="number">1</span>, verbose=<span class="number">1</span>, callbacks=<span class="keyword">None</span>, validation_data=<span class="keyword">None</span>, validation_steps=<span class="keyword">None</span>, validation_freq=<span class="number">1</span>, class_weight=<span class="keyword">None</span>, max_queue_size=<span class="number">10</span>, workers=<span class="number">1</span>, use_multiprocessing=<span class="keyword">False</span>, shuffle=<span class="keyword">True</span>, initial_epoch=<span class="number">0</span>)</div></pre></td></tr></table></figure><blockquote><ul><li><p>Return a <code>History</code> object. Its <code>History.history</code> attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).</p></li><li><p><strong>generator</strong>: A generator or an instance of <code>Sequence</code> (<code>keras.utils.Sequence</code>) object in order to avoid duplicate data when using multiprocessing. The output of the generator must be either</p><ul><li>a tuple <code>(inputs, targets)</code></li><li>a tuple <code>(inputs, targets, sample_weights)</code>.</li></ul><p>This tuple (a single output of the generator) makes a single batch. Therefore, all arrays in this tuple must have the same length (equal to the size of this batch). Different batches may have different sizes. For example, the last batch of the epoch is commonly smaller than the others, if the size of the dataset is not divisible by the batch size. The generator is expected to loop over its data indefinitely. An epoch finishes when <code>steps_per_epoch</code> batches have been seen by the model.</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_arrays_from_file</span><span class="params">(path)</span>:</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">with</span> open(path) <span class="keyword">as</span> f:</div><div class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</div><div class="line">                <span class="comment"># create numpy arrays of input data</span></div><div class="line">                <span class="comment"># and labels, from each line in the file</span></div><div class="line">                x1, x2, y = process_line(line)</div><div class="line">                <span class="keyword">yield</span> (&#123;<span class="string">'input_1'</span>: x1, <span class="string">'input_2'</span>: x2&#125;, &#123;<span class="string">'output'</span>: y&#125;)</div><div class="line"></div><div class="line">model.fit_generator(generate_arrays_from_file(<span class="string">'/my_file.txt'</span>),</div><div class="line">                    steps_per_epoch=<span class="number">10000</span>, epochs=<span class="number">10</span>)</div></pre></td></tr></table></figure><h4 id="evaluate-generator"><a href="#evaluate-generator" class="headerlink" title="evaluate_generator"></a>evaluate_generator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">evaluate_generator(generator, steps=<span class="keyword">None</span>, callbacks=<span class="keyword">None</span>, max_queue_size=<span class="number">10</span>, workers=<span class="number">1</span>, use_multiprocessing=<span class="keyword">False</span>, verbose=<span class="number">0</span>)</div></pre></td></tr></table></figure><blockquote><p>Evaluates the model on a data generator.</p><p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute <code>model.metrics_names</code> will give you the display labels for the scalar outputs.</p></blockquote><h4 id="predict-generator"><a href="#predict-generator" class="headerlink" title="predict_generator"></a>predict_generator</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">predict_generator(generator, steps=<span class="keyword">None</span>, callbacks=<span class="keyword">None</span>, max_queue_size=<span class="number">10</span>, workers=<span class="number">1</span>, use_multiprocessing=<span class="keyword">False</span>, verbose=<span class="number">0</span>)</div></pre></td></tr></table></figure><h4 id="get-layer"><a href="#get-layer" class="headerlink" title="get_layer"></a>get_layer</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">get_layer(name=<span class="keyword">None</span>, index=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><blockquote><p>Retrieves a layer based on either its name (unique) or index.</p><p>If <code>name</code> and <code>index</code> are both provided, <code>index</code> will take precedence.</p><p>Indices are based on order of horizontal graph traversal (bottom-up).</p><p>Return a layer instance.</p></blockquote><h1 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h1><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><h3 id="Get-weights"><a href="#Get-weights" class="headerlink" title="Get weights"></a>Get weights</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">layer.get_weights()</div><div class="line">layer.set_weights(weights)</div><div class="line">layer.get_config():returns a dictionary containing the configuration of the layer.</div></pre></td></tr></table></figure><h3 id="Get-input-output-shape"><a href="#Get-input-output-shape" class="headerlink" title="Get input/output/shape"></a>Get input/output/shape</h3><p>Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a “node” to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2…</p><p>In previous versions of Keras, you could obtain the output tensor of a layer instance via <code>layer.get_output()</code>, or its output shape via <code>layer.output_shape</code>. You still can (except <code>get_output()</code> has been replaced by the property <code>output</code>). But what if a layer is connected to multiple inputs?</p><p>As long as a layer is only connected to one input, there is no confusion, and <code>.output</code> will return the one output of the layer:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = Input(shape=(<span class="number">280</span>, <span class="number">256</span>))</div><div class="line">lstm = LSTM(<span class="number">32</span>)</div><div class="line">encoded_a = lstm(a)</div><div class="line"><span class="keyword">assert</span> lstm.output == encoded_a</div></pre></td></tr></table></figure><p>If we have multiple inputs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">a = Input(shape=(<span class="number">280</span>, <span class="number">256</span>))</div><div class="line">b = Input(shape=(<span class="number">280</span>, <span class="number">256</span>))</div><div class="line">lstm = LSTM(<span class="number">32</span>)</div><div class="line">encoded_a = lstm(a)</div><div class="line">encoded_b = lstm(b)</div><div class="line"><span class="keyword">assert</span> lstm.get_output_at(<span class="number">0</span>) == encoded_a</div><div class="line"><span class="keyword">assert</span> lstm.get_output_at(<span class="number">1</span>) == encoded_b</div></pre></td></tr></table></figure><p>The same is true for the properties <code>input_shape</code> and <code>output_shape</code>: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of “layer output/input shape” is well defined, and that one shape will be returned by <code>layer.output_shape</code>/<code>layer.input_shape</code>. But if, for instance, you apply the same <code>Conv2D</code> layer to an input of shape <code>(32, 32, 3)</code>, and then to an input of shape <code>(64, 64, 3)</code>, the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a = Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))</div><div class="line">b = Input(shape=(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</div><div class="line">conv = Conv2D(<span class="number">16</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>)</div><div class="line">conved_a = conv(a)</div><div class="line"><span class="comment"># Only one input so far, the following will work:</span></div><div class="line"><span class="keyword">assert</span> conv.input_shape == (<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</div><div class="line">conved_b = conv(b)</div><div class="line"><span class="comment"># now the `.input_shape` property wouldn't work, but this does:</span></div><div class="line"><span class="keyword">assert</span> conv.get_input_shape_at(<span class="number">0</span>) == (<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</div><div class="line"><span class="keyword">assert</span> conv.get_input_shape_at(<span class="number">1</span>) == (<span class="keyword">None</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><h3 id="Obtain-output-of-an-intermediate-layer"><a href="#Obtain-output-of-an-intermediate-layer" class="headerlink" title="Obtain output of an intermediate layer"></a>Obtain output of an intermediate layer</h3><p>There are two ways to do it. The first one is to create a new model that outputs the layers we want.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line">complete_model = Model() <span class="comment">#create the original model</span></div><div class="line">layer_name = <span class="string">'my_layer'</span></div><div class="line">intermediate_layer_model = Model(inputs=complete_model.input,outputs=model.get_layer(layer_name).output)</div><div class="line">intermediate_output = intermediate_layer_model.predict(data)</div></pre></td></tr></table></figure><p>Alternatively, we can build a Keras function that will return the output of a certain layer given a certain input, for example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="comment"># with a Sequential model</span></div><div class="line">get_3rd_layer_output = K.function([model.layers[<span class="number">0</span>].input],</div><div class="line">                                  [model.layers[<span class="number">3</span>].output])</div><div class="line">layer_output = get_3rd_layer_output([x])[<span class="number">0</span>]</div></pre></td></tr></table></figure><h2 id="Core-Layers"><a href="#Core-Layers" class="headerlink" title="Core Layers"></a>Core Layers</h2><h3 id="Dense"><a href="#Dense" class="headerlink" title="Dense()"></a>Dense()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.Dense(units, activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>It implements the fully-connected multiplication operation: <code>output=activation(dot(input,kernel)+bias)</code>, where <code>activation</code> is the element-wise activation function passed as the <code>activation</code> argument, <code>kernel</code>is a weights matrix and <code>bias</code> is a bias vector.</li><li>The input’s shape should be like <code>(batch_size,dim)</code>, otherwise, use <code>Flatten()</code> layer to reshape the input.</li><li>The output shape is <code>(batch_size,units)</code></li></ul><h3 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten()"></a>Flatten()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.Flatten()</div></pre></td></tr></table></figure><ul><li>Flattens the input. Does not affect the batch size.</li></ul><h3 id="Input"><a href="#Input" class="headerlink" title="Input()"></a>Input()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Input(shape=())</div></pre></td></tr></table></figure><ul><li>It is used to instantiate a Keras tensor.</li><li><strong>shape</strong>: A shape tuple (integer), not including the batch size. For instance, <code>shape=(32,)</code> indicates that the expected input will be batches of 32-dimensional vectors.</li></ul><h3 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape()"></a>Reshape()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.Reshape(target_shape)</div></pre></td></tr></table></figure><ul><li><strong>target_shape</strong>: target shape. Tuple of integers. Does not include the batch axis. Support shape inference using <code>-1</code> as dimension.</li><li>The shape of output is like <code>(batch_size,)+target_shape</code></li></ul><h3 id="Permute"><a href="#Permute" class="headerlink" title="Permute()"></a>Permute()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.Permute(dims)</div></pre></td></tr></table></figure><ul><li>Permutes the dimensions of the input according to a given pattern.</li></ul><h3 id="RepeatVector"><a href="#RepeatVector" class="headerlink" title="RepeatVector()"></a>RepeatVector()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.RepeatVector(n)</div></pre></td></tr></table></figure><ul><li>Repeats the input n times. Input: 2D tensor of shape <code>(num_samples, features)</code>. output shape: 3D tensor of shape <code>(num_samples, n, features)</code>.</li></ul><h3 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D()"></a>Conv2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.Conv2D(filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>, dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>2D convolution layer (e.g. spatial convolution over images). </li><li>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.</li></ul><h3 id="Conv2DTranspose"><a href="#Conv2DTranspose" class="headerlink" title="Conv2DTranspose()"></a>Conv2DTranspose()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.Conv2DTranspose(filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">'valid'</span>, output_padding=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="keyword">None</span>, use_bias=<span class="keyword">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>, bias_initializer=<span class="string">'zeros'</span>, kernel_regularizer=<span class="keyword">None</span>, bias_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, kernel_constraint=<span class="keyword">None</span>, bias_constraint=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>Transposed convolution layer (sometimes called Deconvolution).</li></ul><h3 id="UpSampling2D"><a href="#UpSampling2D" class="headerlink" title="UpSampling2D()"></a>UpSampling2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.UpSampling2D(size=(<span class="number">2</span>, <span class="number">2</span>), data_format=<span class="keyword">None</span>, interpolation=<span class="string">'nearest'</span>)</div></pre></td></tr></table></figure><ul><li>Repeats the rows and columns of the data by size[0] and size[1] respectively.</li><li>input shape <code>(batch_size,rows,cols,channels)</code>, </li></ul><h3 id="ZeroPadding2D"><a href="#ZeroPadding2D" class="headerlink" title="ZeroPadding2D()"></a>ZeroPadding2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>), data_format=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.</li></ul><h3 id="MaxPooling2D"><a href="#MaxPooling2D" class="headerlink" title="MaxPooling2D()"></a>MaxPooling2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="keyword">None</span>, padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>If None, it will default to <code>pool_size</code>.</li><li>input shape <code>(batch_size, rows, cols, channels)</code>, output shape <code>(batch_size, pooled_rows, pooled_cols, channels)</code></li></ul><h3 id="AveragePooling2D"><a href="#AveragePooling2D" class="headerlink" title="AveragePooling2D()"></a>AveragePooling2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.AveragePooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=<span class="keyword">None</span>, padding=<span class="string">'valid'</span>, data_format=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>input shape <code>(batch_size, rows, cols, channels)</code>, output shape <code>(batch_size, pooled_rows, pooled_cols, channels)</code></li></ul><h3 id="GlobalMaxPooling2D"><a href="#GlobalMaxPooling2D" class="headerlink" title="GlobalMaxPooling2D()"></a>GlobalMaxPooling2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.GlobalMaxPooling2D(data_format=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>input shape <code>(batch_size, rows, cols, channels)</code>, output shape <code>(batch_size,  channels)</code></li></ul><h3 id="GlobalAveragePooling2D"><a href="#GlobalAveragePooling2D" class="headerlink" title="GlobalAveragePooling2D()"></a>GlobalAveragePooling2D()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.layers.GlobalAveragePooling2D(data_format=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>input shape <code>(batch_size, rows, cols, channels)</code>, output shape <code>(batch_size,  channels)</code></li></ul><h1 id="ImageProcessing"><a href="#ImageProcessing" class="headerlink" title="ImageProcessing"></a>ImageProcessing</h1><h2 id="ImageDataGenerator-class"><a href="#ImageDataGenerator-class" class="headerlink" title="ImageDataGenerator class"></a>ImageDataGenerator class</h2><p>In order to make the most of training datasets, somethimes we need to augment the training dataset via a number of random transformations, so that our model can never see the exact same pictures but at the same time we can expand our trianing dataset. This helps prevent overfitting and helps the model generalize better.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keras.preprocessing.image.ImageDataGenerator(featurewise_center=<span class="keyword">False</span>, samplewise_center=<span class="keyword">False</span>, featurewise_std_normalization=<span class="keyword">False</span>, samplewise_std_normalization=<span class="keyword">False</span>, zca_whitening=<span class="keyword">False</span>, zca_epsilon=<span class="number">1e-06</span>, rotation_range=<span class="number">0</span>, width_shift_range=<span class="number">0.0</span>, height_shift_range=<span class="number">0.0</span>, brightness_range=<span class="keyword">None</span>, shear_range=<span class="number">0.0</span>, zoom_range=<span class="number">0.0</span>, channel_shift_range=<span class="number">0.0</span>, fill_mode=<span class="string">'nearest'</span>, cval=<span class="number">0.0</span>, horizontal_flip=<span class="keyword">False</span>, vertical_flip=<span class="keyword">False</span>, rescale=<span class="keyword">None</span>, preprocessing_function=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, validation_split=<span class="number">0.0</span>, dtype=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>In Keras this can be done via the <code>keras.preprocessing.image.ImageDataGenerator</code> class. This class allows you to:</p><ol><li>configure random transformations and normalization operations to be done on your image data during training</li><li>instantiate generators of augmented image batches (and their labels) via <code>.flow(data, labels)</code> or <code>.flow_from_directory(directory)</code>. These generators can then be used with the Keras model methods that accept data generators as inputs, <code>fit_generator</code>, <code>evaluate_generator</code> and <code>predict_generator</code>.</li></ol><p>Let’s go over these parameters quickly by performing these operations on an image.</p><ul><li><p><code>rotation_range</code> is a value in degrees (0-180), a range within which to randomly rotate pictures.</p><p><img src="/2018/07/23/Keras/cat_0_5151.jpg" alt="at_0_515">  ==========   <img src="/2018/07/23/Keras/cat_0_9819.jpg" alt="at_0_981"></p></li><li><p><code>width_shift_range</code> Float, 1-D array-like or int</p><p><img src="/2018/07/23/Keras/cat_0_3682.jpg" alt="at_0_368">  ==========      <img src="/2018/07/23/Keras/cat_0_2380.jpg" alt="at_0_238"></p></li></ul><ul><li><p><code>height_shift_range</code> </p><p><img src="/2018/07/23/Keras/cat_0_3682-4504351.jpg" alt="at_0_3682-450435">   ==========    <img src="/2018/07/23/Keras/cat_0_4790.jpg" alt="at_0_479"></p></li><li><p><code>shear_range</code>: Float. Shear Intensity (Shear angle in counter-clockwise direction in degrees)</p><p><img src="/2018/07/23/Keras/cat_0_3682-4504721.jpg" alt="at_0_3682-450472">    ===========    <img src="/2018/07/23/Keras/cat_0_8162.jpg" alt="at_0_816"></p></li><li><p><code>zoom_range</code>: Float or [lower, upper]. Range for random zoom. If a float, <code>[lower, upper] = [1-zoom_range, 1+zoom_range]</code></p><p><img src="/2018/07/23/Keras/cat_0_3682-4504809.jpg" alt="at_0_3682-450480">     =========== <img src="/2018/07/23/Keras/cat_0_1571.jpg" alt="at_0_157"></p></li></ul><ul><li><p><code>**horizontal_flip**</code> Boolean. Randomly flip inputs horizontally.<img src="/2018/07/23/Keras/cat_0_3682-4504439.jpg" alt="at_0_3682-450443">    ==========  <img src="/2018/07/23/Keras/cat_0_9447.jpg" alt="at_0_944"></p></li><li><p><code>vertical_flip</code> Boolean. Randomly flip inputs vertically.</p><p><img src="/2018/07/23/Keras/cat_0_3021.jpg" alt="at_0_302">      =========     <img src="/2018/07/23/Keras/cat_0_6028.jpg" alt="at_0_602"></p></li><li><p><code>fill_mode</code> is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.</p><p>​</p></li></ul><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator, array_to_img, img_to_array, load_img</div><div class="line"><span class="keyword">import</span> cv2</div><div class="line">datagen = ImageDataGenerator(</div><div class="line">    <span class="comment"># featurewise_center=True,</span></div><div class="line">    <span class="comment"># featurewise_std_normalization=False,</span></div><div class="line">    <span class="comment"># zca_whitening=True,</span></div><div class="line">    <span class="comment"># zca_epsilon=1,</span></div><div class="line">    <span class="comment"># rotation_range=100,</span></div><div class="line">    <span class="comment"># width_shift_range=220.0,</span></div><div class="line">    <span class="comment"># height_shift_range=100.0,</span></div><div class="line">    <span class="comment"># brightness_range=None,</span></div><div class="line">    <span class="comment"># shear_range=0.0,</span></div><div class="line">    <span class="comment"># zoom_range=0.0,</span></div><div class="line">    <span class="comment"># channel_shift_range=0.0,</span></div><div class="line">    <span class="comment"># fill_mode='nearest',</span></div><div class="line">    <span class="comment"># cval=0.0,</span></div><div class="line">    <span class="comment"># horizontal_flip=True,</span></div><div class="line">    vertical_flip=<span class="keyword">True</span>,</div><div class="line">    <span class="comment"># rescale=None,</span></div><div class="line">    <span class="comment"># preprocessing_function=None,</span></div><div class="line">    <span class="comment"># data_format=None,</span></div><div class="line">    <span class="comment"># validation_split=0.0,</span></div><div class="line">    dtype=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">img = load_img(<span class="string">'example.jpg'</span>)  <span class="comment"># this is a PIL image</span></div><div class="line">img = img.resize((<span class="number">224</span>,<span class="number">224</span>))</div><div class="line">x = img_to_array(img)  <span class="comment"># this is a Numpy array with shape (3, 150, 150)</span></div><div class="line">x = x.reshape((<span class="number">1</span>,) + x.shape)  <span class="comment"># this is a Numpy array with shape (1, 3, 150, 150)</span></div><div class="line"></div><div class="line"><span class="comment"># the .flow() command below generates batches of randomly transformed images</span></div><div class="line"><span class="comment"># and saves the results to the `preview/` directory</span></div><div class="line">i = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> datagen.flow(x, batch_size=<span class="number">1</span>,</div><div class="line">                          save_to_dir=<span class="string">'preview'</span>, save_prefix=<span class="string">'cat'</span>, save_format=<span class="string">'jpg'</span>):</div><div class="line">    i += <span class="number">1</span></div><div class="line">    <span class="keyword">if</span> i &gt; <span class="number">1</span>:</div><div class="line">        <span class="keyword">break</span>  <span class="comment"># otherwise the generator would loop indefinitely</span></div></pre></td></tr></table></figure></div></div><h2 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h2><h3 id="apply-transform"><a href="#apply-transform" class="headerlink" title="apply_transform"></a>apply_transform</h3><h3 id="fit-1"><a href="#fit-1" class="headerlink" title="fit"></a>fit</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fit(x,augment=<span class="keyword">False</span>,rounds=<span class="number">1</span>,seed=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>Once we create the ImageDataGenerator and define the data augment parameters, we can use <code>fit</code> to augment the existing training dataset.</p><ul><li><strong>x</strong>: Sample data. Should have rank 4, i.e., <code>shape=(sample_num, height, width, channel)</code>. In case of grayscale data, the channels axis should have value 1, in case of RGB data, it should have value 3, and in case of RGBA data, it should have value 4. </li></ul><h3 id="flow"><a href="#flow" class="headerlink" title="flow"></a>flow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">flow(x, y=<span class="keyword">None</span>, batch_size=<span class="number">32</span>, shuffle=<span class="keyword">True</span>, sample_weight=<span class="keyword">None</span>, seed=<span class="keyword">None</span>, save_to_dir=<span class="keyword">None</span>, save_prefix=<span class="string">''</span>, save_format=<span class="string">'png'</span>, subset=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>Takes data &amp; label arrays, generates batches of augmented data.</p><ul><li><strong>x</strong>: Input data. Numpy array of rank 4 or a tuple. If tuple, the first element should contain the images and the second element another numpy array or a list of numpy arrays that gets passed to the output without any modifications. Can be used to feed the model miscellaneous data along with the images. In case of grayscale data, the channels axis of the image array should have value 1, in case of RGB data, it should have value 3, and in case of RGBA data, it should have value 4.</li><li><strong>y</strong>: Labels.</li><li><strong>batch_size</strong>: Int (default: 32).</li><li><strong>shuffle</strong>: Boolean (default: True).</li><li><strong>sample_weight</strong>: Sample weights.</li><li><strong>seed</strong>: Int (default: None).</li><li><strong>save_to_dir</strong>: None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).</li><li><strong>save_prefix</strong>: Str (default: <code>&#39;&#39;</code>). Prefix to use for filenames of saved pictures (only relevant if <code>save_to_dir</code> is set).</li><li><strong>save_format</strong>: one of “png”, “jpeg” (only relevant if <code>save_to_dir</code> is set). Default: “png”.</li><li><strong>subset</strong>: Subset of data (<code>&quot;training&quot;</code> or <code>&quot;validation&quot;</code>) if <code>validation_split</code> is set in <code>ImageDataGenerator</code>.</li></ul><p><strong>Return</strong></p><p>An <code>Iterator</code> yielding tuples of <code>(x, y)</code> where <code>x</code> is a numpy array of image data (in the case of a single image input) or a list of numpy arrays (in the case with additional inputs) and <code>y</code> is a numpy array of corresponding labels. If ‘sample_weight’ is not None, the yielded tuples are of the form <code>(x, y, sample_weight)</code>. If <code>y</code> is None, only the numpy array <code>x</code> is returned.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</div><div class="line">y_train = np_utils.to_categorical(y_train, num_classes)</div><div class="line">y_test = np_utils.to_categorical(y_test, num_classes)</div><div class="line"></div><div class="line">datagen = ImageDataGenerator(</div><div class="line">    featurewise_center=<span class="keyword">True</span>,</div><div class="line">    featurewise_std_normalization=<span class="keyword">True</span>,</div><div class="line">    rotation_range=<span class="number">20</span>,</div><div class="line">    width_shift_range=<span class="number">0.2</span>,</div><div class="line">    height_shift_range=<span class="number">0.2</span>,</div><div class="line">    horizontal_flip=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># compute quantities required for featurewise normalization</span></div><div class="line"><span class="comment"># (std, mean, and principal components if ZCA whitening is applied)</span></div><div class="line">datagen.fit(x_train)</div><div class="line"></div><div class="line"><span class="comment"># fits the model on batches with real-time data augmentation:</span></div><div class="line">model.fit_generator(datagen.flow(x_train, y_train, batch_size=<span class="number">32</span>),</div><div class="line">                    steps_per_epoch=len(x_train) / <span class="number">32</span>, epochs=epochs)</div><div class="line"></div><div class="line"><span class="comment"># here's a more "manual" example</span></div><div class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</div><div class="line">    print(<span class="string">'Epoch'</span>, e)</div><div class="line">    batches = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> x_batch, y_batch <span class="keyword">in</span> datagen.flow(x_train, y_train, batch_size=<span class="number">32</span>):</div><div class="line">        model.fit(x_batch, y_batch)</div><div class="line">        batches += <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> batches &gt;= len(x_train) / <span class="number">32</span>:</div><div class="line">            <span class="comment"># we need to break the loop by hand because</span></div><div class="line">            <span class="comment"># the generator loops indefinitely</span></div><div class="line">            <span class="keyword">break</span></div></pre></td></tr></table></figure><h3 id="flow-from-directory"><a href="#flow-from-directory" class="headerlink" title="flow_from_directory"></a>flow_from_directory</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">flow_from_directory(directory, target_size=(<span class="number">256</span>, <span class="number">256</span>), color_mode=<span class="string">'rgb'</span>, classes=<span class="keyword">None</span>, class_mode=<span class="string">'categorical'</span>, batch_size=<span class="number">32</span>, shuffle=<span class="keyword">True</span>, seed=<span class="keyword">None</span>, save_to_dir=<span class="keyword">None</span>, save_prefix=<span class="string">''</span>, save_format=<span class="string">'png'</span>, follow_links=<span class="keyword">False</span>, subset=<span class="keyword">None</span>, interpolation=<span class="string">'nearest'</span>)</div></pre></td></tr></table></figure><p>Takes the path to a directory &amp; generates batches of augmented data.</p><ul><li><p><strong>directory</strong>: string, path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator. See <a href="https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d" target="_blank" rel="noopener">this script</a> for more details.</p></li><li><p><strong>target_size</strong>: Tuple of integers <code>(height, width)</code>, default: <code>(256, 256)</code>. The dimensions to which all images found will be resized.</p></li><li><p><strong>color_mode</strong>: One of “grayscale”, “rgb”, “rgba”. Default: “rgb”. Whether the images will be converted to have 1, 3, or 4 channels.</p></li><li><p><strong>classes</strong>: Optional list of class subdirectories (e.g. <code>[&#39;dogs&#39;, &#39;cats&#39;]</code>). Default: None. If not provided, the list of classes will be automatically inferred from the subdirectory names/structure under <code>directory</code>, where each subdirectory will be treated as a different class (and the order of the classes, which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute <code>class_indices</code>.</p></li><li><p>class_mode</p><p>: One of “categorical”, “binary”, “sparse”, “input”, or None. Default: “categorical”. Determines the type of label arrays that are returned:</p><ul><li>“categorical” will be 2D one-hot encoded labels,</li><li>“binary” will be 1D binary labels, “sparse” will be 1D integer labels,</li><li>“input” will be images identical to input images (mainly used to work with autoencoders).</li><li>If None, no labels are returned (the generator will only yield batches of image data, which is useful to use with <code>model.predict_generator()</code>). Please note that in case of class_mode None, the data still needs to reside in a subdirectory of <code>directory</code> for it to work correctly.</li></ul></li><li><p><strong>batch_size</strong>: Size of the batches of data (default: 32).</p></li><li><p><strong>shuffle</strong>: Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.</p></li><li><p><strong>seed</strong>: Optional random seed for shuffling and transformations.</p></li><li><p><strong>save_to_dir</strong>: None or str (default: None). This allows you to optionally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).</p></li><li><p><strong>save_prefix</strong>: Str. Prefix to use for filenames of saved pictures (only relevant if <code>save_to_dir</code> is set).</p></li><li><p><strong>save_format</strong>: One of “png”, “jpeg” (only relevant if <code>save_to_dir</code> is set). Default: “png”.</p></li><li><p><strong>follow_links</strong>: Whether to follow symlinks inside class subdirectories (default: False).</p></li><li><p><strong>subset</strong>: Subset of data (<code>&quot;training&quot;</code> or <code>&quot;validation&quot;</code>) if <code>validation_split</code> is set in <code>ImageDataGenerator</code>.</p></li><li><p><strong>interpolation</strong>: Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are <code>&quot;nearest&quot;</code>, <code>&quot;bilinear&quot;</code>, and <code>&quot;bicubic&quot;</code>. If PIL version 1.1.3 or newer is installed, <code>&quot;lanczos&quot;</code> is also supported. If PIL version 3.4.0 or newer is installed, <code>&quot;box&quot;</code> and <code>&quot;hamming&quot;</code> are also supported. By default, <code>&quot;nearest&quot;</code> is used.</p></li></ul><p><strong>RETURN</strong></p><p>A <code>DirectoryIterator</code> yielding tuples of <code>(x, y)</code> where <code>x</code> is a numpy array containing a batch of images with shape <code>(batch_size, *target_size, channels)</code> and <code>y</code> is a numpy array of corresponding labels.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">train_datagen = ImageDataGenerator(</div><div class="line">        rescale=<span class="number">1.</span>/<span class="number">255</span>,</div><div class="line">        shear_range=<span class="number">0.2</span>,</div><div class="line">        zoom_range=<span class="number">0.2</span>,</div><div class="line">        horizontal_flip=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</div><div class="line"></div><div class="line">train_generator = train_datagen.flow_from_directory(</div><div class="line">        <span class="string">'data/train'</span>,</div><div class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</div><div class="line">        batch_size=<span class="number">32</span>,</div><div class="line">        class_mode=<span class="string">'binary'</span>)</div><div class="line"></div><div class="line">validation_generator = test_datagen.flow_from_directory(</div><div class="line">        <span class="string">'data/validation'</span>,</div><div class="line">        target_size=(<span class="number">150</span>, <span class="number">150</span>),</div><div class="line">        batch_size=<span class="number">32</span>,</div><div class="line">        class_mode=<span class="string">'binary'</span>)</div><div class="line"></div><div class="line">model.fit_generator(</div><div class="line">        train_generator,</div><div class="line">        steps_per_epoch=<span class="number">2000</span>,</div><div class="line">        epochs=<span class="number">50</span>,</div><div class="line">        validation_data=validation_generator,</div><div class="line">        validation_steps=<span class="number">800</span>)</div></pre></td></tr></table></figure><p><strong>Example of transforming images and masks together.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># we create two instances with the same arguments</span></div><div class="line">data_gen_args = dict(featurewise_center=<span class="keyword">True</span>,</div><div class="line">                     featurewise_std_normalization=<span class="keyword">True</span>,</div><div class="line">                     rotation_range=<span class="number">90</span>,</div><div class="line">                     width_shift_range=<span class="number">0.1</span>,</div><div class="line">                     height_shift_range=<span class="number">0.1</span>,</div><div class="line">                     zoom_range=<span class="number">0.2</span>)</div><div class="line">image_datagen = ImageDataGenerator(**data_gen_args)</div><div class="line">mask_datagen = ImageDataGenerator(**data_gen_args)</div><div class="line"></div><div class="line"><span class="comment"># Provide the same seed and keyword arguments to the fit and flow methods</span></div><div class="line">seed = <span class="number">1</span></div><div class="line">image_datagen.fit(images, augment=<span class="keyword">True</span>, seed=seed)</div><div class="line">mask_datagen.fit(masks, augment=<span class="keyword">True</span>, seed=seed)</div><div class="line"></div><div class="line">image_generator = image_datagen.flow_from_directory(</div><div class="line">    <span class="string">'data/images'</span>,</div><div class="line">    class_mode=<span class="keyword">None</span>,</div><div class="line">    seed=seed)</div><div class="line"></div><div class="line">mask_generator = mask_datagen.flow_from_directory(</div><div class="line">    <span class="string">'data/masks'</span>,</div><div class="line">    class_mode=<span class="keyword">None</span>,</div><div class="line">    seed=seed)</div><div class="line"></div><div class="line"><span class="comment"># combine generators into one which yields image and masks</span></div><div class="line">train_generator = zip(image_generator, mask_generator)</div><div class="line"></div><div class="line">model.fit_generator(</div><div class="line">    train_generator,</div><div class="line">    steps_per_epoch=<span class="number">2000</span>,</div><div class="line">    epochs=<span class="number">50</span>)</div></pre></td></tr></table></figure><p> Since we only have few examples, our number one concern should be <strong>overfitting</strong>. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three, images of people who are sailors, and among them only one lumberjack wears a cap, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.</p><p>Data augmentation is one way to fight overfitting, but it isn’t enough since our augmented samples are still highly correlated. Your main focus for fighting overfitting should be the entropic capacity of your model —how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.</p><p>There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.</p><p>In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure><h1 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h1><h2 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h2><h3 id="模型保存-1"><a href="#模型保存-1" class="headerlink" title="模型保存"></a>模型保存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">json_string = model.to_json()</div><div class="line">open(<span class="string">'model.json'</span>, <span class="string">'w'</span>).write(json_string)</div><div class="line">model.save_weights(<span class="string">'weights.h5'</span>)</div></pre></td></tr></table></figure><h3 id="模型载入"><a href="#模型载入" class="headerlink" title="模型载入"></a>模型载入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> model_from_json</div><div class="line">model = model_from_json(open(<span class="string">'keras_modelB/model.json'</span>).read())</div><div class="line">model.load_weights(<span class="string">'keras_modelB/weights.h5'</span>)</div></pre></td></tr></table></figure><h2 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</div><div class="line">model.save(<span class="string">'my_model.h5'</span>)</div><div class="line">model = load_model(<span class="string">'my_model.h5'</span>)</div></pre></td></tr></table></figure><h1 id="Keras-Callback"><a href="#Keras-Callback" class="headerlink" title="Keras-Callback"></a>Keras-Callback</h1><h2 id="ModelCheckpoint"><a href="#ModelCheckpoint" class="headerlink" title="ModelCheckpoint"></a><a href="https://machinelearningmastery.com/check-point-deep-learning-models-keras/" target="_blank" rel="noopener">ModelCheckpoint</a></h2><h3 id="Checkpoint-Neural-Network-Model-Improvements"><a href="#Checkpoint-Neural-Network-Model-Improvements" class="headerlink" title="Checkpoint Neural Network Model Improvements"></a>Checkpoint Neural Network Model Improvements</h3><p>Checkpoint is an approach where a snapshot of the state of the system is taken in case of system failure. If there is a problem, not all is lost. The checkpoint may be used directly, or used as the starting point for a new run, picking up where it left off. When training deep learning models, the checkpoint is the weights of the model. These weights can be used to make predictions as is, or used as the basis for ongoing training.</p><p>The ModelCheckpoint callback class allows you to define where to checkpoint the model weights, how the file should named and under what circumstances to make a checkpoint of the model. The API allows you to specify which metric to monitor, such as loss or accuracy on the training or validation dataset. You can specify whether to look for an improvement in maximizing or minimizing the score. Finally, the filename that you use to store the weights can include variables like the epoch number or metric. The ModelCheckpoint can then be passed to the training process when calling the fit() function on the model.</p><p>Note, you may need to install the <a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py library</a> to output network weights in HDF5 format.</p><p>Checkpointing is setup to save the network weights only when there is an improvement in classification accuracy on the validation dataset (monitor=’val_acc’ and mode=’max’). The weights are stored in a file that includes the score in the filename (weights-improvement-{val_acc=.2f}.hdf5).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Checkpoint the weights when validation accuracy improves</span></div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div><div class="line"><span class="comment"># load pima indians dataset</span></div><div class="line">dataset = numpy.loadtxt(<span class="string">"pima-indians-diabetes.csv"</span>, delimiter=<span class="string">","</span>)</div><div class="line"><span class="comment"># split into input (X) and output (Y) variables</span></div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">8</span>]</div><div class="line">Y = dataset[:,<span class="number">8</span>]</div><div class="line"><span class="comment"># create model</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">12</span>, input_dim=<span class="number">8</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">8</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># Compile model</span></div><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line"><span class="comment"># checkpoint</span></div><div class="line">filepath=<span class="string">"weights-improvement-&#123;epoch:02d&#125;-&#123;val_acc:.2f&#125;.hdf5"</span></div><div class="line">checkpoint = ModelCheckpoint(filepath, monitor=<span class="string">'val_acc'</span>, verbose=<span class="number">1</span>, save_best_only=<span class="keyword">True</span>, mode=<span class="string">'max'</span>)</div><div class="line">callbacks_list = [checkpoint]</div><div class="line"><span class="comment"># Fit the model</span></div><div class="line">model.fit(X, Y, validation_split=<span class="number">0.33</span>, epochs=<span class="number">150</span>, batch_size=<span class="number">10</span>, callbacks=callbacks_list, verbose=<span class="number">0</span>)</div></pre></td></tr></table></figure><p>Running the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">Epoch <span class="number">00134</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00135</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00136</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00137</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00138</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00139</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00140</span>: val_acc improved <span class="keyword">from</span> <span class="number">0.83465</span> to <span class="number">0.83858</span>, saving model to weights-improvement<span class="number">-140</span><span class="number">-0.84</span>.hdf5</div><div class="line">Epoch <span class="number">00141</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00142</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00143</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00144</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00145</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00146</span>: val_acc improved <span class="keyword">from</span> <span class="number">0.83858</span> to <span class="number">0.84252</span>, saving model to weights-improvement<span class="number">-146</span><span class="number">-0.84</span>.hdf5</div><div class="line">Epoch <span class="number">00147</span>: val_acc did <span class="keyword">not</span> improve</div><div class="line">Epoch <span class="number">00148</span>: val_acc improved <span class="keyword">from</span> <span class="number">0.84252</span> to <span class="number">0.84252</span>, saving model to weights-improvement<span class="number">-148</span><span class="number">-0.84</span>.hdf5</div><div class="line">Epoch <span class="number">00149</span>: val_acc did <span class="keyword">not</span> improve</div></pre></td></tr></table></figure><h3 id="Loading-a-Check-Pointed-Neural-Network-Model"><a href="#Loading-a-Check-Pointed-Neural-Network-Model" class="headerlink" title="Loading a Check-Pointed Neural Network Model"></a>Loading a Check-Pointed Neural Network Model</h3><p>Now that you have seen how to checkpoint your deep learning models during training, you need to review how to load and use a checkpointed model. </p><p>In the example below, the model structure is known and the best weights are loaded from the previous experiment, stored in the working directory in the weights.best.hdf5 file.</p><p>The model is then used to make predictions on the entire dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># How to load and use weights from a checkpoint</span></div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div><div class="line"><span class="comment"># create model</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">12</span>, input_dim=<span class="number">8</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">8</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># load weights</span></div><div class="line">model.load_weights(<span class="string">"weights.best.hdf5"</span>)</div><div class="line"><span class="comment"># Compile model (required to make predictions)</span></div><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">print(<span class="string">"Created model and loaded weights from file"</span>)</div><div class="line"><span class="comment"># load pima indians dataset</span></div><div class="line">dataset = numpy.loadtxt(<span class="string">"pima-indians-diabetes.csv"</span>, delimiter=<span class="string">","</span>)</div><div class="line"><span class="comment"># split into input (X) and output (Y) variables</span></div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">8</span>]</div><div class="line">Y = dataset[:,<span class="number">8</span>]</div><div class="line"><span class="comment"># estimate accuracy on whole dataset using loaded weights</span></div><div class="line">scores = model.evaluate(X, Y, verbose=<span class="number">0</span>)</div><div class="line">print(<span class="string">"%s: %.2f%%"</span> % (model.metrics_names[<span class="number">1</span>], scores[<span class="number">1</span>]*<span class="number">100</span>))</div></pre></td></tr></table></figure><h2 id="LearningRateScheduler"><a href="#LearningRateScheduler" class="headerlink" title="LearningRateScheduler"></a><a href="https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/" target="_blank" rel="noopener">LearningRateScheduler</a></h2><p>Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure.</p><p>Two popular and easy to use learning rate schedules are as follows:</p><ul><li>Decrease the learning rate gradually based on the epoch.</li><li>Decrease the learning rate using punctuated large drops at specific epochs.</li></ul><p>Next, we will look at how you can use each of these learning rate schedules in turn with Keras.</p><h3 id="Time-Based-Learning-Rate-Schedule"><a href="#Time-Based-Learning-Rate-Schedule" class="headerlink" title="Time-Based Learning Rate Schedule"></a>Time-Based Learning Rate Schedule</h3><p>Keras has a time-based learning rate schedule built in.</p><p>The stochastic gradient descent optimization algorithm implementation in the SGD class has an argument called decay. This argument is used in the time-based learning rate decay schedule equation as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LearningRate = LearningRate * <span class="number">1</span>/(<span class="number">1</span> + decay * epoch)</div></pre></td></tr></table></figure><p>When the decay argument is zero (the default), this has no effect on the learning rate.</p><p>When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.</p><p>For example, if we use the initial learning rate value of 0.1 and the decay of 0.001, the first 5 epochs will adapt the learning rate as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">EpochLearning Rate</div><div class="line"><span class="number">1</span><span class="number">0.1</span></div><div class="line"><span class="number">2</span><span class="number">0.0999000999</span></div><div class="line"><span class="number">3</span><span class="number">0.0997006985</span></div><div class="line"><span class="number">4</span><span class="number">0.09940249103</span></div><div class="line"><span class="number">5</span><span class="number">0.09900646517</span></div></pre></td></tr></table></figure><p>Extending this out to 100 epochs will produce the following graph of learning rate (y axis) versus epoch (x axis):</p><p><img src="/2018/07/23/Keras/Time-Based-Learning-Rate-Schedule.png" alt="ime-Based-Learning-Rate-Schedul"></p><p>You can create a nice default schedule by setting the decay value as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Decay = LearningRate / Epochs</div><div class="line">Decay = <span class="number">0.1</span> / <span class="number">100</span></div><div class="line">Decay = <span class="number">0.001</span></div></pre></td></tr></table></figure><p>The example below demonstrates using the time-based learning rate adaptation schedule in Keras. The learning rate for stochastic gradient descent has been set to a higher value of 0.1. The model is trained for 50 epochs and the decay argument has been set to 0.002, calculated as 0.1/50. Additionally, it can be a good idea to use momentum when using an adaptive learning rate. In this case we use a momentum value of 0.8.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Time Based Learning Rate Decay</span></div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div><div class="line"><span class="comment"># load dataset</span></div><div class="line">dataframe = read_csv(<span class="string">"ionosphere.csv"</span>, header=<span class="keyword">None</span>)</div><div class="line">dataset = dataframe.values</div><div class="line"><span class="comment"># split into input (X) and output (Y) variables</span></div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">34</span>].astype(float)</div><div class="line">Y = dataset[:,<span class="number">34</span>]</div><div class="line"><span class="comment"># encode class values as integers</span></div><div class="line">encoder = LabelEncoder()</div><div class="line">encoder.fit(Y)</div><div class="line">Y = encoder.transform(Y)</div><div class="line"><span class="comment"># create model</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">34</span>, input_dim=<span class="number">34</span>, kernel_initializer=<span class="string">'normal'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'normal'</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># Compile model</span></div><div class="line">epochs = <span class="number">50</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line">decay_rate = learning_rate / epochs</div><div class="line">momentum = <span class="number">0.8</span></div><div class="line">sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=<span class="keyword">False</span>)</div><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=sgd, metrics=[<span class="string">'accuracy'</span>])</div><div class="line"><span class="comment"># Fit the model</span></div><div class="line">model.fit(X, Y, validation_split=<span class="number">0.33</span>, epochs=epochs, batch_size=<span class="number">28</span>, verbose=<span class="number">2</span>)</div></pre></td></tr></table></figure><h3 id="Drop-Based-Learning-Rate-Schedule"><a href="#Drop-Based-Learning-Rate-Schedule" class="headerlink" title="Drop-Based Learning Rate Schedule"></a>Drop-Based Learning Rate Schedule</h3><p>Another popular learning rate schedule used with deep learning models is to systematically drop the learning rate at specific times during training.</p><p>Often this method is implemented by dropping the learning rate by half every fixed number of epochs. For example, we may have an initial learning rate of 0.1 and drop it by 0.5 every 10 epochs. The first 10 epochs of training would use a value of 0.1, in the next 10 epochs a learning rate of 0.05 would be used, and so on.</p><p>If we plot out the learning rates for this example out to 100 epochs you get the graph below showing learning rate (y axis) versus epoch (x axis).</p><p><img src="/2018/07/23/Keras/Drop-Based-Learning-Rate-Schedule-2237380.png" alt="rop-Based-Learning-Rate-Schedule-223738"></p><p>We can implement this in Keras using a the <a href="http://keras.io/callbacks/" target="_blank" rel="noopener">LearningRateScheduler</a> callback when fitting the model.</p><p>The LearningRateScheduler callback allows us to define a function to call that takes the epoch number as an argument and returns the learning rate to use in stochastic gradient descent. When used, the learning rate specified by stochastic gradient descent is ignored.</p><p>In the code below, we use the same example before of a single hidden layer network on the Ionosphere dataset. A new step_decay() function is defined that implements the equation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">LearningRate = InitialLearningRate * DropRate^floor(Epoch / EpochDrop)</div></pre></td></tr></table></figure><p>Where InitialLearningRate is the initial learning rate such as 0.1, the DropRate is the amount that the learning rate is modified each time it is changed such as 0.5, Epoch is the current epoch number and EpochDrop is how often to change the learning rate such as 10.</p><p>Notice that we set the learning rate in the SGD class to 0 to clearly indicate that it is not used. Nevertheless, you can set a momentum term in SGD if you want to use momentum with this learning rate schedule.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Drop-Based Learning Rate Decay</span></div><div class="line"><span class="keyword">import</span> pandas</div><div class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> LearningRateScheduler</div><div class="line"></div><div class="line"><span class="comment"># learning rate schedule</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_decay</span><span class="params">(epoch)</span>:</span></div><div class="line">initial_lrate = <span class="number">0.1</span></div><div class="line">drop = <span class="number">0.5</span></div><div class="line">epochs_drop = <span class="number">10.0</span></div><div class="line">lrate = initial_lrate * math.pow(drop, math.floor((<span class="number">1</span>+epoch)/epochs_drop))</div><div class="line"><span class="keyword">return</span> lrate</div><div class="line"></div><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div><div class="line"><span class="comment"># load dataset</span></div><div class="line">dataframe = read_csv(<span class="string">"ionosphere.csv"</span>, header=<span class="keyword">None</span>)</div><div class="line">dataset = dataframe.values</div><div class="line"><span class="comment"># split into input (X) and output (Y) variables</span></div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">34</span>].astype(float)</div><div class="line">Y = dataset[:,<span class="number">34</span>]</div><div class="line"><span class="comment"># encode class values as integers</span></div><div class="line">encoder = LabelEncoder()</div><div class="line">encoder.fit(Y)</div><div class="line">Y = encoder.transform(Y)</div><div class="line"><span class="comment"># create model</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">34</span>, input_dim=<span class="number">34</span>, kernel_initializer=<span class="string">'normal'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'normal'</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># Compile model</span></div><div class="line">sgd = SGD(lr=<span class="number">0.0</span>, momentum=<span class="number">0.9</span>, decay=<span class="number">0.0</span>, nesterov=<span class="keyword">False</span>)</div><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=sgd, metrics=[<span class="string">'accuracy'</span>])</div><div class="line"><span class="comment"># learning schedule callback</span></div><div class="line">lrate = LearningRateScheduler(step_decay)</div><div class="line">callbacks_list = [lrate]</div><div class="line"><span class="comment"># Fit the model</span></div><div class="line">model.fit(X, Y, validation_split=<span class="number">0.33</span>, epochs=<span class="number">50</span>, batch_size=<span class="number">28</span>, callbacks=callbacks_list, verbose=<span class="number">2</span>)</div></pre></td></tr></table></figure><h2 id="History"><a href="#History" class="headerlink" title="History"></a><a href="https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/" target="_blank" rel="noopener">History</a></h2><p>History callback records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) as well as the loss and accuracy for the validation dataset, if one is set. The history object is returned from calls to the fit() function used to train the model. Metrics are stored in a dictionary in the history member of the object returned.</p><p>For example, you can list the metrics collected in a history object using the following snippet of code after a model is trained:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># list all data in history</span></div><div class="line">print(history.history.keys())</div></pre></td></tr></table></figure><p>For example, for a model trained on a classification problem with a validation dataset, this might produce the following listing:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="string">'acc'</span>, <span class="string">'loss'</span>, <span class="string">'val_acc'</span>, <span class="string">'val_loss'</span>]</div></pre></td></tr></table></figure><h3 id="Visualize-Model-Training-History-in-Keras"><a href="#Visualize-Model-Training-History-in-Keras" class="headerlink" title="Visualize Model Training History in Keras"></a>Visualize Model Training History in Keras</h3><p>The example collects the history, returned from training the model and creates two charts:</p><ol><li>A plot of accuracy on the training and validation datasets over training epochs.</li><li>A plot of loss on the training and validation datasets over training epochs.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Visualize training history</span></div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div><div class="line"><span class="comment"># load pima indians dataset</span></div><div class="line">dataset = numpy.loadtxt(<span class="string">"pima-indians-diabetes.csv"</span>, delimiter=<span class="string">","</span>)</div><div class="line"><span class="comment"># split into input (X) and output (Y) variables</span></div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">8</span>]</div><div class="line">Y = dataset[:,<span class="number">8</span>]</div><div class="line"><span class="comment"># create model</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">12</span>, input_dim=<span class="number">8</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">8</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'uniform'</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line"><span class="comment"># Compile model</span></div><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line"><span class="comment"># Fit the model</span></div><div class="line">history = model.fit(X, Y, validation_split=<span class="number">0.33</span>, epochs=<span class="number">150</span>, batch_size=<span class="number">10</span>, verbose=<span class="number">0</span>)</div><div class="line"><span class="comment"># list all data in history</span></div><div class="line">print(history.history.keys())</div><div class="line"><span class="comment"># summarize history for accuracy</span></div><div class="line">plt.plot(history.history[<span class="string">'acc'</span>])</div><div class="line">plt.plot(history.history[<span class="string">'val_acc'</span>])</div><div class="line">plt.title(<span class="string">'model accuracy'</span>)</div><div class="line">plt.ylabel(<span class="string">'accuracy'</span>)</div><div class="line">plt.xlabel(<span class="string">'epoch'</span>)</div><div class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</div><div class="line">plt.show()</div><div class="line"><span class="comment"># summarize history for loss</span></div><div class="line">plt.plot(history.history[<span class="string">'loss'</span>])</div><div class="line">plt.plot(history.history[<span class="string">'val_loss'</span>])</div><div class="line">plt.title(<span class="string">'model loss'</span>)</div><div class="line">plt.ylabel(<span class="string">'loss'</span>)</div><div class="line">plt.xlabel(<span class="string">'epoch'</span>)</div><div class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The plots are provided below. The history for the validation dataset is labeled test by convention as it is indeed a test dataset for the model.</p><p>From the plot of accuracy we can see that the model could probably be trained a little more as the trend for accuracy on both datasets is still rising for the last few epochs. We can also see that the model has not yet over-learned the training dataset, showing comparable skill on both datasets.</p><p><img src="/2018/07/23/Keras/history_training_dataset.png" alt="istory_training_datase"></p><p>From the plot of loss, we can see that the model has comparable performance on both train and validation datasets (labeled test). If these parallel plots start to depart consistently, it might be a sign to stop training at an earlier epoch.</p><p><img src="/2018/07/23/Keras/history_validation_dataset.png" alt="istory_validation_datase"></p><h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p><a href="http://www.moxleystratton.com/tensorflow-visualizing-weights/" target="_blank" rel="noopener">Visualizing Neuron Weights During Training</a> <a href="**https**://www.jianshu.com/p/25e30055d7ac">如何在使用train_on_batch时候调用tensorboard</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_tensorboard</span><span class="params">(self,generator_step, summary_writer,losses)</span>:</span></div><div class="line"></div><div class="line">        summary = tf.Summary()</div><div class="line"></div><div class="line">        value = summary.value.add()</div><div class="line">        value.simple_value = losses[<span class="number">1</span>]</div><div class="line">        value.tag = <span class="string">'Critic Real Loss'</span></div><div class="line"></div><div class="line">        value = summary.value.add()</div><div class="line">        value.simple_value = losses[<span class="number">2</span>]</div><div class="line">        value.tag = <span class="string">'Critic Fake Loss'</span></div><div class="line"></div><div class="line">        value = summary.value.add()</div><div class="line">        value.simple_value = losses[<span class="number">3</span>]</div><div class="line">        value.tag = <span class="string">'Generator Loss'</span></div><div class="line"></div><div class="line">        value = summary.value.add()</div><div class="line">        value.simple_value = losses[<span class="number">1</span>] - losses[<span class="number">2</span>]</div><div class="line">        value.tag = <span class="string">'Critic Loss (D_real - D_fake)'</span></div><div class="line"></div><div class="line">        value = summary.value.add()</div><div class="line">        value.simple_value = losses[<span class="number">1</span>] + losses[<span class="number">2</span>]</div><div class="line">        value.tag = <span class="string">'Critic Loss (D_fake + D_real)'</span></div><div class="line"></div><div class="line">        summary_writer.add_summary(summary, generator_step)</div><div class="line">        summary_writer.flush()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,epochs,batch_size=<span class="number">10</span>,sample_interval=<span class="number">100</span>)</span>:</span></div><div class="line">        summary_writer = tf.summary.FileWriter(<span class="string">'./logs/trainBoth'</span>)</div><div class="line">        generator_step = <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(range(<span class="number">1</span>, epochs + <span class="number">1</span>)):</div><div class="line">            <span class="keyword">for</span> e, (figure_imgs, pose_imgs) <span class="keyword">in</span> tqdm(enumerate(self.data_loader.load_batch(batch_size=batch_size))):</div><div class="line">                    <span class="comment"># Train the critic</span></div><div class="line"></div><div class="line">                    figure_loss_real = self.figure_critic.train_on_batch(figure_imgs, valid)</div><div class="line">                    figure_loss_fake = self.figure_critic.train_on_batch(gen_figure_imgs, fake)</div><div class="line">                    d_figure_loss = <span class="number">0.5</span> * np.add(figure_loss_real, figure_loss_fake)</div><div class="line"></div><div class="line">                print(self.figure_critic.metrics_names,d_figure_loss)</div><div class="line">                losses = np.empty(shape=<span class="number">1</span>)</div><div class="line">                losses = np.append(losses, figure_loss_real)</div><div class="line">                losses = np.append(losses, figure_loss_fake)</div><div class="line"></div><div class="line">                <span class="comment"># ---------------------</span></div><div class="line">                <span class="comment">#  Train Generator</span></div><div class="line">                <span class="comment"># ---------------------</span></div><div class="line"></div><div class="line">                figure_loss = self.figure_EN.train_on_batch([figure_noise,pose_imgs],valid)</div><div class="line">                print(self.figure_EN.metrics_names,figure_loss)</div><div class="line">                losses = np.append(losses, figure_loss)</div><div class="line">                self.write_to_tensorboard(generator_step, summary_writer, losses)</div><div class="line">                generator_step += <span class="number">1</span></div></pre></td></tr></table></figure><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line">SUMMARY_DIR = <span class="string">"./graph"</span></div><div class="line">BATCH_SIZE = <span class="number">100</span></div><div class="line">TRAIN_STEPS = <span class="number">3000</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span><span class="params">(var, name)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</div><div class="line">        tf.summary.histogram(name, var)</div><div class="line">        mean = tf.reduce_mean(var)</div><div class="line">        tf.summary.scalar(<span class="string">'mean/'</span> + name, mean)</div><div class="line">        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</div><div class="line">        tf.summary.scalar(<span class="string">'stddev/'</span> + name, stddev)</div><div class="line"></div><div class="line"><span class="comment"># 2. 生成一层全链接的神经网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_layer</span><span class="params">(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(layer_name):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">            weights = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=<span class="number">0.1</span>))</div><div class="line">            variable_summaries(weights, layer_name + <span class="string">'/weights'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[output_dim]))</div><div class="line">            variable_summaries(biases, layer_name + <span class="string">'/biases'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">            preactivate = tf.matmul(input_tensor, weights) + biases</div><div class="line">            tf.summary.histogram(layer_name + <span class="string">'/pre_activations'</span>, preactivate)</div><div class="line">        activations = act(preactivate, name=<span class="string">'activation'</span>)</div><div class="line"></div><div class="line">        <span class="comment"># 记录神经网络节点输出在经过激活函数之后的分布。</span></div><div class="line">        tf.summary.histogram(layer_name + <span class="string">'/activations'</span>, activations)</div><div class="line">        <span class="keyword">return</span> activations</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    mnist = input_data.read_data_sets(<span class="string">"D:\pyprogram"</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'input'</span>):</div><div class="line">        x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'x-input'</span>)</div><div class="line">        y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">'y-input'</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'input_reshape'</span>):</div><div class="line">        image_shaped_input = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">        tf.summary.image(<span class="string">'input'</span>, image_shaped_input, <span class="number">10</span>)</div><div class="line"></div><div class="line">    hidden1 = nn_layer(x, <span class="number">784</span>, <span class="number">500</span>, <span class="string">'layer1'</span>)</div><div class="line">    y = nn_layer(hidden1, <span class="number">500</span>, <span class="number">10</span>, <span class="string">'layer2'</span>, act=tf.identity)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cross_entropy'</span>):</div><div class="line">        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))</div><div class="line">        tf.summary.scalar(<span class="string">'cross_entropy'</span>, cross_entropy)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">        train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</div><div class="line">            correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</div><div class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">        tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</div><div class="line"></div><div class="line">    merged = tf.summary.merge_all()</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        summary_writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)</div><div class="line">        tf.global_variables_initializer().run()</div><div class="line"></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAIN_STEPS):</div><div class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</div><div class="line">            <span class="comment"># 运行训练步骤以及所有的日志生成操作，得到这次运行的日志。</span></div><div class="line">            summary, _ = sess.run([merged, train_step], feed_dict=&#123;x: xs, y_: ys&#125;)</div><div class="line">            <span class="comment"># 将得到的所有日志写入日志文件，这样TensorBoard程序就可以拿到这次运行所对应的</span></div><div class="line">            <span class="comment"># 运行信息。</span></div><div class="line">            summary_writer.add_summary(summary, i)</div><div class="line"></div><div class="line">    summary_writer.close()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><p><img src="/2018/07/23/Keras/Screen Shot 2019-03-13 at 10.25.34 PM.png" alt="creen Shot 2019-03-13 at 10.25.34 P"></p><h1 id="Customerize-Optimizer"><a href="#Customerize-Optimizer" class="headerlink" title="Customerize Optimizer"></a>Customerize Optimizer</h1><p>The following example is an example of modifing the implementation of Adam optimizer, so that it can support differential privacy.</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Optimizer</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip_norm</span><span class="params">(g, c, n)</span>:</span></div><div class="line">    <span class="keyword">if</span> c &gt; <span class="number">0</span>:</div><div class="line">        g = K.switch(n &gt;= c, g * c / n, g)</div><div class="line">    <span class="keyword">return</span> g</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoisyAdam</span><span class="params">(Optimizer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.001</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>,</span></span></div><div class="line"><span class="function"><span class="params">                 epsilon=<span class="number">1e-8</span>, decay=<span class="number">0.</span>, noise=<span class="number">0.</span>, **kwargs)</span>:</span></div><div class="line">        super(NoisyAdam, self).__init__(**kwargs)</div><div class="line">        self.iterations = K.variable(<span class="number">0</span>, name=<span class="string">'iterations'</span>)</div><div class="line">        self.lr = K.variable(lr, name=<span class="string">'lr'</span>)</div><div class="line">        self.beta_1 = K.variable(beta_1, name=<span class="string">'beta_1'</span>)</div><div class="line">        self.beta_2 = K.variable(beta_2, name=<span class="string">'beta_2'</span>)</div><div class="line">        self.epsilon = epsilon</div><div class="line">        self.decay = K.variable(decay, name=<span class="string">'decay'</span>)</div><div class="line">        self.initial_decay = decay</div><div class="line">        self.noise = noise</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gradients</span><span class="params">(self, loss, params)</span>:</span></div><div class="line">        grads = K.gradients(loss, params)</div><div class="line">        <span class="keyword">if</span> hasattr(self, <span class="string">'clipnorm'</span>) <span class="keyword">and</span> self.clipnorm &gt; <span class="number">0</span>:</div><div class="line">            norm = K.sqrt(sum([K.sum(K.square(g)) <span class="keyword">for</span> g <span class="keyword">in</span> grads]))</div><div class="line">            grads = [clip_norm(g, self.clipnorm, norm) <span class="keyword">for</span> g <span class="keyword">in</span> grads]</div><div class="line">        <span class="keyword">if</span> hasattr(self, <span class="string">'clipvalue'</span>) <span class="keyword">and</span> self.clipvalue &gt; <span class="number">0</span>:</div><div class="line">            grads = [K.clip(g, -self.clipvalue, self.clipvalue) <span class="keyword">for</span> g <span class="keyword">in</span> grads]</div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.noise &gt; <span class="number">0</span>:</div><div class="line">            grads = [(g + K.random_normal(g.shape, mean=<span class="number">0</span>,</div><div class="line">                                          stddev=(self.noise * self.clipnorm)))</div><div class="line">                     <span class="keyword">for</span> g <span class="keyword">in</span> grads]</div><div class="line">        <span class="keyword">return</span> grads</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_updates</span><span class="params">(self, loss,params)</span>:</span></div><div class="line">        grads = self.get_gradients(loss, params)</div><div class="line">        self.updates = [K.update_add(self.iterations, <span class="number">1</span>)]</div><div class="line"></div><div class="line">        lr = self.lr</div><div class="line">        <span class="keyword">if</span> self.initial_decay &gt; <span class="number">0</span>:</div><div class="line">            lr *= (<span class="number">1.</span> / (<span class="number">1.</span> + self.decay * self.iterations))</div><div class="line"></div><div class="line">        t = self.iterations + <span class="number">1</span></div><div class="line">        lr_t = lr * (K.sqrt(<span class="number">1.</span> - K.pow(self.beta_2, t)) /</div><div class="line">                     (<span class="number">1.</span> - K.pow(self.beta_1, t)))</div><div class="line"></div><div class="line">        shapes = [K.get_variable_shape(p) <span class="keyword">for</span> p <span class="keyword">in</span> params]</div><div class="line">        ms = [K.zeros(shape) <span class="keyword">for</span> shape <span class="keyword">in</span> shapes]</div><div class="line">        vs = [K.zeros(shape) <span class="keyword">for</span> shape <span class="keyword">in</span> shapes]</div><div class="line">        self.weights = [self.iterations] + ms + vs</div><div class="line"></div><div class="line">        <span class="keyword">for</span> p, g, m, v <span class="keyword">in</span> zip(params, grads, ms, vs):</div><div class="line">            m_t = (self.beta_1 * m) + (<span class="number">1.</span> - self.beta_1) * g</div><div class="line">            v_t = (self.beta_2 * v) + (<span class="number">1.</span> - self.beta_2) * K.square(g)</div><div class="line">            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)</div><div class="line"></div><div class="line">            self.updates.append(K.update(m, m_t))</div><div class="line">            self.updates.append(K.update(v, v_t))</div><div class="line">            new_p = p_t</div><div class="line">            <span class="comment"># Apply constraints.</span></div><div class="line">            <span class="keyword">if</span> getattr(p, <span class="string">'constraint'</span>, <span class="keyword">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                new_p = p.constraint(new_p)</div><div class="line"></div><div class="line">            self.updates.append(K.update(p, new_p))</div><div class="line">        <span class="keyword">return</span> self.updates</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></div><div class="line">        config = &#123;<span class="string">'lr'</span>: float(K.get_value(self.lr)),</div><div class="line">                  <span class="string">'beta_1'</span>: float(K.get_value(self.beta_1)),</div><div class="line">                  <span class="string">'beta_2'</span>: float(K.get_value(self.beta_2)),</div><div class="line">                  <span class="string">'decay'</span>: float(K.get_value(self.decay)),</div><div class="line">                  <span class="string">'epsilon'</span>: self.epsilon&#125;</div><div class="line">        base_config = super(NoisyAdam, self).get_config()</div><div class="line">        <span class="keyword">return</span> dict(list(base_config.items()) + list(config.items()))</div></pre></td></tr></table></figure></div></div><h1 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h1><h2 id="VGG-Feature-Loss"><a href="#VGG-Feature-Loss" class="headerlink" title="VGG Feature Loss"></a>VGG Feature Loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG19</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vgg</span><span class="params">()</span>:</span></div><div class="line">    vgg = VGG19(weights=<span class="string">"imagenet"</span>)</div><div class="line">    vgg.outputs = [vgg.layers[<span class="number">9</span>].output]</div><div class="line">    img = layers.Input(shape=self.img_shape)</div><div class="line">    img_features = vgg(img)</div><div class="line">    <span class="keyword">return</span> Model(img, img_features)</div><div class="line"><span class="comment">##########################################</span></div><div class="line">vgg = build_vgg()</div><div class="line">vgg.trainable = <span class="keyword">False</span></div><div class="line">vgg.compile(loss=<span class="string">'mse'</span>,optimizer=optimizer,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="comment">##########################################</span></div><div class="line">fake_pose_vgg_feature = vgg(pose_recons)</div><div class="line">pose_ende = Model(pose_img,fake_pose_vgg_feature)</div><div class="line"> </div><div class="line"><span class="comment">##########################################</span></div><div class="line">pose_real_vgg_feature = vgg.predict(pose_imgs)</div><div class="line">pose_loss = pose_ende.train_on_batch(pose_imgs,pose_real_vgg_feature)</div></pre></td></tr></table></figure><h2 id="Fine-tune-ref"><a href="#Fine-tune-ref" class="headerlink" title="Fine tune ref"></a>Fine tune <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" target="_blank" rel="noopener">ref</a></h2><h1 id="Print-Gradients-ref"><a href="#Print-Gradients-ref" class="headerlink" title="Print Gradients ref"></a>Print Gradients <a href="http://sujitpal.blogspot.com/2017/10/debugging-keras-networks.html" target="_blank" rel="noopener">ref</a></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradients</span><span class="params">(self,inputs, groundtruth, model)</span>:</span></div><div class="line">    opt = model.optimizer</div><div class="line">    loss = model.total_loss</div><div class="line">    weights = model.weights</div><div class="line">    grads = opt.get_gradients(loss, weights)</div><div class="line">    grad_fn = K.function(inputs=[model.inputs[<span class="number">0</span>],</div><div class="line">                                 model.sample_weights[<span class="number">0</span>],</div><div class="line">                                 model.targets[<span class="number">0</span>],</div><div class="line">                                 K.learning_phase()],</div><div class="line">                         outputs=grads)</div><div class="line">    grad_values = grad_fn([inputs, np.ones(len(inputs)), groundtruth, <span class="number">1</span>])</div><div class="line">    <span class="keyword">return</span> grad_values</div></pre></td></tr></table></figure><p>Then in the main function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">gradients = self.get_gradients(imgs,valid,self.discriminator)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(gradients)):</div><div class="line">    print(i, np.shape(gradients[i]))</div></pre></td></tr></table></figure><h1 id="HyperparametersSearching"><a href="#HyperparametersSearching" class="headerlink" title="HyperparametersSearching"></a>HyperparametersSearching</h1><p><a href="https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/tune_mnist_keras.py" target="_blank" rel="noopener">source</a></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Original Code here:</span></div><div class="line"><span class="comment"># https://github.com/pytorch/examples/blob/master/mnist/main.py</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> argparse</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</div><div class="line"></div><div class="line"><span class="comment"># Training settings</span></div><div class="line">parser = argparse.ArgumentParser(description=<span class="string">"PyTorch MNIST Example"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--batch-size"</span>,</div><div class="line">    type=int,</div><div class="line">    default=<span class="number">64</span>,</div><div class="line">    metavar=<span class="string">"N"</span>,</div><div class="line">    help=<span class="string">"input batch size for training (default: 64)"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--test-batch-size"</span>,</div><div class="line">    type=int,</div><div class="line">    default=<span class="number">1000</span>,</div><div class="line">    metavar=<span class="string">"N"</span>,</div><div class="line">    help=<span class="string">"input batch size for testing (default: 1000)"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--epochs"</span>,</div><div class="line">    type=int,</div><div class="line">    default=<span class="number">1</span>,</div><div class="line">    metavar=<span class="string">"N"</span>,</div><div class="line">    help=<span class="string">"number of epochs to train (default: 1)"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--lr"</span>,</div><div class="line">    type=float,</div><div class="line">    default=<span class="number">0.01</span>,</div><div class="line">    metavar=<span class="string">"LR"</span>,</div><div class="line">    help=<span class="string">"learning rate (default: 0.01)"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--momentum"</span>,</div><div class="line">    type=float,</div><div class="line">    default=<span class="number">0.5</span>,</div><div class="line">    metavar=<span class="string">"M"</span>,</div><div class="line">    help=<span class="string">"SGD momentum (default: 0.5)"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--no-cuda"</span>,</div><div class="line">    action=<span class="string">"store_true"</span>,</div><div class="line">    default=<span class="keyword">False</span>,</div><div class="line">    help=<span class="string">"disables CUDA training"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--seed"</span>,</div><div class="line">    type=int,</div><div class="line">    default=<span class="number">1</span>,</div><div class="line">    metavar=<span class="string">"S"</span>,</div><div class="line">    help=<span class="string">"random seed (default: 1)"</span>)</div><div class="line">parser.add_argument(</div><div class="line">    <span class="string">"--smoke-test"</span>, action=<span class="string">"store_true"</span>, help=<span class="string">"Finish quickly for testing"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_mnist</span><span class="params">(args, config, reporter)</span>:</span></div><div class="line">    vars(args).update(config)</div><div class="line">    args.cuda = <span class="keyword">not</span> args.no_cuda <span class="keyword">and</span> torch.cuda.is_available()</div><div class="line"></div><div class="line">    torch.manual_seed(args.seed)</div><div class="line">    <span class="keyword">if</span> args.cuda:</div><div class="line">        torch.cuda.manual_seed(args.seed)</div><div class="line"></div><div class="line">    kwargs = &#123;<span class="string">"num_workers"</span>: <span class="number">1</span>, <span class="string">"pin_memory"</span>: <span class="keyword">True</span>&#125; <span class="keyword">if</span> args.cuda <span class="keyword">else</span> &#123;&#125;</div><div class="line">    train_loader = torch.utils.data.DataLoader(</div><div class="line">        datasets.MNIST(</div><div class="line">            <span class="string">"~/data"</span>,</div><div class="line">            train=<span class="keyword">True</span>,</div><div class="line">            download=<span class="keyword">False</span>,</div><div class="line">            transform=transforms.Compose([</div><div class="line">                transforms.ToTensor(),</div><div class="line">                transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))</div><div class="line">            ])),</div><div class="line">        batch_size=args.batch_size,</div><div class="line">        shuffle=<span class="keyword">True</span>,</div><div class="line">        **kwargs)</div><div class="line">    test_loader = torch.utils.data.DataLoader(</div><div class="line">        datasets.MNIST(</div><div class="line">            <span class="string">"~/data"</span>,</div><div class="line">            train=<span class="keyword">False</span>,</div><div class="line">            transform=transforms.Compose([</div><div class="line">                transforms.ToTensor(),</div><div class="line">                transforms.Normalize((<span class="number">0.1307</span>, ), (<span class="number">0.3081</span>, ))</div><div class="line">            ])),</div><div class="line">        batch_size=args.test_batch_size,</div><div class="line">        shuffle=<span class="keyword">True</span>,</div><div class="line">        **kwargs)</div><div class="line"></div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">            super(Net, self).__init__()</div><div class="line">            self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</div><div class="line">            self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</div><div class="line">            self.conv2_drop = nn.Dropout2d()</div><div class="line">            self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</div><div class="line">            self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">            x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))</div><div class="line">            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))</div><div class="line">            x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</div><div class="line">            x = F.relu(self.fc1(x))</div><div class="line">            x = F.dropout(x, training=self.training)</div><div class="line">            x = self.fc2(x)</div><div class="line">            <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</div><div class="line"></div><div class="line">    model = Net()</div><div class="line">    <span class="keyword">if</span> args.cuda:</div><div class="line">        model.cuda()</div><div class="line"></div><div class="line">    optimizer = optim.SGD(</div><div class="line">        model.parameters(), lr=args.lr, momentum=args.momentum)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(epoch)</span>:</span></div><div class="line">        model.train()</div><div class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader):</div><div class="line">            <span class="keyword">if</span> args.cuda:</div><div class="line">                data, target = data.cuda(), target.cuda()</div><div class="line">            optimizer.zero_grad()</div><div class="line">            output = model(data)</div><div class="line">            loss = F.nll_loss(output, target)</div><div class="line">            loss.backward()</div><div class="line">            optimizer.step()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></div><div class="line">        model.eval()</div><div class="line">        test_loss = <span class="number">0</span></div><div class="line">        correct = <span class="number">0</span></div><div class="line">        <span class="keyword">with</span> torch.no_grad():</div><div class="line">            <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</div><div class="line">                <span class="keyword">if</span> args.cuda:</div><div class="line">                    data, target = data.cuda(), target.cuda()</div><div class="line">                output = model(data)</div><div class="line">                <span class="comment"># sum up batch loss</span></div><div class="line">                test_loss += F.nll_loss(output, target, reduction=<span class="string">"sum"</span>).item()</div><div class="line">                <span class="comment"># get the index of the max log-probability</span></div><div class="line">                pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">True</span>)</div><div class="line">                correct += pred.eq(</div><div class="line">                    target.data.view_as(pred)).long().cpu().sum()</div><div class="line"></div><div class="line">        test_loss = test_loss / len(test_loader.dataset)</div><div class="line">        accuracy = correct.item() / len(test_loader.dataset)</div><div class="line">        reporter(mean_loss=test_loss, mean_accuracy=accuracy)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, args.epochs + <span class="number">1</span>):</div><div class="line">        train(epoch)</div><div class="line">        test()</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    datasets.MNIST(<span class="string">"~/data"</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>)</div><div class="line">    args = parser.parse_args()</div><div class="line"></div><div class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">    <span class="keyword">import</span> ray</div><div class="line">    <span class="keyword">from</span> ray <span class="keyword">import</span> tune</div><div class="line">    <span class="keyword">from</span> ray.tune.schedulers <span class="keyword">import</span> AsyncHyperBandScheduler</div><div class="line"></div><div class="line">    ray.init()</div><div class="line">    sched = AsyncHyperBandScheduler(</div><div class="line">        time_attr=<span class="string">"training_iteration"</span>,</div><div class="line">        reward_attr=<span class="string">"neg_mean_loss"</span>,</div><div class="line">        max_t=<span class="number">400</span>,</div><div class="line">        grace_period=<span class="number">20</span>)</div><div class="line">    tune.register_trainable(</div><div class="line">        <span class="string">"TRAIN_FN"</span>,</div><div class="line">        <span class="keyword">lambda</span> config, reporter: train_mnist(args, config, reporter))</div><div class="line">    tune.run(</div><div class="line">        <span class="string">"TRAIN_FN"</span>,</div><div class="line">        name=<span class="string">"exp"</span>,</div><div class="line">        scheduler=sched,</div><div class="line">        **&#123;</div><div class="line">            <span class="string">"stop"</span>: &#123;</div><div class="line">                <span class="string">"mean_accuracy"</span>: <span class="number">0.98</span>,</div><div class="line">                <span class="string">"training_iteration"</span>: <span class="number">1</span> <span class="keyword">if</span> args.smoke_test <span class="keyword">else</span> <span class="number">20</span></div><div class="line">            &#125;,</div><div class="line">            <span class="string">"resources_per_trial"</span>: &#123;</div><div class="line">                <span class="string">"cpu"</span>: <span class="number">3</span>,</div><div class="line">                <span class="string">"gpu"</span>: int(<span class="keyword">not</span> args.no_cuda)</div><div class="line">            &#125;,</div><div class="line">            <span class="string">"num_samples"</span>: <span class="number">1</span> <span class="keyword">if</span> args.smoke_test <span class="keyword">else</span> <span class="number">10</span>,</div><div class="line">            <span class="string">"config"</span>: &#123;</div><div class="line">                <span class="string">"lr"</span>: tune.sample_from(</div><div class="line">                    <span class="keyword">lambda</span> spec: np.random.uniform(<span class="number">0.001</span>, <span class="number">0.1</span>)),</div><div class="line">                <span class="string">"momentum"</span>: tune.sample_from(</div><div class="line">                    <span class="keyword">lambda</span> spec: np.random.uniform(<span class="number">0.1</span>, <span class="number">0.9</span>)),</div><div class="line">            &#125;</div><div class="line">        &#125;)</div></pre></td></tr></table></figure></div></div>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> CNN </tag>
            
            <tag> Neural Network </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>AutoEncoder</title>
      <link href="/2018/07/22/AutoEncoder/"/>
      <url>/2018/07/22/AutoEncoder/</url>
      <content type="html"><![CDATA[<p>AutoEncoder通过设计encode和decode过程使输入和输出越来越接近，是一种自监督学习过程，输入图片通过encode进行处理，得到code，再经过decode处理得到输出，我们可以控制encode的输出维数，就相当于强迫encode过程以低维参数学习高维特征。</p><a id="more"></a><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><a href="https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4" target="_blank" rel="noopener">ref1</a> </p><p><img src="/2018/07/22/AutoEncoder/1_eLieLQIprDp4BgC63288kQ.png" alt="1_eLieLQIprDp4BgC63288kQ"></p><p>自编码器包含两个部分：encoder和decoder。</p><p><strong>encoder</strong>的输入是$x$，输出是$h$，通常$h$的维度比$x$少。比如：我们输入encoder一张大小为$100\times 100$大小的图片，输出潜在编码$h$，大小是$100\times 1$。这种情况下，encoder将输入图片映射到了一个低维空间，这样我们只需要很少的空间就可以存储$h$，当然这样的压缩也会丢失一些原始数据的信息。</p><p><strong>decoder</strong>的输入是潜在编码$h$，并且尝试重构出一张图片。假如$h$的大小是$100\times 1$，decoder试图利用$h$重构一张大小为$100\times100$的图片，我们训练decoder使之输出的图片尽量接近原图。</p><p>只有当输入是彼此关联的（比如图片是来自同一个领域），降维才能有效。如果输入是完全随机的图片，降维的效果是很差的。同时这个过程中我们没有使用任何label数据，所以自编码器属于无监督模型。</p><p><a href="https://blog.csdn.net/marsjhao/article/details/73480859" target="_blank" rel="noopener">intuition1</a> </p><p><a href="https://www.youtube.com/watch?v=Tk5B4seA-AU&amp;t=0s&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=26" target="_blank" rel="noopener">video1</a> </p><p><img src="/2018/07/22/AutoEncoder/Screen Shot 2018-07-22 at 12.42.04 AM.png" alt="Screen Shot 2018-07-22 at 12.42.04 AM"></p><p><img src="/2018/07/22/AutoEncoder/Screen Shot 2018-07-22 at 12.33.57 AM.png" alt="Screen Shot 2018-07-22 at 12.33.57 AM"></p><p><img src="/2018/07/22/AutoEncoder/Screen Shot 2018-07-22 at 12.36.22 AM.png" alt="Screen Shot 2018-07-22 at 12.36.22 AM"></p><h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><p><a href="https://blog.csdn.net/u013719780/article/details/53908852" target="_blank" rel="noopener">Tensorflow - Denoising AE</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Flatten, Dense, Reshape</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AE</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.rows = <span class="number">28</span></div><div class="line">        self.cols = <span class="number">28</span></div><div class="line">        self.chanels = <span class="number">1</span></div><div class="line">        self.input_shape = (self.rows,self.cols,self.chanels)</div><div class="line">        self.latent_dim = <span class="number">2</span></div><div class="line"></div><div class="line">        opt = Adam(lr=<span class="number">0.002</span>,beta_1=<span class="number">0.9</span>)</div><div class="line">        self.encoder = self.make_encoder()</div><div class="line">        en_input = Input(self.input_shape)</div><div class="line">        en_out = self.encoder(en_input)</div><div class="line">        self.decoder = self.make_decoder()</div><div class="line">        out_img = self.decoder(en_out)</div><div class="line">        self.ae = Model(en_input,out_img)</div><div class="line">        self.ae.compile(optimizer=opt,loss=<span class="string">'mse'</span>,metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_encoder</span><span class="params">(self)</span>:</span></div><div class="line">        input = Input(self.input_shape)</div><div class="line">        h = Flatten()(input)</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">        h = Dense(<span class="number">1000</span>,activation=<span class="string">'relu'</span>)(h)</div><div class="line">        output = Dense(self.latent_dim)(h)</div><div class="line">        encoder = Model(input,output)</div><div class="line">        encoder.summary()</div><div class="line">        <span class="keyword">return</span> encoder</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_decoder</span><span class="params">(self)</span>:</span></div><div class="line">        input = Input((self.latent_dim,))</div><div class="line">        h = Dense(<span class="number">1000</span>, activation=<span class="string">'relu'</span>)(input)</div><div class="line">        h = Dense(<span class="number">1000</span>, activation=<span class="string">'relu'</span>)(h)</div><div class="line">        h = Dense(np.prod(self.input_shape),activation=<span class="string">'tanh'</span>)(h)</div><div class="line">        output = Reshape(self.input_shape)(h)</div><div class="line">        decoder = Model(input,output)</div><div class="line">        decoder.summary()</div><div class="line">        <span class="keyword">return</span> decoder</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,batch_size=<span class="number">128</span>,epoches=<span class="number">100</span>)</span>:</span></div><div class="line">        (x_train,y_train),(x_test,y_test) = mnist.load_data()</div><div class="line">        x_train = (x_train<span class="number">-127.5</span>)/<span class="number">127.5</span></div><div class="line">        x_test = (x_test<span class="number">-127.5</span>)/<span class="number">127.5</span></div><div class="line">        x_train = np.expand_dims(x_train,axis=<span class="number">3</span>)</div><div class="line">        x_test = np.expand_dims(x_test,axis=<span class="number">3</span>)</div><div class="line">        losses = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(epoches)):</div><div class="line">            idx = np.random.randint(<span class="number">0</span>,len(x_train),batch_size)</div><div class="line">            input_imgs = x_train[idx,:,:,:]</div><div class="line">            loss = self.ae.train_on_batch(input_imgs,input_imgs)</div><div class="line">            losses.append(loss[<span class="number">0</span>])</div><div class="line">        losses = np.array(losses)</div><div class="line">        plt.plot(range(<span class="number">1</span>,len(losses)+<span class="number">1</span>),losses)</div><div class="line">        plt.show()</div><div class="line">        self.pred(x_test)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pred</span><span class="params">(self,x_test)</span>:</span></div><div class="line">        idx = np.random.randint(<span class="number">0</span>,len(x_test),<span class="number">3</span>)</div><div class="line">        image = x_test[idx, :, :, :]</div><div class="line">        generated_img = self.ae.predict(image)</div><div class="line">        generated_img = np.reshape(generated_img,(<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(generated_img)):</div><div class="line">            pic = generated_img[i]</div><div class="line">            pic = pic*<span class="number">127.5</span>+<span class="number">127.5</span></div><div class="line">            img = Image.fromarray(pic)</div><div class="line">            plt.imshow(img)</div><div class="line">            plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    ae = AE()</div><div class="line">    ae.train()</div></pre></td></tr></table></figure><p><img src="/2018/07/22/AutoEncoder/image-20180809104701466.png" alt="image-20180809104701466"></p></div></div>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> AutoEncoder </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Generative Adversarial Networks</title>
      <link href="/2018/07/20/Generative-Adversarial-Networks/"/>
      <url>/2018/07/20/Generative-Adversarial-Networks/</url>
      <content type="html"><![CDATA[<p>Richard Feynman说“如果要真正理解一个东西，我们必须要能够把它创造出来。”</p><p>GAN 启发自博弈论中的二人零和博弈（two-player game），GAN 模型中的两位博弈方分别由生成式模型（generative model）和判别式模型（discriminative model）充当。<strong>生成模型 G 捕捉样本数据的分布</strong>，<strong>用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本</strong>，追求效果是越像真实样本越好；判别模型 D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。</p><a id="more"></a><p><a href="https://blog.csdn.net/on2way/article/details/72773771" target="_blank" rel="noopener">intuition1</a> <a href="https://ctmakro.github.io/site/on_learning/gan.html" target="_blank" rel="noopener">intuition2</a> </p><p><a href="https://zhuanlan.zhihu.com/p/27440393" target="_blank" rel="noopener">code1</a> <a href="http://baijiahao.baidu.com/s?id=1596173515001448323&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">tensorflow-gan</a> </p><p><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html" target="_blank" rel="noopener">video</a> </p><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>通常，我们会用下面这个例子来说明 GAN 的原理：将警察视为判别器，制造假币的犯罪分子视为生成器。一开始，犯罪分子会首先向警察展示一张假币。警察识别出该假币，并向犯罪分子反馈哪些地方是假的。接着，根据警察的反馈，犯罪分子改进工艺，制作一张更逼真的假币给警方检查。这时警方再反馈，犯罪分子再改进工艺。不断重复这一过程，直到警察识别不出真假，那么模型就训练成功了。</p><p><strong>GAN强大之处在于可以自动的学习原始真实样本集的数据分布</strong>，GAN的生成模型最后可以通过噪声生成一个完整的真实数据（比如人脸），说明生成模型已经掌握了从随机噪声到人脸数据的分布规律了，有了这个规律，想生成人脸还不容易。然而这个规律我们开始知道吗？显然不知道，如果让你说从随机噪声到人脸应该服从什么分布，你不可能知道。这是一层层映射之后组合起来的非常复杂的分布映射规律。然而GAN的机制可以学习到，也就是说GAN学习到了真实样本集的数据分布。</p><p><img src="/2018/07/20/Generative-Adversarial-Networks/20170526212621356.png" alt="20170526212621356"></p><p>这张图表明的是GAN的生成网络如何一步步<strong>从均匀分布学习到正太分布</strong>的。原始数据x服从正太分布，这个过程你也没告诉生成网络说你得用正太分布来学习，但是生成网络学习到了。假设你改一下x的分布，不管什么分布，生成网络可能也能学到。</p><h1 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h1><p>假设现在生成网络模型已经有了（当然可能不是最好的生成网络），那么给一堆随机数组，就会得到一堆假的样本集（因为不是最终的生成模型，那么现在生成网络可能就处于劣势，导致生成的样本就不咋地，可能很容易就被判别网络判别出来了说这货是假冒的），但是先不管这个，假设我们现在有了这样的假样本集，真样本集一直都有，现在我们人为的定义真假样本集的标签，因为我们希望真样本集的输出尽可能为1，假样本集为0，很明显这里我们就已经默认真样本集所有的类标签都为1，而假样本集的所有类标签都为0. 有人会说，在真样本集里面的人脸中，可能张三人脸和李四人脸不一样呀，对于这个问题我们需要理解的是，我们现在的任务是什么，我们是想分样本真假，而不是分真样本中那个是张三label、那个是李四label。况且我们也知道，原始真样本的label我们是不知道的。回过头来，我们现在有了真样本集以及它们的label（都是1）、假样本集以及它们的label（都是0），这样单就判别网络来说，此时问题就变成了一个再简单不过的<strong>有监督的二分类问题</strong>了，直接送到神经网络模型中训练就完事了。假设训练完了，下面我们来看生成网络。</p><h1 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h1><p>对于生成网络，想想我们的目的，是生成尽可能逼真的样本。那么原始的生成网络生成的样本你怎么知道它真不真呢？就是送到判别网络中，所以在训练生成网络的时候，我们需要联合判别网络一起才能达到训练的目的。什么意思？就是如果我们单单只用生成网络，那么想想我们怎么去训练？误差来源在哪里？细想一下没有，但是如果我们把刚才的判别网络串接在生成网络的后面，这样我们就知道真假了，也就有了误差了。所以对于生成网络的训练其实是对生成-判别网络串接的训练。好了那么现在来分析一下样本，原始的噪声数组Z我们有，也就是生成了假样本我们有，此时很关键的一点来了，我们要<strong>把这些假样本的标签都设置为1</strong>，也就是认为这些假样本在生成网络训练的时候是真样本。那么为什么要这样呢？我们想想，是不是这样才能起到迷惑判别器的目的，也才能使得生成的假样本逐渐逼近为正样本。好了，重新顺一下思路，现在对于生成网络的训练，我们有了样本集（只有假样本集，没有真样本集），有了对应的label（全为1），是不是就可以训练了？有人会问，这样只有一类样本，训练啥呀？谁说一类样本就不能训练了？只要有误差就行。还有人说，你这样一训练，<strong>判别网络的网络参数不是也跟着变吗</strong>？没错，这很关键，所以在训练这个串接的网络的时候，一个很重要的操作就是<strong>不要判别网络的参数发生变化</strong>，也就是不让它参数发生更新，只是把误差一直传，传到生成网络那块后更新生成网络的参数。这样就完成了生成网络的训练了。</p><p>在完成生成网络训练好，那么我们是不是可以根据目前新的生成网络再对先前的那些噪声Z生成新的假样本了，没错，并且训练后的假样本应该是更真了才对。然后又有了新的真假样本集（其实是新的假样本集），这样又可以重复上述过程了。我们把这个过程称作为单独交替训练。我们可以实现定义一个迭代次数，交替迭代到一定次数后停止即可。这个时候我们再去看一看噪声Z生成的假样本会发现，原来它已经很真了。</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>对于判别器$D$，其优化函数：</p><p><img src="/2018/07/20/Generative-Adversarial-Networks/Screen Shot 2018-07-24 at 5.13.45 PM.png" alt="Screen Shot 2018-07-24 at 5.13.45 PM"></p><p>对于生成器$G$，其优化函数：</p><p><img src="/2018/07/20/Generative-Adversarial-Networks/Screen Shot 2018-07-24 at 5.14.26 PM.png" alt="Screen Shot 2018-07-24 at 5.14.26 PM"></p><p><img src="/2018/07/20/Generative-Adversarial-Networks/Screen Shot 2018-07-21 at 11.26.58 AM.png" alt="Screen Shot 2018-07-21 at 11.26.58 AM"></p><h1 id="GAN的特殊理解"><a href="#GAN的特殊理解" class="headerlink" title="GAN的特殊理解"></a>GAN的特殊理解</h1><p>GAN的目标是学习一个从指定分布数据到目标分布数据的映射，一旦学习好了这个映射，我们则需要比较这两个分布的相近程度，注意不是比较样本之间的差距。通常我们会用考KL距离来描述这两个分布的差异，但是GAN并没有指定任何距离，而是学习出一个距离度量。</p><p>假设我们的GAN的网络参数是$\theta$, 目标分布是$z_i$，拟合出来的分布是$y_i$，那么GAN有一个独立的距离度量网络$L$，输入$z_i$和$y_i$，自动计算分布的差异，即：</p><script type="math/tex; mode=display">L(\{z_i\}_{i=1}^{M},\{y_i\}_{i=1}^{N},\theta)</script><p>考虑到$\{z_i\}_{i=1}^{M}$是已知的，我们可以将之视作非变量，当成模型的一部分，则上式可以简写为：</p><script type="math/tex; mode=display">L(\{y_i\}_{i=1}^{N},\theta)</script><p>但是，我们比较的是分布间的距离，即分布本身与各个$y_i$出现的顺序是没有关系的，因此，尽管$L$是各个$y_i$和函数，但它必须全对称，</p><script type="math/tex; mode=display">L = \frac{1}{M!}\sum_{对y_1, ..., y_M所有的排列求和}D(y_1,y_2,...,\Theta)</script><p>也就是说，我们先找⼀个有序的函数$D$，然后对所有可能的序求平均，那么就得到⽆序的函数了。当然，这样的计算量是$O(M!)$，显然也不靠谱，那么我们就选择最简单的⼀种：</p><script type="math/tex; mode=display">L=\frac{1}{M} \sum_{i=1}^{M} D\left(y_{i}, \Theta\right)</script><p>这便是⽆序的最简单实现，可以简单的理解为：分布之间的距离，等于单个样本的距离的平均。</p><p>因为$D(Y,\theta)$的均值, 也就是$L$, 是度量两个分布的差异程度, 这就意味着，要能够将两个分布区分开来，即$L$越⼤越好；但是我们最终的⽬的，是希望通过均匀分布⽽⽣成我们指定的分布，所以$G(X,\theta)$则希望两个分布越来越接近，即$L$越⼩越好。</p><p>训练$D(Y,\Theta)$的时候，我们随机初始化$G(X,\theta)$，固定它，然后生成一批$Y$。我们再从目标样本中采样一批$Z$，则</p><script type="math/tex; mode=display">\begin{array}{l}{\Theta=\arg \min L=\underset{\Theta}{\arg \min } \frac{1}{N} \sum_{i=1}^{N} D\left(z_{i}, \Theta\right)} \\ {\Theta=\arg \max L=\underset{\Theta}{\arg \max } \frac{1}{M} \sum_{i=1}^{M} D\left(y_{i}, \Theta\right)}\end{array}</script><p>然⽽有两个⽬标并不容易平衡，所以⼲脆都取同样的样本数$B$（⼀个batch），然后⼀起训练就好：</p><script type="math/tex; mode=display">\begin{array}{l}{\Theta=\arg \min _{\boldsymbol{\theta}} L_{1}} \\ {\quad=\underset{\Theta}{\arg \min } \frac{1}{B} \sum_{i=1}^{B}\left[D\left(z_{i}, \Theta\right)-D\left(y_{i}, \Theta\right)\right]}\end{array}</script><p>而$G(X,\theta)$希望⽣成的样本越接近真实样本越好，因此这时候把$D$的参数$\Theta$固定，只训练 $\theta$让$L$越来越⼩：</p><script type="math/tex; mode=display">\begin{aligned} \theta &=\underset{\theta}{\arg \min } L_{2} \\ &=\underset{\theta}{\arg \min } \frac{1}{B} \sum_{i=1}^{B}\left[D\left(G\left(x_{i}, \theta\right), \Theta\right)\right] \end{aligned}</script><blockquote><p>实际上，传统的GAN公式如下</p><p><img src="/2018/07/20/Generative-Adversarial-Networks/Screen Shot 2019-08-10 at 6.11.56 PM.png" alt="creen Shot 2019-08-10 at 6.11.56 P"></p></blockquote><h1 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h1><p><a href="http://www.360doc.com/content/17/0705/19/99071_669134770.shtml" target="_blank" rel="noopener">existing problems</a> </p><p>在手写体数字生成过程中，我们无法得知什么样的噪声z可以用来生成数字1，什么样的噪声z可以用来生成数字3，我们对这些一无所知，这从一点程度上限制了我们对GAN的使用。</p><h1 id="GAN实现"><a href="#GAN实现" class="headerlink" title="GAN实现"></a>GAN实现</h1><h2 id="GAN训练伪代码"><a href="#GAN训练伪代码" class="headerlink" title="GAN训练伪代码"></a><a href="http://36kr.com/p/5086889.html" target="_blank" rel="noopener">GAN训练伪代码</a></h2><p><img src="/2018/07/20/Generative-Adversarial-Networks/beb95bnndbwkgz4x!1200.jpeg" alt="beb95bnndbwkgz4x!1200"></p><p><strong>这里红框圈出的部分是我们要额外注意的</strong>。第一步我们训练D，D是希望V(G, D)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(G, D)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。</p><h2 id="keras实现手写体"><a href="#keras实现手写体" class="headerlink" title="keras实现手写体"></a><a href="https://github.com/roatienza/Deep-Learning-Experiments/blob/master/Experiments/Tensorflow/GAN/dcgan_mnist.py" target="_blank" rel="noopener">keras实现手写体</a></h2><p><a href="https://www.leiphone.com/news/201703/Y5vnDSV9uIJIQzQm.html" target="_blank" rel="noopener">文字版</a> <a href="# https://github.com/jiqizhixin/ML-Tutorial-Experiment/blob/master/Experiments/tf_GAN.ipynb">Tensorflow实现手写体</a></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''</span></div><div class="line"><span class="string">DCGAN on MNIST using Keras</span></div><div class="line"><span class="string">Author: Rowel Atienza</span></div><div class="line"><span class="string">Project: https://github.com/roatienza/Deep-Learning-Experiments</span></div><div class="line"><span class="string">Dependencies: tensorflow 1.0 and keras 2.0</span></div><div class="line"><span class="string">Usage: python3 dcgan_mnist.py</span></div><div class="line"><span class="string">'''</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Flatten, Reshape</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, Conv2DTranspose, UpSampling2D</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LeakyReLU, Dropout</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> BatchNormalization</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam, RMSprop</div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ElapsedTimer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.start_time = time.time()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">elapsed</span><span class="params">(self,sec)</span>:</span></div><div class="line">        <span class="keyword">if</span> sec &lt; <span class="number">60</span>:</div><div class="line">            <span class="keyword">return</span> str(sec) + <span class="string">" sec"</span></div><div class="line">        <span class="keyword">elif</span> sec &lt; (<span class="number">60</span> * <span class="number">60</span>):</div><div class="line">            <span class="keyword">return</span> str(sec / <span class="number">60</span>) + <span class="string">" min"</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> str(sec / (<span class="number">60</span> * <span class="number">60</span>)) + <span class="string">" hr"</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">elapsed_time</span><span class="params">(self)</span>:</span></div><div class="line">        print(<span class="string">"Elapsed: %s "</span> % self.elapsed(time.time() - self.start_time) )</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DCGAN</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_rows=<span class="number">28</span>, img_cols=<span class="number">28</span>, channel=<span class="number">1</span>)</span>:</span></div><div class="line"></div><div class="line">        self.img_rows = img_rows</div><div class="line">        self.img_cols = img_cols</div><div class="line">        self.channel = channel</div><div class="line">        self.D = <span class="keyword">None</span>   <span class="comment"># discriminator</span></div><div class="line">        self.G = <span class="keyword">None</span>   <span class="comment"># generator</span></div><div class="line">        self.AM = <span class="keyword">None</span>  <span class="comment"># adversarial model</span></div><div class="line">        self.DM = <span class="keyword">None</span>  <span class="comment"># discriminator model</span></div><div class="line"></div><div class="line">    <span class="comment"># (W−F+2P)/S+1</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.D:</div><div class="line">            <span class="keyword">return</span> self.D</div><div class="line">        self.D = Sequential()</div><div class="line">        depth = <span class="number">64</span></div><div class="line">        dropout = <span class="number">0.4</span></div><div class="line">        <span class="comment"># In: 28 x 28 x 1, depth = 1</span></div><div class="line">        <span class="comment"># Out: 14 x 14 x 1, depth=64</span></div><div class="line">        input_shape = (self.img_rows, self.img_cols, self.channel)</div><div class="line">        self.D.add(Conv2D(depth*<span class="number">1</span>, <span class="number">5</span>, strides=<span class="number">2</span>, input_shape=input_shape,\</div><div class="line">            padding=<span class="string">'same'</span>))</div><div class="line">        self.D.add(LeakyReLU(alpha=<span class="number">0.2</span>))</div><div class="line">        self.D.add(Dropout(dropout))</div><div class="line"></div><div class="line">        self.D.add(Conv2D(depth*<span class="number">2</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.D.add(LeakyReLU(alpha=<span class="number">0.2</span>))</div><div class="line">        self.D.add(Dropout(dropout))</div><div class="line"></div><div class="line">        self.D.add(Conv2D(depth*<span class="number">4</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.D.add(LeakyReLU(alpha=<span class="number">0.2</span>))</div><div class="line">        self.D.add(Dropout(dropout))</div><div class="line"></div><div class="line">        self.D.add(Conv2D(depth*<span class="number">8</span>, <span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.D.add(LeakyReLU(alpha=<span class="number">0.2</span>))</div><div class="line">        self.D.add(Dropout(dropout))</div><div class="line"></div><div class="line">        <span class="comment"># Out: 1-dim probability</span></div><div class="line">        self.D.add(Flatten())</div><div class="line">        self.D.add(Dense(<span class="number">1</span>))</div><div class="line">        self.D.add(Activation(<span class="string">'sigmoid'</span>))</div><div class="line">        self.D.summary()</div><div class="line">        <span class="keyword">return</span> self.D</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.G:</div><div class="line">            <span class="keyword">return</span> self.G</div><div class="line">        self.G = Sequential()</div><div class="line">        dropout = <span class="number">0.4</span></div><div class="line">        depth = <span class="number">64</span>+<span class="number">64</span>+<span class="number">64</span>+<span class="number">64</span></div><div class="line">        dim = <span class="number">7</span></div><div class="line">        <span class="comment"># In: 100</span></div><div class="line">        <span class="comment"># Out: dim x dim x depth</span></div><div class="line">        self.G.add(Dense(dim*dim*depth, input_dim=<span class="number">100</span>))</div><div class="line">        self.G.add(BatchNormalization(momentum=<span class="number">0.9</span>))</div><div class="line">        self.G.add(Activation(<span class="string">'relu'</span>))</div><div class="line">        self.G.add(Reshape((dim, dim, depth)))</div><div class="line">        self.G.add(Dropout(dropout))</div><div class="line"></div><div class="line">        <span class="comment"># In: dim x dim x depth</span></div><div class="line">        <span class="comment"># Out: 2*dim x 2*dim x depth/2</span></div><div class="line">        self.G.add(UpSampling2D())</div><div class="line">        self.G.add(Conv2DTranspose(int(depth/<span class="number">2</span>), <span class="number">5</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.G.add(BatchNormalization(momentum=<span class="number">0.9</span>))</div><div class="line">        self.G.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line">        self.G.add(UpSampling2D())</div><div class="line">        self.G.add(Conv2DTranspose(int(depth/<span class="number">4</span>), <span class="number">5</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.G.add(BatchNormalization(momentum=<span class="number">0.9</span>))</div><div class="line">        self.G.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line">        self.G.add(Conv2DTranspose(int(depth/<span class="number">8</span>), <span class="number">5</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.G.add(BatchNormalization(momentum=<span class="number">0.9</span>))</div><div class="line">        self.G.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line">        <span class="comment"># Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix</span></div><div class="line">        self.G.add(Conv2DTranspose(<span class="number">1</span>, <span class="number">5</span>, padding=<span class="string">'same'</span>))</div><div class="line">        self.G.add(Activation(<span class="string">'sigmoid'</span>))</div><div class="line">        self.G.summary()</div><div class="line">        <span class="keyword">return</span> self.G</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">discriminator_model</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.DM:</div><div class="line">            <span class="keyword">return</span> self.DM</div><div class="line">        optimizer = RMSprop(lr=<span class="number">0.0002</span>, decay=<span class="number">6e-8</span>)</div><div class="line">        self.DM = Sequential()</div><div class="line">        self.DM.add(self.discriminator())</div><div class="line">        self.DM.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=optimizer,\</div><div class="line">            metrics=[<span class="string">'accuracy'</span>])</div><div class="line">        <span class="keyword">return</span> self.DM</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">adversarial_model</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.AM:</div><div class="line">            <span class="keyword">return</span> self.AM</div><div class="line">        optimizer = RMSprop(lr=<span class="number">0.0001</span>, decay=<span class="number">3e-8</span>)</div><div class="line">        self.AM = Sequential()</div><div class="line">        self.AM.add(self.generator())</div><div class="line">        self.AM.add(self.discriminator())</div><div class="line">        self.AM.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=optimizer,\</div><div class="line">            metrics=[<span class="string">'accuracy'</span>])</div><div class="line">        <span class="keyword">return</span> self.AM</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST_DCGAN</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.img_rows = <span class="number">28</span></div><div class="line">        self.img_cols = <span class="number">28</span></div><div class="line">        self.channel = <span class="number">1</span></div><div class="line"></div><div class="line">        self.x_train = input_data.read_data_sets(<span class="string">"mnist"</span>,\</div><div class="line">        one_hot=<span class="keyword">True</span>).train.images</div><div class="line">        self.x_train = self.x_train.reshape(<span class="number">-1</span>, self.img_rows,\</div><div class="line">        self.img_cols, <span class="number">1</span>).astype(np.float32)</div><div class="line"></div><div class="line">        self.DCGAN = DCGAN()</div><div class="line">        self.discriminator =  self.DCGAN.discriminator_model()</div><div class="line">        self.adversarial = self.DCGAN.adversarial_model()</div><div class="line">        self.generator = self.DCGAN.generator()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_steps=<span class="number">2000</span>, batch_size=<span class="number">256</span>, save_interval=<span class="number">0</span>)</span>:</span></div><div class="line">        noise_input = <span class="keyword">None</span></div><div class="line">        <span class="keyword">if</span> save_interval&gt;<span class="number">0</span>:</div><div class="line">            noise_input = np.random.uniform(<span class="number">-1.0</span>, <span class="number">1.0</span>, size=[<span class="number">16</span>, <span class="number">100</span>])</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(train_steps):</div><div class="line">            images_train = self.x_train[np.random.randint(<span class="number">0</span>,</div><div class="line">                self.x_train.shape[<span class="number">0</span>], size=batch_size), :, :, :]</div><div class="line">            noise = np.random.uniform(<span class="number">-1.0</span>, <span class="number">1.0</span>, size=[batch_size, <span class="number">100</span>])</div><div class="line">            images_fake = self.generator.predict(noise)</div><div class="line">            x = np.concatenate((images_train, images_fake))</div><div class="line">            y = np.ones([<span class="number">2</span>*batch_size, <span class="number">1</span>])</div><div class="line">            y[batch_size:, :] = <span class="number">0</span></div><div class="line">            d_loss = self.discriminator.train_on_batch(x, y)</div><div class="line"></div><div class="line">            y = np.ones([batch_size, <span class="number">1</span>])</div><div class="line">            noise = np.random.uniform(<span class="number">-1.0</span>, <span class="number">1.0</span>, size=[batch_size, <span class="number">100</span>])</div><div class="line">            a_loss = self.adversarial.train_on_batch(noise, y)</div><div class="line">            log_mesg = <span class="string">"%d: [D loss: %f, acc: %f]"</span> % (i, d_loss[<span class="number">0</span>], d_loss[<span class="number">1</span>])</div><div class="line">            log_mesg = <span class="string">"%s  [A loss: %f, acc: %f]"</span> % (log_mesg, a_loss[<span class="number">0</span>], a_loss[<span class="number">1</span>])</div><div class="line">            print(log_mesg)</div><div class="line">            <span class="keyword">if</span> save_interval&gt;<span class="number">0</span>:</div><div class="line">                <span class="keyword">if</span> (i+<span class="number">1</span>)%save_interval==<span class="number">0</span>:</div><div class="line">                    self.plot_images(save2file=<span class="keyword">True</span>, samples=noise_input.shape[<span class="number">0</span>],\</div><div class="line">                        noise=noise_input, step=(i+<span class="number">1</span>))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_images</span><span class="params">(self, save2file=False, fake=True, samples=<span class="number">16</span>, noise=None, step=<span class="number">0</span>)</span>:</span></div><div class="line">        filename = <span class="string">'mnist.png'</span></div><div class="line">        <span class="keyword">if</span> fake:</div><div class="line">            <span class="keyword">if</span> noise <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                noise = np.random.uniform(<span class="number">-1.0</span>, <span class="number">1.0</span>, size=[samples, <span class="number">100</span>])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                filename = <span class="string">"mnist_%d.png"</span> % step</div><div class="line">            images = self.generator.predict(noise)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            i = np.random.randint(<span class="number">0</span>, self.x_train.shape[<span class="number">0</span>], samples)</div><div class="line">            images = self.x_train[i, :, :, :]</div><div class="line"></div><div class="line">        plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(images.shape[<span class="number">0</span>]):</div><div class="line">            plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i+<span class="number">1</span>)</div><div class="line">            image = images[i, :, :, :]</div><div class="line">            image = np.reshape(image, [self.img_rows, self.img_cols])</div><div class="line">            plt.imshow(image, cmap=<span class="string">'gray'</span>)</div><div class="line">            plt.axis(<span class="string">'off'</span>)</div><div class="line">        plt.tight_layout()</div><div class="line">        <span class="keyword">if</span> save2file:</div><div class="line">            plt.savefig(filename)</div><div class="line">            plt.close(<span class="string">'all'</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            plt.show()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    mnist_dcgan = MNIST_DCGAN()</div><div class="line">    timer = ElapsedTimer()</div><div class="line">    mnist_dcgan.train(train_steps=<span class="number">10000</span>, batch_size=<span class="number">256</span>, save_interval=<span class="number">500</span>)</div><div class="line">    timer.elapsed_time()</div><div class="line">    mnist_dcgan.plot_images(fake=<span class="keyword">True</span>)</div><div class="line">    mnist_dcgan.plot_images(fake=<span class="keyword">False</span>, save2file=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></div></div>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Generative Adversarial Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Variational AutoEncoder</title>
      <link href="/2018/07/20/Variational-AutoEncoder/"/>
      <url>/2018/07/20/Variational-AutoEncoder/</url>
      <content type="html"><![CDATA[<p> AutoEncoder（自编码器）本质上是数据特定的数据压缩。虽然自编码器中的重构损失函数确保了编码过程原始数据不会丢失过多，但是没有对约束特征$z$做出约束。</p><p>作为一种无监督的学习方法，VAE（Variational Auto-Encoder，变分自编码器）是一个产生式模型，其在ae的基础上约束潜变量$z$服从于某个已知的先验分布$p(z|x)$，比如希望$z$的每个特征相互独立并且符合高斯分布等。</p><a id="more"></a><h1 id="判别式模型-VS-生成式模型"><a href="#判别式模型-VS-生成式模型" class="headerlink" title="判别式模型 VS 生成式模型"></a>判别式模型 VS 生成式模型</h1><p><a href="http://www.cnblogs.com/kemaswill/p/3427422.html" target="_blank" rel="noopener">ref1</a> <a href="https://blog.csdn.net/heyc861221/article/details/80130968" target="_blank" rel="noopener">ref2</a> </p><blockquote><p>给定数据样本$D=\{x_1,x_2,…,x_n\}$，及其对应的标签$\{y_1,y_2,…,y_n\}$.</p><p>判别式模型：直接对$p(y|x)$进行建模</p><p>生成式模型：对$x$和$y$的联合分布$p(x,y)$进行建模，然后通过贝叶斯公式来求得$p(y|x)$，最后选择使$p(y|x)$最大的$y_i$。</p></blockquote><p>判别式模型大多有下面的规律：已知观察变量X，和隐含变量z，判别式模型对p(z|X)进行建模，它根据输入的观察变量x得到隐含变量z出现的可能性。生成式模型则是将两者的顺序反过来，它要对p(X|z)进行建模，输入是隐含变量，输出是观察变量的概率。</p><p>可以想象，不同的模型结构自然有不同的用途。判别模型在判别工作上更适合，生成模型在分布估计等问题上更有优势。如果想用生成式模型去解决判别问题，就需要利用贝叶斯公式把这个问题转换成适合自己处理的样子：</p><script type="math/tex; mode=display">p(z | X)=\frac{p(X | z) p(z)}{p(X)}</script><p>对于一些简单的问题，上面的公式还是比较容易解出的，但对于一些复杂的问题，找出从隐含变量到观察变量之间的关系是一件很困难的事情，生成式模型的建模过程会非常困难，所以对于判别类问题，判别式模型一般更适合。</p><p>但对于“随机生成满足某些隐含变量特点的数据”这样的问题来说，判别式模型就会显得力不从心。如果用判别式模型生成数据，就要通过类似于下面这种方式的方法进行。</p><p>第一步，利用简单随机一个X。</p><p>第二步，用判别式模型计算p(z|X)概率，如果概率满足，则找到了这个观察数据，如果不满足，返回第一步。</p><p>这样用判别式模型生成数据的效率可能会十分低下。而生成式模型解决这个问题就十分简单，首先确定好z的取值，然后根据p(X|z)的分布进行随机采样就行了</p><h1 id="Intuition-behind-VAE"><a href="#Intuition-behind-VAE" class="headerlink" title="Intuition behind VAE"></a>Intuition behind VAE</h1><p>该部分参考了<a href="https://zhuanlan.zhihu.com/p/25269592" target="_blank" rel="noopener">ZhiHu-VAE</a>.</p><p>考虑MNIST数据集，数据集里有10种数字，每种数字下有几千个不同的样本，我们能不能照猫画虎，模仿已有的数字生成一个同样可辨识，但却与现有的样本都不同的的数字呢？</p><p>要解决这个问题，需要对数字的分布进行建模。我们需要知道一个数字“一般而言长什么样”。如果我们得到了数据集X的分布P(X)，那么数据集中的每个图片，也不过就是从P(X)采样得到的一个样本而已。可以说掌握了P(X)，我们就算把这个数据集的底裤都扒下来了，到时候搓扁捏圆，任君所愿。</p><p>然而从有限的样本中估计出数据原来的分布情况，却不是一件容易的事，别说更复杂的数据集了，就是MNIST你也做不好。为了能估计P(X)，我们做一个隐变量假设，假设数据集X实际上是由一组我们观察不到的隐变量Z经过某个复杂的映射$P(z|x)$产生的，给定一个z，我就能通过某种方法生成一个样本$\hat x$。如果能得到z的分布和$P(z|x)$，那P(X)我们也算知道了。</p><p>这个假设是有道理的，z尽管是隐变量，但不妨碍我们对它的物理含义做出猜测。比方说z里面有控制笔画粗细的变量，有控制笔划角度的变量，有控制数字大小的变量等等。这些变量一旦确定，写出来的数字大致长什么样就确定了。</p><p>但是，P(X)分布的形式未知这点很好理解，但凭什么P(Z)能服从标准n维高斯分布呢？比方z里面有个控制比划粗细的变量，它怎么可能是高斯分布呢？答案隐藏在$P(z|x)$中：<strong>对于任意d维随机变量，不管他们实际上服从什么分布，我总可以用d个服从标准高斯分布的随机变量通过一个足够复杂的函数去逼近它。</strong>我们可以这样理解，z其实是“隐变量的隐变量”，函数$P(z|x)$实际上首先将z映射到某一组隐变量z’，这个z’可能就是上面说的笔划粗细啊，角度什么的有物理含义的东西，然后接着再把z’映射到X得到样本。</p><h1 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h1><p><img src="/2018/07/20/Variational-AutoEncoder/v2-0533fd3dc73184f807038b4f08e8681a_hd.jpg" alt="v2-0533fd3dc73184f807038b4f08e8681a_hd"></p><p>变分编码器和自动编码器的区别就在于，传统自动编码器的隐变量z的分布是不知道的，因此我们无法采样得到新的z，也就无法通过解码器得到新的x。下面我们来变分，我们现在不要从x中直接得到z，而是得到z的均值和方差，然后再迫使它逼近正态分布的均值和方差，则网络变成下面的样子：</p><p><img src="/2018/07/20/Variational-AutoEncoder/v2-89c7d409bcb7a8218c80b3134b015818_hd.jpg" alt="v2-89c7d409bcb7a8218c80b3134b015818_hd"></p><p>看上去不错，从Q(z|x)估计出来的值跟标准正态分布不一样没关系，训练过程中慢慢逼近就行了。假定z服从高斯分布的好处之一在这里就能体现出来，只要估计均值和方差，我们就完全了解这个高斯分布了，也就能从其中采样了。</p><p>然而上面这个网络最大的问题是，它是<strong>断开</strong>的。前半截是从数据集估计z的分布，后半截是从一个z的样本重构输入。最关键的采样这一步，恰好不是一个我们传统意义上的操作。这个网络没法求导，因为梯度传到f(z)以后没办法往前走了。</p><p>为了使得整个网络得以训练，使用一种叫<strong>reparemerization</strong>的trick，使得网络对均值和方差可导，把网络连起来。这个trick的idea见下图：</p><p><img src="/2018/07/20/Variational-AutoEncoder/Screen Shot 2018-07-24 at 1.05.03 PM.png" alt="Screen Shot 2018-07-24 at 1.05.03 PM"></p><h1 id="Math-in-VAE"><a href="#Math-in-VAE" class="headerlink" title="Math in VAE"></a>Math in VAE</h1><p><a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">REF</a> </p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>假设我们有一批数据样本$\left\{X_{1}, \ldots, X_{n}\}\right.$, 其整体用$X$来描述，我们想用$\left\{X_{1}, \ldots, X_{n}\}\right.$得到$X$的分布$p(X)$，如果可以实现的话，那我们就可以根据$p(X)$来采样，就可能得到所有可能的$X$了，包括$\left\{X_{1}, \ldots, X_{n}\}\right.$以外的样本，这就是一个生成模型了。但是这个很难去实现，于是我们将分布改一下：</p><script type="math/tex; mode=display">p(X)=\sum_{Z} p(X | Z) p(Z)</script><p>此时$p(X|Z)$描述了一个由$Z$生成$X$的模型，如果我们假设$Z$服从标准正态分布，即$p(Z)=\mathcal{N}(0, I)$ 。</p><p>如果这个实现，我们就可以先从标准正态分布中采样一个$Z$，然后根据$Z$来算一个$X$，也是一个很棒的生成模型。接下来就是结合自编码器来实现重构，保证有效信息没有丢失，再加上一系列的推导，最后把模型实现。框架的示意图如下：</p><p><img src="/2018/07/20/Variational-AutoEncoder/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/Variational-AutoEncoder/2689906124.png" alt="68990612"></p><p>但是，在整个VAE模型中，我们并没有去使用$p(Z)$（隐变量空间的分布）是正态分布的假设，我们用的是假设$p(Z|X)$（后验分布）是正态分布！！</p><p>具体来说，给定一个真实样本$Xk$，我们假设存在一个专属于$Xk$的分布$p(Z|X_k)$（学名叫后验分布），并进一步假设这个分布是（独立的、多元的）正态分布。为什么要强调“专属”呢？因为我们后面要训练一个生成器$X=g(Z)$，希望能够把从分布$p(Z|X_k)$采样出来的一个$Z_k$还原为$X_k$。如果假设$p(Z)$是正态分布，然后从$p(Z)$中采样一个$Z$，那么我们怎么知道这个$Z$对应于哪个真实的$X$呢？现在$p(Z|X_k)$专属于$X_k$，我们有理由说从这个分布采样出来的ZZ应该要还原到$X_k$中去。</p><p>这时候每一个$X_k$都配上了一个专属的正态分布，才方便后面的生成器做还原。但这样有多少个$X$就有多少个正态分布了。我们知道正态分布有两组参数：均值$\mu$和方差$\sigma^2$（多元的话，它们都是向量），那我怎么找出专属于$X_k$的正态分布$p(Z|X_k)$的均值和方差呢？用神经网络来拟合出来吧！</p><p>于是我们构建两个神经网络$\mu_k=f_1(X_k),log\sigma^{2}_k=f_2(X_k)$来算它们了。我们选择拟合$log\sigma^2_k$而不是直接拟合$\sigma^2_k$，是因为$\sigma^2_k$总是非负的，需要加激活函数处理，而拟合$log\sigma^2_k$不需要加激活函数，因为它可正可负。到这里，我能知道专属于$X_k$的均值和方差了，也就知道它的正态分布长什么样了，然后从这个专属分布中采样一个$Z_k$出来，然后经过一个生成器得到$\hat X_k=g(Z_k)$，现在我们可以放心地最小化$\mathcal{D}\left(\hat{X}_{k}, X_{k}\right)^{2}$，因为$Z_k$是从专属$X_k$的分布中采样出来的，这个生成器应该要把开始的$X_k$还原回来。于是可以画出VAE的示意图</p><p><img src="/2018/07/20/Variational-AutoEncoder/8da0009b298c462b9dba190f21f594f5.jpeg" alt="8da0009b298c462b9dba190f21f594f5"></p><p>让我们来思考一下，根据上图的训练过程，最终会得到什么结果。</p><p>首先，我们希望重构$X$，也就是最小化$\mathcal{D}\left(\hat{X}_{k}, X_{k}\right)^{2}$，但是这个重构过程受到噪声的影响，因为$Z_k$是通过重新采样过的，不是直接由encoder算出来的。显然噪声会增加重构的难度，不过好在这个噪声强度（也就是方差）通过一个神经网络算出来的，所以最终模型为了重构得更好，肯定会想尽办法让方差为0。而方差为0的话，也就没有随机性了，所以不管怎么采样其实都只是得到确定的结果（也就是均值），只拟合一个当然比拟合多个要容易，而均值是通过另外一个神经网络算出来的。</p><p>说白了，模型会慢慢退化成普通的AutoEncoder，噪声不再起作用。</p><p>这样不就白费力气了吗？说好的生成模型呢？</p><p>别急别急，其实<strong>VAE还让所有的$p(Z|X)$都向标准正态分布看齐</strong>，这样就防止了噪声为零，同时保证了模型具有生成能力。怎么理解“保证了生成能力”呢？如果所有的$p(Z|X)$都很接近标准正态分布$N(0,I)$，那么根据定义</p><script type="math/tex; mode=display">p(Z)=\sum_{X} p(Z | X) p(X)=\sum_{X} \mathcal{N}(0, I) p(X)=\mathcal{N}(0, I) \sum_{X} p(X)=\mathcal{N}(0, I)</script><p>这样我们就能达到我们的先验假设：$p(Z)$是标准正态分布。然后我们就可以放心地从$N(0,I)$中采样来生成图像了。</p><p>那如何让$p(Z|X)$接近标准正态分布$N(0,I)$呢？其实最直接的方法应该是在重构误差的基础上中加入额外的loss：</p><script type="math/tex; mode=display">\mathcal{L}_{\mu}=\left\|f_{1}\left(X_{k}\right)\right\|^{2} \quad \mathbb{and}  \quad \mathcal{L}_{\sigma^{2}}=\left\|f_{2}\left(X_{k}\right)\right\|^{2}</script><p>因为它们分别代表了均值$\mu_k$和方差的对数$\log \sigma_{k}^{2}$，达到$N(0,I)$就是希望二者尽量接近0。不过，这又会面临着这两个损失的比例要怎么选取的问题，选取得不好，生成的图像会比较模糊。所以，原论文直接算了一般（各分量独立的）正态分布与标准正态分布的KL散度$K L\left(N\left(\mu, \sigma^{2}\right) | N(0, I)\right)$作为这个额外的loss，计算结果为：</p><script type="math/tex; mode=display">\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)</script><p>这里$d$是隐变量$Z$的维度，而$\mu_{(i)}$和$\log \sigma_{(i)}^{2}$分别代表一般正态分布的均值向量和方差向量的第$i$个分量。直接用这个式子做补充loss，就不用考虑均值损失和方差损失的相对比例问题了。显然，这个loss也可以分两部分理解：</p><script type="math/tex; mode=display">\begin{array}{l}{\mathcal{L}_{\mu, \sigma^{2}}=\mathcal{L}_{\mu}+\mathcal{L}_{\sigma^{2}}} \\ {\mathcal{L}_{\mu}=\frac{1}{2} \sum_{i=1}^{d} \mu_{(i)}^{2}=\frac{1}{2}\left\|f_{1}(X)\right\|^{2}} \\ {\mathcal{L}_{\sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)}\end{array}</script><blockquote><p>推导</p><p>由于我们考虑的是各分量独立的多元正态分布，因此只需要推导一元正态分布的情形即可，根据定义我们可以写出</p><script type="math/tex; mode=display">\begin{aligned} & K L\left(N\left(\mu, \sigma^{2}\right) \| N(0,1)\right) \\=& \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-(x-\mu)^{2} / 2 \sigma^{2}}\left(\log \frac{e^{-(x-\mu)^{2} / 2 \sigma^{2}} / \sqrt{2 \pi \sigma^{2}}}{e^{-x^{2} / 2} / \sqrt{2 \pi}}\right) d x \\=& \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-(x-\mu)^{2} / 2 \sigma^{2}} \log \left\{\frac{1}{\sqrt{\sigma^{2}}} \exp \left\{\frac{1}{2}\left[x^{2}-(x-\mu)^{2} / \sigma^{2}\right]\right\}\right\} d x \\=& \frac{1}{2} \int \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-(x-\mu)^{2} / 2 \sigma^{2}}\left[-\log \sigma^{2}+x^{2}-(x-\mu)^{2} / \sigma^{2}\right] d x \end{aligned}</script><p>整个结果分为三项积分，第一项实际上就是$-\log \sigma^{2}$乘以概率密度的积分（也就是1），所以结果是$-\log \sigma^{2}$; 第二项实际是正态分布的二阶矩，熟悉正态分布的朋友应该都清楚正态分布的二阶矩为$\mu^{2}+\sigma^{2}$；而根据定义，第三项实际上就是“-方差除以方差=-1”。所以总结果就是</p><script type="math/tex; mode=display">K L\left(N\left(\mu, \sigma^{2}\right) \| N(0,1)\right)=\frac{1}{2}\left(-\log \sigma^{2}+\mu^{2}+\sigma^{2}-1\right)</script></blockquote><h3 id="Reparemerization-重参"><a href="#Reparemerization-重参" class="headerlink" title="Reparemerization(重参)"></a>Reparemerization(重参)</h3><p>其实很简单，就是我们要从$p(Z|X_k)$中采样一个$Z_k$出来，尽管我们知道了$p(Z|X_k)$是正态分布，但是均值方差都是靠模型算出来的，我们要靠这个过程反过来优化均值方差的模型，但是“采样”这个操作是不可导的，而采样的结果是可导的。我们利用</p><script type="math/tex; mode=display">\begin{aligned} & \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(z-\mu)^{2}}{2 \sigma^{2}}\right) d z \\=& \frac{1}{\sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^{2}\right] d\left(\frac{z-\mu}{\sigma}\right) \end{aligned}</script><p>这说明$(z-\mu) / \sigma=\varepsilon$是服从均值为0、方差为1的标准正态分布的，要同时把$dz$考虑进去，是因为乘上$dz$才算是概率，去掉$dz$是概率密度而不是概率。这时候我们得到：</p><p>从$\mathcal{N}\left(\mu, \sigma^{2}\right)$ 中采样一个$Z$，相当于从$N(0,I)$中采样一个$\boldsymbol{\varepsilon}$ ，然后让$Z=\mu+\varepsilon \times c$。 </p><p>于是，我们将从$\mathcal{N}\left(\mu, \sigma^{2}\right)$采样变成了从$N(0,I)$中采样，然后通过参数变换得到从$\mathcal{N}\left(\mu, \sigma^{2}\right)$ 中采样的结果。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型可训练了。</p><p><img src="/2018/07/20/Variational-AutoEncoder/Screen Shot 2018-07-23 at 11.15.34 PM.png" alt="Screen Shot 2018-07-23 at 11.15.34 PM"></p><blockquote><p><a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">intuition</a> </p><ul><li><p>采样操作是不可导，采样之后的加减操作是可导，所以利用了重参技巧？</p><p>可以参考这篇<a href="http://lib.csdn.net/article/deeplearning/68143?knId=1748" target="_blank" rel="noopener">文章</a>. 如下图，左边是直接sample隐变量，显然，无法对这个“动作”进行任何数学计算；但是，使用了重参，即右边部分将隐变量的sample变成了一个数学计算，$z = \epsilon*\sigma+\mu$，显然可导。</p></li><li><p>​</p></li></ul></blockquote><p><img src="/2018/07/20/Variational-AutoEncoder/Screen Shot 2018-07-23 at 11.16.40 PM.png" alt="Screen Shot 2018-07-23 at 11.16.40 PM"></p><blockquote><ol><li>由于训练过程只用到 X（同时作为输入和目标输出），而与 X的标签无关，因此，这是无监督学习。</li><li>​</li></ol></blockquote><p><img src="/2018/07/20/Variational-AutoEncoder/Screen Shot 2018-07-22 at 1.12.06 AM.png" alt="Screen Shot 2018-07-22 at 1.12.06 AM"></p><blockquote><p>The generated image is linear combination of existing images.</p></blockquote><p><a href="http://www.360doc.com/content/17/0930/22/99071_691460743.shtml" target="_blank" rel="noopener">VAE数据流动和损失计算</a> </p><p><img src="/2018/07/20/Variational-AutoEncoder/11.jpg" alt="11"></p><h1 id="VAE的Keras实现"><a href="#VAE的Keras实现" class="headerlink" title="VAE的Keras实现"></a>VAE的Keras实现</h1><p><a href="https://blog.csdn.net/A_a_ron/article/details/79004163" target="_blank" rel="noopener">detailed explanation</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</div><div class="line"> </div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Lambda</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> objectives</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.utils.visualize_util <span class="keyword">import</span> plot</div><div class="line"><span class="keyword">import</span> sys</div></pre></td></tr></table></figure></div></div><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>输入是$n$维，输出$x\times m$维</p><p><img src="/2018/07/20/Variational-AutoEncoder/20161214175120686.png" alt="20161214175120686"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">batch_size = <span class="number">100</span></div><div class="line">original_dim = <span class="number">784</span>  <span class="comment"># 28*28</span></div><div class="line">latent_dim = <span class="number">2</span> <span class="comment"># encoder output size</span></div><div class="line">intermediate_dim = <span class="number">256</span> <span class="comment"># neural number of hidden layer</span></div><div class="line">nb_epoch = <span class="number">50</span></div><div class="line">epsilon_std = <span class="number">1.0</span> </div><div class="line"></div><div class="line">x = Input(batch_shape=(batch_size, original_dim))</div><div class="line">h = Dense(intermediate_dim, activation=<span class="string">'relu'</span>)(x)</div><div class="line">z_mean = Dense(latent_dim)(h)</div><div class="line">z_log_var = Dense(latent_dim)(h)</div></pre></td></tr></table></figure></div></div><h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>将encoder的大小为$(2\times m)$的输出视作$m$个高斯多元分布的均值$z_{mean}$和方差的对数$z _ log _ var$</p><p><img src="/2018/07/20/Variational-AutoEncoder/20161214175146348.png" alt="20161214175146348"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling</span><span class="params">(args)</span>:</span></div><div class="line">  z_mean, z_log_var = args</div><div class="line">  epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=<span class="number">0.</span>,stddev=epsilon_std)</div><div class="line">  <span class="keyword">return</span> z_mean + K.exp(z_log_var / <span class="number">2</span>) * epsilon</div><div class="line"></div><div class="line">z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])</div></pre></td></tr></table></figure></div></div><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p><img src="/2018/07/20/Variational-AutoEncoder/20161214175029514.png" alt="20161214175029514"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">decoder_h = Dense(intermediate_dim, activation=<span class="string">'relu'</span>)</div><div class="line">decoder_mean = Dense(original_dim, activation=<span class="string">'sigmoid'</span>)</div><div class="line">h_decoded = decoder_h(z)</div><div class="line">x_decoded_mean = decoder_mean(h_decoded)</div></pre></td></tr></table></figure></div></div><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vae_loss</span><span class="params">(x, x_decoded_mean)</span>:</span></div><div class="line">  <span class="comment"># my tips:logloss</span></div><div class="line">  xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)</div><div class="line">  <span class="comment"># my tips:see paper's appendix B</span></div><div class="line">  kl_loss = - <span class="number">0.5</span> * K.sum(<span class="number">1</span> + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=<span class="number">-1</span>)</div><div class="line">  <span class="keyword">return</span> xent_loss + kl_loss</div></pre></td></tr></table></figure></div></div><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">vae = Model(x, x_decoded_mean)</div><div class="line">vae.compile(optimizer=<span class="string">'rmsprop'</span>, loss=vae_loss)</div><div class="line"></div><div class="line"><span class="comment"># train the VAE on MNIST digits</span></div><div class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data(path=<span class="string">'mnist.pkl.gz'</span>)</div><div class="line"></div><div class="line">x_train = x_train.astype(<span class="string">'float32'</span>) / <span class="number">255.</span> <span class="comment"># input normalization</span></div><div class="line">x_test = x_test.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[<span class="number">1</span>:])))</div><div class="line">x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[<span class="number">1</span>:])))</div><div class="line"></div><div class="line">vae.fit(x_train, <span class="comment"># regarded as "x", which is "training data"</span></div><div class="line">        x_train, <span class="comment"># regarded as "y", which is "target/label data"</span></div><div class="line">        shuffle=<span class="keyword">True</span>,</div><div class="line">        nb_epoch=nb_epoch,</div><div class="line">        verbose=<span class="number">2</span>,<span class="comment"># Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.</span></div><div class="line">        batch_size=batch_size,</div><div class="line">        validation_data=(x_test, x_test))</div></pre></td></tr></table></figure></div></div><h2 id="Latent-space-display"><a href="#Latent-space-display" class="headerlink" title="Latent space display"></a>Latent space display</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># build a model to project inputs on the latent space</span></div><div class="line">encoder = Model(x, z_mean)</div><div class="line"><span class="comment"># display a 2D plot of the digit classes in the latent space</span></div><div class="line">x_test_encoded = encoder.predict(x_test, batch_size=batch_size)</div><div class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))</div><div class="line">plt.scatter(x_test_encoded[:, <span class="number">0</span>], x_test_encoded[:, <span class="number">1</span>], c=y_test)</div><div class="line">plt.colorbar()</div><div class="line">plt.show()</div></pre></td></tr></table></figure></div></div><p><img src="/2018/07/20/Variational-AutoEncoder/image-20180724163307750.png" alt="image-20180724163307750"></p><h2 id="Generated-pics-display"><a href="#Generated-pics-display" class="headerlink" title="Generated pics display"></a>Generated pics display</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># build a digit generator that can sample from the learned distribution</span></div><div class="line">decoder_input = Input(shape=(latent_dim,))</div><div class="line">_h_decoded = decoder_h(decoder_input)</div><div class="line">_x_decoded_mean = decoder_mean(_h_decoded)</div><div class="line">generator = Model(decoder_input, _x_decoded_mean)</div><div class="line"></div><div class="line"><span class="comment"># display a 2D manifold of the digits</span></div><div class="line">n = <span class="number">15</span>  <span class="comment"># figure with 15x15 digit pics</span></div><div class="line">digit_size = <span class="number">28</span> <span class="comment"># size of the pic is 28*28</span></div><div class="line">figure = np.zeros((digit_size * n, digit_size * n))</div><div class="line"></div><div class="line">xy = np.linspace(<span class="number">0.05</span>, <span class="number">0.95</span>, n) <span class="comment"># generate n number in range(0.05,0.95)</span></div><div class="line">grid_x = norm.ppf(xy) <span class="comment"># ppf is a inverse function of CDF, e.g. xy=0.1,then ppf return x such that p(X&lt;x)=0.1</span></div><div class="line">grid_y = norm.ppf(xy)</div><div class="line"></div><div class="line"><span class="comment">#plotting</span></div><div class="line"><span class="keyword">for</span> i, yi <span class="keyword">in</span> enumerate(grid_x):</div><div class="line">    <span class="keyword">for</span> j, xi <span class="keyword">in</span> enumerate(grid_y):</div><div class="line">        z_sample = np.array([[xi, yi]])<span class="comment">#1*2</span></div><div class="line">        x_decoded = generator.predict(z_sample)</div><div class="line">        digit = x_decoded[<span class="number">0</span>].reshape(digit_size, digit_size)<span class="comment">#the generated image</span></div><div class="line">        figure[i * digit_size: (i + <span class="number">1</span>) * digit_size,</div><div class="line">               j * digit_size: (j + <span class="number">1</span>) * digit_size] = digit</div><div class="line"></div><div class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">plt.imshow(figure, cmap=<span class="string">'Greys_r'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></div></div><p><img src="/2018/07/20/Variational-AutoEncoder/image-20180724163319485.png" alt="image-20180724163319485"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.cnblogs.com/huangshiyu13/p/6209016.html" target="_blank" rel="noopener">AE to VAE</a> <a href="https://www.cnblogs.com/wangxiaocvpr/p/6231019.html" target="_blank" rel="noopener">AE vs VAE</a> <a href="http://chengjunwen.github.io/2017/01/02/VAE/" target="_blank" rel="noopener">intuition3</a> <a href="https://zhuanlan.zhihu.com/p/25269592" target="_blank" rel="noopener">gotcha</a> </p><p><a href="https://blog.csdn.net/jackytintin/article/details/53641885" target="_blank" rel="noopener">ref1</a> <a href="https://blog.csdn.net/u011534057/article/details/78911902" target="_blank" rel="noopener">ref2</a> <a href="http://lib.csdn.net/article/deeplearning/68143?knId=1748" target="_blank" rel="noopener">ref3</a> <a href="http://www.dengfanxin.cn/?p=334" target="_blank" rel="noopener">ref4</a> <a href="https://yq.aliyun.com/articles/68410" target="_blank" rel="noopener">[intuition about learn latent gaussian distribution]</a> <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/" target="_blank" rel="noopener">ref7</a> <a href="https://blog.csdn.net/heyc861221/article/details/80130968" target="_blank" rel="noopener">VAE vs GAN</a> </p><p><a href="https://www.youtube.com/watch?v=8zomhgKrsmQ" target="_blank" rel="noopener">video1</a> <a href="https://zhuanlan.zhihu.com/p/25401928" target="_blank" rel="noopener">公式</a> </p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Variational AutoEncoder </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Density Map</title>
      <link href="/2018/07/17/Density-Map/"/>
      <url>/2018/07/17/Density-Map/</url>
      <content type="html"><![CDATA[<p>This is some implementations of create ground truth density maps for crowd counting.  </p><a id="more"></a><p>This one is from <a href="https://github.com/svishwa/crowdcount-mcnn" target="_blank" rel="noopener">Single Image Crowd Counting via MCNN (Unofficial Implementation)</a>. The author uses fixed gaussian kernel size. </p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">clc; </div><div class="line">dataset = <span class="string">'A'</span>;</div><div class="line">dataset_name = [<span class="string">'shanghaitech_part_'</span> dataset ];</div><div class="line">path = [<span class="string">'../data/original/shanghaitech/part_'</span> dataset <span class="string">'_final/test_data/images/'</span>];</div><div class="line">gt_path = [<span class="string">'../data/original/shanghaitech/part_'</span> dataset <span class="string">'_final/test_data/ground-truth/'</span>];</div><div class="line">gt_path_csv = [<span class="string">'../data/original/shanghaitech/part_'</span> dataset <span class="string">'_final/test_data/ground_truth_csv/'</span>];</div><div class="line"></div><div class="line">mkdir(gt_path_csv);</div><div class="line"><span class="keyword">if</span> (dataset == <span class="string">'A'</span>)</div><div class="line">    num_images = <span class="number">182</span>;</div><div class="line"><span class="keyword">else</span></div><div class="line">    num_images = <span class="number">316</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">gt_people_count = [];</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:num_images  </div><div class="line">    <span class="keyword">if</span> (<span class="built_in">mod</span>(<span class="built_in">i</span>,<span class="number">10</span>)==<span class="number">0</span>)</div><div class="line">        fprintf(<span class="number">1</span>,<span class="string">'Processing %3d/%d files\n'</span>, <span class="built_in">i</span>, num_images);</div><div class="line">    <span class="keyword">end</span></div><div class="line">    gt_file_name = [gt_path,<span class="string">'GT_IMG_'</span>,num2str(i),<span class="string">'.mat'</span>];</div><div class="line">    load(gt_file_name);</div><div class="line">    input_img_name = strcat(path,<span class="string">'IMG_'</span>,num2str(<span class="built_in">i</span>),<span class="string">'.jpg'</span>);</div><div class="line">    im = imread(input_img_name);</div><div class="line">    [h, w, c] = <span class="built_in">size</span>(im);</div><div class="line">    <span class="keyword">if</span> (c == <span class="number">3</span>)</div><div class="line">        im = rgb2gray(im);</div><div class="line">    <span class="keyword">end</span>     </div><div class="line">    annPoints =  image_info&#123;<span class="number">1</span>&#125;.location;</div><div class="line">    gt_people_count = [ gt_people_count;length(annPoints)];</div><div class="line">    [h, w, c] = <span class="built_in">size</span>(im);</div><div class="line">    im_density = get_density_map_gaussian(im,annPoints);    </div><div class="line">    csvwrite([gt_path_csv ,<span class="string">'IMG_'</span>,num2str(i) <span class="string">'.csv'</span>], im_density);       </div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">im_density</span> = <span class="title">get_density_map_gaussian</span><span class="params">(im,points)</span></span></div><div class="line"></div><div class="line"></div><div class="line">im_density = <span class="built_in">zeros</span>(<span class="built_in">size</span>(im)); </div><div class="line">[h,w] = <span class="built_in">size</span>(im_density);</div><div class="line"></div><div class="line"><span class="keyword">if</span>( <span class="built_in">isempty</span>(points))</div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="built_in">disp</span>(<span class="string">'The total points are'</span>);</div><div class="line"><span class="built_in">disp</span>(<span class="built_in">length</span>(points));</div><div class="line"><span class="keyword">if</span>(<span class="built_in">length</span>(points(:,<span class="number">1</span>))==<span class="number">1</span>)</div><div class="line">    x1 = max(<span class="number">1</span>,min(w,<span class="built_in">round</span>(points(<span class="number">1</span>,<span class="number">1</span>))));</div><div class="line">    y1 = max(<span class="number">1</span>,min(h,<span class="built_in">round</span>(points(<span class="number">1</span>,<span class="number">2</span>))));</div><div class="line">    im_density(y1,x1) = <span class="number">255</span>;</div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(points) </div><div class="line">    f_sz = <span class="number">15</span>;</div><div class="line">    sigma = <span class="number">4.0</span>;</div><div class="line">    H = fspecial(<span class="string">'Gaussian'</span>,[f_sz, f_sz],sigma);</div><div class="line">    x = min(w,max(<span class="number">1</span>,<span class="built_in">abs</span>(int32(<span class="built_in">floor</span>(points(<span class="built_in">j</span>,<span class="number">1</span>))))));</div><div class="line">    y = min(h,max(<span class="number">1</span>,<span class="built_in">abs</span>(int32(<span class="built_in">floor</span>(points(<span class="built_in">j</span>,<span class="number">2</span>))))));</div><div class="line">    <span class="keyword">if</span>(x &gt; w || y &gt; h)</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    </div><div class="line">    x1 = x - int32(<span class="built_in">floor</span>(f_sz/<span class="number">2</span>)); y1 = y - int32(<span class="built_in">floor</span>(f_sz/<span class="number">2</span>));</div><div class="line">    x2 = x + int32(<span class="built_in">floor</span>(f_sz/<span class="number">2</span>)); y2 = y + int32(<span class="built_in">floor</span>(f_sz/<span class="number">2</span>));</div><div class="line">    dfx1 = <span class="number">0</span>; dfy1 = <span class="number">0</span>; dfx2 = <span class="number">0</span>; dfy2 = <span class="number">0</span>;</div><div class="line">    change_H = false;</div><div class="line">    </div><div class="line">    <span class="keyword">if</span>(x1 &lt; <span class="number">1</span>)</div><div class="line">        dfx1 = <span class="built_in">abs</span>(x1)+<span class="number">1</span>;</div><div class="line">        x1 = <span class="number">1</span>;</div><div class="line">        change_H = true;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    <span class="keyword">if</span>(y1 &lt; <span class="number">1</span>)</div><div class="line">        dfy1 = <span class="built_in">abs</span>(y1)+<span class="number">1</span>;</div><div class="line">        y1 = <span class="number">1</span>;</div><div class="line">        change_H = true;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    <span class="keyword">if</span>(x2 &gt; w)</div><div class="line">        dfx2 = x2 - w;</div><div class="line">        x2 = w;</div><div class="line">        change_H = true;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    <span class="keyword">if</span>(y2 &gt; h)</div><div class="line">        dfy2 = y2 - h;</div><div class="line">        y2 = h;</div><div class="line">        change_H = true;</div><div class="line">    <span class="keyword">end</span></div><div class="line">    x1h = <span class="number">1</span>+dfx1; y1h = <span class="number">1</span>+dfy1; x2h = f_sz - dfx2; y2h = f_sz - dfy2;</div><div class="line">    <span class="keyword">if</span> (change_H == true)</div><div class="line">        H =  fspecial(<span class="string">'Gaussian'</span>,[double(y2h-y1h+<span class="number">1</span>), double(x2h-x1h+<span class="number">1</span>)],sigma);</div><div class="line">    <span class="keyword">end</span></div><div class="line">    im_density(y1:y2,x1:x2) = im_density(y1:y2,x1:x2) +  H;</div><div class="line">     </div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>The second one is modified by me based on the code above. In this version, it is created for UCSD dataset where the intensities outside ROI is set to 0.</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">im_density</span> = <span class="title">ucsd_density_map</span><span class="params">(im,points,xi,yi)</span></span></div><div class="line"></div><div class="line">im_density = <span class="built_in">zeros</span>(<span class="built_in">size</span>(im)); </div><div class="line">[h,w] = <span class="built_in">size</span>(im_density); <span class="comment">%h=160,w=240</span></div><div class="line"><span class="keyword">if</span>( <span class="built_in">isempty</span>(points))</div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="keyword">if</span>(<span class="built_in">length</span>(points(:,<span class="number">1</span>))==<span class="number">1</span>)</div><div class="line">    x1 = max(<span class="number">1</span>,min(w,<span class="built_in">round</span>(points(<span class="number">1</span>,<span class="number">1</span>))));</div><div class="line">    y1 = max(<span class="number">1</span>,min(h,<span class="built_in">round</span>(points(<span class="number">1</span>,<span class="number">2</span>))));</div><div class="line">    im_density(y1,x1) = <span class="number">255</span>;</div><div class="line">    <span class="keyword">return</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(points) </div><div class="line">    sigma = <span class="number">4.0</span>;</div><div class="line">    H = fspecial(<span class="string">'Gaussian'</span>,sigma);</div><div class="line">    x = min(w,max(<span class="number">1</span>,<span class="built_in">abs</span>(int32(<span class="built_in">floor</span>(points(<span class="built_in">j</span>,<span class="number">1</span>))))));</div><div class="line">    y = min(h,max(<span class="number">1</span>,<span class="built_in">abs</span>(int32(<span class="built_in">floor</span>(points(<span class="built_in">j</span>,<span class="number">2</span>))))));</div><div class="line">    <span class="keyword">if</span>(x &gt; w || y &gt; h)</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">    <span class="keyword">end</span> </div><div class="line">    </div><div class="line">    mask_area = roipoly(im,<span class="built_in">floor</span>(xi),<span class="built_in">floor</span>(yi));</div><div class="line">    </div><div class="line">    imm = <span class="built_in">zeros</span>(h,w);</div><div class="line">    imm(y,x) = <span class="number">1</span>; <span class="comment">% attention: not imm(x,y) = 1 </span></div><div class="line">    H_ROI = roifilt2(H,imm,mask_area);</div><div class="line">    im_density = im_density +  H_ROI;</div><div class="line">         </div><div class="line"><span class="keyword">end</span></div><div class="line">fprintf("the sum of sum is %f",sum(sum(im_density)));</div><div class="line">fprintf("  the length is %f\n",length(points));</div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>The third one is from <a href="https://zhuanlan.zhihu.com/p/39424587" target="_blank" rel="noopener">人群密度图生成-Python实现</a>. In this one, the author uses geometry-adaptive Gaussian kernel for density map creation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</div><div class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</div><div class="line"><span class="keyword">from</span> scipy.ndimage <span class="keyword">import</span> filters</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_filter_density</span><span class="params">(gt)</span>:</span></div><div class="line">    pts = np.array(list(zip(np.nonzero(gt)[<span class="number">1</span>], np.nonzero(gt)[<span class="number">0</span>]))) <span class="comment">#np.nonzero return two arrays, one is x_index of nonzero value and the other is y_index of nonzero value</span></div><div class="line">    neighbors = NearestNeighbors(n_neighbors=<span class="number">4</span>, algorithm=<span class="string">'kd_tree'</span>, leaf_size=<span class="number">1200</span>)</div><div class="line">    neighbors.fit(pts.copy())</div><div class="line">    distances, _ = neighbors.kneighbors()</div><div class="line">    density = np.zeros(gt.shape, dtype=np.float32)</div><div class="line">    type(distances)</div><div class="line">    sigmas = distances.sum(axis=<span class="number">1</span>) * <span class="number">0.075</span> <span class="comment"># 0.075 = 0.3/4</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(pts)):</div><div class="line">        pt = pts[i]</div><div class="line">        pt2d = np.zeros(shape=gt.shape, dtype=np.float32)</div><div class="line">        pt2d[pt[<span class="number">1</span>]][pt[<span class="number">0</span>]] = <span class="number">1</span></div><div class="line">        density += filters.gaussian_filter(pt2d, sigmas[i], mode=<span class="string">'constant'</span>)</div><div class="line">    <span class="keyword">return</span> density</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_density</span><span class="params">(gts, d_map_h, d_map_w)</span>:</span></div><div class="line">    res = np.zeros(shape=[d_map_h, d_map_w]) <span class="comment"># res store head positions</span></div><div class="line">    bool_res = (gts[:, <span class="number">0</span>] &lt; d_map_w) &amp; (gts[:, <span class="number">1</span>] &lt; d_map_h)</div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(gts)):</div><div class="line">        gt = gts[k]</div><div class="line">        <span class="keyword">if</span> (bool_res[k] == <span class="keyword">True</span>):</div><div class="line">            res[int(gt[<span class="number">1</span>])][int(gt[<span class="number">0</span>])] = <span class="number">1</span></div><div class="line">    pts = np.array(list(zip(np.nonzero(res)[<span class="number">1</span>], np.nonzero(res)[<span class="number">0</span>])))</div><div class="line">    neighbors = NearestNeighbors(n_neighbors=<span class="number">4</span>, algorithm=<span class="string">'kd_tree'</span>, leaf_size=<span class="number">1200</span>)</div><div class="line">    neighbors.fit(pts.copy())</div><div class="line">    distances, _ = neighbors.kneighbors()</div><div class="line">    map_shape = [d_map_h, d_map_w]</div><div class="line">    density = np.zeros(shape=map_shape, dtype=np.float32)</div><div class="line">    sigmas = distances.sum(axis=<span class="number">1</span>) * <span class="number">0.075</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(pts)):</div><div class="line">        pt = pts[i]</div><div class="line">        pt2d = np.zeros(shape=map_shape, dtype=np.float32)</div><div class="line">        pt2d[pt[<span class="number">1</span>]][pt[<span class="number">0</span>]] = <span class="number">1</span></div><div class="line">        t1 = filters.gaussian_filter(pt2d, sigmas[i])</div><div class="line">        density += t1 <span class="comment">#bigger the sigma, blurrer the picture</span></div><div class="line">    <span class="keyword">return</span> density</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    train_img = <span class="string">'../data/original/shanghaitech/part_A_final/train_data/images'</span></div><div class="line">    train_gt = <span class="string">'../data/original/shanghaitech/part_A_final/train_data/ground-truth'</span></div><div class="line">    out_path = <span class="string">'../data/formatted_trainval/shanghaitech_adaptive_gaussian/true_crowd_counting/'</span></div><div class="line">    validation_num = <span class="number">15</span></div><div class="line"></div><div class="line">    img_names = os.listdir(train_img)</div><div class="line">    num = len(img_names)</div><div class="line">    num_list = np.arange(<span class="number">1</span>, num + <span class="number">1</span>)</div><div class="line">    <span class="comment"># random.shuffle(num_list)</span></div><div class="line">    global_step = <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> num_list:</div><div class="line">        full_img = train_img + <span class="string">'/IMG_'</span> + str(i) + <span class="string">'.jpg'</span></div><div class="line">        full_gt = train_gt + <span class="string">'/GT_IMG_'</span> + str(i) + <span class="string">'.mat'</span></div><div class="line">        img = mpimg.imread(full_img)</div><div class="line">        data = sio.loadmat(full_gt)</div><div class="line">        gts = data[<span class="string">'image_info'</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># shape is (num_count, 2)</span></div><div class="line">        count = <span class="number">1</span></div><div class="line"></div><div class="line">        shape = img.shape</div><div class="line">        <span class="keyword">if</span> (len(shape) &lt; <span class="number">3</span>):</div><div class="line">            img = img.reshape([shape[<span class="number">0</span>], shape[<span class="number">1</span>], <span class="number">1</span>])</div><div class="line"></div><div class="line">        d_map_h = math.floor(math.floor(float(img.shape[<span class="number">0</span>]) / <span class="number">2.0</span>) / <span class="number">2.0</span>)</div><div class="line">        d_map_w = math.floor(math.floor(float(img.shape[<span class="number">1</span>]) / <span class="number">2.0</span>) / <span class="number">2.0</span>)</div><div class="line">        den_map = create_density(gts / <span class="number">4</span>, d_map_h, d_map_w)</div><div class="line"></div><div class="line"></div><div class="line">        p_h = math.floor(float(img.shape[<span class="number">0</span>]) / <span class="number">3.0</span>)</div><div class="line">        p_w = math.floor(float(img.shape[<span class="number">1</span>]) / <span class="number">3.0</span>)</div><div class="line">        d_map_ph = math.floor(math.floor(p_h / <span class="number">2.0</span>) / <span class="number">2.0</span>)</div><div class="line">        d_map_pw = math.floor(math.floor(p_w / <span class="number">2.0</span>) / <span class="number">2.0</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (global_step &lt; validation_num):</div><div class="line">            mode = <span class="string">'val'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            mode = <span class="string">'train'</span></div><div class="line">        py = <span class="number">1</span></div><div class="line">        py2 = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">4</span>):</div><div class="line">            px = <span class="number">1</span></div><div class="line">            px2 = <span class="number">1</span></div><div class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">4</span>):</div><div class="line">                final_image = img[py - <span class="number">1</span>: py + p_h - <span class="number">1</span>, px - <span class="number">1</span>: px + p_w - <span class="number">1</span>, :]</div><div class="line">                final_gt = den_map[py2 - <span class="number">1</span>: py2 + d_map_ph - <span class="number">1</span>, px2 - <span class="number">1</span>: px2 + d_map_pw - <span class="number">1</span>]</div><div class="line">                px = px + p_w</div><div class="line">                px2 = px2 + d_map_pw</div><div class="line">                <span class="keyword">if</span> final_image.shape[<span class="number">2</span>] &lt; <span class="number">3</span>:</div><div class="line">                    final_image = np.tile(final_image, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]) <span class="comment">#tile : repeat "final_image" once in axis=0 and axis = 1 while repeat three times in axis=2</span></div><div class="line">                image_final_name = out_path + mode + <span class="string">'_img/'</span> <span class="string">'IMG_'</span> + str(i) + <span class="string">'_'</span> + str(count) + <span class="string">'.jpg'</span></div><div class="line">                gt_final_name = out_path + mode + <span class="string">'_gt/'</span> + <span class="string">'GT_IMG_'</span> + str(i) + <span class="string">'_'</span> + str(count)</div><div class="line">                <span class="comment">#Image.fromarray(final_image).convert('RGB').save(image_final_name)</span></div><div class="line">                <span class="comment">#np.save(gt_final_name, final_gt)</span></div><div class="line">                count = count + <span class="number">1</span></div><div class="line">            py = py + p_h</div><div class="line">            py2 = py2 + d_map_ph</div><div class="line">        global_step = global_step + <span class="number">1</span></div></pre></td></tr></table></figure><p>Some other implementation <a href="https://blog.csdn.net/snwang_miss/article/details/80410501" target="_blank" rel="noopener">密度图生成1</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Delta Function</title>
      <link href="/2018/07/16/Delta-Function/"/>
      <url>/2018/07/16/Delta-Function/</url>
      <content type="html"><![CDATA[<p>Technically speaking, the Dirac delta function is not actually a function. It is what we may call a generalized function. Nevertheless, its definition is intuitive and it simplifies dealing with probability distributions.</p><a id="more"></a><p><a href="https://www.youtube.com/watch?v=ECslmuGlu-U" target="_blank" rel="noopener">Intuition</a> <a href="https://www.probabilitycourse.com/chapter4/4_3_2_delta_function.php" target="_blank" rel="noopener">ref2</a> <a href="https://www.youtube.com/watch?v=oQx1myAYZoY" target="_blank" rel="noopener">begin to understand</a> </p><p><a href="http://www.stat.cmu.edu/~cshalizi/350/lectures/28/lecture-28.pdf" target="_blank" rel="noopener">Ref1</a> <a href="https://www.google.com/search?q=empirical+distribution+delta+function&amp;oq=empirical+distribution+delta+function&amp;aqs=chrome..69i57.6208j0j7&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">Google search</a> </p><h2 id="Unit-Step-function"><a href="#Unit-Step-function" class="headerlink" title="Unit Step function"></a><strong>Unit Step function</strong></h2><p>在介绍delta函数之前，我们先了解一下阶跃函数(step function)，定义如下：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 11.05.35 AM.png" alt="Screen Shot 2018-07-17 at 11.05.35 AM"></p><p>即当$t&lt;0$时，函数值为0；否则，函数值是1。其图像如下：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 11.05.41 AM.png" alt="Screen Shot 2018-07-17 at 11.05.41 AM"></p><p>那么我们来求出阶跃函数的导数，显然当$t<0$和$t>0$时，导数是0，那么$t=0$呢？直觉上，在$t=0$处，函数的斜率是$\infin$，而实际上$t=0$处的导数就是$\infin$，证明可以参看<a href="https://blog.csdn.net/cinmyheart/article/details/21133039" target="_blank" rel="noopener">阶跃函数的导数为什么是冲击函数?</a></0$和$t></p><h2 id="delta-Function"><a href="#delta-Function" class="headerlink" title="$\delta$ Function"></a><strong>$\delta$ Function</strong></h2><p>这样我们就得到了delta函数，$\delta$函数是在实数线上的一个函数，在原点上无限，在所有其他点上为零，</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 11.31.14 AM.png" alt="Screen Shot 2018-07-17 at 11.31.14 AM"></p><p>其图像是：</p><p><img src="/2018/07/16/Delta-Function/1024px-Dirac_distribution_PDF.svg.png" alt="1024px-Dirac_distribution_PDF.svg"></p><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a><strong>Properties</strong></h2><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 11.34.17 AM.png" alt="Screen Shot 2018-07-17 at 11.34.17 AM"></p><blockquote><p>对于第三条性质，由于delta function是阶跃函数的导数，那么对其积分就对阶跃函数求函数值：</p><script type="math/tex; mode=display">\int_{-\epsilon}^{\epsilon}\delta(x)dx=u(x)|_{-\epsilon}^{\epsilon}=u(\epsilon)-u(-\epsilon)=1-0=1</script><p>对于第四条性质，令$F(x)=g(x)\delta(x-x_0)$，根据$\delta$函数的定义，如果$x-x_0\ne 0$,即$x\ne x_0$，$\delta()=0$，即$F(x)=0$，那么可以改写上述为$F(x)=g(x_0)\delta (x-x_0)$，则</p><script type="math/tex; mode=display">\int_{-\infin}^{\infin}g(x_0)\delta(x)dx=g(x_0)\int_{-\infin}^{\infin}\delta(x)dx=g(x_0)</script></blockquote><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><h3 id="Delta-Function-in-PDFs"><a href="#Delta-Function-in-PDFs" class="headerlink" title="Delta Function in PDFs"></a>Delta Function in PDFs</h3><p>无论是离散还是连续变量，它们都有连续累积函数(<strong>CDF</strong>)；而概率密度函数(<strong>PDF</strong>)，只存在于连续型变量；离散型变量有相应的概率质量函数(<strong>PMF</strong>)。使用$\delta$函数可以帮助我们对离散型变量和混合型变量定义一个<strong>PDF</strong>。</p><p>假设我们有一随机变量$X$，其取值为$R_X=\{x_1,x_2,x_3,…\}$，对应的PMF是$P_X(x_k)$。那么离散随机变量的CDF是一个阶梯函数，如下入所示</p><p><img src="/2018/07/16/Delta-Function/image-20180717155151850.png" alt="image-20180717155151850"></p><p>图中每一个跳跃点对应到横轴上都是一个$X$的取值，那么对于每一个$x_k$，其PMF为：</p><script type="math/tex; mode=display">P_X(x_k)u(x-x_k)</script><p>即只有当$x=x_k$时，其概率是$P_X(x_k)$.</p><p>那么该随机变量$X$的CDF可以写作：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 3.55.43 PM.png" alt="Screen Shot 2018-07-17 at 3.55.43 PM"></p><p>那么我们可以通过对CDF求导得到PDF，</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 3.56.49 PM.png" alt="Screen Shot 2018-07-17 at 3.56.49 PM"></p><p>我们称之为<strong>generalized PDF</strong></p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 3.57.52 PM.png" alt="Screen Shot 2018-07-17 at 3.57.52 PM"></p><p><strong>期望</strong></p><p>对一个具有上述PDF的离散型变量X，其期望为：</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}E(X)&=\int_{-\infin}^{\infin}xf_X(x)dx\\&=\int_{-\infin}^{\infin}x\sum_{x_k\in R_X}P_X(x_k)\delta (x-x_k)dx\\&=\sum_{x_k\in R_X}P_X(x_k)\int_{-\infin}^{\infin}x\delta (x-x_k)dx\\&=\sum_{x_k\in R_X}P_X(x_k)x_k          ------\text{4th property}\end{aligned}\end{equation}</script><p>可以看到，其期望值与离散型随机变量的定义一样。</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.12.12 PM.png" alt="Screen Shot 2018-07-17 at 4.12.12 PM"></p><p><strong>例子</strong></p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.28.56 PM.png" alt="Screen Shot 2018-07-17 at 4.28.56 PM"></p><blockquote><p>a. 画出CDF图像如下示：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.30.18 PM.png" alt="Screen Shot 2018-07-17 at 4.30.18 PM"></p><p>可以看到有两个跳跃点$x=0$和$x=1$，同时在区间$[0,1)$和$[1,\infin]$，CDF又是连续的，所以随机变量$X$是一个混合型变量。</p><p>b. CDF有两个跳跃点，且每一个跳跃点的概率大小都是0.25，所以CDF有两个$\delta$函数：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.35.03 PM.png" alt="Screen Shot 2018-07-17 at 4.35.03 PM"></p><p>c. 利用CDF，我们有：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.37.40 PM.png" alt="Screen Shot 2018-07-17 at 4.37.40 PM"></p><p>利用PDF，我们有：</p><p><img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.38.41 PM.png" alt="Screen Shot 2018-07-17 at 4.38.41 PM"></p><p>d. <img src="/2018/07/16/Delta-Function/Screen Shot 2018-07-17 at 4.39.32 PM.png" alt="Screen Shot 2018-07-17 at 4.39.32 PM"></p></blockquote>]]></content>
      
      <categories>
          
          <category> Probability and Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Delta Function </tag>
            
            <tag> Probability </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Books</title>
      <link href="/2018/07/16/Books/"/>
      <url>/2018/07/16/Books/</url>
      <content type="html"><![CDATA[<h1 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a>Productivity</h1><h2 id="Feynman-Technique"><a href="#Feynman-Technique" class="headerlink" title="Feynman Technique"></a>Feynman Technique</h2><ul><li><a href="https://fs.blog/2012/04/feynman-technique/" target="_blank" rel="noopener">The Feynman Technique: The Best Way to Learn Anything</a> </li><li><a href="https://curiosity.com/topics/learn-anything-in-four-steps-with-the-feynman-technique-curiosity/" target="_blank" rel="noopener">Learn Anything in Four Steps With the Feynman Technique</a> </li></ul><h1 id="Habits"><a href="#Habits" class="headerlink" title="Habits"></a>Habits</h1><ol><li>The single most important change you can make in your working habits is to switch to creative work first, reactive work second. This means blocking off a large chunk of time every day for creative work on your own priorities, with the phone and e-mail off.</li><li>Notice when you seem to have the most energy during the day, and dedicate those valuable periods to your most important creative work. Never book a meeting during this time if you can help it. And don’t waste any of it on administrative work!</li></ol><h1 id="Logic"><a href="#Logic" class="headerlink" title="Logic"></a>Logic</h1><p><a href="https://www.zhihu.com/question/19599216" target="_blank" rel="noopener">ref</a> </p><p>A4纸横放，每张纸写一个主题，1页写4至6行，每行8字到12字，一张纸控制在2分钟之内，每天写10页，即每天用10-20分钟的时间写笔记。</p><ul><li>写标题和正文时，别思考太多，想到的事，不论是什么，先写下来。</li><li>严格坚持每页1分钟，一想到就立刻写下来，这会让你不拖延。</li><li>随身准备好A4纸和纸板，在任何地方都可以写，或者把A4纸折成三折</li><li>可以把写好的笔记，4-6行的内容做为正文，继续深入下去</li><li>对于，同一个标题可以用不同的角度来写很多页，处理问题和视野都能扩展开来</li></ul><h1 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h1><ul><li>《放学后》- 东野圭吾</li></ul><h1 id="Art"><a href="#Art" class="headerlink" title="Art"></a>Art</h1><h2 id="Evening-Silence"><a href="#Evening-Silence" class="headerlink" title="Evening Silence"></a>Evening Silence</h2><p><img src="/2018/07/16/Books/IMG_4517.JPG" alt="MG_451"></p><p><img src="/2018/07/16/Books/IMG_4520.JPG" alt="MG_452"></p>]]></content>
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Books </tag>
            
            <tag> Habits </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Image_Processing</title>
      <link href="/2018/07/15/Image-Processing/"/>
      <url>/2018/07/15/Image-Processing/</url>
      <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/dengtaocs/article/details/38022153" target="_blank" rel="noopener">opencv中感兴趣区域以及mask的使用</a></p><p><a href="https://blog.csdn.net/matlab_matlab/article/details/54015331" target="_blank" rel="noopener">不规则区域提取</a></p><p><a href="https://ask.helplib.com/matlab/post_5759524" target="_blank" rel="noopener">使用预先定义的ROI Matlab进行图像裁剪</a></p><h1 id="CV2"><a href="#CV2" class="headerlink" title="CV2"></a>CV2</h1><p>RGB =<code>(height, width, 3)</code></p><p>图片读取<code>cv2.imread()</code>第一个参数是文件路径，第二个参数指定读取方式</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> cv2</div><div class="line"><span class="comment"># 以灰度模式读取图片</span></div><div class="line">img = cv2.imread(<span class="string">'lena.jpg'</span>,<span class="number">0</span>)   //  cv2.IMREAD_GRAYSCALE</div></pre></td></tr></table></figure><blockquote><ul><li>cv2.IMREAD_COLOR：加载彩色图片，这个是默认参数。</li><li>cv2.IMREAD_GRAYSCALE：以灰度模式加载图片。</li><li>cv2.IMREAD_UNCHANGED：包括alpha。</li><li>上面三个flag分别对应的值为1，0，-1</li></ul></blockquote></div></div><p>使用<code>cv2.imshow()</code>显示一个图像窗口，窗口大小根据图像自动调整。第一个参数是窗口名称，第二个参数是图片。</p><p>使用函数<code>cv2.imwrite()</code>保存图像到文件,第一个参数是文件名，第二个参数是你要保存的文件</p><p>使用<code>cv2.resize(img,(width, height))</code>来改变图像大小。</p><h1 id="VIDEOS"><a href="#VIDEOS" class="headerlink" title="VIDEOS"></a>VIDEOS</h1><p>从视频中提取帧 <a href="https://blog.csdn.net/JNingWei/article/details/77869078" target="_blank" rel="noopener">ref</a>  </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding=utf-8</span></div><div class="line"></div><div class="line"><span class="comment"># 全局变量</span></div><div class="line">VIDEO_PATH = <span class="string">'./1.avi'</span> <span class="comment"># 视频地址</span></div><div class="line">EXTRACT_FOLDER = <span class="string">'./extract_folder'</span> <span class="comment"># 存放帧图片的位置</span></div><div class="line">EXTRACT_FREQUENCY = <span class="number">100</span> <span class="comment"># 帧提取频率</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_frames</span><span class="params">(video_path, dst_folder, index)</span>:</span></div><div class="line">    <span class="comment"># 主操作</span></div><div class="line">    <span class="keyword">import</span> cv2</div><div class="line">    video = cv2.VideoCapture()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> video.open(video_path):</div><div class="line">        print(<span class="string">"can not open the video"</span>)</div><div class="line">        exit(<span class="number">1</span>)</div><div class="line">    count = <span class="number">1</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        _, frame = video.read()</div><div class="line">        <span class="keyword">if</span> frame <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">if</span> count % EXTRACT_FREQUENCY == <span class="number">0</span>:</div><div class="line">            save_path = <span class="string">"&#123;&#125;/&#123;:&gt;03d&#125;.jpg"</span>.format(dst_folder, index)</div><div class="line">            cv2.imwrite(save_path, frame)</div><div class="line">            index += <span class="number">1</span></div><div class="line">        count += <span class="number">1</span></div><div class="line">    video.release()</div><div class="line">    <span class="comment"># 打印出所提取帧的总数</span></div><div class="line">    print(<span class="string">"Totally save &#123;:d&#125; pics"</span>.format(index<span class="number">-1</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 递归删除之前存放帧图片的文件夹，并新建一个</span></div><div class="line">    <span class="keyword">import</span> shutil</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        shutil.rmtree(EXTRACT_FOLDER)</div><div class="line">    <span class="keyword">except</span> OSError:</div><div class="line">        <span class="keyword">pass</span></div><div class="line">    <span class="keyword">import</span> os</div><div class="line">    os.mkdir(EXTRACT_FOLDER)</div><div class="line">    <span class="comment"># 抽取帧图片，并保存到指定路径</span></div><div class="line">    extract_frames(VIDEO_PATH, EXTRACT_FOLDER, <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cv2</div><div class="line"><span class="keyword">import</span> shutil</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">dataset = <span class="string">"handwaving"</span></div><div class="line">inpath = <span class="string">"/Users/daniel/Desktop/%s/"</span>%dataset</div><div class="line">outpath = <span class="string">"/Users/daniel/Downloads/Nutstore/PythonFile/DATASET/KTH/%s/"</span>%dataset</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_frames</span><span class="params">(video_path,dst_folder,index)</span>:</span></div><div class="line">    video = cv2.VideoCapture()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> video.open(video_path):</div><div class="line">        print(<span class="string">"can not open the video"</span>)</div><div class="line">        exit(<span class="number">1</span>)</div><div class="line">    success = <span class="keyword">True</span></div><div class="line">    <span class="keyword">while</span> success:</div><div class="line">        success, frame = video.read()</div><div class="line">        save_path = dst_folder+<span class="string">"%d.png"</span>%index</div><div class="line">        cv2.imwrite(save_path, frame)</div><div class="line">        index += <span class="number">1</span></div><div class="line">    video.release()</div><div class="line">    <span class="comment"># 打印出所提取帧的总数</span></div><div class="line">    print(<span class="string">"Totally save &#123;:d&#125; pics"</span>.format(index<span class="number">-1</span>))</div><div class="line">    <span class="keyword">return</span> index</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    index = <span class="number">1</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        shutil.rmtree(outpath)</div><div class="line">    <span class="keyword">except</span> OSError:</div><div class="line">        <span class="keyword">pass</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(outpath):</div><div class="line">        os.makedirs(outpath)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</div><div class="line">            video_path = inpath + <span class="string">"person%02d_handwaving_d%d_uncomp.avi"</span> % (i, j)</div><div class="line">            index = extract_frames(video_path,outpath,index)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure></div></div><h1 id="Depth-Images"><a href="#Depth-Images" class="headerlink" title="Depth Images"></a>Depth Images</h1><p><a href="https://rgbd-dataset.cs.washington.edu/software.html" target="_blank" rel="noopener">rgbd images processing</a> </p><p><a href="http://www.atifanwer.xyz/blog/loading-and-visualizing-kinect-depth-images-in-matlab/" target="_blank" rel="noopener">Loading and Visualizing Kinect Depth images in Matlab</a> </p><p>MSR3D-Daily Activities Dataset</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line">clear;close all;clc;</div><div class="line">binPath = <span class="string">'/Users/daniel/Downloads/Nutstore/PythonFile/DATASET/msr3d/Depth/'</span>;</div><div class="line">output_path = <span class="string">'/Users/daniel/Downloads/Nutstore/PythonFile/DATASET/msr3d/Depth_Image/'</span>;</div><div class="line">idx = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> si = <span class="number">1</span>:<span class="number">1</span></div><div class="line">    <span class="keyword">for</span> ai = <span class="number">1</span>:<span class="number">1</span></div><div class="line">        <span class="keyword">for</span> ei = <span class="number">1</span>:<span class="number">1</span></div><div class="line">            <span class="comment">%%%%%%%%%%%%%</span></div><div class="line">            <span class="comment">%%%%%%%%%%%%%</span></div><div class="line">            acsr = num2str(ai,<span class="string">'%02d'</span>);</div><div class="line">            susr = num2str(si,<span class="string">'%02d'</span>);</div><div class="line">            exsr = num2str(ei,<span class="string">'%02d'</span>);</div><div class="line">            <span class="comment">%%%%%% getsr(ai,si,ei) convert ai,si,ei to double bits</span></div><div class="line">            <span class="comment">%%%%%% for example, if ai=3, acsr is 03</span></div><div class="line">            <span class="comment">%%%%%%%%%%%</span></div><div class="line"></div><div class="line">            binfile = [binPath,<span class="string">'a'</span>,acsr,<span class="string">'_s'</span>,susr,<span class="string">'_e'</span>,exsr,<span class="string">'_depth.bin'</span>]; </div><div class="line">            <span class="keyword">if</span> ~exist(binfile,<span class="string">'file'</span>)</div><div class="line">                <span class="built_in">disp</span>(<span class="string">'error'</span>);</div><div class="line">                <span class="keyword">continue</span>;</div><div class="line">            <span class="keyword">end</span></div><div class="line"></div><div class="line">            fileread = fopen(binfile);      </div><div class="line">            <span class="keyword">if</span> fileread&lt;<span class="number">0</span></div><div class="line">               <span class="built_in">disp</span>(<span class="string">'no such file.'</span>);</div><div class="line">               <span class="keyword">return</span>;</div><div class="line">            <span class="keyword">end</span></div><div class="line"></div><div class="line">       </div><div class="line">            header = fread(fileread,<span class="number">3</span>,<span class="string">'uint=&gt;uint'</span>);</div><div class="line">            nnof = header(<span class="number">1</span>); ncols = header(<span class="number">2</span>); nrows = header(<span class="number">3</span>);</div><div class="line">            </div><div class="line"></div><div class="line">            depths = <span class="built_in">zeros</span>(ncols, nrows, nnof);</div><div class="line">            <span class="keyword">for</span> f = <span class="number">1</span>:nnof</div><div class="line">                idx = idx+<span class="number">1</span>;</div><div class="line">                fprintf(<span class="string">'Processing %d/10, %d/16, %d/2, %d/%d\n'</span>,si,ai,ei,f,nnof);</div><div class="line">                frame = <span class="built_in">zeros</span>( ncols, nrows);</div><div class="line">                <span class="keyword">for</span> row = <span class="number">1</span>:nrows</div><div class="line">                    tempRow = fread(fileread, ncols, <span class="string">'uint=&gt;uint'</span>);</div><div class="line">                    tempRowID = fread(fileread, ncols, <span class="string">'uint8'</span>);<span class="comment">%%%%%</span></div><div class="line">                    frame(:,row) = tempRow;</div><div class="line">                <span class="keyword">end</span></div><div class="line">                depth(:,:,f) = frame;</div><div class="line">                [m,n] = <span class="built_in">size</span>(depth(:,:,f));</div><div class="line">                </div><div class="line">                pic_name = [output_path,num2str(idx),<span class="string">'.jpg'</span>];</div><div class="line">                pic = imrotate(depth(:,:,f),<span class="number">270</span>);</div><div class="line">                <span class="comment">%imshow(depth(:,:,f));</span></div><div class="line">                <span class="comment">%imwrite(uint8(pic),pic_name);</span></div><div class="line">                </div><div class="line">                </div><div class="line">                <span class="comment">%plot3k(depth(:,:,1));</span></div><div class="line">                </div><div class="line">                </div><div class="line"><span class="comment">%                 fig = imagesc(depth(:,:,1)); axis off; % truesize;</span></div><div class="line"><span class="comment">%                 title('Depth in color map');</span></div><div class="line"><span class="comment">%                 colormap parula;</span></div><div class="line"><span class="comment">%                 c = colorbar; c.Label.String = 'Distance in mm';</span></div><div class="line"><span class="comment">%                 clear tempRow tempRowID;</span></div><div class="line">                </div><div class="line">                </div><div class="line"><span class="comment">%                 [counts,binLocations] = imhist(depth(:,:,1),4000);</span></div><div class="line"><span class="comment">%                 binLocations = binLocations(1:4000,1);</span></div><div class="line"><span class="comment">%                 bar( binLocations,counts); xlim([450 4000])</span></div><div class="line"><span class="comment">%                 title('Histogram of depth values');</span></div><div class="line">            <span class="keyword">end</span></div><div class="line"></div><div class="line"></div><div class="line">            fclose(fileread);</div><div class="line">        <span class="keyword">end</span></div><div class="line">    <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></div></div>]]></content>
      
      <categories>
          
          <category> Image Processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Processing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Markov Chains</title>
      <link href="/2018/07/12/Markov-Chains/"/>
      <url>/2018/07/12/Markov-Chains/</url>
      <content type="html"><![CDATA[<p><a href="">[Ref1]</a></p>]]></content>
      
      <categories>
          
          <category> Probability and Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Probability </tag>
            
            <tag> Statistic </tag>
            
            <tag> Markov Chains </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Git</title>
      <link href="/2018/07/12/Git/"/>
      <url>/2018/07/12/Git/</url>
      <content type="html"><![CDATA[<p>文章参考自<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">[Ref1]</a></p><p>Git一个是分布式版本控制系统</p><a id="more"></a><h1 id="创建版本库"><a href="#创建版本库" class="headerlink" title="创建版本库"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013743256916071d599b3aed534aaab22a0db6c4e07fd0000" target="_blank" rel="noopener">创建版本库</a></h1><p>版本库又名仓库，英文名<strong>repository</strong>，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。</p><ol><li><p>创建版本库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir file_name</div><div class="line">cd file_name</div><div class="line">pwd</div></pre></td></tr></table></figure><blockquote><p>pwd:显示当前目录</p></blockquote></li><li><p>通过<code>git init</code>命令把这个目录变成Git可以管理的仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git init</div></pre></td></tr></table></figure></li><li><p>添加文件到版本库</p><p>创建一个文件<code>readme.md</code>，放置与版本库下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git add readme.md</div><div class="line">git commit -m 'wrote a readme file'</div></pre></td></tr></table></figure><blockquote><p>简单解释一下<code>git commit</code>命令，<code>-m</code>后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。</p></blockquote><p>添加大量文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git add .</div></pre></td></tr></table></figure><p>添加当前目录下的所有文件和子目录</p></li><li><p>查看历史不同</p><p>修改<code>readme.md</code>内容，使用<code>git status</code>命令查看，命令输出告诉我们，<code>readme.txt</code>被修改过了，但还没有准备提交的修改。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git diff readme.md</div></pre></td></tr></table></figure><p>该命令显示修改前后的对比</p></li></ol><h1 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013744142037508cf42e51debf49668810645e02887691000" target="_blank" rel="noopener">版本回退</a></h1><ol><li><p>查看历史提交记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git log</div><div class="line">git log --pretty=oneline //简化输出信息</div></pre></td></tr></table></figure></li><li><p>回退</p><p>首先，Git必须知道当前版本是哪个版本，在Git中，用<code>HEAD</code>表示当前版本，上一个版本就是<code>HEAD^</code>，上上一个版本就是<code>HEAD^^</code>，当然往上100个版本写100个<code>^</code>比较容易数不过来，所以写成<code>HEAD~100</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git reset --hard HEAD^</div></pre></td></tr></table></figure></li></ol><h1 id="工作区和暂存区"><a href="#工作区和暂存区" class="headerlink" title="工作区和暂存区"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013745374151782eb658c5a5ca454eaa451661275886c6000" target="_blank" rel="noopener">工作区和暂存区</a></h1><p><strong>工作区</strong>：本地文件夹</p><p><strong>版本库</strong>：工作区有一个隐藏目录<code>.git</code>，这个不算工作区，而是Git的版本库。</p><p>Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支<code>master</code>，以及指向<code>master</code>的一个指针叫<code>HEAD</code>。</p><p><img src="/2018/07/12/Git/Screen Shot 2018-07-13 at 6.16.48 PM.png" alt="Screen Shot 2018-07-13 at 6.16.48 PM"></p><p><img src="/2018/07/12/Git/Screen Shot 2018-07-13 at 6.16.56 PM.png" alt="Screen Shot 2018-07-13 at 6.16.56 PM"></p><h1 id="管理修改"><a href="#管理修改" class="headerlink" title="管理修改"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374829472990293f16b45df14f35b94b3e8a026220c5000" target="_blank" rel="noopener">管理修改</a></h1><p>Git跟踪并管理的是修改，而非文件。</p><p>你会问，什么是修改？比如你新增了一行，这就是一个修改，删除了一行，也是一个修改，更改了某些字符，也是一个修改，删了一些又加了一些，也是一个修改，甚至创建一个新文件，也算一个修改。</p><h1 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374831943254ee90db11b13d4ba9a73b9047f4fb968d000" target="_blank" rel="noopener">撤销修改</a></h1><p><strong>丢掉工作区的修改</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git checkout -- file_name</div></pre></td></tr></table></figure><p>命令<code>git checkout -- readme.txt</code>意思就是，把<code>readme.txt</code>文件在工作区的修改全部撤销，这里有两种情况：</p><p>一种是<code>readme.txt</code>自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；</p><p>一种是<code>readme.txt</code>已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。</p><p>总之，就是让这个文件回到最近一次<code>git commit</code>或<code>git add</code>时的状态。</p><p> <strong>丢掉暂存区的修改</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git reset HEAD file_name</div></pre></td></tr></table></figure><p><code>git reset</code>命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用<code>HEAD</code>时，表示最新的版本。</p><h1 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758392816224cafd33c44b4451887cc941e6716805c000" target="_blank" rel="noopener">删除文件</a></h1><p>删除工作区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rm file_name</div></pre></td></tr></table></figure><p>这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，<code>git status</code>命令会立刻告诉你哪些文件被删除了</p><p>现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令<code>git rm</code>删掉，并且<code>git commit</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git rm file_name</div><div class="line">git commit -m "xxx"</div></pre></td></tr></table></figure><p>另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> git checkout -- test.txt</div></pre></td></tr></table></figure><p><code>git checkout</code>其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。</p><h1 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374385852170d9c7adf13c30429b9660d0eb689dd43a000" target="_blank" rel="noopener">远程仓库</a></h1><h2 id="添加到远程仓库"><a href="#添加到远程仓库" class="headerlink" title="添加到远程仓库"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013752340242354807e192f02a44359908df8a5643103a000" target="_blank" rel="noopener">添加到远程仓库</a></h2><p>Git是分布式版本控制系统，同一个Git仓库，可以分布到不同的机器上。怎么分布呢？最早，肯定只有一台机器有一个原始版本库，此后，别的机器可以“克隆”这个原始版本库，而且每台机器的版本库其实都是一样的，并没有主次之分。</p><p>实际情况往往是这样，找一台电脑充当服务器的角色，每天24小时开机，其他每个人都从这个“服务器”仓库克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交。</p><p>完全可以自己搭建一台运行Git的服务器，不过现阶段，为了学Git先搭个服务器绝对是小题大作。好在这个世界上有个叫<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>的神奇的网站，从名字就可以看出，这个网站就是提供Git仓库托管服务的，所以，只要注册一个GitHub账号，就可以免费获得Git远程仓库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git remote add origin git@github.com:gzwq/learngit.git</div></pre></td></tr></table></figure><p>把上面的<code>gzwq</code>替换成你自己的GitHub账户名,添加后，远程库的名字就是<code>origin</code>，这是Git默认的叫法，也可以改成别的，但是<code>origin</code>这个名字一看就知道是远程库。</p><p>下一步，就可以把本地库的所有内容推送到远程库上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git push -u origin master</div></pre></td></tr></table></figure><p>把本地库的内容推送到远程，用<code>git push</code>命令，实际上是把当前分支<code>master</code>推送到远程。由于远程库是空的，我们第一次推送<code>master</code>分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的<code>master</code>分支，还会把本地的<code>master</code>分支和远程的<code>master</code>分支关联起来，在以后的推送或者拉取时就可以简化命令。</p><p>从现在起，只要本地作了提交，就可以通过命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> git push origin master</div></pre></td></tr></table></figure><p>把本地<code>master</code>分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！</p><h2 id="从远程仓库克隆"><a href="#从远程仓库克隆" class="headerlink" title="从远程仓库克隆"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375233990231ac8cf32ef1b24887a5209f83e01cb94b000" target="_blank" rel="noopener">从远程仓库克隆</a></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone xxxxx</div></pre></td></tr></table></figure><h1 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013743862006503a1c5bf5a783434581661a3cc2084efa000" target="_blank" rel="noopener">分支管理</a></h1><p><img src="/2018/07/12/Git/Screen Shot 2018-07-16 at 12.22.45 AM.png" alt="Screen Shot 2018-07-16 at 12.22.45 AM"></p><h2 id="分支的创建和合并"><a href="#分支的创建和合并" class="headerlink" title="分支的创建和合并"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840038939c291467cc7c747b1810aab2fb8863508000" target="_blank" rel="noopener">分支的创建和合并</a></h2><p><strong>分支创建</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git checkout -b dev</div></pre></td></tr></table></figure><p><code>git checkout</code>命令加上<code>-b</code>参数表示创建并切换，相当于以下两条命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> git branch dev</div><div class="line"><span class="meta">$</span> git checkout dev</div></pre></td></tr></table></figure><p>然后，用<code>git branch</code>命令查看当前分支：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> git branch</div><div class="line">* dev</div><div class="line">  master</div></pre></td></tr></table></figure><p><code>git branch</code>命令会列出所有分支，当前分支前面会标一个<code>*</code>号。</p><p><strong>分支合并</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git merge dev</div></pre></td></tr></table></figure><p><strong>分支删除</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git branch -d dev</div></pre></td></tr></table></figure><h2 id="解决冲突"><a href="#解决冲突" class="headerlink" title="解决冲突"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001375840202368c74be33fbd884e71b570f2cc3c0d1dcf000" target="_blank" rel="noopener">解决冲突</a></h2><p>手动解决。</p><h2 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0013758410364457b9e3d821f4244beb0fd69c61a185ae0000" target="_blank" rel="noopener">分支管理策略</a></h2><p>通常，合并分支时，如果可能，Git会用<code>Fast forward</code>模式，但这种模式下，删除分支后，会丢掉分支信息。</p><p>如果要强制禁用<code>Fast forward</code>模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。</p>]]></content>
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ML-Sth</title>
      <link href="/2018/07/10/ML-Gradient-Descent/"/>
      <url>/2018/07/10/ML-Gradient-Descent/</url>
      <content type="html"><![CDATA[<p>adaptive moment estimation </p><a id="more"></a><p><strong>Hyperparameter</strong></p><p>It cannot be learnt from the training dataset.</p><p><strong>Validation set</strong></p><p>While test dataset is used to estimate the generalization error of a model, validation dataset is used to choose the model, i.e., determining hyperparameters. It is constructed from the training dataset.</p><p><strong>Variance and Standard Variance</strong></p><p>The variance or standard error, of an estimator provides a measure of how we would expect the estimator we compute from data to vary as we independently resample the dataset from the underlying data-generating process.</p><p><strong>Bias</strong></p><script type="math/tex; mode=display">bias(\hat \theta_m)=E(\hat \theta_m)-\theta</script><p>where the expectation is over the data (seen as samples from a random variable) and $\hat \theta$ is the true underlying value of $\theta$ used to deﬁne the data-generating distribution.</p><p>Bias and variance measure two diﬀerent sources of error in an estimator. Bias measures the expected deviation from the true value of the function or parameter. Variance on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause.</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Crowd Counting</title>
      <link href="/2018/07/04/DP-Crowd-Counting/"/>
      <url>/2018/07/04/DP-Crowd-Counting/</url>
      <content type="html"><![CDATA[<h1 id="Q"><a href="#Q" class="headerlink" title="Q"></a>Q</h1><ol><li><p>About perspective normalization, the closer the people is to the camera, the bigger he is. So the weight should be small, which is consistent to the <a href="http://www.svcl.ucsd.edu/publications/journal/2008/pami/pami08-dytexmix.pdf" target="_blank" rel="noopener">paper</a>. While in <a href="http://www.ee.cuhk.edu.hk/~xgwang/papers/zhangLWYcvpr15.pdf" target="_blank" rel="noopener">paper</a>, the closer people is, the hotter the color, meaning bigger weight of people close to camera.</p></li><li><p>down-sample the training pics by $\frac{1}{4}$ before training, it wouldn’t change the density? </p><p>delta function to model the image with $N$ heads?</p></li><li></li></ol><p>CNN based</p><p><a href="https://github.com/gjy3035/Awesome-Crowd-Counting" target="_blank" rel="noopener">All kinds of papers and code</a></p><a id="more"></a><p><a href="http://www.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf" target="_blank" rel="noopener">2D Gaussian Kernel</a></p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-05 at 10.47.30 AM.png" alt="Screen Shot 2018-07-05 at 10.47.30 AM"></p><blockquote><ol><li>The $\sigma$ determines the width of the Gaussian kernel.</li></ol></blockquote><h1 id="2008-AB-Chan"><a href="#2008-AB-Chan" class="headerlink" title="[2008-AB Chan]"></a>[2008-AB Chan]</h1><p><a href="https://dsp.stackexchange.com/questions/37245/python-how-to-calculate-image-perimeter-orientation-histogram" target="_blank" rel="noopener">[chapter3.1 | 3.3 understanding ]</a></p><h2 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h2><p>指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取最重要的一个特性是“可重复性”：同一场景的不同图像所提取的特征应该是相同的。 </p><p>特征提取是图象处理中的一个初级运算，也就是说它是对一个图像进行的第一个运算处理。它检查每个像素来确定该像素是否代表一个特征。</p><h3 id="Segment-Features"><a href="#Segment-Features" class="headerlink" title="Segment Features"></a>Segment Features</h3><h4 id="Perimeter"><a href="#Perimeter" class="headerlink" title="Perimeter"></a>Perimeter</h4><p><a href="https://dsp.stackexchange.com/questions/37245/python-how-to-calculate-image-perimeter-orientation-histogram" target="_blank" rel="noopener">[perimeter finding]</a> : </p><p><strong>definition</strong> : total number of pixels on the segment perimeter, computed with morphological operators.</p><h4 id="Perimeter-Edge-Orientation"><a href="#Perimeter-Edge-Orientation" class="headerlink" title="Perimeter Edge Orientation"></a>Perimeter Edge Orientation</h4><p><strong>definition</strong> : for pixels in the edge, applying a set of <a href="https://www.swarthmore.edu/NatSci/mzucker1/e27_s2016/filter-slides.pdf" target="_blank" rel="noopener">orientation gaussian filtering</a> to the pixel and maximum output is considered as the orientation of that pixel, where orientation histagram is is a histogram of a set of integer numbers in the range [0…5].</p><h3 id="Edge-Features"><a href="#Edge-Features" class="headerlink" title="Edge Features"></a><a href="https://blog.csdn.net/icamera0/article/details/50521442" target="_blank" rel="noopener">Edge Features</a></h3><p>不同图像灰度不同，边界处一般会有明显的边缘，利用此特征可以分割图像。需要说明的是：边缘和物体间的边界并不等同，<strong>边缘指的是图像中像素的值有突变的地方</strong>，而物体间的边界指的是现实场景中的存在于物体之间的边界。</p><p>在实际的图像分割中，往往只用到一阶和二阶导数，虽然，原理上，可以用更高阶的导数，但是，因为噪声的影响，在纯粹二阶的导数操作中就会出现对噪声的敏感现象，三阶以上的导数信息往往失去了应用价值。二阶导数还可以说明灰度突变的类型。在有些情况下，如灰度变化均匀的图像，只利用一阶导数可能找不到边界，此时二阶导数就能提供很有用的信息。二阶导数对噪声也比较敏感，解决的方法是先对图像进行平滑滤波，消除部分噪声，再进行边缘检测。不过，利用二阶导数信息的算法是基于过零检测的，因此得到的边缘点数比较少，有利于后继的处理和识别工作。</p><h4 id="Minkowski-Dimension"><a href="#Minkowski-Dimension" class="headerlink" title="Minkowski Dimension"></a><a href="https://brilliant.org/wiki/minkowski-dimension/" target="_blank" rel="noopener">Minkowski Dimension</a></h4><h3 id="Texture-Features"><a href="#Texture-Features" class="headerlink" title="Texture Features"></a><a href="https://blog.csdn.net/ajianyingxiaoqinghan/article/details/71482296" target="_blank" rel="noopener">Texture Features</a></h3><p>纹理是一种反映图像中同质现象的视觉特征，它体现了物体表面的具有缓慢变化或者周期性变化的表面结构组织排列属性。纹理具有三大标志：</p><ul><li>某种局部序列性不断重复；</li><li>非随机排列；</li><li>纹理区域内大致为均匀的统一体；</li></ul><h4 id="GLCM-For-Texture-Abstracting"><a href="#GLCM-For-Texture-Abstracting" class="headerlink" title="GLCM For Texture Abstracting"></a><a href="https://blog.csdn.net/carson2005/article/details/38442533" target="_blank" rel="noopener">GLCM For Texture Abstracting</a></h4><p><strong>定义</strong>：灰度共生矩阵是图像中相距为D的两个灰度像素同时出现的联合概率分布。</p><p><strong>意义</strong>：共生矩阵方法用条件概率来反映文理，是相邻像素的灰度相关性的表现。</p><p>假设灰度集合$\{0,1,2,3\}$, 那么共生矩阵大小是$4\times 4$, 对于$\forall i,j\in{0,1,2,3} $ 每个矩阵entry值是$f(i,j|d,\theta)$ , $d$和$\theta$给定。<a href="https://blog.csdn.net/guanyuqiu/article/details/53117507" target="_blank" rel="noopener">[实现方法]</a> : </p><p>在计算得到共生矩阵之后，往往不是直接应用计算的灰度共生矩阵，而是在此基础上计算纹理特征量，我们经常用反差、能量、熵、相关性等特征量来表示纹理特征。</p><blockquote><p>能量：是灰度共生矩阵各元素值的平方和，是对图像纹理的灰度变化稳定程度的度量，反应了图像灰度分布均匀程度和纹理粗细度。能量值大表明当前纹理是一种规则变化较为稳定的纹理。     </p><p>熵：是图像包含信息量的随机性度量。当共生矩阵中所有值均相等或者像素值表现出最大的随机性时，熵最大；因此熵值表明了图像灰度分布的复杂程度，熵值越大，图像越复杂。           </p><p>相关性：也称为同质性，用来度量图像的灰度级在行或列方向上的相似程度，因此值的大小反应了局部灰度相关性，值越大，相关性也越大。</p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-08 at 12.17.26 AM.png" alt="Screen Shot 2018-07-08 at 12.17.26 AM"></p></blockquote><h1 id="2016-Yingying-Zhang"><a href="#2016-Yingying-Zhang" class="headerlink" title="[2016-Yingying Zhang]"></a>[2016-Yingying Zhang]</h1><p><a href="https://blog.csdn.net/u011285477/article/details/51954989" target="_blank" rel="noopener">[PaperUsding1]</a></p><p><a href="https://www.cnblogs.com/wmr95/p/8134692.html" target="_blank" rel="noopener">Multi-Papers</a></p><p><a href="https://www.paperweekly.site/papers/notes/268" target="_blank" rel="noopener">[PaperWeekly]</a> </p><h1 id="Knowledge"><a href="#Knowledge" class="headerlink" title="Knowledge"></a>Knowledge</h1><ol><li><p><a href="https://blog.csdn.net/ccblogger/article/details/72875497" target="_blank" rel="noopener">Downsampling an image</a></p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-09 at 1.01.17 AM.png" alt="Screen Shot 2018-07-09 at 1.01.17 AM"></p></li><li><p>FPS 每秒传输帧数(Frames Per Second) </p><p>FPS是图像领域中的定义，是指画面每秒传输帧数，通俗来讲就是指动画或视频的画面数, 电影以每秒24张画面的速度播放，也就是一秒钟内在屏幕上连续投射出24张静止画面,那么我们就说电影是24fps.</p></li><li><p><a href="http://www.svcl.ucsd.edu/publications/journal/2008/pami/pami08-dytexmix.pdf" target="_blank" rel="noopener">A dynamic texture</a></p><p>A dynamic texture (DT) is the temporal extension of 2D texture, which is considered as a spatio-temporal generative model for video, which represents video sequences as observations from a linear dynamical system.</p></li></ol><p>   <strong>Definition</strong> : For a frame at time $t$, we have two variables $y_t$ and $x_t$, which encode the frame appearance component and the evolution of the video over time respectively.</p><p>   <img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-08 at 10.01.38 AM.png" alt="Screen Shot 2018-07-08 at 10.01.38 AM"></p><ol><li><p>Pixel</p><p>图像是由的小方格即所谓的像素(pixel)组成的，这些小方块都有一个明确的位置和被分配的色彩数值，像素是整个图像中不可分割的单位或者是元素。e.g. 图片分辨率为72，即每英寸像素为72，1英寸等于2.54厘米，那么通过换算可以得出每厘米等于28像素(72 / 2.54)；又如15x15厘米长度的图片，等于420*420像素的长度。</p></li><li><p><a href="https://baike.baidu.com/item/%E7%81%B0%E5%BA%A6%E5%80%BC/10259111" target="_blank" rel="noopener">Grey-Scale Value</a></p><p>把白色与黑色之间按对数关系分成若干级，称为“灰度等级”。范围一般从0到255，白色为255，黑色为0，故黑白图片也称灰度图像。</p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-08 at 9.38.43 AM.png" alt="Screen Shot 2018-07-08 at 9.38.43 AM"></p></li><li><p>Gaussian Filtering (高斯滤波)</p><p><a href="https://blog.csdn.net/L_inYi/article/details/8915116" target="_blank" rel="noopener">ref1</a> <a href="https://blog.csdn.net/sunmc1204953974/article/details/50634652" target="_blank" rel="noopener">ref2</a> </p><blockquote><p>高斯滤波在图像处理概念下，将图像频域处理和时域处理相联系，作为低通滤波器使用，可以将低频能量（比如噪声）滤去，起到图像平滑作用。</p><p>高斯滤波是一种线性平滑滤波，适用于消除高斯噪声，广泛应用于图像处理的减噪过程。<strong>通俗的讲，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到</strong>。高斯滤波的具体操作是：<strong>用一个模板（或称卷积、掩模）扫描图像中的每一个像素，用模板确定的邻域内像素的加权平均灰度值去替代模板中心像素点的值用。</strong>高斯平滑滤波器对于抑制服从正态分布的噪声非常有效。</p></blockquote></li><li><p><a href="https://www.swarthmore.edu/NatSci/mzucker1/e27_s2016/filter-slides.pdf" target="_blank" rel="noopener">Orientation Gaussian Filtering</a></p><blockquote><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-07 at 4.15.43 PM.png" alt="Screen Shot 2018-07-07 at 4.15.43 PM"></p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-07 at 11.08.24 PM.png" alt="Screen Shot 2018-07-07 at 11.08.24 PM"></p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-07 at 11.08.31 PM.png" alt="Screen Shot 2018-07-07 at 11.08.31 PM"></p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-07 at 11.08.38 PM.png" alt="Screen Shot 2018-07-07 at 11.08.38 PM"></p></blockquote></li></ol><ol><li><p><a href="http://homepages.inf.ed.ac.uk/rbf/BOOKS/BANDB/LIB/bandb2_2.pdf" target="_blank" rel="noopener">image modeling</a></p></li><li><p>Multivariate Gaussian Distributions</p><p><a href="https://blog.csdn.net/wjheha/article/details/78832416" target="_blank" rel="noopener">Blog1</a> <a href="https://www.youtube.com/watch?v=eho8xH3E6mE" target="_blank" rel="noopener">YouTube</a></p><p>$n$元正态分布中的协方差矩阵$Cov(X)$:</p><p><img src="/2018/07/04/DP-Crowd-Counting/Screen Shot 2018-07-08 at 11.50.02 AM.png" alt="Screen Shot 2018-07-08 at 11.50.02 AM"></p><blockquote><p>For one variate gaussian distribution, the variance is $Var(x)=E[(x-E(x))(x-E(x))]$, so for the multivariate, the variacne is $Var(x_1,x_2)=E[(x_1-E(x_1))(x_2-E(x_2))]$</p></blockquote><p><img src="/2018/07/04/DP-Crowd-Counting/20171218134527847.png" alt="20171218134527847"></p></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.robots.ox.ac.uk/~vgg/publications/2010/Lempitsky10b/lempitsky10b.pdf" target="_blank" rel="noopener">[2010-Victor Lempitsky]</a> Learning To Count Objects in Images </p><p><a href="http://www.di.ens.fr/willow/pdfscurrent/rodriguez11b.pdf" target="_blank" rel="noopener">[2011-Mikel Rodriguez]</a> Density-aware person detection and tracking in crowds</p><p><a href="https://www.robots.ox.ac.uk/~vgg/publications/2014/Arteta14/arteta14.pdf" target="_blank" rel="noopener">[2014-Carlos Arteta]</a> Interactive Object Counting</p><p><a href="https://arxiv.org/pdf/1705.10118.pdf" target="_blank" rel="noopener">[2018-Di Kang]</a> Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks - Counting, Detection, and Tracking</p><p><a href="http://www.svcl.ucsd.edu/publications/journal/2008/pami/pami08-dytexmix.pdf" target="_blank" rel="noopener">[2008-AB Chan]</a> Privacy Preserving Crowd Monitoring: Counting People without People Models or Tracking</p><p><a href="http://www.ee.cuhk.edu.hk/~xgwang/papers/zhangLWYcvpr15.pdf" target="_blank" rel="noopener">[2015- Cong Zhang]</a> Cross-scene Crowd Counting via Deep Convolutional Neural Networks</p><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780439" target="_blank" rel="noopener">[2016-Yingying Zhang]</a> Single-image crowd counting via multi-column convolutional neural network</p><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099912" target="_blank" rel="noopener">[2015-Chuan Wang]</a> Deep People Counting in Extremely Dense Crowds</p><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099912&amp;tag=1" target="_blank" rel="noopener">[2017-Deepak Babu Sam]</a> Switching Convolutional Neural Network for Crowd Counting</p><p><a href="http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf" target="_blank" rel="noopener">[2016-Daniel O˜noro-Rubio ]</a> Towards perspective-free object counting with deep learning</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Software Installation</title>
      <link href="/2018/07/04/Software-Installation/"/>
      <url>/2018/07/04/Software-Installation/</url>
      <content type="html"><![CDATA[<p>Instructions of various sofware installation.</p><a id="more"></a><h1 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> pip install tensorflow</div></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> python</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;</span>&gt;&gt; import tensorflow as tf</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;</span>&gt;&gt; hello = tf.constant('Hello, TensorFlow!')</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;</span>&gt;&gt; sess = tf.Session()</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;</span>&gt;&gt; print(sess.run(hello))</div></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Hello, TensorFlow!</div></pre></td></tr></table></figure><h2 id="Gpu"><a href="#Gpu" class="headerlink" title="Gpu"></a>Gpu</h2><p>基于tensorflow的keras中GPU的使用<a href="https://blog.csdn.net/batuwuhanpei/article/details/62217025" target="_blank" rel="noopener">ref</a> </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CUDA_VISIBLE_DEVICES="" python3 train.py</div></pre></td></tr></table></figure><h2 id="Dlib"><a href="#Dlib" class="headerlink" title="Dlib"></a>Dlib</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda install -c menpo dlib</div></pre></td></tr></table></figure><h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><p>基于Anaconda</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span> conda install pytorch torchvision -c soumith</div></pre></td></tr></table></figure><h1 id="Octave"><a href="#Octave" class="headerlink" title="Octave"></a>Octave</h1><p><a href="https://sourceforge.net/projects/octave/files/Octave%20MacOSX%20Binary/2013-12-30%20binary%20installer%20of%20Octave%203.8.0%20for%20OSX%2010.9.1%20%28beta%29/GNU_Octave_3.8.0-6.dmg/download" target="_blank" rel="noopener">download</a></p><p><a href="https://blog.csdn.net/weixin_37325825/article/details/73010883" target="_blank" rel="noopener">命令行安装</a></p><h1 id="Ffmpeg"><a href="#Ffmpeg" class="headerlink" title="Ffmpeg"></a>Ffmpeg</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">brew install ffmpeg</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">输入命令行ffmpeg -i src01.avi %d.jpg</div><div class="line">回车，即可自动输出1.jpg;2.jpg……</div></pre></td></tr></table></figure><h1 id="Matlab"><a href="#Matlab" class="headerlink" title="Matlab"></a>Matlab</h1><p><a href="https://www.jb51.net/softjc/578585.html" target="_blank" rel="noopener">[download and instruction]</a></p><h1 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h1><p>anaconda 2/3 切换</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">source activate /xx/xx/anaconda2</div><div class="line">conda info --envs 查看具体位置</div></pre></td></tr></table></figure><h1 id="移动硬盘"><a href="#移动硬盘" class="headerlink" title="移动硬盘"></a>移动硬盘</h1><p>用mac磁盘工具，插上移动硬盘，点击抹掉之后，格式选择exfat就可以了。</p><h1 id="sh"><a href="#sh" class="headerlink" title=".sh"></a>.sh</h1><p><img src="/2018/07/04/Software-Installation/Screen Shot 2019-04-06 at 12.00.31 PM.png" alt="creen Shot 2019-04-06 at 12.00.31 P"></p><h1 id="Colab读文件"><a href="#Colab读文件" class="headerlink" title="Colab读文件"></a>Colab读文件</h1><p><a href="https://blog.csdn.net/code_nie/article/details/85029074" target="_blank" rel="noopener">colab 读入 google drive 的文件</a></p><ol><li><p>首先需要让colab获得google drive的授权，在google colab里执行如下代码：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">!apt-get install -y -qq software-properties-common python-software-properties module-init-tools</div><div class="line">!add-apt-repository -y ppa:alessandro-strada/ppa 2&gt;&amp;1 &gt; /dev/null</div><div class="line">!apt-get update -qq 2&gt;&amp;1 &gt; /dev/null</div><div class="line">!apt-get -y install -qq google-drive-ocamlfuse fuse</div><div class="line">from google.colab import auth</div><div class="line">auth.authenticate_user()</div><div class="line">from oauth2client.client import GoogleCredentials</div><div class="line">creds = GoogleCredentials.get_application_default()</div><div class="line">import getpass</div><div class="line">!google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125; &lt; /dev/null 2&gt;&amp;1 | grep URL</div><div class="line">vcode = getpass.getpass()</div><div class="line">!<span class="built_in">echo</span> &#123;vcode&#125; | google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125;</div></pre></td></tr></table></figure><p>执行时会打印一个网址和一个授权码的输入框，此时你需要点击网址，登陆google account，授权colab使用你的google drive，然后系统会分配给你一个授权码，你需要将授权码粘贴到输入框里，回车继续。</p></li><li><p>关联Google drive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span><span class="bash"> 指定Google Drive云端硬盘的根目录，名为drive</span></div><div class="line">!mkdir -p drive</div><div class="line">!google-drive-ocamlfuse drive</div></pre></td></tr></table></figure><p>此时colab中出现drive的文件夹，里面就是你的google drive的根目录文件</p></li><li><p>更换工作目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">os.chdir("drive/Colab Notebooks")</div></pre></td></tr></table></figure></li></ol><h1 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h1><h2 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h2><p>The useful link <a href="https://ljvmiranda921.github.io/notebook/2018/01/31/running-a-jupyter-notebook/" target="_blank" rel="noopener">1</a> </p><p>Usually a remote server is equipped with GPUs. What we want is to run a notebook from server, but have the graphical interface show up in our local machine’s web browser. The libraries, hardware, and all backend-related stuff depends on your remote machine, but the GUI is seen from your laptop.</p><p><strong>Set-up:</strong> Here, let’s define the local user and host as <code>localuser</code> and <code>localhost</code> respectively. Similarly, let’s define the remote user and remote host as <code>remoteuser</code> and <code>remotehost</code>. Needless to say, make sure that <a href="http://jupyter.readthedocs.io/en/latest/install.html" target="_blank" rel="noopener">Jupyter notebook and all its dependencies</a> are installed in both machines. Here’s a quick diagram of the whole process, I will discuss them one-by-one in the next section:</p><p><img src="/2018/07/04/Software-Installation/Screen Shot 2020-01-22 at 5.56.01 PM-9738535.png" alt="creen Shot 2020-01-22 at 5.56.01 PM-973853"></p><ol><li><p>Run Jupyter Notebook from remote machine</p><p>Log-in to remote machine the usual way we do. Once the console shows, type the following:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">remoteuser@remotehost: jupyter notebook --no-browser --port=XXXX</div></pre></td></tr></table></figure><blockquote><p>Change XXXX to the port you want. Usually the default is 8888. You can try 8889 or 8890 as well.</p></blockquote></li><li><p>Forward port XXXX to YYYY and listen to it</p><p>In your remote, the notebook is now running at the port <code>XXXX</code> that you specified. What you’ll do next is forward this to port <code>YYYY</code> <em>of your machine</em> so that you can listen and run it from your browser. To achieve this, we write the following command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh -N -f -L localhost:YYYY:localhost:XXXX remoteuser@remotehost</div></pre></td></tr></table></figure><blockquote><p>ssh -N -f -L localhost:8890:localhost:8888 qwang35@mars.ece.lsu.edu</p></blockquote></li><li><p>Fire up Jupyter Notebook</p><p>To open up the Jupyter notebook from your remote machine, simply start your browser and type the following in your address bar:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">localhost:YYYY</div></pre></td></tr></table></figure></li></ol><p>In your first connection, you may be prompted to enter an Access Token as typical to most Jupyter notebooks. Normally, I’d just copy-paste it from my terminal, but to make things easier for you, you can <a href="http://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup" target="_blank" rel="noopener">set-up your own notebook password</a>.</p><p><img src="/2018/07/04/Software-Installation/Screen Shot 2020-01-22 at 6.01.51 PM-9738527.png" alt="creen Shot 2020-01-22 at 6.01.51 PM-973852"></p>]]></content>
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Installation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-CNN</title>
      <link href="/2018/07/04/DP-CNN/"/>
      <url>/2018/07/04/DP-CNN/</url>
      <content type="html"><![CDATA[<h1 id="Q"><a href="#Q" class="headerlink" title="Q"></a>Q</h1><ul><li><p><strong>比如S2 -&gt; C3，是不是C3的每一个感受野要感受S2的6张map？</strong></p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-08 at 9.29.19 PM.png" alt="Screen Shot 2018-07-08 at 9.29.19 PM"></p><p>事实是：每一个卷积核要感受前一层的所有feature_maps</p></li></ul><p><strong><a href="http://www.jeyzhang.com/cnn-learning-notes-1.html" target="_blank" rel="noopener">卷积神经网络(Convolutional Neural Network, CNN)</a></strong>是深度学习技术中极具代表的网络结构之一, CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。</p><p>图像处理中，往往会将图像看成是一个或多个的二维向量，比如MNIST手写体图片就可以看做是一个28 × 28的二维向量（黑白图片，只有一个颜色通道；如果是RGB表示的彩色图片则有三个颜色通道，可表示为三张二维向量）。传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而CNN则通过<strong>局部连接</strong>、<strong>权值共享</strong>等方法避免这一困难。</p><ul><li><p><strong>Why using the patches of images as input instead of the whole images?</strong></p><p>I saw this phenomenon when I read papers about crowd counting with CNN, like <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Sam_Switching_Convolutional_Neural_CVPR_2017_paper.pdf" target="_blank" rel="noopener">S-CNN</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780439" target="_blank" rel="noopener">MCNN</a>. And according to the paper S-CNN, the complete image is divided in 9 non-overlapping patches so that crowd characteristics like density, appearance etc. can be assumed to be consistent in a given patch for a crowd scene.</p></li><li><p><strong>What is fine-tune of CNN model?</strong> fine tuning</p></li><li><p>如何理解“卷积运算，可以使原信号特征增强，并且降低噪音”？？</p></li><li><p>为什么神经网络可以提取特征？？</p></li><li><p>模型的鲁棒性？？？？</p></li></ul><h1 id="A"><a href="#A" class="headerlink" title="A"></a>A</h1><ol><li><p>In machine learning terms, this flashlight is called a <strong>filter</strong>(or sometimes referred to as a <strong>neuron</strong> or a <strong>kernel</strong>) and the region that it is shining over is called the <strong>receptive field</strong>.</p></li><li><p>A very important note is that the depth of this filter has to be the same as the depth of the input (this makes sure that the math works out), so the dimensions of this filter is 5 x 5 x 3.</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-03-02 at 5.14.02 PM.png" alt="creen Shot 2019-03-02 at 5.14.02 P"></p></li><li><p>For an input image with size $N \times N \times C$, after going through a filter $5 \times 5$, we have an output feature map with size $(N-5+1) \times (N-5+1) \times 1$.</p><p>output_size = (Height-Filter)/stride+1.</p></li><li><p>weight number</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-03-02 at 5.25.55 PM.png" alt="creen Shot 2019-03-02 at 5.25.55 P"></p></li><li><p>$1 \times 1$ filter size</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-03-02 at 5.28.39 PM.png" alt="creen Shot 2019-03-02 at 5.28.39 P"></p></li><li><p>​</p></li></ol><a id="more"></a><h1 id="CNN的引入"><a href="#CNN的引入" class="headerlink" title="CNN的引入"></a>CNN的引入</h1><p><a href="https://www.leiphone.com/news/201807/RQ4sBWYqLkGV4ZAW.html" target="_blank" rel="noopener">直观理解深度学习卷积部分</a> </p><p><a href="https://blog.csdn.net/cxmscb/article/details/71023576" target="_blank" rel="noopener">ThumbsUp</a></p><p>在人工的全连接神经网络中，每相邻两层之间的每个神经元之间都是有边相连的。当输入层的特征维度变得很高时，这时全连接网络需要训练的参数就会增大很多，计算速度就会变得很慢，例如一张黑白的 28×28 的手写数字图片，输入层的神经元就有784个，如下图所示：</p><p><img src="/2018/07/04/DP-CNN/20170430134321810.png" alt="20170430134321810"></p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 11.02.43 AM.png" alt="Screen Shot 2018-07-10 at 11.02.43 AM"></p><h1 id="CNN层次结构"><a href="#CNN层次结构" class="headerlink" title="CNN层次结构"></a>CNN层次结构</h1><h2 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h2><p>CNN的输入层的输入格式保留了图片本身的结构。</p><p>对于黑白的 28×2828×28 的图片，CNN的输入是一个 28×2828×28 的的二维神经元，如下图所示：</p><p><img src="/2018/07/04/DP-CNN/20170430143339630.png" alt="20170430143339630"></p><p>而对于RGB格式的$28\times 28$图片，CNN的输入则是一个$3\times 28\times 28$的三维神经元（RGB中的每一个颜色通道都有一个 $28\times 28$的矩阵）</p><p><img src="/2018/07/04/DP-CNN/20170430144413942.png" alt="20170430144413942"></p><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>两个重要的概念：</p><ul><li>local receptive fields（局部视野野）</li><li>shared weights（权值共享）</li></ul><h3 id="局部感受野"><a href="#局部感受野" class="headerlink" title="局部感受野"></a>局部感受野</h3><p>相比传统的每个神经元与前一层的所有神经元全连接，CNN中每个神经元只与前一层的部分神经元连接，俗称局部感受。如下图，假设输入的是一个 $28\times 28$的的二维神经元，我们定义$5\times 5$的 一个 local receptive fields（感受视野），即 隐藏层的神经元与输入层的$5\times 5$个神经元相连，这个5*5的区域就称之为Local Receptive Fields，每一条直线对应一个权重$w$。</p><p><img src="/2018/07/04/DP-CNN/20170430145326135.png" alt="20170430145326135"></p><p>每个神经元只与$5\times5$大小范围的神经元连接，如果步长是1，则两个神经元之间的感受区域有重叠，所以下一层共需要神经元个数是$(28\times 28) \div(5\times5)=(28-(5-1))\times(28-(5-1))=24\times24$</p><p><img src="/2018/07/04/DP-CNN/20170430145733209.png" alt="20170430145733209"></p><p>移动的步长为1：从左到右扫描，每次移动 1 格，扫描完之后，再向下移动一格，再次从左到右扫描。</p><p><img src="/2018/07/04/DP-CNN/20170430150003326.png" alt="20170430150003326"></p><p><img src="/2018/07/04/DP-CNN/20170430161936244.gif" alt="20170430161936244"></p><p><strong>过滤器-卷积核-Filters-权重$w$-(神经元局部感受的模板)</strong></p><p>一个感受野带有一个卷积核，将感受野对输入的扫描间隔称为步长(Stride)，当步长比较大时(stride&gt;1)，为了扫描到边缘的一些特征，感受视野可能会“出界”，这时需要对<strong>边界扩充(pad)</strong>，边界扩充可以设为 00 或 其他值。步长 和 边界扩充值的大小由用户来定义。</p><blockquote><p>比如前一层是$5\times5$, 感受野大小是$2\times2$, 步长2，可以发现最右的边界是无法扫描，所以可以对前一层进行padding，扩充成$6\times6$。</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 12.34.21 PM.png" alt="Screen Shot 2018-07-10 at 12.34.21 PM"></p></blockquote><p>卷积核的权重矩阵的值，便是卷积神经网络的参数，为了有一个偏移项 $b$，卷积核可附带一个偏移项 bb ，它们的初值可以随机来生成，可通过训练进行变化。</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 12.19.02 PM.png" alt="Screen Shot 2018-07-10 at 12.19.02 PM"></p><p>我们将通过 一个带有<strong>卷积核</strong>的<strong>感受视野</strong> 扫描生成的下一层神经元矩阵 称为 一个<strong>feature map (特征映射图)</strong>，如下图的右边便是一个 feature map：</p><p><img src="/2018/07/04/DP-CNN/20170430145733209-1243208.png" alt="20170430145733209-1243208"></p><p>因此在同一个 <strong>feature map</strong> 上的神经元使用的卷积核是相同的，因此这些神经元 shared weights，共享卷积核中的权值和附带的偏移。一个 feature map 对应 一个卷积核，若我们使用 3 个不同的卷积核，可以输出3个feature map：（感受视野：5×5，布长stride：1）</p><p><img src="/2018/07/04/DP-CNN/20170430160824104.png" alt="20170430160824104"></p><p>因此在CNN的卷积层，我们需要训练的参数大大地减少到了 (5×5+1)×3=78个。</p><p>假设输入的是 $28\times28$的RGB图片，即输入的是一个 $3×28×28$的的二维神经元，这时卷积核的大小不只用长和宽来表示，还有深度，感受视野也对应的有了深度。如下图所示：</p><p><img src="/2018/07/04/DP-CNN/20170430170435760.png" alt="20170430170435760"></p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 12.35.21 PM.png" alt="Screen Shot 2018-07-10 at 12.35.21 PM"></p><blockquote><p>RGB图像在CNN中如何进行convolution? - 知乎 <a href="https://www.zhihu.com/question/46607672" target="_blank" rel="noopener">https://www.zhihu.com/question/46607672</a></p><p><a href="https://blog.csdn.net/u014114990/article/details/51125776" target="_blank" rel="noopener">https://blog.csdn.net/u014114990/article/details/51125776</a></p></blockquote><h2 id="池化层-下采样"><a href="#池化层-下采样" class="headerlink" title="池化层(下采样)"></a>池化层(下采样)</h2><p>当输入经过卷积层时，若感受视野比较小，布长stride比较小，得到的feature map （特征图）还是比较大，可以通过池化层来对每一个 feature map 进行降维操作，输出的深度还是不变的，依然为 feature map 的个数。</p><p>池化层也有一个“池化视野（filter）”来对feature map矩阵进行扫描，对“池化视野”中的矩阵值进行计算，一般有两种计算方式：</p><ul><li>Max pooling：取“池化视野”矩阵中的最大值</li><li>Average pooling：取“池化视野”矩阵中的平均值</li></ul><p>扫描的过程中同样地会涉及的扫描布长stride，扫描方式同卷积层一样，先从左到右扫描，结束则向下移动布长大小，再从左到右。如下图示例所示：</p><p><img src="/2018/07/04/DP-CNN/20170430172832308.png" alt="20170430172832308"></p><p>最后可将 3 个 $24\times24$ 的 feature map 下采样得到 3 个 $24\times24$ 的特征矩阵：</p><p><img src="/2018/07/04/DP-CNN/20170430204102339.png" alt="20170430204102339"></p><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>全连接层主要对特征进行重新拟合，减少特征信息的丢失。用户指定全连接层的神经元个数，然后每个神经元都与前一层的所有神经元连接，计算。</p><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><p>输出层主要准备做好最后目标结果的输出，用户指定神经元个数，如果是2分类问题，就设置两个神经元，每一个神经元输出某种分类的概率；十分类就10个神经元，属于全连接。</p><h2 id="归一化层-Batch-Normalization"><a href="#归一化层-Batch-Normalization" class="headerlink" title="归一化层(Batch Normalization)"></a>归一化层(Batch Normalization)</h2><p>实现了在神经网络层的中间进行预处理的操作，即在上一层的输入归一化处理后再进入网络的下一层，这样可有效地防止“梯度弥散”，加速网络训练。在卷积神经网络中进行批量归一化时，一般对 未进行ReLu激活的 feature map进行批量归一化，输出后再作为激励层的输入，可达到调整激励函数偏导的作用。</p><p><img src="/2018/07/04/DP-CNN/20170430173715000.png" alt="20170430173715000"></p><h2 id="近邻归一化-Local-Response-Normalization"><a href="#近邻归一化-Local-Response-Normalization" class="headerlink" title="近邻归一化(Local Response Normalization)"></a>近邻归一化(Local Response Normalization)</h2><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 3.29.15 PM.png" alt="Screen Shot 2018-07-10 at 3.29.15 PM"></p><h2 id="融合层"><a href="#融合层" class="headerlink" title="融合层"></a>融合层</h2><p>融合层可以对切分层进行融合，也可以对不同大小的卷积核学习到的特征进行融合。例如在GoogleLeNet 中，使用多种分辨率的卷积核对目标特征进行学习，通过 <strong>padding</strong> 使得每一个 <strong>feature map</strong> 的长宽都一致，之后再将多个 feature map 在深度上拼接在一起： </p><p>融合的方法有几种，一种是特征矩阵之间的拼接级联，另一种是在特征矩阵进行运算 (+,−,x,max,conv)(+,−,x,max,conv)。</p><p><a href="https://www.cnblogs.com/denny402/p/5071126.html" target="_blank" rel="noopener">parameters setting</a></p><p><a href="https://blog.csdn.net/u011276025/article/details/76050377" target="_blank" rel="noopener">图像归一化</a> </p><h1 id="CNN-in-NLP"><a href="#CNN-in-NLP" class="headerlink" title="CNN in NLP"></a>CNN in NLP</h1><p>Since CNNs, unlike RNNs, can output only fixed sized vectors, the natural fit for them seem to be in the classification tasks such as <strong>Sentiment Analysis, Spam Detection or Topic Categorization.</strong></p><p>In computer vision tasks, the filters used in CNNs slide over patches of an image whereas in NLP tasks, the filters slide over the sentence matrices, a few words at a time.</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Instead of image pixels, the input to most NLP tasks are sentences or documents represented as a matrix. Each row of the matrix corresponds to one token, typically a word, but it could be a character. That is, each row is vector that represents a word. Typically, these vectors are <em>word embeddings</em> (low-dimensional representations) like <a href="https://code.google.com/p/word2vec/" target="_blank" rel="noopener">word2vec</a> or <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a>, but they could also be one-hot vectors that index the word into a vocabulary. For a 10 word sentence using a 100-dimensional embedding we would have a 10×100 matrix as our input. That’s our “image”.</p><h2 id="Fliter"><a href="#Fliter" class="headerlink" title="Fliter"></a>Fliter</h2><p>In vision, our filters slide over local patches of an image, but in NLP we typically use filters that slide over full rows of the matrix (words). Thus, the “width” of our filters is usually the same as the width of the input matrix. The height, or <em>region size</em>, may vary, but sliding windows over 2-5 words at a time is typical.</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.17.54 AM.png" alt="creen Shot 2019-07-15 at 11.17.54 A"></p><h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>We also have zero-padding in nlp, where all elements that would fall outside of the matrix are taken to be zero. By doing this you can apply the filter to every element of your input matrix, and get a larger or equally sized output. Adding zero-padding is also called <strong><em>wide convolution**</em></strong>,<strong> and not using (zero-)padding would be a *</strong>narrow convolution<em>*</em>.</p><h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><p>The last concept we need to understand are <em>channels</em>. Channels are different “views” of your input data. For example, in image recognition you typically have RGB (red, green, blue) channels. You can apply convolutions across channels, either with different or equal weights. In NLP you could imagine having various channels as well: You could have a separate channels for different word embeddings (<a href="https://code.google.com/p/word2vec/" target="_blank" rel="noopener">word2vec</a> and <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> for example), or you could have a channel for the same sentence represented in different languages, or phrased in different ways.</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.20.42 AM.png" alt="creen Shot 2019-07-15 at 11.20.42 A"></p><h2 id="Stride-Size"><a href="#Stride-Size" class="headerlink" title="Stride Size"></a>Stride Size</h2><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.32.18 AM.png" alt="creen Shot 2019-07-15 at 11.32.18 A"></p><h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>Why pooling? There are a couple of reasons. One property of pooling is that it provides a fixed size output matrix, which typically is required for classification. For example, if you have 1,000 filters and you apply max pooling to each, you will get a 1000-dimensional output, regardless of the size of your filters, or the size of your input. This allows you to use variable size sentences, and variable size filters, but always get the same output dimensions to feed into a classifier.</p><p>Pooling also reduces the output dimensionality but (hopefully) keeps the most salient information. You can think of each filter as detecting a specific feature, such as detecting if the sentence contains a negation like “not amazing” for example. If this phrase occurs somewhere in the sentence, the result of applying the filter to that region will yield a large value, but a small value in other regions. By performing the max operation you  are keeping information about whether or not the feature appeared in the sentence, but you are losing information about where exactly it appeared. But isn’t this information about locality really useful? Yes, it  is and it’s a bit similar to what a bag of n-grams model is doing. You are losing global information about locality (where in a sentence something happens), but you are keeping local information captured by your filters, like “not amazing” being very different from “amazing not”.</p><p>In imagine recognition, pooling also provides basic invariance to translating (shifting) and rotation. When you are pooling over a region, the output will stay approximately the same even if you shift/rotate the image by a few pixels, because the max operations will pick out the same value regardless.</p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.21.19 AM.png" alt="creen Shot 2019-07-15 at 11.21.19 A"></p><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-15 at 11.21.39 AM.png" alt="creen Shot 2019-07-15 at 11.21.39 A"></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><a href="http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/" target="_blank" rel="noopener">link</a> </p><p><img src="/2018/07/04/DP-CNN/Screen-Shot-2015-11-06-at-12.05.40-PM.png" alt="creen-Shot-2015-11-06-at-12.05.40-P"></p><p>llustration of a Convolutional Neural Network (CNN) architecture for sentence classification. Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states. Source: Zhang, Y., &amp; Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification.</p><p><strong>Input</strong></p><p>The example is “I like this movie very much!”, there are 6 words here and the exclamation mark is treated like a word – some researchers do this differently and disregard the exclamation mark – in total there are 7 words in the sentence. The authors chose 5 to be the dimension of the word vectors. We let <em>s</em> denote the length of sentence and <em>d</em> denote the dimension of the word vector, hence we now have a sentence matrix of the shape <em>s</em> x <em>d</em>, or 7 x 5.</p><p><strong>Filters</strong></p><p>One of the desirable properties of CNN is that it preserves 2D spatial orientation in computer vision. Texts, like pictures, have an orientation. Instead of 2-dimensional, texts have a one-dimensional structure where words sequence matter. We also recall that all words in the example are each replaced by a 5-dimensional word vector, hence we fix one dimension of the filter to match the word vectors (5) and vary the region size, <em>h</em>. Region size refers to the number of rows – representing word – of the sentence matrix that would be filtered.</p><p>In the figure, #filters are the illustrations of the filters, not what has been filtered out from the sentence matrix by the filter, the next paragraph would make this distinction clearer. Here, the authors chose to use 6 filters – 2 complementary filters to consider (2,3,4) words.</p><p><strong>Featuremaps</strong></p><p>For filters with size being 4, the output featuremap size should be (7-4)/1+1=4. Similarly, we have output featuremaps of 5 and 6 respectively for filter size 3 and 2.</p><p><strong>Maxpooling</strong></p><p>For each featuremap, we perform 1-max pooling and extract the largest number.</p><p><strong>Softmax</strong></p><p>After 1-max pooling, we are certain to have a fixed-length vector of 6 elements ( = number of filters = number of filters per region size (2) x number of region size considered (3)). This fixed length vector can then be fed into a softmax (fully-connected) layer to perform the classification. The error from the classification is then back-propagated back into the following parameters as part of learning:</p><ul><li>The <strong>w</strong> matrices that produced <strong>o</strong></li><li>The bias term that is added to <strong>o</strong> to produce <strong>c</strong></li><li>Word vectors (optional, use validation performance to decide)</li></ul><h2 id="CNN-Applications-in-NLP"><a href="#CNN-Applications-in-NLP" class="headerlink" title="CNN Applications in NLP"></a>CNN Applications in NLP</h2><p>The most natural fit for CNNs seem to be classifications tasks, such as Sentiment Analysis, Spam Detection or Topic Categorization. Convolutions and pooling operations lose information about the local order of words, so that sequence tagging as in PoS Tagging or Entity Extraction is a bit harder to fit into a pure CNN architecture (though not impossible, you can add positional features to the input).</p><h2 id="Pytorch-Implementation"><a href="#Pytorch-Implementation" class="headerlink" title="Pytorch Implementation"></a>Pytorch Implementation</h2><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p><a href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/" target="_blank" rel="noopener">link</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</div><div class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> spacy</div><div class="line"></div><div class="line"><span class="comment"># https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb</span></div><div class="line"></div><div class="line">SEED = <span class="number">1234</span></div><div class="line"></div><div class="line">torch.manual_seed(SEED)</div><div class="line">torch.backends.cudnn.deterministic = <span class="keyword">True</span></div><div class="line"></div><div class="line">spacy_en = spacy.load(<span class="string">'en'</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span><span class="params">(text)</span>:</span> <span class="comment"># create a tokenizer function</span></div><div class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</div><div class="line"></div><div class="line">TEXT = data.Field(tokenize = tokenizer)</div><div class="line">LABEL = data.LabelField(dtype = torch.float)</div><div class="line"></div><div class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</div><div class="line"></div><div class="line">train_data, valid_data = train_data.split(random_state = random.seed(SEED))</div><div class="line"></div><div class="line">MAX_VOCAB_SIZE = <span class="number">25</span>_000</div><div class="line"></div><div class="line">TEXT.build_vocab(train_data,</div><div class="line">                 max_size = MAX_VOCAB_SIZE,</div><div class="line">                 vectors = <span class="string">"glove.6B.100d"</span>,</div><div class="line">                 unk_init = torch.Tensor.normal_)</div><div class="line"></div><div class="line">LABEL.build_vocab(train_data)</div><div class="line">BATCH_SIZE = <span class="number">64</span></div><div class="line"></div><div class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</div><div class="line"></div><div class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</div><div class="line">    (train_data, valid_data, test_data),</div><div class="line">    batch_size = BATCH_SIZE,</div><div class="line">    device = device)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></div><div class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></div><div class="line"><span class="string">    """</span></div><div class="line">    <span class="comment">#round predictions to the closest integer</span></div><div class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</div><div class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division</span></div><div class="line">    acc = correct.sum() / len(correct)</div><div class="line">    <span class="keyword">return</span> acc</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></div><div class="line">    epoch_loss = <span class="number">0</span></div><div class="line">    epoch_acc = <span class="number">0</span></div><div class="line">    model.train()</div><div class="line"></div><div class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</div><div class="line">        optimizer.zero_grad()</div><div class="line"></div><div class="line">        batch_ = batch.text <span class="comment">#torch.Size([1225, 64])</span></div><div class="line"></div><div class="line">        predictions = model(batch_).squeeze(<span class="number">1</span>)</div><div class="line"></div><div class="line">        loss = criterion(predictions, batch.label)</div><div class="line"></div><div class="line">        acc = binary_accuracy(predictions, batch.label)</div><div class="line"></div><div class="line">        loss.backward()</div><div class="line"></div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line">        epoch_loss += loss.item()</div><div class="line">        epoch_acc += acc.item()</div><div class="line"></div><div class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx)</span>:</span></div><div class="line">        super(TextCNN,self).__init__()</div><div class="line">        self.embedding = nn.Embedding(vocab_size,embedding_dim,pad_idx)</div><div class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>,out_channels=n_filters,kernel_size=(filter_sizes[<span class="number">0</span>],embedding_dim))</div><div class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">1</span>,out_channels=n_filters,kernel_size=(filter_sizes[<span class="number">1</span>],embedding_dim))</div><div class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">1</span>,out_channels=n_filters,kernel_size=(filter_sizes[<span class="number">2</span>],embedding_dim))</div><div class="line">        self.fc = nn.Linear(n_filters*len(filter_sizes),output_dim)</div><div class="line">        self.dropout = nn.Dropout(p=dropout)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></div><div class="line">        <span class="comment">#text.shape = (sent_length,batch_size), sent_length is varing</span></div><div class="line">        text = torch.transpose(text, <span class="number">0</span>, <span class="number">1</span>) <span class="comment">#(batch_size,sent_length)=(64,1225)</span></div><div class="line">        text_embedding = self.embedding(text) <span class="comment"># shape = (batch_size,sent_length,embedding_size)=(64,1225,100)</span></div><div class="line">        </div><div class="line">        <span class="comment"># since we use conv2d, the input should be (batch,channel,height,width)</span></div><div class="line">        text_embedding = torch.unsqueeze(text_embedding,dim=<span class="number">1</span>)</div><div class="line">        </div><div class="line">        <span class="comment">#kernel_size is (?,embedding_dim), so output is (batch,filter_num,??,1)</span></div><div class="line">        conv1 = F.relu(self.conv1(text_embedding)) <span class="comment">#Size([64, 100, 1223, 1])</span></div><div class="line">        conv2 = F.relu(self.conv2(text_embedding)) <span class="comment">#Size([64, 100, 1222, 1])</span></div><div class="line">        conv3 = F.relu(self.conv3(text_embedding)) <span class="comment">#Size([64, 100, 1221, 1])</span></div><div class="line"></div><div class="line">        pooling1 = F.max_pool1d(conv1.squeeze(),conv1.size(<span class="number">2</span>))</div><div class="line">        pooling2 = F.max_pool1d(conv2.squeeze(),conv2.size(<span class="number">2</span>))</div><div class="line">        pooling3 = F.max_pool1d(conv3.squeeze(),conv3.size(<span class="number">2</span>))</div><div class="line"></div><div class="line">        cat = torch.cat((pooling1,pooling2,pooling3),dim=<span class="number">1</span>).squeeze() <span class="comment">#Size([64,300,1,1]-&gt;[64,300])</span></div><div class="line">        dropout = self.dropout(cat)</div><div class="line">        output = self.fc(dropout) <span class="comment">#fully-connected layers can only be (batch_size,dim)</span></div><div class="line">        <span class="keyword">return</span> output</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    INPUT_DIM = len(TEXT.vocab) <span class="comment">#25002</span></div><div class="line">    EMBEDDING_DIM = <span class="number">100</span></div><div class="line">    N_FILTERS = <span class="number">100</span></div><div class="line">    FILTER_SIZES = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</div><div class="line">    OUTPUT_DIM = <span class="number">1</span></div><div class="line">    DROPOUT = <span class="number">0.5</span></div><div class="line">    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</div><div class="line">    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</div><div class="line">    print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</div><div class="line">    pretrained_embeddings = TEXT.vocab.vectors</div><div class="line">    model.embedding.weight.data.copy_(pretrained_embeddings)</div><div class="line">    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</div><div class="line">    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</div><div class="line">    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</div><div class="line">    optimizer = optim.Adam(model.parameters())</div><div class="line">    criterion = nn.BCEWithLogitsLoss()</div><div class="line">    model = model.to(device)</div><div class="line">    criterion = criterion.to(device)</div><div class="line">    N_EPOCHS = <span class="number">5</span></div><div class="line">    best_valid_loss = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</div><div class="line">        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</div><div class="line">        print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</div></pre></td></tr></table></figure><p><img src="/2018/07/04/DP-CNN/Screen Shot 2019-07-16 at 9.20.27 PM.png" alt="creen Shot 2019-07-16 at 9.20.27 P"></p></div></div><h1 id="Tensorflow代码"><a href="#Tensorflow代码" class="headerlink" title="Tensorflow代码"></a>Tensorflow代码</h1><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><h2 id="卷积层tf-nn-conv2d"><a href="#卷积层tf-nn-conv2d" class="headerlink" title="卷积层tf.nn.conv2d()"></a>卷积层tf.nn.conv2d()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=<span class="keyword">None</span>, data_format=<span class="keyword">None</span>, name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 3.38.52 PM.png" alt="Screen Shot 2018-07-10 at 3.38.52 PM"></p><h2 id="池化层tf-nn-max-pool"><a href="#池化层tf-nn-max-pool" class="headerlink" title="池化层tf.nn.max_pool()"></a>池化层tf.nn.max_pool()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tf.nn.max_pool( value, ksize,strides,padding,data_format=’NHWC’,name=<span class="keyword">None</span>) </div><div class="line"><span class="comment">#或者 </span></div><div class="line">tf.nn.avg_pool(…)</div></pre></td></tr></table></figure><p><img src="/2018/07/04/DP-CNN/Screen Shot 2018-07-10 at 3.41.42 PM.png" alt="Screen Shot 2018-07-10 at 3.41.42 PM"></p><h2 id="归一化层tf-nn-batch-normalization"><a href="#归一化层tf-nn-batch-normalization" class="headerlink" title="归一化层tf.nn.batch_normalization()"></a>归一化层tf.nn.batch_normalization()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">batch_normalization( x,mean,variance,offset,scale, variance_epsilon,name=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><ul><li>mean 和 variance 通过 tf.nn.moments 来进行计算：<br>batch_mean, batch_var = tf.nn.moments(x, axes = [0, 1, 2], keep_dims=True)，注意axes的输入。对于以feature map 为维度的全局归一化，若feature map 的shape 为[batch, height, width, depth]，则将axes赋值为[0, 1, 2]</li><li>x 为输入的feature map 四维数据，offset、scale为一维Tensor数据，shape 等于 feature map 的深度depth。</li></ul><h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p>通过搭建卷积神经网络来实现sklearn库中的手写数字识别，搭建的卷积神经网络结构如下图所示：</p><p><img src="/2018/07/04/DP-CNN/20170501131714709.png" alt="20170501131714709"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">digits = load_digits()</div><div class="line">x_data = digits.data.astype(np.float32)</div><div class="line">y_data = digits.target.astype(np.float32)</div><div class="line">print(y_data) <span class="comment"># [0. 1. 2. ... 8. 9. 8.]</span></div><div class="line">y_data = y_data.reshape(<span class="number">-1</span>,<span class="number">1</span>)</div><div class="line">print(y_data)</div><div class="line"><span class="comment"># [[0.]</span></div><div class="line"><span class="comment">#  [1.]</span></div><div class="line"><span class="comment">#  [2.]</span></div><div class="line"><span class="comment">#  ...</span></div><div class="line"><span class="comment">#  [8.]</span></div><div class="line"><span class="comment">#  [9.]</span></div><div class="line"><span class="comment">#  [8.]]</span></div><div class="line">print(x_data.shape) <span class="comment">#(1797,64)</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler <span class="comment"># 将属性缩放到一个指定的最大和最小值（通常是1-0）之间</span></div><div class="line">scaler = MinMaxScaler()</div><div class="line">x_data = scaler.fit_transform(x_data)</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</div><div class="line">y = OneHotEncoder().fit_transform(y_data).todense()</div><div class="line">print(y)</div><div class="line"><span class="comment"># [[1. 0. 0. ... 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 1. 0. ... 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 1. ... 0. 0. 0.]</span></div><div class="line"><span class="comment">#  ...</span></div><div class="line"><span class="comment">#  [0. 0. 0. ... 0. 1. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 0. ... 0. 0. 1.]</span></div><div class="line"><span class="comment">#  [0. 0. 0. ... 0. 1. 0.]]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#转换为图片的格式 （batch，height，width，channels）</span></div><div class="line">x = x_data.reshape(<span class="number">-1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>) <span class="comment">#-1表示适应大小，原来是64长度，变成8*8，则适应大小是1，即one batch的图片数目是1</span></div><div class="line">batch_size = <span class="number">8</span> <span class="comment"># 使用MBGD算法，设定batch_size为8</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generatebatch</span><span class="params">(x,y,n_examples,batch_size)</span>:</span></div><div class="line">    <span class="keyword">for</span> batch_i <span class="keyword">in</span> range(n_examples // batch_size):</div><div class="line">        start = batch_i * batch_size</div><div class="line">        end = start + batch_size</div><div class="line">        batch_xs = x[start:end]</div><div class="line">        batch_ys = y[start:end]</div><div class="line">        <span class="keyword">yield</span> batch_xs, batch_ys  <span class="comment"># 生成每一个batch</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#输入层</span></div><div class="line">tf.reset_default_graph()</div><div class="line">tf_x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>]) <span class="comment">#灰度图片，CHANEL是1</span></div><div class="line">tf_y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</div><div class="line"><span class="comment">#卷积层+激活层</span></div><div class="line">conv_filter_w1 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">10</span>])) <span class="comment">#shape:10个filter，size是3*3，灰度图片1</span></div><div class="line">conv_filter_b1 = tf.Variable(tf.random_normal([<span class="number">10</span>])) <span class="comment">#10个filter</span></div><div class="line">relu_feature_maps1 = tf.nn.relu(</div><div class="line">    tf.nn.conv2d(tf.x,conv_filter_w1,</div><div class="line">                 strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],<span class="comment">#batch和in_chanel是1，1*1是移动步</span></div><div class="line">                 padding=<span class="string">'same'</span>)</div><div class="line">    +conv_filter_b1)</div><div class="line"><span class="comment">#池化层</span></div><div class="line">max_pool1 = tf.nn.max_pool(relu_feature_maps1,</div><div class="line">                           ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],</div><div class="line">                           strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</div><div class="line">                           padding=<span class="string">'same'</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#卷积层</span></div><div class="line">conv_filter_w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">10</span>,<span class="number">5</span>])) <span class="comment">#10是in_chanel,与输入的in_chanel相同，5是feature_maps数目</span></div><div class="line">conv_filter_b2 = tf.Variable(tf.random_normal([<span class="number">5</span>])) <span class="comment">#与神经元个数相同</span></div><div class="line">conv_out2 = tf.nn.conv2d(relu_feature_maps1,conv_filter_w2,strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)+conv_filter_b2</div><div class="line"></div><div class="line"><span class="comment">#BN归一化层+激励层</span></div><div class="line">batch_mean, batch_var = tf.nn.moments(conv_out2, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], keep_dims=<span class="keyword">True</span>)</div><div class="line">shift = tf.Variable(tf.zeros([<span class="number">5</span>]))</div><div class="line">scale = tf.Variable(tf.ones([<span class="number">5</span>]))</div><div class="line">epsilon = <span class="number">1e-3</span></div><div class="line">BN_out = tf.nn.batch_normalization(conv_out2, batch_mean, batch_var, shift, scale, epsilon)</div><div class="line">relu_BN_maps2 = tf.nn.relu(BN_out)</div><div class="line"></div><div class="line"><span class="comment">#池化层</span></div><div class="line">max_pool2 = tf.nn.max_pool(BN_out,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#特征图展开，输入大小是8*8，两次pooling，变成2*2，而且有5个神经元，则展开大小是2*2*5</span></div><div class="line">max_pool2_flat = tf.reshape(max_pool2,[<span class="number">-1</span>,<span class="number">2</span>*<span class="number">2</span>*<span class="number">5</span>])</div><div class="line"><span class="comment">#全连接层</span></div><div class="line">fc_w1 = tf.Variable(tf.random_normal([<span class="number">2</span>*<span class="number">2</span>*<span class="number">5</span>,<span class="number">50</span>])) <span class="comment">#50神经元</span></div><div class="line">fc_b1 = tf.Variable(tf.random_normal([<span class="number">50</span>]))</div><div class="line">fc_out1 = tf.nn.relu(tf.matmul(max_pool2_flat,fc_w1)+fc_b1)</div><div class="line"><span class="comment">#输出层</span></div><div class="line">out_w1 = tf.Variable(tf.random_normal([<span class="number">50</span>,<span class="number">10</span>]))</div><div class="line">out_b1 = tf.Variable(tf.random_normal([<span class="number">10</span>]))</div><div class="line">pred = tf.nn.softmax(tf.matmul(fc_out1,out_w1)+out_b1)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">loss = -tf.reduce_mean(tf_y*tf.log(tf.clip_by_value(pred,<span class="number">1e-11</span>,<span class="number">1.0</span>)))</div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</div><div class="line">y_pred = tf.arg_max(pred,<span class="number">1</span>)</div><div class="line">bool_pred = tf.equal(tf.arg_max(tf_y,<span class="number">1</span>),y_pred)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(bool_pred,tf.float32)) <span class="comment"># 准确率</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment"># 迭代1000个周期</span></div><div class="line">        <span class="keyword">for</span> batch_xs,batch_ys <span class="keyword">in</span> generatebatch(x,y,y.shape[<span class="number">0</span>],batch_size): <span class="comment"># 每个周期进行MBGD算法</span></div><div class="line">            sess.run(train_step,feed_dict=&#123;tf_x:batch_xs , tf_y:batch_ys&#125;)</div><div class="line">        <span class="keyword">if</span>(epoch%<span class="number">100</span>==<span class="number">0</span>):</div><div class="line">            res = sess.run(accuracy,feed_dict=&#123;tf_x:x,tf_y:y&#125;)</div><div class="line">            <span class="keyword">print</span> (epoch,res)</div><div class="line">    res_ypred = y_pred.eval(feed_dict=&#123;tf_x:x,tf_y:y&#125;).flatten() <span class="comment"># 只能预测一批样本，不能预测一个样本</span></div><div class="line">    <span class="keyword">print</span> (res_ypred)</div></pre></td></tr></table></figure><p>在第100次个batch size 迭代时，准确率就快速接近收敛了，这得归功于Batch Normalization 的作用！需要注意的是，这个模型还不能用来预测单个样本，因为在进行BN层计算时，单个样本的均值和方差都为0，会得到相反的预测效果，解决方法详见归一化层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span>  accuracy_score</div><div class="line">print(accuracy_score(Y_data,res_ypred.reshape(<span class="number">-1</span>,<span class="number">1</span>)))</div><div class="line"><span class="comment"># 0.998887033945</span></div></pre></td></tr></table></figure></div></div><h1 id="CNN结构扩展"><a href="#CNN结构扩展" class="headerlink" title="CNN结构扩展"></a>CNN结构扩展</h1><h2 id="GAP-Global-Average-Pooling"><a href="#GAP-Global-Average-Pooling" class="headerlink" title="GAP(Global Average Pooling)"></a>GAP(Global Average Pooling)</h2><p><a href="https://blog.csdn.net/yimingsilence/article/details/79227668" target="_blank" rel="noopener">[Ref1]</a> <a href="https://blog.csdn.net/Losteng/article/details/51520555" target="_blank" rel="noopener">[Ref2]</a></p><p>GAP是用来代替全连接层，由于全连接层过多的参数重要到会造成过拟合，所以GAP放弃了对前一层的全连接，使用pooling来降低参数。global average pooling 与 average pooling 的差别就在 “global” 这一个字眼上。global 与 local 在字面上都是用来形容 pooling 窗口区域的。 local 是取 feature map 的一个子区域求平均值，然后滑动这个子区域； global 显然就是对整个 feature map 求平均值了。因此，global average pooling 的最后输出结果仍然是 10 个 feature map，而不是一个，只不过每个 feature map 只剩下一个像素罢了。这个像素就是求得的平均值。</p><p>举个例子</p><p>假如，最后的一层的数据是10个6<em>6的特征图，global average pooling是将每一张特征图计算所有像素点的均值，输出一个数据值，这样10 个特征图就会输出10个数据点，将这些数据点组成一个1</em>10的向量的话，就成为一个特征向量，就可以送入到softmax的分类中计算了</p><p><img src="/2018/07/04/DP-CNN/20160528001522947.jpg" alt="20160528001522947"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://towardsdatascience.com/how-to-build-a-gated-convolutional-neural-network-gcnn-for-natural-language-processing-nlp-5ba3ee730bfb" target="_blank" rel="noopener">Building a convolutional neural network for natural language processing</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ML-Kernel Function</title>
      <link href="/2018/07/04/ML-Kernel-Function/"/>
      <url>/2018/07/04/ML-Kernel-Function/</url>
      <content type="html"><![CDATA[<p>kernel function的核心是，只要我们对整个空间给定一个对距离相关性的度量标准，那么我们因为这个度量标准可以推测出别处的数据(可能的)分布。</p><p>机器学习里的 kernel 是指什么？ - 知乎 <a href="https://www.zhihu.com/question/30371867" target="_blank" rel="noopener">https://www.zhihu.com/question/30371867</a></p><a id="more"></a><h1 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h1><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><h3 id="一元高斯分布"><a href="#一元高斯分布" class="headerlink" title="一元高斯分布"></a>一元高斯分布</h3><ul><li><strong>定义</strong></li></ul><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 10.52.44 PM-1626785.png" alt="Screen Shot 2018-07-14 at 10.52.44 PM-1626785"></p><blockquote><p>$Var(X)=E[(X-E(X))(X-E(X))]$</p></blockquote><ul><li><p><strong>标准正态</strong></p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 11.05.52 PM.png" alt="Screen Shot 2018-07-14 at 11.05.52 PM"></p></li></ul><h3 id="二元高斯分布"><a href="#二元高斯分布" class="headerlink" title="二元高斯分布"></a>二元高斯分布</h3><ul><li><strong>定义</strong></li></ul><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 10.55.09 PM.png" alt="Screen Shot 2018-07-14 at 10.55.09 PM"></p><blockquote><p>其中,$\rho$是变量$x$和$y$的协方差，$\rho=cov(x,y)=E(x-E(x))E(y-E(y))$.</p></blockquote><ul><li><p><strong>边缘概率密度</strong></p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 10.59.42 PM.png" alt="Screen Shot 2018-07-14 at 10.59.42 PM"></p></li><li><p><strong>独立性</strong></p><p>对于二维正态随机变量$(X,Y)$, $X$和$Y$相互独立的充要条件是参数$\rho=0$.也即二维正态随机变量独立和不相关可以互推。</p></li></ul><h3 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h3><p><a href="https://www.zhihu.com/question/36339816/answer/67043318" target="_blank" rel="noopener">Ref1</a> <a href="https://www.zhihu.com/question/36339816/answer/385944057" target="_blank" rel="noopener">Ref2</a> <a href="https://www.datalearner.com/blog/1051485590815771" target="_blank" rel="noopener">Ref3</a></p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 11.08.08 PM.png" alt="Screen Shot 2018-07-14 at 11.08.08 PM"></p><h3 id="多元高斯下的条件分布"><a href="#多元高斯下的条件分布" class="headerlink" title="多元高斯下的条件分布"></a>多元高斯下的条件分布</h3><p>整体服从多维高斯分布,条件分布也服从相应的正态分布，并且均值和协方差矩阵可以被唯一确定表示。</p><p>下图以2元高斯分布为例，$x_1$和$x_2$服从联合高斯分布，给定$X_1=x_1$，那么$P(x_2|x_1)$服从高斯分布。</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-27 at 9.14.48 PM.png" alt="Screen Shot 2018-07-27 at 9.14.48 PM"></p><h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><h2 id="Gaussian-Process-Regression"><a href="#Gaussian-Process-Regression" class="headerlink" title="Gaussian Process Regression"></a>Gaussian Process Regression</h2><p><a href="http://dataunion.org/17089.html" target="_blank" rel="noopener">intuition1</a> <a href="https://www.zhihu.com/question/46631426" target="_blank" rel="noopener">intuition2</a></p><p><a href="https://www.youtube.com/watch?v=4vGiHC35j9s" target="_blank" rel="noopener">video1</a> <a href="https://www.youtube.com/watch?v=MfHKW5z-OOA&amp;index=10&amp;list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6&amp;t=19s" target="_blank" rel="noopener">video2</a> <a href="https://blog.csdn.net/lj6052317/article/details/78772494" target="_blank" rel="noopener">Blog1</a> <a href="https://zhuanlan.zhihu.com/p/27555501" target="_blank" rel="noopener">ZhiHu2</a> </p><p><a href="https://www.zhihu.com/question/46631426/answer/102553087**" target="_blank" rel="noopener">问题理解</a>：你不知道函数长什么样。但是你有一些样本，还有这些样本的函数值（带噪声）。高斯过程可以通过kernel距离，给你把这个函数补出来：给一个新的样本，高斯过程可以告诉你函数值是多少。</p><h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>假设我们有3个样本点及其函数值：$(x_1,f_1)$,$(x_2,f_2)$,$(x_3,f_3)$.我们假设没有噪声，则$f(x)=wx$，高斯回归的关键假设是：给定一些$X$的值，我们对$Y$建模，并这些$Y$服从联合正态分布，故：</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 11.28.33 PM.png" alt="Screen Shot 2018-07-14 at 11.28.33 PM"></p><p>那么我们关心的是如何计算协方差矩阵，即如何衡量两个点之间的相似度？为了解答这个问题，我们进行了另一个重要假设：如果两个x 比较相似（eg, 离得比较近），那么对应的y值的相关性也就较高。换言之，协方差矩阵是 X 的函数。（而不是y的函数）。我们借助核函数来计算协方差矩阵，采用常见的高斯核函数：</p><script type="math/tex; mode=display">K_{ij}=e^{-\frac{\lambda|x_i-x_j|^2}{2\sigma^2}}</script><blockquote><p>可以看到核函数中有个超参数$\lambda$，我们可以通过极大似然法求出。</p></blockquote><p>现在我们有了一个新的点$x^<em>$, 我们使用回归方法去预测其对应的函数值$f^</em>$.</p><p>我们假设 $f^*$和 训练集里的$f_1,f_2,f_3$同属于一个 （4维的）联合正态分布！</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 11.36.53 PM.png" alt="Screen Shot 2018-07-14 at 11.36.53 PM"></p><p>首先，黄色部分是训练集的3维联合分布计算得来的，绿色部分是由预测点$x^*$与训练集求解出来，</p><p>这样整个联合分布就可以知道了。又正态分布的条件概率也是正态分布<a href="https://www.zhihu.com/question/46631426/answer/122929183" target="_blank" rel="noopener">Ref</a>，即$P(f*|f)$为：</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-14 at 11.49.32 PM.png" alt="Screen Shot 2018-07-14 at 11.49.32 PM"></p><p>所以这是一种贝叶斯方法，和OLS回归不同，这个方法给出了预测值所隶属的整个（后验）概率分布的。再强调一下，我们得到的是$f^*$的整个分布！</p><blockquote><ul><li>高斯分布是针对向量，而高斯过程是针对函数的；给定均值向量和协方差矩阵，我们可以确定一个高斯分布，同样给定<strong>一个均值函数和协方差函数，我们也可以惟一确定一个高斯过程</strong>。</li></ul></blockquote><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><h4 id="Noiseless-GPR"><a href="#Noiseless-GPR" class="headerlink" title="Noiseless GPR"></a>Noiseless GPR</h4><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-27 at 9.50.36 PM.png" alt="Screen Shot 2018-07-27 at 9.50.36 PM"></p><blockquote><p>可以看到，kernel中有两个超参数$\sigma_f$和带宽$l$。</p></blockquote><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-27 at 9.52.00 PM.png" alt="Screen Shot 2018-07-27 at 9.52.00 PM"></p><h4 id="Noise-GPR"><a href="#Noise-GPR" class="headerlink" title="Noise GPR"></a>Noise GPR</h4><p>当观测点有噪声的时候，即$y=f(x)+\epsilon$，其中$\epsilon \sim N(0,\sigma^2)$，那么我们的高斯过程回归：</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-08-05 at 11.07.50 AM.png" alt="Screen Shot 2018-08-05 at 11.07.50 AM"></p><p>那么条件概率的高斯分布的均值和方差为：</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-08-05 at 11.09.25 AM.png" alt="Screen Shot 2018-08-05 at 11.09.25 AM"></p><p>可以看到，均值函数就是观测点值得线性组合；同时，如果令$\alpha=(K+\sigma^2_nI)^{-1}y$，又可以看做是多个核函数的线性组合，</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-08-05 at 11.13.54 AM.png" alt="Screen Shot 2018-08-05 at 11.13.54 AM"></p><p>分析：<a href="https://www.zhihu.com/question/46631426/answer/122929183" target="_blank" rel="noopener">知乎1</a></p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-07 at 4.47.21 PM.png" alt="Screen Shot 2018-07-07 at 4.47.21 PM"></p><h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><p><a href="https://blog.dominodatalab.com/fitting-gaussian-process-models-python/" target="_blank" rel="noopener">code1</a> <a href="https://github.com/fonnesbeck/gp_tutorial_pydata/blob/master/1-Introduction.ipynb" target="_blank" rel="noopener">code2</a> </p><p>首先定义一个kernel函数来计算数据点之间的协方差矩阵，我们使用squared exponential，$\theta_1$和$\theta_2$是两个超参数，这样越靠近的两个点，它们的kernel函数值也会越接近于1。</p><script type="math/tex; mode=display">\begin{aligned}k(x,x^{\prime}) &= \theta_1\exp\left(-\frac{\theta_2}{2}(x-x^{\prime})^2\right)\end{aligned}</script><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(data1,data2,params)</span>:</span></div><div class="line">    theta_1 = params[<span class="number">0</span>]</div><div class="line">    theta_2 = params[<span class="number">1</span>]</div><div class="line">    kernel_func = theta_1*np.exp(<span class="number">-0.5</span>*theta_2*np.subtract.outer(data1,data2)**<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> kernel_func</div></pre></td></tr></table></figure><p>可视化协方差矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cov_matrix</span><span class="params">()</span>:</span></div><div class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">5</span>))</div><div class="line">    xrange = np.linspace(<span class="number">0</span>, <span class="number">5</span>)  <span class="comment"># generate 50 points</span></div><div class="line">    ax1.plot(xrange, kernel(<span class="number">0</span>, xrange, [<span class="number">1</span>, <span class="number">1</span>]))</div><div class="line">    ax1.set_xlabel(<span class="string">'x'</span>)</div><div class="line">    ax1.set_ylabel(<span class="string">'cov(0, x)'</span>)</div><div class="line"></div><div class="line">    cov_matrix = kernel(xrange, xrange, [<span class="number">1</span>, <span class="number">1</span>])</div><div class="line">    z = np.array(cov_matrix)  <span class="comment"># shape is (50,50)</span></div><div class="line">    ims = ax2.imshow(z, cmap=<span class="string">"inferno"</span>,</div><div class="line">                     interpolation=<span class="string">'none'</span>,</div><div class="line">                     extent=(<span class="number">0</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>))</div><div class="line">    ax2.set_xlabel(<span class="string">'x'</span>)</div><div class="line">    ax2.set_ylabel(<span class="string">'x'</span>)</div><div class="line">    plt.colorbar(ims, ax=ax2)</div><div class="line"></div><div class="line">    plt.tight_layout()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804212108124.png" alt="image-20180804212108124"></p></div></div><p>定义好了kernel函数之后，我们便可以利用kernel由样本点求出未知点的函数值的分布，我们假定未知点和已知点共同构成多变量的正态分布，而且多变量的正态分布的条件概率也是多变量正态分布，</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-08-04 at 9.44.00 PM.png" alt="Screen Shot 2018-08-04 at 9.44.00 PM"></p><p>所以只要根据上述公式计算出新分布的均值和协方差矩阵，那么我们就知道了未知点的多变量正态分布。所以，我们现在计算均值和协方差矩阵，可以看到它们两个都是由$K$, $KK_$, $K_K$和$K_$组合而来，所以根据kernel_function来计算：</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_mean_var</span><span class="params">(new_data,old_data,y,params)</span>:</span></div><div class="line"></div><div class="line">    K = kernel(old_data,old_data,params)</div><div class="line">    K_xing = kernel(new_data,old_data,params)</div><div class="line">    K_xingxing = kernel(new_data, new_data, params)</div><div class="line"></div><div class="line">    K_inv = np.linalg.inv(K)</div><div class="line"></div><div class="line">    mu = K_xing.dot(K_inv).dot(y)</div><div class="line">    var = K_xingxing - K_xing.dot(K_inv).dot(K_xing)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> mu,var</div></pre></td></tr></table></figure></div></div><p>现在假设我们有一个观测点$(0,0)$，那么我们根据该点画出高斯过程先验</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gaussian_prior</span><span class="params">()</span>:</span></div><div class="line">    thetas = [<span class="number">1</span>,<span class="number">10</span>]</div><div class="line">    sigma_0 = kernel(<span class="number">0</span>,<span class="number">0</span>,thetas)</div><div class="line">    x = np.arange(<span class="number">-3</span>,<span class="number">3</span>,step=<span class="number">0.01</span>)</div><div class="line">    plt.errorbar(x,np.zeros(len(x)),yerr=sigma_0)</div><div class="line">    plt.ylim(<span class="number">-3</span>,<span class="number">3</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804221943887.png" alt="image-20180804221943887"></p></div></div><p>现在，假设我们有一个已知点$(x=1.0,y=0.4967141530112327)$，那么我们就根据这个点取预测新样本的值，即计算出正态分布的均值和方差，然后采样即可。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">seed = np.random.seed(<span class="number">42</span>)</div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    thetas = [<span class="number">1</span>, <span class="number">10</span>]</div><div class="line">    sigma_0 = kernel(<span class="number">0</span>, <span class="number">0</span>, thetas)</div><div class="line">    x = [<span class="number">1.0</span>]</div><div class="line">    y = [np.random.normal(scale=sigma_0)]</div><div class="line">    print(y)</div><div class="line">    <span class="comment"># [0.4967141530112327]</span></div><div class="line">    x_pred = np.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">1000</span>)</div><div class="line">    predictions = [cal_mean_var(i,x,y,thetas) <span class="keyword">for</span> i <span class="keyword">in</span> x_pred]</div><div class="line">    y_pred, sigmas = np.transpose(predictions)</div><div class="line">    plt.errorbar(x_pred, y_pred, yerr=sigmas)</div><div class="line">    plt.plot(x, y, <span class="string">"ro"</span>)</div><div class="line">    plt.xlim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">    plt.ylim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804233007785.png" alt="image-20180804233007785"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">m, s = cal_mean_var([<span class="number">-0.7</span>], x, y, thetas)</div><div class="line">y2 = np.random.normal(m, s)</div><div class="line">x.append(<span class="number">-0.7</span>)</div><div class="line">y.append(y2)</div><div class="line">predictions = [cal_mean_var(i, x, y, thetas) <span class="keyword">for</span> i <span class="keyword">in</span> x_pred]</div><div class="line">y_pred, sigmas = np.transpose(predictions)</div><div class="line">plt.errorbar(x_pred, y_pred, yerr=sigmas)</div><div class="line">plt.plot(x, y, <span class="string">"ro"</span>)</div><div class="line">plt.xlim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">plt.ylim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804233150258.png" alt="image-20180804233150258"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">x_more = [<span class="number">-2.1</span>, <span class="number">-1.5</span>, <span class="number">0.3</span>, <span class="number">1.8</span>, <span class="number">2.5</span>]</div><div class="line">    m, s = cal_mean_var(x_more, x, y, thetas)</div><div class="line">    y_more = np.random.multivariate_normal(m, s)</div><div class="line">    print(y_more)</div><div class="line">    <span class="comment"># [-1.5128756   0.52371713 -0.13952425 -0.93665367 -1.29343995]</span></div><div class="line">    x += x_more</div><div class="line">    y += y_more.tolist()</div><div class="line">    predictions = [cal_mean_var(i, x, y, thetas) <span class="keyword">for</span> i <span class="keyword">in</span> x_pred]</div><div class="line">    y_pred, sigmas = np.transpose(predictions)</div><div class="line">    plt.errorbar(x_pred, y_pred, yerr=sigmas)</div><div class="line">    plt.plot(x, y, <span class="string">"ro"</span>)</div><div class="line">    plt.xlim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">    plt.ylim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804233352771.png" alt="image-20180804233352771"> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">10</span>)</div><div class="line">m,s = cal_mean_var(x,[],[],thetas)</div><div class="line">y = np.random.multivariate_normal(m,s)</div><div class="line">print(y)</div><div class="line"><span class="comment"># [-0.10972138 -0.68964403 -1.35001851  0.12093873  0.66659681 -1.58789444</span></div><div class="line">  <span class="number">1.01459254</span> <span class="number">-0.69464233</span> <span class="number">-0.8580216</span>   <span class="number">1.1130274</span> ]</div><div class="line">plt.plot(x, y, <span class="string">"ro"</span>)</div><div class="line">predictions = [cal_mean_var(i, x, y, thetas) <span class="keyword">for</span> i <span class="keyword">in</span> x_pred]</div><div class="line">y_pred, sigmas = np.transpose(predictions)</div><div class="line">plt.errorbar(x_pred, y_pred, yerr=sigmas)</div><div class="line">plt.ylim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804233917970.png" alt="image-20180804233917970"></p></div></div><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    x_pred = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>)</div><div class="line">    thetas = [<span class="number">1</span>,<span class="number">10</span>]</div><div class="line">    x = np.linspace(<span class="number">-3</span>,<span class="number">3</span>)</div><div class="line">    m,s = cal_mean_var(x,[],[],thetas)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        y = np.random.multivariate_normal(m,s)</div><div class="line">        predictions = [cal_mean_var(i,x,y,thetas) <span class="keyword">for</span> i <span class="keyword">in</span> x_pred]</div><div class="line">        y_pred,sigmas = np.transpose(predictions)</div><div class="line">        plt.plot(x_pred, y_pred, <span class="string">'LightSeaGreen'</span>, alpha=<span class="number">0.3</span>)</div><div class="line">        plt.ylim(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">    plt.xlabel(<span class="string">'X'</span>)</div><div class="line">    plt.ylabel(<span class="string">'y'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180804234353362.png" alt="image-20180804234353362"></p></div></div><h3 id="GPR中的参数选择"><a href="#GPR中的参数选择" class="headerlink" title="GPR中的参数选择"></a>GPR中的参数选择</h3><p><a href="http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process" target="_blank" rel="noopener">x</a> <a href="https://www.jiqizhixin.com/articles/2018-02-16" target="_blank" rel="noopener">ref1</a> <a href="https://github.com/fonnesbeck/gp_tutorial_pydata/blob/master/3-Marginal_likelihood.ipynb" target="_blank" rel="noopener">ref2</a> </p><h3 id="GPR-on-CO2-data"><a href="#GPR-on-CO2-data" class="headerlink" title="GPR on CO2 data"></a>GPR on CO2 data</h3><p><a href="http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process" target="_blank" rel="noopener">sklearn-example</a> </p><h4 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h4><p>该问题意在求解出$CO_2$含量关于时间的函数。</p><h2 id="Density-Estimation"><a href="#Density-Estimation" class="headerlink" title="Density Estimation"></a>Density Estimation</h2><p><a href="http://qr.ae/TUp6rs" target="_blank" rel="noopener">[ref1]</a> <a href="http://www.mvstat.net/tduong/research/seminars/seminar-2001-05/" target="_blank" rel="noopener">[ref2]</a> <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener">[wiki]</a> <a href="http://sklearn.lzjqsdd.com/modules/density.html" target="_blank" rel="noopener">[sklearn-implement]</a> </p><p><a href="https://www.zhihu.com/question/27301358/answer/105267357" target="_blank" rel="noopener">[Begin]</a> <a href="https://wenku.baidu.com/view/364bc23931126edb6f1a104b.html" target="_blank" rel="noopener">[deepen]</a> <a href="https://blog.csdn.net/liangzuojiayi/article/details/78152180" target="_blank" rel="noopener">[deepen]</a> <a href="https://blog.csdn.net/unixtch/article/details/78556499" target="_blank" rel="noopener">[close to totally]</a> <a href="https://blog.csdn.net/pipisorry/article/details/53635895" target="_blank" rel="noopener">[closer]</a> <a href="http://math.sjtu.edu.cn/faculty/chengwang/files/2015fall/ch4.pdf" target="_blank" rel="noopener">[sjtu]</a> <a href="https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E5%9F%BA%E6%9C%AC%E7%BB%9F%E8%AE%A1/kernel-density-estimation.md" target="_blank" rel="noopener">[intuition]</a> <a href="http://lotabout.me/2018/kernel-density-estimation/" target="_blank" rel="noopener">[形象生动]</a></p><p>核密度估计方法，是概率论中用来估计未知的(概率)密度函数，属于非参数检验方法之一，不利用有关数据分布的先验知识，对数据分布不附加任何假定，是一种从数据样本本身出发研究数据分布特征的方法。</p><h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><p>由给定样本集合求解随机变量的分布密度函数问题是概率统计学的基本问题之一。解决这一问题的方法包括参数估计和非参数估计。</p><p>对于<strong>参数估计</strong>，假设$(x_1,x_2,…,x_n)$是一组独立同分布的样本点，我们想要估计该组样本点的概率密度函数$f$。我们会假设样本点服从某一分布，比如高斯分布$x\sim N(\mu,\sigma ^2)$。 那么我们可以通过样本点取求解两个未知参数：</p><script type="math/tex; mode=display">\mu = \frac{1}{n}\sum_{i=1}^{n}x_i \\\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2</script><p>而<strong>非参数估计</strong>，核密度估计为：</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-13 at 4.51.58 PM.png" alt="Screen Shot 2018-07-13 at 4.51.58 PM"></p><p>核密度函数的好坏依赖于核函数和宽带$h$的选择，但是$h$对密度的估计影响比核函数选取大。通常我们考虑核函数为关于原点对称的且其积分为1.常见的核函数。</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-10 at 10.02.58 AM.png" alt="Screen Shot 2018-07-10 at 10.02.58 AM"></p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-10 at 10.03.29 AM.png" alt="Screen Shot 2018-07-10 at 10.03.29 AM"></p><blockquote><p>Intuition:</p><ul><li><p>这些核函数存在共同点：在数据点处为波峰；曲线下方面积为1 ；故理解为在每一个样本点$x_i$都放置了一个高斯核，该核以$x_i$为轴对称。</p></li><li><p>在“非参数估计”的语境下，“核”是一个函数，用来提供权重。上式就是一个加权平均，离$x$越近的$x_i$其权重越高。在每一个样本点处都放了一个核函数；</p></li><li><p>核密度函数的原理比较简单，在我们知道某一事物的概率分布的情况下，如果某一个数在观察中出现了，我们可以认为这个数的概率密度很大，和这个数比较近的数的概率密度也会比较大，而那些离这个数远的数的概率密度会比较小。</p><p>基于这种想法，针对观察中的第一个数，我们可以用K去拟合我们想象中的那个远小近大概率密度。对每一个观察数拟合出的多个概率密度分布函数，取平均。如果某些数是比较重要的，则可以取加权平均。需要说明的一点是，核密度的估计并不是找到真正的分布函数。</p></li><li><p>带宽的理解：随着点距离的增大，即$(x-x_i)$增大，核函数愈小，并且当距离大于带宽$h$时，核函数接近0。</p></li></ul></blockquote><h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p><strong>概率分布函数</strong>是描述随机变量取值分布规律的数学表示。对于任何实数x，事件[X&lt;x]的概率是一个x的函数：$F(x)=P(X\le x)$。</p><p><strong>概率密度函数(Probability Density Function)</strong>是概率分布函数的一阶导数，</p><p>暂时不谈概率密度，我们从概率分布说起，因为概率分布导数就是密度函数。对于一个点，它的概率是$P(X=x_i)=\frac{# x_i}{N}$, 即它出现的次数与点总数的商。</p><p><strong><a href="http://blog.shaochuancs.com/statistics-histogram/" target="_blank" rel="noopener">直方图</a>。</strong> 将数据值所在范围分成若干个区间，然后在图上描画每个区间中数据点的个数。</p><p><strong>分布函数的一阶导数。</strong></p><p>密度函数，从其定义出发，是分布函数的一阶导数。那么自然的，对于给定的样本集，我们通过估计其分布函数，然后对其求导得到密度函数。一个最简单而有效的估计分布函数的方法是所谓的「经验分布函数（empirical distribution function」：</p><script type="math/tex; mode=display">\hat{F_n}(t)=\frac{\# sample\ value\le t}{n}=\frac{1}{n}\sum_{i=1}^{n}1_{x_i\le t}</script><p>$\hat{F_n}(t)$的估计为所有小于t的样本的概率。但是这个EDF是不可导的，不能直接求一阶导数得到密度函数。</p><p>密度函数可以记为：</p><script type="math/tex; mode=display">f(x) = lim_{h\to0}\frac{F(x+h)-F(x-h)}{2h}</script><p>我们把分布函数用上面的经验分布函数替代，那么上式分子上就是落在[x-h,x+h]区间的点的个数。我们可以把f(x)的估计写成：</p><p><img src="/2018/07/04/ML-Kernel-Function/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/ML-Kernel-Function/equation.svg" alt="equation"></p><p>面的这个估计看起来还可以，但是还不够好，得到的密度函数不是光滑的。如果记$K_0(t)=\frac{1}{2}\cdot 1(t&lt;1)$,则</p><p><img src="/2018/07/04/ML-Kernel-Function/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/ML-Kernel-Function/Screen%20Shot%202018-07-10%20at%209.33.04%20AM.png" alt="Screen Shot 2018-07-10 at 9.33.04 AM"></p><p><img src="/2018/07/04/ML-Kernel-Function/equation-1171263.svg" alt="equation-1171263"></p><p>那么一个自然的想法是，我们是不是可以换其他的函数形式呢？比如其他的分布的密度函数作为K？如果我们使用标准正态分布的密度函数作为K,那估计就变成了：</p><p><img src="/2018/07/04/ML-Kernel-Function/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/ML-Kernel-Function/equation-1171731.svg" alt="equation-1171731"></p><p>由高斯积分$\int_{-\infin}^{+\infin}e^{-x^2}dx=\sqrt{\pi}$,上述积分$\int_{-\infin}^{+\infin}\hat{f_h}(x)dx=\frac{1}{\sqrt{2\pi}}\int_{-\infin}^{+\infin}e^{-x^2/2}dx=1$</p></div></div><h3 id="h的选择"><a href="#h的选择" class="headerlink" title="h的选择"></a><a href="http://blog.shaochuancs.com/statistics-kde/" target="_blank" rel="noopener">h的选择</a></h3><p>如何选定核函数的“方差”呢？这其实是由带宽h来决定，不同的带宽下的核函数估计结果差异很大。</p><p>如果是高斯核，令$\sigma$为样本方差，那么$h=(\frac{4}{3n})^{\frac{1}{5}}\sigma$. </p><p>带宽反映了KDE曲线整体的平坦程度，也即观察到的数据点在KDE曲线形成过程中所占的比重。带宽越大，观察到的数据点在最终形成的曲线形状中所占比重越小，KDE整体曲线就越平坦；带宽越小，观察到的数据点在最终形成的曲线形状中所占比重越大，KDE整体曲线就越陡峭。</p><p>假设有3个数据，生成的KDE曲线如下：</p><p><img src="/2018/07/04/ML-Kernel-Function/Screen Shot 2018-07-13 at 5.10.25 PM.png" alt="Screen Shot 2018-07-13 at 5.10.25 PM"></p><p>K函数内部的h分母用于调整KDE曲线的宽幅，而K函数外部的h分母则用于保证曲线下方的面积符合KDE的规则(KDE曲线下方面积和为1)。</p><p>带宽的选择很大程度上取决于主观判断：如果认为真实的概率分布曲线是比较平坦的，那么就选择较大的带宽；相反，如果认为真实的概率分布曲线是比较陡峭的，那么就选择较小的带宽。</p><p>如果带宽不是固定的，其变化取决于估计的位置（balloon  estimator）或样本点（逐点估计pointwise estimator），由此可以产产生一个非常强大的方法称为自适应或可变带宽核密度估计。</p><h3 id="Python-Implement"><a href="#Python-Implement" class="headerlink" title="Python Implement"></a><a href="http://scikit-learn.org/stable/modules/density.html#density-estimation" target="_blank" rel="noopener">Python Implement</a></h3><p>1、<a href="https://www.homeworkhelponline.net/blog/math/tutorial-kde" target="_blank" rel="noopener">Several Examples</a></p><p><strong>One data point example</strong></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(samples,sigma=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(-samples**<span class="number">2</span>/(<span class="number">2</span>*sigma**<span class="number">2</span>))/(sigma*np.sqrt(<span class="number">2</span>*np.pi))</div><div class="line">observed = np.array([<span class="number">2.2</span>]) <span class="comment">#shape = (1,)</span></div><div class="line">samples = np.linspace(<span class="number">-1</span>,<span class="number">6</span>,<span class="number">100</span>) <span class="comment"># shape = (100,)</span></div><div class="line">fig,ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>,sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>,figsize=(<span class="number">14</span>,<span class="number">8</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># bins 如果是整数，指定bin(箱子)的个数,也就是总共有几条条状图</span></div><div class="line"><span class="comment"># 如果是数组，则指明箱子的边界,[1,2,3,4,5]，则共4个箱子，区间是[1,2],[2,3],[3,4],[4,5]</span></div><div class="line">bins = np.linspace(<span class="number">-1</span>,<span class="number">5</span>,<span class="number">16</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].hist(observed,bins=bins,normed=<span class="keyword">False</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].text(<span class="number">0</span>,<span class="number">1.25</span>,<span class="string">'Histogram, Not normed, bin=0.4'</span>)</div><div class="line"></div><div class="line">bins = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">9</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].hist(observed,bins=bins,normed=<span class="keyword">True</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].text(<span class="number">0</span>,<span class="number">1.25</span>,<span class="string">'Histogram, normed, bin=0.75'</span>)</div><div class="line"></div><div class="line">bins = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">5</span>)</div><div class="line">ax[<span class="number">0</span>, <span class="number">2</span>].hist(observed, bins=bins, normed=<span class="keyword">True</span>)</div><div class="line">ax[<span class="number">0</span>, <span class="number">2</span>].text(<span class="number">0</span>, <span class="number">1.25</span>, <span class="string">"Histogram, normed, bin=1.5"</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].fill(samples, gaussian(samples-observed[<span class="number">0</span>]))</div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].text(<span class="number">0</span>, <span class="number">1.25</span>, <span class="string">"Gaussian, middle on X=2.2, b=1"</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].fill(samples, gaussian(samples-observed[<span class="number">0</span>],<span class="number">0.6</span>))</div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].text(<span class="number">0</span>, <span class="number">1.25</span>, <span class="string">"Gaussian, middle on X=2.2, b=0.6"</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].fill(samples, gaussian(samples-observed[<span class="number">0</span>],<span class="number">0.35</span>))</div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].text(<span class="number">0</span>, <span class="number">1.25</span>, <span class="string">"Gaussian, middle on X=2.2, b=0.35"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180720093134022.png" alt="image-20180720093134022"></p></div></div><p><strong>Two data point example</strong>.</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(samples,sigma=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(-samples**<span class="number">2</span>/(<span class="number">2</span>*sigma**<span class="number">2</span>))/(sigma*np.sqrt(<span class="number">2</span>*np.pi))</div><div class="line">observed = np.array([<span class="number">2.2</span>,<span class="number">3.9</span>])</div><div class="line">samples = np.linspace(<span class="number">-1</span>,<span class="number">7</span>,<span class="number">100</span>)</div><div class="line">fig,ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>,sharex=<span class="keyword">True</span>,sharey=<span class="keyword">True</span>,figsize=(<span class="number">14</span>,<span class="number">8</span>))</div><div class="line"></div><div class="line"><span class="comment"># bins 如果是整数，指定bin(箱子)的个数,也就是总共有几条条状图</span></div><div class="line"><span class="comment"># 如果是数组，则指明箱子的边界,[1,2,3,4,5]，则共4个箱子，区间是[1,2],[2,3],[3,4],[4,5]</span></div><div class="line">bins = np.linspace(<span class="number">-1</span>,<span class="number">7</span>,<span class="number">26</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].hist(observed,bins=bins,normed=<span class="keyword">False</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].text(<span class="number">-1</span>,<span class="number">1</span>,<span class="string">'Histogram, Not normed, bin=0.32'</span>) <span class="comment"># (7-(-1))/(26-1)</span></div><div class="line"></div><div class="line">bins = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">9</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].hist(observed,bins=bins,normed=<span class="keyword">True</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].text(<span class="number">-1</span>,<span class="number">1</span>,<span class="string">'Histogram, normed, bin=0.75'</span>)</div><div class="line"></div><div class="line">bins = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">5</span>)</div><div class="line">ax[<span class="number">0</span>, <span class="number">2</span>].hist(observed, bins=bins, normed=<span class="keyword">True</span>)</div><div class="line">ax[<span class="number">0</span>, <span class="number">2</span>].text(<span class="number">-1</span>,<span class="number">1</span>, <span class="string">"Histogram, normed, bin=1.5"</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].fill(samples, (gaussian(samples-observed[<span class="number">0</span>])+gaussian(samples-observed[<span class="number">1</span>]))/<span class="number">2</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].text(<span class="number">-1</span>,<span class="number">1</span>, <span class="string">"Gaussian, middle on X=2.2, b=1"</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].plot(samples, (gaussian(samples-observed[<span class="number">0</span>])/<span class="number">2</span>), <span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].plot(samples, (gaussian(samples-observed[<span class="number">1</span>])/<span class="number">2</span>), <span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].fill(samples, (gaussian(samples-observed[<span class="number">0</span>],<span class="number">0.6</span>)+gaussian(samples-observed[<span class="number">1</span>],<span class="number">0.6</span>))/<span class="number">2</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].text(<span class="number">-1</span>,<span class="number">1</span>, <span class="string">"Gaussian, middle on X=2.2, b=0.6"</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].plot(samples, (gaussian(samples-observed[<span class="number">0</span>],<span class="number">0.6</span>)/<span class="number">2</span>),<span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].plot(samples, (gaussian(samples-observed[<span class="number">1</span>],<span class="number">0.6</span>)/<span class="number">2</span>),<span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].fill(samples, (gaussian(samples-observed[<span class="number">0</span>],<span class="number">0.35</span>)+gaussian(samples-observed[<span class="number">1</span>],<span class="number">0.35</span>))/<span class="number">2</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].text(<span class="number">-1</span>,<span class="number">1</span>, <span class="string">"Gaussian, middle on X=2.2, b=0.35"</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].plot(samples, (gaussian(samples-observed[<span class="number">0</span>],<span class="number">0.35</span>))/<span class="number">2</span>,<span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].plot(samples, (gaussian(samples-observed[<span class="number">1</span>],<span class="number">0.35</span>))/<span class="number">2</span>,<span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180720104627333.png" alt="image-20180720104627333"></p></div></div><p><strong>Multiple data points example</strong></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(samples,sigma=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(-samples**<span class="number">2</span>/(<span class="number">2</span>*sigma**<span class="number">2</span>))/(sigma*np.sqrt(<span class="number">2</span>*np.pi))</div><div class="line">np.random.seed(<span class="number">2</span>)</div><div class="line">observed = np.random.normal(<span class="number">2.5</span>,<span class="number">0.9</span>,<span class="number">14</span>) <span class="comment"># 14 samples</span></div><div class="line">n = len(observed)</div><div class="line">samples = np.linspace(<span class="number">-1</span>,<span class="number">7</span>,<span class="number">100</span>)</div><div class="line">fig,ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>,sharex=<span class="keyword">True</span>,figsize=(<span class="number">14</span>,<span class="number">8</span>))</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].set_ylim([<span class="number">0</span>,<span class="number">3.1</span>])</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].set_ylim([<span class="number">0</span>,<span class="number">1</span>])</div><div class="line">ax[<span class="number">0</span>,<span class="number">2</span>].set_ylim([<span class="number">0</span>,<span class="number">1</span>])</div><div class="line">ax[<span class="number">1</span>,<span class="number">0</span>].set_ylim([<span class="number">0</span>,<span class="number">1</span>])</div><div class="line">ax[<span class="number">1</span>,<span class="number">1</span>].set_ylim([<span class="number">0</span>,<span class="number">1</span>])</div><div class="line">ax[<span class="number">1</span>,<span class="number">2</span>].set_ylim([<span class="number">0</span>,<span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># bins 如果是整数，指定bin(箱子)的个数,也就是总共有几条条状图</span></div><div class="line"><span class="comment"># 如果是数组，则指明箱子的边界,[1,2,3,4,5]，则共4个箱子，区间是[1,2],[2,3],[3,4],[4,5]</span></div><div class="line">bins = np.linspace(<span class="number">-1</span>,<span class="number">5</span>,<span class="number">20</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].hist(observed,bins=bins+<span class="number">0.75</span>,normed=<span class="keyword">False</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">0</span>].text(<span class="number">-1</span>,<span class="number">3</span>,<span class="string">'Histogram, Not normed, bin=0.32'</span>) <span class="comment"># (7-(-1))/(26-1)</span></div><div class="line"></div><div class="line">bins = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">9</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].hist(observed,bins=bins+<span class="number">0.75</span>,normed=<span class="keyword">True</span>)</div><div class="line">ax[<span class="number">0</span>,<span class="number">1</span>].text(<span class="number">-1</span>,<span class="number">1</span>,<span class="string">'Histogram, normed, bin=0.75'</span>)</div><div class="line"></div><div class="line">bins = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">5</span>)</div><div class="line">ax[<span class="number">0</span>, <span class="number">2</span>].hist(observed, bins=bins+<span class="number">0.75</span>, normed=<span class="keyword">True</span>)</div><div class="line">ax[<span class="number">0</span>, <span class="number">2</span>].text(<span class="number">-1</span>,<span class="number">1</span>, <span class="string">"Histogram, normed, bin=1.5"</span>)</div><div class="line"></div><div class="line"><span class="comment"># 3 figures</span></div><div class="line">sum1 = np.zeros(len(samples))</div><div class="line">sum2 = np.zeros(len(samples))</div><div class="line">sum3 = np.zeros(len(samples))</div><div class="line"><span class="keyword">for</span> obs <span class="keyword">in</span> observed:</div><div class="line">    ax[<span class="number">1</span>,<span class="number">0</span>].plot(samples,gaussian(samples-obs)/n,<span class="string">'-k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line">    sum1 += gaussian(samples-obs)</div><div class="line">    ax[<span class="number">1</span>, <span class="number">1</span>].plot(samples, gaussian(samples - obs,<span class="number">0.6</span>) / n, <span class="string">'-k'</span>, linestyle=<span class="string">'dashed'</span>)</div><div class="line">    sum2 += gaussian(samples-obs,<span class="number">0.6</span>)</div><div class="line">    ax[<span class="number">1</span>, <span class="number">2</span>].plot(samples, gaussian(samples - obs,<span class="number">0.35</span>) / n, <span class="string">'-k'</span>, linestyle=<span class="string">'dashed'</span>)</div><div class="line">    sum3 += gaussian(samples-obs,<span class="number">0.35</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].fill(samples, sum1/n)</div><div class="line">ax[<span class="number">1</span>, <span class="number">0</span>].text(<span class="number">1</span>,<span class="number">0.7</span>, <span class="string">"Gaussian, b=1"</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].fill(samples, sum2/n)</div><div class="line">ax[<span class="number">1</span>, <span class="number">1</span>].text(<span class="number">1</span>,<span class="number">0.7</span>, <span class="string">"Gaussian, b=0.6"</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].fill(samples, sum3/n)</div><div class="line">ax[<span class="number">1</span>, <span class="number">2</span>].text(<span class="number">1</span>,<span class="number">0.7</span>, <span class="string">"Gaussian, b=0.35"</span>)</div><div class="line"></div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180720104014770.png" alt="image-20180720104014770"></p></div></div><p>2、<a href="https://www.homeworkhelponline.net/blog/programming/predict-pdf-generate-data" target="_blank" rel="noopener">Practical task using KDE</a></p><p>给定样本数据集，预测其密度函数，并使用预测的密度函数生成样本</p><p><a href="https://github.com/HomeworkHelpOnline/KDE-Task/blob/master/dataset1.txt" target="_blank" rel="noopener">dataset</a></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x,sigma=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(-x**<span class="number">2</span>/(<span class="number">2</span>*sigma**<span class="number">2</span>))/(sigma*np.sqrt(<span class="number">2</span>*np.pi))</div><div class="line"></div><div class="line"><span class="comment"># load data</span></div><div class="line">data = np.loadtxt(<span class="string">'data1.txt'</span>) <span class="comment">#shape = (1000,)</span></div><div class="line">data_len = len(data)</div><div class="line"></div><div class="line">hist,bins = np.histogram(data,bins=<span class="number">100</span>,range=(np.min(data),np.max(data)),density=<span class="keyword">True</span>)</div><div class="line">dx = bins[<span class="number">1</span>]-bins[<span class="number">0</span>]</div><div class="line">width = <span class="number">0.7</span>*dx</div><div class="line">center = (bins[:<span class="number">-1</span>]+bins[<span class="number">1</span>:])/<span class="number">2</span></div><div class="line"></div><div class="line">sumPdf = np.zeros(len(center))</div><div class="line">h = <span class="number">1.06</span>*np.std(data)*data_len**(<span class="number">-1</span>/<span class="number">5.0</span>) <span class="comment">#Silverman's Rule to find optimal bandwidth</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> data:</div><div class="line">    sumPdf += gaussian(center-ele,h)</div><div class="line">sumPdf /= data_len</div><div class="line"></div><div class="line"><span class="comment">#使用估计的KDE进行数据采样1000个点</span></div><div class="line">samples = np.zeros(<span class="number">1000</span>)</div><div class="line">i = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> i &lt; <span class="number">1000</span>:</div><div class="line">    sample = np.random.rand(<span class="number">1</span>)*(np.max(data)-np.min(data))-np.abs(np.min(data)) <span class="comment">#np.random.rand()采样范围是[0,1)</span></div><div class="line">    <span class="keyword">if</span> np.random.rand(<span class="number">1</span>) &lt;= np.sum(gaussian(sample-data,h)/data_len):</div><div class="line">        samples[i] = sample</div><div class="line">        i += <span class="number">1</span></div><div class="line"></div><div class="line">plt.plot(center, sumPdf, color=<span class="string">'k'</span>,linestyle=<span class="string">'dashed'</span>)</div><div class="line">hist2,bins2 = np.histogram(samples,bins=<span class="number">100</span>,range=(np.min(data),np.max(data)),density=<span class="keyword">True</span>)</div><div class="line">plt.bar(center,hist2,align=<span class="string">'center'</span>,width=width)</div><div class="line">plt.title(<span class="string">'KDE in dashed vs Generated data bar, bandwidth h=%.2f'</span> %h)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180721000547637.png" alt="image-20180721000547637"></p></div></div><p>3、<strong>Python.sklaern工具包</strong></p><p><code>sklearn.neighbors.KernelDensity</code>实现了核密度估计，类声明如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kde = KernelDensity(bandwidth=1.0,kernel=&apos;gaussian&apos;)</div></pre></td></tr></table></figure><ul><li>bandwidth：核带宽</li><li>kernel：’gaussian’ , ‘tophat’ , ‘epanechnikov’ , ‘exponential’ , ‘linear’ , ‘cosine’. 默认’gaussian’</li></ul><p><strong>方法</strong></p><p><code>fit(X)</code>: 观测样本, X是二维的, shape = ( m , n ) </p><p><code>score_samples(X)</code>:对观测样本的概率取以e为底的对数</p><p><strong>Simple 1D Kernel Density Estimation</strong></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KernelDensity</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">N = <span class="number">100</span></div><div class="line">np.random.seed(<span class="number">1</span>)</div><div class="line">X = np.concatenate((np.random.normal(<span class="number">0</span>, <span class="number">1</span>, int(<span class="number">0.3</span> * N)),</div><div class="line">                    np.random.normal(<span class="number">5</span>, <span class="number">1</span>, int(<span class="number">0.7</span> * N))))</div><div class="line">print(X)<span class="comment">#是(100,)</span></div><div class="line">X = X[:, np.newaxis]<span class="comment">#增加一个轴</span></div><div class="line">print(X)<span class="comment">#(100,1)</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">X_plot = np.linspace(<span class="number">-5</span>, <span class="number">10</span>, <span class="number">1000</span>)[:, np.newaxis]</div><div class="line"></div><div class="line">true_dens = (<span class="number">0.3</span> * norm(<span class="number">0</span>, <span class="number">1</span>).pdf(X_plot[:, <span class="number">0</span>]) <span class="comment">#X_plot[:, 0] shape = (1000,)</span></div><div class="line">             + <span class="number">0.7</span> * norm(<span class="number">5</span>, <span class="number">1</span>).pdf(X_plot[:, <span class="number">0</span>]))</div><div class="line"></div><div class="line">fig, ax = plt.subplots()</div><div class="line">ax.fill(X_plot[:, <span class="number">0</span>], true_dens, fc=<span class="string">'black'</span>, alpha=<span class="number">0.2</span>,</div><div class="line">        label=<span class="string">'input distribution'</span>)</div><div class="line">ax.plot(X[:, <span class="number">0</span>], <span class="number">-0.005</span> - <span class="number">0.01</span> * np.random.random(X.shape[<span class="number">0</span>]), <span class="string">'+k'</span>)</div><div class="line">ax.text(<span class="number">6</span>, <span class="number">0.38</span>, <span class="string">"N=&#123;0&#125; points"</span>.format(N))</div><div class="line">ax.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180713221657920.png" alt="image-20180713221657920"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> kernel <span class="keyword">in</span> [<span class="string">'gaussian'</span>, <span class="string">'tophat'</span>, <span class="string">'epanechnikov'</span>]:</div><div class="line">    kde = KernelDensity(kernel=kernel, bandwidth=<span class="number">0.5</span>).fit(X)<span class="comment">#输入X必须是2维</span></div><div class="line">    log_dens = kde.score_samples(X_plot)</div><div class="line">    ax.plot(X_plot[:, <span class="number">0</span>], np.exp(log_dens), <span class="string">'-'</span>,</div><div class="line">            label=<span class="string">"kernel = '&#123;0&#125;'"</span>.format(kernel))</div><div class="line"></div><div class="line">ax.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">ax.set_xlim(<span class="number">-4</span>, <span class="number">9</span>)</div><div class="line">ax.set_ylim(<span class="number">-0.02</span>, <span class="number">0.4</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180713220740270.png" alt="image-20180713220740270"></p></div></div><p><strong>物种分布密度拟合</strong></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_species_distributions</div><div class="line"><span class="keyword">from</span> sklearn.datasets.species_distributions <span class="keyword">import</span> construct_grids</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KernelDensity</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">from</span> mpl_toolkits.basemap <span class="keyword">import</span> Basemap</div><div class="line">    basemap = <span class="keyword">True</span></div><div class="line"><span class="keyword">except</span> ImportError:</div><div class="line">    basemap = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># data是一个字典（dictionary）,</span></div><div class="line"><span class="comment"># 里面储存着Bradypus variegatus和Microryzomys minutus两个物种的地理分布数据</span></div><div class="line"><span class="comment"># 对于每一个物种，储存着8个key-values对</span></div><div class="line">data = fetch_species_distributions()</div><div class="line">species_names = [<span class="string">'Bradypus Variegatus'</span>, <span class="string">'Microryzomys Minutus'</span>]</div><div class="line"></div><div class="line"><span class="comment"># train是一个record array,它的shape 是 (1624,)。</span></div><div class="line"><span class="comment"># 每一个数据点有三个fields: 物种名字：train['species']</span></div><div class="line"><span class="comment"># 经度（单位：度）： train['dd long']</span></div><div class="line"><span class="comment"># 维度（单位：度）： train['dd lat']</span></div><div class="line">Xtrain = np.vstack([data[<span class="string">'train'</span>][<span class="string">'dd lat'</span>],</div><div class="line">                    data[<span class="string">'train'</span>][<span class="string">'dd long'</span>]]).T <span class="comment">#Xtrain的shape=(1624, 2),不转置是（2，1624）</span></div><div class="line">ytrain = np.array([d.decode(<span class="string">'ascii'</span>).startswith(<span class="string">'micro'</span>)</div><div class="line">                  <span class="keyword">for</span> d <span class="keyword">in</span> data[<span class="string">'train'</span>][<span class="string">'species'</span>]], dtype=<span class="string">'int'</span>)</div><div class="line">print(data[<span class="string">'train'</span>][<span class="string">'species'</span>])</div><div class="line"><span class="comment"># [b'microryzomys_minutus' b'microryzomys_minutus'... b'microryzomys_minutus']</span></div><div class="line"></div><div class="line">Xtrain *= np.pi / <span class="number">180</span> <span class="comment">## Convert lat/long to 弧度</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Set up the data grid for the contour plot</span></div><div class="line">xgrid, ygrid = construct_grids(data)</div><div class="line">X, Y = np.meshgrid(xgrid[::<span class="number">5</span>], ygrid[::<span class="number">5</span>][::<span class="number">-1</span>])</div><div class="line">land_reference = data.coverages[<span class="number">6</span>][::<span class="number">5</span>, ::<span class="number">5</span>]</div><div class="line">land_mask = (land_reference &gt; <span class="number">-9999</span>).ravel()</div><div class="line"></div><div class="line">xy = np.vstack([Y.ravel(), X.ravel()]).T</div><div class="line">xy = xy[land_mask]</div><div class="line">xy *= np.pi / <span class="number">180.</span></div><div class="line"></div><div class="line"><span class="comment"># Plot map of South America with distributions of each species</span></div><div class="line">fig = plt.figure()</div><div class="line">fig.subplots_adjust(left=<span class="number">0.05</span>, right=<span class="number">0.95</span>, wspace=<span class="number">0.05</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</div><div class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, i + <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># construct a kernel density estimate of the distribution</span></div><div class="line">    print(<span class="string">" - computing KDE in spherical coordinates"</span>)</div><div class="line">    kde = KernelDensity(bandwidth=<span class="number">0.04</span>, metric=<span class="string">'haversine'</span>,</div><div class="line">                        kernel=<span class="string">'gaussian'</span>, algorithm=<span class="string">'ball_tree'</span>)</div><div class="line">    kde.fit(Xtrain[ytrain == i])</div><div class="line"></div><div class="line">    <span class="comment"># evaluate only on the land: -9999 indicates ocean</span></div><div class="line">    Z = <span class="number">-9999</span> + np.zeros(land_mask.shape[<span class="number">0</span>])</div><div class="line">    Z[land_mask] = np.exp(kde.score_samples(xy))</div><div class="line">    Z = Z.reshape(X.shape)</div><div class="line"></div><div class="line">    <span class="comment"># plot contours of the density</span></div><div class="line">    levels = np.linspace(<span class="number">0</span>, Z.max(), <span class="number">25</span>)</div><div class="line">    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> basemap:</div><div class="line">        print(<span class="string">" - plot coastlines using basemap"</span>)</div><div class="line">        m = Basemap(projection=<span class="string">'cyl'</span>, llcrnrlat=Y.min(),</div><div class="line">                    urcrnrlat=Y.max(), llcrnrlon=X.min(),</div><div class="line">                    urcrnrlon=X.max(), resolution=<span class="string">'c'</span>)</div><div class="line">        m.drawcoastlines()</div><div class="line">        m.drawcountries()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        print(<span class="string">" - plot coastlines from coverage"</span>)</div><div class="line">        plt.contour(X, Y, land_reference,</div><div class="line">                    levels=[<span class="number">-9999</span>], colors=<span class="string">"k"</span>,</div><div class="line">                    linestyles=<span class="string">"solid"</span>)</div><div class="line">        plt.xticks([])</div><div class="line">        plt.yticks([])</div><div class="line"></div><div class="line">    plt.title(species_names[i])</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/07/04/ML-Kernel-Function/image-20180713223436731.png" alt="image-20180713223436731"></p></div></div>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Kernel Function </tag>
            
            <tag> Gaussian Distribution </tag>
            
            <tag> Density Estimation </tag>
            
            <tag> Gaussian Process Regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Lectures by Dwork</title>
      <link href="/2018/07/02/DP-Lectures-by-Dwork/"/>
      <url>/2018/07/02/DP-Lectures-by-Dwork/</url>
      <content type="html"><![CDATA[<h1 id="Lecture1"><a href="#Lecture1" class="headerlink" title="Lecture1"></a><a href="https://www.youtube.com/watch?v=OfWj89oRD7g" target="_blank" rel="noopener">Lecture1</a></h1><h1 id="Lecture2"><a href="#Lecture2" class="headerlink" title="Lecture2"></a><a href="https://www.youtube.com/watch?v=1LTi6MmJf48" target="_blank" rel="noopener">Lecture2</a></h1><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.14.21 PM.png" alt="Screen Shot 2018-07-02 at 6.14.21 PM"></p><blockquote><ol><li>$P(|x|\ge tb)=e^{-t}$ </li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.25.02 PM.png" alt="Screen Shot 2018-07-02 at 6.25.02 PM"></p><blockquote></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.24.38 PM.png" alt="Screen Shot 2018-07-02 at 6.24.38 PM"></p><blockquote><p>x</p></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.34.57 PM.png" alt="Screen Shot 2018-07-02 at 6.34.57 PM"></p><blockquote><p>sensitivity of utility $\Delta u$: how much in the worst case can one person’s data affect the ytility.</p></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.40.25 PM.png" alt="Screen Shot 2018-07-02 at 6.40.25 PM"></p><blockquote><p>xx</p></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.44.44 PM (2" alt="Screen Shot 2018-07-02 at 6.44.44 PM (2)">.png)</p><blockquote><ol><li>for the utility above, $u(x,y)$, it is defined according to the query error between the databases; if the query error is small, then should have a good utility, that’s why we need a negative sign in front of the max.</li><li>the sensitivity of utility????</li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.54.26 PM (2" alt="Screen Shot 2018-07-02 at 6.54.26 PM (2)">.png)</p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 6.55.13 PM.png" alt="Screen Shot 2018-07-02 at 6.55.13 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 7.02.17 PM.png" alt="Screen Shot 2018-07-02 at 7.02.17 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 7.02.47 PM.png" alt="Screen Shot 2018-07-02 at 7.02.47 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 7.03.48 PM.png" alt="Screen Shot 2018-07-02 at 7.03.48 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-02 at 7.05.11 PM.png" alt="Screen Shot 2018-07-02 at 7.05.11 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.34.45 AM.png" alt="Screen Shot 2018-07-03 at 9.34.45 AM"></p><blockquote><ol><li>In the video, she says the mechanism should be saperated from the database????</li><li></li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.38.58 AM.png" alt="Screen Shot 2018-07-03 at 9.38.58 AM"></p><blockquote><ol><li><strong>uncoordinated responses</strong>: ask one question and I add some nosie to the true answer and return it to you ; ask another question and do the same thing. They are independent of everything I did in the past.</li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.43.24 AM.png" alt="Screen Shot 2018-07-03 at 9.43.24 AM"></p><blockquote><ol><li>Stateless Mechanism : it does not remember what it does before. Answering the subsequent queries doesn’t depend on the previous queries.</li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.50.47 AM.png" alt="Screen Shot 2018-07-03 at 9.50.47 AM"></p><blockquote><ol><li>density response?????</li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.52.36 AM.png" alt="Screen Shot 2018-07-03 at 9.52.36 AM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 10.03.33 AM.png" alt="Screen Shot 2018-07-03 at 10.03.33 AM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.56.54 AM.png" alt="Screen Shot 2018-07-03 at 9.56.54 AM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 9.58.36 AM.png" alt="Screen Shot 2018-07-03 at 9.58.36 AM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-03 at 10.00.51 AM.png" alt="Screen Shot 2018-07-03 at 10.00.51 AM"></p><h1 id="Lecture3"><a href="#Lecture3" class="headerlink" title="Lecture3"></a><a href="https://www.youtube.com/watch?v=585hLnKDdUM" target="_blank" rel="noopener">Lecture3</a></h1><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 5.38.25 PM.png" alt="Screen Shot 2018-07-10 at 5.38.25 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 5.40.29 PM.png" alt="Screen Shot 2018-07-10 at 5.40.29 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 5.43.16 PM.png" alt="Screen Shot 2018-07-10 at 5.43.16 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 5.44.13 PM.png" alt="Screen Shot 2018-07-10 at 5.44.13 PM"></p><blockquote><ol><li>$D(q)$ is the probability so is bounded by 1.</li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 5.48.18 PM.png" alt="Screen Shot 2018-07-10 at 5.48.18 PM"></p><p> <img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.06.02 PM.png" alt="Screen Shot 2018-07-10 at 6.06.02 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.07.31 PM.png" alt="Screen Shot 2018-07-10 at 6.07.31 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.10.34 PM.png" alt="Screen Shot 2018-07-10 at 6.10.34 PM"></p><blockquote><ol><li>There are two databases $x$ and $x’$, where $x$ has property $p2,p3,p6$ and $p8$ and $x’$ has property 1 through 4 but not 5 to 8. And we have a set of queries results $Y_i$ and say we release the result with some noise, where for $Y_1,Y_2,Y_3,Y_4$ we add positive noise while negative noise for $Y_5,Y_6,Y_7,Y_8$.</li><li></li></ol></blockquote><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.17.02 PM.png" alt="Screen Shot 2018-07-10 at 6.17.02 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.18.22 PM.png" alt="Screen Shot 2018-07-10 at 6.18.22 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.23.33 PM.png" alt="Screen Shot 2018-07-10 at 6.23.33 PM"></p><p><img src="/2018/07/02/DP-Lectures-by-Dwork/Screen Shot 2018-07-10 at 6.27.49 PM.png" alt="Screen Shot 2018-07-10 at 6.27.49 PM"></p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-DEEP LEARNING</title>
      <link href="/2018/06/28/DP-DEEP-LEARNING/"/>
      <url>/2018/06/28/DP-DEEP-LEARNING/</url>
      <content type="html"><![CDATA[<p>Deep learning is the process of learning nonlinear features and functions from complex data. Deep learning has been shown to outperform traditional techniques for speech recognition, image recognition, and face detection. Deep learning aims to extract complex features from high-dimensional data and use them to build a model that relates inputs to outputs (e.g., classes). Deep learning architectures are usually constructed as multi-layer networks so that more abstract features are computed as nonlinear functions of lower-level features.</p><p>Privacy in deep learning consists of three aspects: privacy of the data used for learning a model or as input to an existing model, privacy of the model, and privacy of the model’s output.</p><a id="more"></a><p>This paper provides a <strong>detailed and clear background</strong>. <a href="https://www.cs.cornell.edu/~shmat/shmat_ccs15.pdf" target="_blank" rel="noopener">[2015-Reza Shokri]</a> design, implement, and evaluate a practical system that enables multiple parties to jointly learn an accurate neural network model for a given objective without sharing their input datasets, their system achieves all three privacy objectives in the context of collaborative neural-network training: it protects privacy of the training data, enables participants to control the learning objective and how much to reveal about their individual models, and lets them apply the jointly learned model to their own inputs without revealing the inputs or the outputs. </p><p><a href="https://arxiv.org/pdf/1607.00133.pdf" target="_blank" rel="noopener">[2016-Martín Abadi]</a> develop differential privacy SGD for neural network by applying gaussian noise with tighter error bounds in centralized setting.</p><p><a href="">[2016-NhatHai Phan]</a> </p><p><a href="https://arxiv.org/pdf/1709.05750.pdf" target="_blank" rel="noopener">[2017-NhatHai Phan]</a> criticizes all three methods above and aiming at following three objects:</p><ol><li>It is totally independent of the number of training epochs in consuming privacy budget</li><li>It has the ability to adaptively inject noise into features based on the contribution of each to the model output</li><li>It can be applied in a variety of deep neural networks</li></ol><h1 id="2016-Martin-Abadi"><a href="#2016-Martin-Abadi" class="headerlink" title="[2016-Martín Abadi]"></a>[2016-Martín Abadi]</h1><p>The following summary is from paper <a href="https://arxiv.org/pdf/1709.05750.pdf" target="_blank" rel="noopener">[2017-NhatHai Phan]</a>. </p><p><img src="/2018/06/28/DP-DEEP-LEARNING/Screen Shot 2018-06-28 at 5.04.15 PM.png" alt="Screen Shot 2018-06-28 at 5.04.15 PM"></p><p><img src="/2018/06/28/DP-DEEP-LEARNING/Screen Shot 2018-06-28 at 5.04.27 PM.png" alt="Screen Shot 2018-06-28 at 5.04.27 PM"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[2015-Reza Shokri] Privacy-Preserving Deep Learning</p><p>[2016-Martín Abadi] Deep Learning with Differential Privacy</p><p>[2016-NhatHai Phan] Differential privacy preservation for deep auto-encoders: an application of human behavior prediction</p><p>[2017-NhatHai Phan] Adaptive Laplace Mechanism: Differential Privacy<br>Preservation in Deep Learning</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python</title>
      <link href="/2018/06/26/ML-Python/"/>
      <url>/2018/06/26/ML-Python/</url>
      <content type="html"><![CDATA[<p><a href="https://www.yiibai.com/python3/python_date_time.html#article-start" target="_blank" rel="noopener">last</a></p><h1 id="琐碎"><a href="#琐碎" class="headerlink" title="琐碎"></a>琐碎</h1><ol><li><p><strong>warning ignore</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</div></pre></td></tr></table></figure></li><li><p><strong>中文注释乱码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding:utf8 -*-</span></div></pre></td></tr></table></figure><p>放在python脚本的第一行。</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,...]) <span class="comment"># n</span></div><div class="line">b = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,...]) <span class="comment"># m</span></div><div class="line">c = a - b <span class="comment"># error because n != m</span></div><div class="line">c = a.reshape(n,<span class="number">1</span>)-b <span class="comment"># produce a two-dim array (n,m)</span></div></pre></td></tr></table></figure></li><li><p>​</p></li></ol><h1 id="Number"><a href="#Number" class="headerlink" title="Number"></a>Number</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Python支持不同的数字类型 -</p><ul><li><p>int (有符号整数): 它们通常被称为只是整数或整数，是正的或负的整数，没有小数点。 Python3整数是无限的大小。Python 2中有两个整数类型 - int 和 long。</p><p> 在Python3中不再有 “长整型”了。</p></li><li><p>float (点实数值) : 也叫浮点数，它们代表实数，并用小数点分割整数和小数部分。浮点数也可以用科学记数法，使用 e 或 E 表示10的幂 (2.5e2 = 2.5 x 102 = 250).</p></li><li><p>complex (复数) : 格式是 a + bJ，其中a和b是浮点数，而J(或j)代表-1的平方根(这是一个虚数)。 实数是a的一部分，而虚部为b。复数不经常使用在 Python 编程了。</p></li></ul><h2 id="数值类型转换"><a href="#数值类型转换" class="headerlink" title="数值类型转换"></a>数值类型转换</h2><ul><li>类型 int(X)是将x转换为纯整数</li><li>类型 long(x) 将 x 转换为一个长整型</li><li>类型 float(x) 将 x 转换为浮点数</li><li>类型 complex(x) 将 x 转换成具有实数部分x和虚部为零的复数</li><li>类型 complex(x, y) x和y转换成一个带x实部和y为虚部的复数。x和y是数值表达式</li></ul><h2 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 11.09.15 AM.png" alt="Screen Shot 2018-06-27 at 11.09.15 AM"></p><h2 id="随机函数"><a href="#随机函数" class="headerlink" title="随机函数"></a>随机函数</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 11.10.03 AM.png" alt="Screen Shot 2018-06-27 at 11.10.03 AM"></p><h3 id="random-choice"><a href="#random-choice" class="headerlink" title="random.choice()"></a>random.choice()</h3><p>以下是 choice() 方法的语法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">random.choice( seq ) <span class="comment">##seq是一个列表、元组或者字符串，返回一个随机选择的值</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line">print(random.choice(range(<span class="number">100</span>)))</div><div class="line"><span class="comment"># 31</span></div><div class="line">print(random.choice(<span class="string">'HelleWorld'</span>))</div><div class="line"><span class="comment"># r</span></div><div class="line">print(random.choice([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">9</span>]))</div><div class="line"><span class="comment"># 3</span></div></pre></td></tr></table></figure><h3 id="random-randrange"><a href="#random-randrange" class="headerlink" title="random.randrange()"></a>random.randrange()</h3><p>randrange()方法的语法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">randrange ([start,] stop [,step]) <span class="comment">##该方法从给定范围内返回一个随机项</span></div></pre></td></tr></table></figure><ul><li>start — 范围的开始点。这个起点包括在该范围内。默认值为0</li><li>stop — 停止的范围点。这个点不包含在这个范围内</li><li>step — 递增值。默认值为1</li></ul><h3 id="random-random"><a href="#random-random" class="headerlink" title="random.random()"></a>random.random()</h3><p>返回范围[0,1]内的随机值</p><h3 id="random-shuffle"><a href="#random-shuffle" class="headerlink" title="random.shuffle()"></a>random.shuffle()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">random.shuffle (lst) <span class="comment">##返回重新洗牌列表。</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line">list = [<span class="number">20</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">5</span>];</div><div class="line">random.shuffle(list)</div><div class="line">print(list)</div><div class="line"><span class="comment"># [16, 20, 10, 5]</span></div><div class="line">random.shuffle(list)</div><div class="line">print(list)</div><div class="line"><span class="comment"># [10, 16, 20, 5]</span></div></pre></td></tr></table></figure><h3 id="random-unform"><a href="#random-unform" class="headerlink" title="random.unform()"></a>random.unform()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">random.uniform(x, y) <span class="comment">##介于[x,y)的均分布随机值,返回一个浮点数 r，使得 x &lt;= r &lt; y</span></div></pre></td></tr></table></figure><h2 id="三角函数"><a href="#三角函数" class="headerlink" title="三角函数"></a>三角函数</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.14.54 PM.png" alt="Screen Shot 2018-06-27 at 12.14.54 PM"></p><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.16.18 PM.png" alt="Screen Shot 2018-06-27 at 12.16.18 PM"></p><h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><h2 id="字符串访问"><a href="#字符串访问" class="headerlink" title="字符串访问"></a>字符串访问</h2><p>要访问子字符串，用方括号以及索引或索引来获得子切片</p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.18.32 PM.png" alt="Screen Shot 2018-06-27 at 12.18.32 PM"></p><h2 id="字符串更新"><a href="#字符串更新" class="headerlink" title="字符串更新"></a>字符串更新</h2><p>字符串是常量，不能直接修改，但是可以通过字符串拼接，修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">s = <span class="string">'hello x world'</span></div><div class="line">s = s[:<span class="number">6</span>]+<span class="string">"wq "</span>+s[<span class="number">8</span>:]</div><div class="line">print(s)</div><div class="line"><span class="comment"># hello wq world</span></div></pre></td></tr></table></figure><h2 id="字符串特殊操作符"><a href="#字符串特殊操作符" class="headerlink" title="字符串特殊操作符"></a>字符串特殊操作符</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.22.40 PM.png" alt="Screen Shot 2018-06-27 at 12.22.40 PM"></p><h3 id="字符串格式化"><a href="#字符串格式化" class="headerlink" title="字符串格式化%"></a>字符串格式化%</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"My name is %s and weight is %d kg!"</span> % (<span class="string">'Zara'</span>, <span class="number">21</span>)) </div><div class="line"><span class="comment"># My name is Zara and weight is 21 kg!</span></div></pre></td></tr></table></figure><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.24.44 PM.png" alt="Screen Shot 2018-06-27 at 12.24.44 PM"></p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.25.15 PM.png" alt="Screen Shot 2018-06-27 at 12.25.15 PM"></p><h2 id="字符串内置函数"><a href="#字符串内置函数" class="headerlink" title="字符串内置函数"></a>字符串内置函数</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.26.32 PM.png" alt="Screen Shot 2018-06-27 at 12.26.32 PM"></p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.26.45 PM.png" alt="Screen Shot 2018-06-27 at 12.26.45 PM"></p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.27.04 PM.png" alt="Screen Shot 2018-06-27 at 12.27.04 PM"></p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-27 at 12.27.16 PM.png" alt="Screen Shot 2018-06-27 at 12.27.16 PM"></p><h1 id="List-列表"><a href="#List-列表" class="headerlink" title="List(列表)"></a>List(列表)</h1><p>逗号分隔列表中元素，列表重要的一点是，在列表中的项目不必是同一类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">list1 = [<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>];</div><div class="line">list2 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> ];</div><div class="line">list3 = [<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span>];</div></pre></td></tr></table></figure><blockquote><p>与字符串索引类似，列表的索引从0开始，并列出可切片，联接，等等。</p></blockquote><h2 id="访问列表值"><a href="#访问列表值" class="headerlink" title="访问列表值"></a>访问列表值</h2><p>要访问列表值，请使用方括号连同索引或索引切片获得索引对应可用的值。例如 -</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">list1 = [<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>]</div><div class="line">list2 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span> ]</div><div class="line"><span class="keyword">print</span> (<span class="string">"list1[0]: "</span>, list1[<span class="number">0</span>])</div><div class="line"><span class="keyword">print</span> (<span class="string">"list2[1:5]: "</span>, list2[<span class="number">1</span>:<span class="number">5</span>])</div></pre></td></tr></table></figure><p>当执行上面的代码，它产生以下结果 -</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">list1[<span class="number">0</span>]:  physics</div><div class="line">list2[<span class="number">1</span>:<span class="number">5</span>]:  [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</div></pre></td></tr></table></figure><h2 id="列表值更新"><a href="#列表值更新" class="headerlink" title="列表值更新"></a>列表值更新</h2><p>可以通过给赋值运算符到左侧切片更新列表中的单个或多个元素， 并且可以使用 append()方法中加入一元素。例如 -</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">list = [<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>]</div><div class="line"><span class="keyword">print</span> (<span class="string">"Value available at index 2 : "</span>, list[<span class="number">2</span>])</div><div class="line">list[<span class="number">2</span>] = <span class="number">2001</span></div><div class="line"><span class="keyword">print</span> (<span class="string">"New value available at index 2 : "</span>, list[<span class="number">2</span>])</div></pre></td></tr></table></figure><p>当执行上面的代码，它产生以下结果 -</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Value available at index <span class="number">2</span> :</div><div class="line"><span class="number">1997</span></div><div class="line">New value available at index <span class="number">2</span> :</div><div class="line"><span class="number">2001</span></div></pre></td></tr></table></figure><h2 id="删除列表值"><a href="#删除列表值" class="headerlink" title="删除列表值"></a>删除列表值</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">del list[index] #删除index处的值</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">list = [<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>]</div><div class="line"><span class="keyword">print</span> (list)</div><div class="line"><span class="keyword">del</span> list[<span class="number">2</span>]</div><div class="line"><span class="keyword">print</span> (<span class="string">"After deleting value at index 2 : "</span>, list)</div><div class="line"><span class="comment"># ['physics', 'chemistry', 1997, 2000]</span></div><div class="line"><span class="comment"># After deleting value at index 2 :  ['physics', 'chemistry', 2000]</span></div></pre></td></tr></table></figure><h2 id="列表基本操作"><a href="#列表基本操作" class="headerlink" title="列表基本操作"></a>列表基本操作</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-06-28 at 12.16.11 PM.png" alt="Screen Shot 2018-06-28 at 12.16.11 PM"></p><h2 id="列表内置函数"><a href="#列表内置函数" class="headerlink" title="列表内置函数"></a>列表内置函数</h2><div class="table-container"><table><thead><tr><th style="text-align:center">函数名</th><th style="text-align:center">功能描述</th></tr></thead><tbody><tr><td style="text-align:center">cmp(list1, list2)</td><td style="text-align:center">列表元素比较，返回0,1,-1,</td></tr><tr><td style="text-align:center">len(list)</td><td style="text-align:center">列表长度</td></tr><tr><td style="text-align:center">max(list)</td><td style="text-align:center">返回列表中最大值</td></tr><tr><td style="text-align:center">min(list)</td><td style="text-align:center">列表中最小值</td></tr><tr><td style="text-align:center">list(seq)</td><td style="text-align:center">转化为list</td></tr><tr><td style="text-align:center">append(ele)</td><td style="text-align:center">元素添加到列表</td></tr><tr><td style="text-align:center">count(ele)</td><td style="text-align:center">统计元素在列表中出现次数</td></tr><tr><td style="text-align:center">extend(list)</td><td style="text-align:center">合并list</td></tr><tr><td style="text-align:center">index(ele)</td><td style="text-align:center">返回列表中 ele 对象对应最低索引值</td></tr><tr><td style="text-align:center">insert(index, ele)</td><td style="text-align:center">插入 ele 对象到列表的 index 索引位置,index必须给出</td></tr><tr><td style="text-align:center">pop([index=-1])</td><td style="text-align:center">根据index删除元素，默认删除最后一个</td></tr><tr><td style="text-align:center">remove(ele)</td><td style="text-align:center">删除ele元素</td></tr><tr><td style="text-align:center">reverse()</td><td style="text-align:center">列表翻转</td></tr><tr><td style="text-align:center">sort()</td><td style="text-align:center">列表元素排序</td></tr></tbody></table></div><h3 id="append"><a href="#append" class="headerlink" title="append()"></a>append()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">list1 = [<span class="string">'C++'</span>, <span class="string">'Java'</span>, <span class="string">'Python'</span>]</div><div class="line">list1.append(<span class="string">'C#'</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">"updated list : "</span>, list1)</div><div class="line"><span class="comment"># updated list :  ['C++', 'Java', 'Python', 'C#']</span></div></pre></td></tr></table></figure><h3 id="extend"><a href="#extend" class="headerlink" title="extend()"></a>extend()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">list1 = [<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="string">'maths'</span>]</div><div class="line">list2=list(range(<span class="number">5</span>)) </div><div class="line">list1.extend( list2)</div><div class="line"><span class="keyword">print</span> (<span class="string">'Extended List :'</span>,list1)</div><div class="line"><span class="comment"># Extended List : ['physics', 'chemistry', 'maths', 0, 1, 2, 3, 4]</span></div></pre></td></tr></table></figure><h3 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">aList = [<span class="number">123</span>, <span class="string">'xyz'</span>, <span class="string">'zara'</span>, <span class="string">'abc'</span>, <span class="number">123</span>];</div><div class="line"><span class="keyword">print</span> (<span class="string">"Count for 123 : "</span>, aList.count(<span class="number">123</span>))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Count for zara : "</span>, aList.count(<span class="string">'zara'</span>))</div><div class="line"><span class="comment"># Count for 123 :  2</span></div><div class="line"><span class="comment"># Count for zara :  1</span></div></pre></td></tr></table></figure><h1 id="Tuple元组"><a href="#Tuple元组" class="headerlink" title="Tuple元组"></a>Tuple元组</h1><p>元组是<strong>不可变</strong>的Python对象的序列，元组序列就像列表，元组和列表之间的区别是，元组不像列表那样不能被改变以及元组使用<strong>圆括号</strong>，而列表使用方括号，创建一个元组是将不同的逗号分隔值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">tup1 = (<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>)</div><div class="line">tup2 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> )</div><div class="line">tup3 = <span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span></div><div class="line">print(tup1,tup2,tup3)</div><div class="line"><span class="comment"># ('physics', 'chemistry', 1997, 2000) (1, 2, 3, 4, 5) ('a', 'b', 'c', 'd')</span></div></pre></td></tr></table></figure><p>为了编写含有一个单一的值，必须包含逗号，即使只有一个值的元组 −</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tup1 = (<span class="number">50</span>,)</div></pre></td></tr></table></figure><h2 id="元素访问"><a href="#元素访问" class="headerlink" title="元素访问"></a>元素访问</h2><p>要访问值元组，用方括号带索引或索引切片来获得可用的索引值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tup1 = (<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>)</div><div class="line">tup2 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> )</div><div class="line">tup3 = <span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"d"</span></div><div class="line">print(tup1[<span class="number">1</span>]) <span class="comment"># chemistry</span></div><div class="line">print(tup2[:<span class="number">2</span>]) <span class="comment"># (1, 2)</span></div><div class="line">print(tup3[<span class="number">2</span>:]) <span class="comment"># ('c', 'd')</span></div></pre></td></tr></table></figure><h2 id="元素更新"><a href="#元素更新" class="headerlink" title="元素更新"></a>元素更新</h2><p>元组是不可变的，这意味着我们不可以更新或更改元组元素的值。如下面的例子说明了可以把现有的元组创建新的元组的部分 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tup1 = (<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>)</div><div class="line"></div><div class="line"><span class="comment">#TypeError: 'tuple' object does not support item assignment</span></div><div class="line"><span class="comment">#tup1[0] = 100</span></div><div class="line">t_add = (<span class="number">100</span>,)</div><div class="line">tup1 = t_add + tup1[<span class="number">1</span>:]</div><div class="line">print(tup1)</div></pre></td></tr></table></figure><h2 id="元素删除"><a href="#元素删除" class="headerlink" title="元素删除"></a>元素删除</h2><p>移除个元组的别元素是不可能的。如要明确删除整个元组，只需要用 del 语句。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tup = (<span class="string">'physics'</span>, <span class="string">'chemistry'</span>, <span class="number">1997</span>, <span class="number">2000</span>);</div><div class="line"><span class="keyword">del</span> tup;</div></pre></td></tr></table></figure><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.07.25 PM.png" alt="Screen Shot 2018-07-01 at 4.07.25 PM"></p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.08.13 PM.png" alt="Screen Shot 2018-07-01 at 4.08.13 PM"></p><h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.09.04 PM.png" alt="Screen Shot 2018-07-01 at 4.09.04 PM"></p><h1 id="Python字典"><a href="#Python字典" class="headerlink" title="Python字典"></a>Python字典</h1><p>每个键是从它的值由冒号(:)，即在项目之间用逗号隔开，整个东西是包含在大括号中。没有任何项目一个空字典只写两个大括号，就像这样：{}.</p><p>键在一个字典中是唯一的，而值则可以重复。字典的值可以是任何类型，但键必须是不可变的数据的类型，例如：字符串，数字或元组这样的类型。</p><h2 id="访问字典中的值"><a href="#访问字典中的值" class="headerlink" title="访问字典中的值"></a>访问字典中的值</h2><p>要访问字典元素，你可以使用方括号和对应键，以获得其对应的值。</p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.11.11 PM.png" alt="Screen Shot 2018-07-01 at 4.11.11 PM"></p><h2 id="更新字典"><a href="#更新字典" class="headerlink" title="更新字典"></a>更新字典</h2><p>可以通过添加新条目或键值对，修改现有条目，或删除现有条目</p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.12.07 PM.png" alt="Screen Shot 2018-07-01 at 4.12.07 PM"></p><h2 id="删除字典"><a href="#删除字典" class="headerlink" title="删除字典"></a>删除字典</h2><p>可以删除单个字典元素或清除字典的全部内容。也可以在一个单一的操作删除整个词典。</p><p>要明确删除整个词典，只要用 del 语句就可以做到。</p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.13.11 PM.png" alt="Screen Shot 2018-07-01 at 4.13.11 PM"></p><h2 id="字典键"><a href="#字典键" class="headerlink" title="字典键"></a>字典键</h2><ol><li><p>每个键对应多个条目是不允许的。这意味着重复键是不允许的。当键分配过程中遇到重复，以最后分配的为准。例如 -</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dict = &#123;<span class="string">'Name'</span>: <span class="string">'Zara'</span>, <span class="string">'Age'</span>: <span class="number">7</span>, <span class="string">'Name'</span>: <span class="string">'Manni'</span>&#125;</div><div class="line"><span class="keyword">print</span> (<span class="string">"dict['Name']: "</span>, dict[<span class="string">'Name'</span>])</div><div class="line"><span class="comment">## dict['Name']:  Manni</span></div></pre></td></tr></table></figure></li><li><p>键必须是不可变的。这意味着可以使用字符串，数字或元组作为字典的键，但是像[‘key’]是不允许的。</p></li></ol><h2 id="内置函数-1"><a href="#内置函数-1" class="headerlink" title="内置函数"></a>内置函数</h2><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.18.40 PM.png" alt="Screen Shot 2018-07-01 at 4.18.40 PM"></p><ol><li><p>fromkeys()</p><p>使用seq的值作为键，来设置创建新的字典。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dict.fromkeys(seq[, value]))</div></pre></td></tr></table></figure><p><strong>参数</strong></p><ul><li>seq — 这是将用于字典键准备值的列表。</li><li>value — 这是可选的，如果提供的话则这个值将被设置为字典的值</li></ul><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.19.56 PM.png" alt="Screen Shot 2018-07-01 at 4.19.56 PM"></p></li><li><p>get()</p><p>返回给定键的值。如果键不可用，那么返回默认值 - None。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dict.get(key, default=None)</div></pre></td></tr></table></figure><p><strong>参数</strong></p><ul><li>key — 这是在字典中被搜索的键。</li><li>default — 这是以防键不存在对应值时，则使用这个值返回。</li></ul><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.21.11 PM.png" alt="Screen Shot 2018-07-01 at 4.21.11 PM"></p></li><li><p>update()</p><p>添加字典dict2键值对到字典dict</p><p><img src="/2018/06/26/ML-Python/Screen Shot 2018-07-01 at 4.22.45 PM.png" alt="Screen Shot 2018-07-01 at 4.22.45 PM"></p></li></ol>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Smart Metering</title>
      <link href="/2018/06/26/DP-Smart-Metering/"/>
      <url>/2018/06/26/DP-Smart-Metering/</url>
      <content type="html"><![CDATA[<p>The several features of smart metering:</p><ol><li><a id="more"></a></li></ol><p>There is an overview <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6461626" target="_blank" rel="noopener">[2013-Zekeriya Erkin]</a>.</p><p>In <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6847974" target="_blank" rel="noopener">[2014-Jing Zhao] </a>, the battery with capacity C is connected to the original house hold electricity network, and charging or discharging it (whose maximum rates are both $\beta$) adds noises (i.e., the battery energy b(t)) to the real load of the appliances d(t), which constitutes the smart meter’s reading s(t) = b(t)+d(t), and the c(t) is the energy stored at the battery at time t.</p><p>In order to achieve non-trusted center, the methods based on multipaty secure computatio are proposed. <a href="http://planete.inrialpes.fr/~ccastel/PAPERS/IH_TR.pdf" target="_blank" rel="noopener">[2011-Gergely Acs]</a> and <a href="http://delivery.acm.org/10.1145/2670000/2664263/p316-eigner.pdf?ip=167.96.157.132&amp;id=2664263&amp;acc=ACTIVE%20SERVICE&amp;key=A79D83B43E50B5B8%2E43B53F8E9794E57B%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1530070185_156cc5b8210c462923292fba625dd354" target="_blank" rel="noopener">[2014-Fabienne Eigner]</a> proposes three methods based on secure multiparties computation.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[2013-Zekeriya Erkin] Privacy-Preserving Data Aggregation in Smart Metering Systems</p><p>[2014-Jing Zhao] Achieving Differential Privacy of Data Disclosure in the Smart Grid</p><p>[2014-Fabienne Eigner] Differentially Private Data Aggregation with Optimal Utility</p><p>[2011-Gergely Acs] I have a DREAM! (DIffeRentially PrivatE smart Metering)</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Experiment-Metric</title>
      <link href="/2018/06/25/Experiment-Metric/"/>
      <url>/2018/06/25/Experiment-Metric/</url>
      <content type="html"><![CDATA[<h2 id="箱线图"><a href="#箱线图" class="headerlink" title="箱线图"></a><a href="https://blog.csdn.net/zhanghongju/article/details/18446131" target="_blank" rel="noopener">箱线图</a></h2><p>箱线图（Boxplot）也称箱须图（Box-whisker Plot），可以用于异常值检测。它是用一组数据中的最小值、第一四分位数、中位数、第三四分位数和最大值来反映数据分布的中心位置和散布范围，可以粗略地看出数据是否具有对称性。通过将多组数据的箱线图画在同一坐标上，则可以清晰地显示各组数据的分布差异，为发现问题、改进流程提供线索。</p><p><img src="/2018/06/25/Experiment-Metric/2009624114634640.JPG" alt="2009624114634640"></p><h3 id="四分位数"><a href="#四分位数" class="headerlink" title="四分位数"></a>四分位数</h3><p>所谓四分位数，就是把组中所有数据由小到大排列并分成四等份，处于三个分割点位置的数字就是四分位数。</p><ul><li>第一四分位数（Q1），又称“较小四分位数”或“下四分位数”，等于该样本中所有数值由小到大排列后第25%的数字。</li><li>第二四分位数（Q2），又称“中位数”，等于该样本中所有数值由小到大排列后第50%的数字。</li><li>第三四分位数（Q3），又称“较大四分位数”或“上四分位数”，等于该样本中所有数值由小到大排列后第75%的数字。</li><li>第三四分位数与第一四分位数的差距又称四分位间距（InterQuartile Range，IQR）。</li></ul><h4 id="四分位数计算"><a href="#四分位数计算" class="headerlink" title="四分位数计算"></a>四分位数计算</h4><ol><li><p>确定Q1、Q2、Q3的位置（n表示数字的总个数）</p><ul><li>Q1的位置=（n+1）/4</li><li>Q2的位置=（n+1）/2</li><li>Q3的位置=3（n+1）/4</li></ul><p>对于数字个数为奇数的，其四分位数比较容易确定。例如，数字“5、47、48、15、42、41、7、39、45、40、35”共有11项，由小到大排列的结果为“5、7、15、35、39、40、41、42、45、47、48”，计算结果如下：</p><ul><li>Q1的位置=（11+1）/4=3，该位置的数字是15。</li><li>Q2的位置=（11+1）/2=6，该位置的数字是40。</li><li>Q3的位置=3（11+1）/4=9，该位置的数字是45。</li></ul><p>而对于数字个数为偶数的，其四分位数确定起来稍微繁琐一点。例如，数字“8、17、38、39、42、44”共有6项，位置计算结果如下：</p><ul><li>Q1的位置=（6+1）/4=1.75</li><li>Q2的位置=（6+1）/2=3.5</li><li>Q3的位置=3（6+1）/4=5.25</li></ul><p>这时的数字以数据连续为前提，由所确定位置的前后两个数字共同确定。例如，Q2的位置为3.5，则由第3个数字38和第4个数字39共同确定，计算方法是：38+（39-38）×3.5的小数部分，即38+1×0.5=38.5。该结果实际上是38和39的平均数。</p><p>同理，Q1、Q3的计算结果如下：</p><ul><li>Q1 = 8+（17-8）×0.75=14.75</li><li>Q3 = 42+（44-42）×0.25=42.5</li></ul></li></ol><h2 id="MAE-vs-MSE"><a href="#MAE-vs-MSE" class="headerlink" title="MAE vs MSE"></a>MAE vs MSE</h2><p><a href="https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d" target="_blank" rel="noopener">ref1</a></p><h3 id="MAE-Mean-Absolute-Error"><a href="#MAE-Mean-Absolute-Error" class="headerlink" title="MAE(Mean Absolute Error)"></a>MAE(Mean Absolute Error)</h3><p><img src="/2018/06/25/Experiment-Metric/Screen Shot 2018-07-16 at 9.10.11 PM.png" alt="Screen Shot 2018-07-16 at 9.10.11 PM"></p><h3 id="MSE-Mean-Squared-Error"><a href="#MSE-Mean-Squared-Error" class="headerlink" title="MSE(Mean Squared Error)"></a>MSE(Mean Squared Error)</h3><p><img src="/2018/06/25/Experiment-Metric/Screen Shot 2018-07-16 at 9.11.04 PM.png" alt="Screen Shot 2018-07-16 at 9.11.04 PM"></p><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><ol><li><p>They are both used in crowd counting as evaluation metric.</p></li><li><p>Roughly speaking, MAE indicates the accuracy of the estimates, and MSE indicates the robustness of the estimates.</p><blockquote><p>This is because for mse, the errors are squared before they are averaged, the MSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.</p></blockquote></li><li></li></ol><h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><p>A confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs. Example for a classification algorithm: Predicting if the patient has cancer. Here, there can only be two outputs i.e. Yes or No.</p><p>A confusion matrix gives us a better idea of what our classification model is predicting right and what types of errors it is making.</p><p>Below is what an Confusion Matrix looks like:</p><p><img src="/2018/06/25/Experiment-Metric/Screen Shot 2019-08-06 at 9.01.39 PM.png" alt="creen Shot 2019-08-06 at 9.01.39 P"></p><p><strong>True Positive:</strong> You predicted positive and your are right.</p><p><strong>True Negative:</strong> You predicted negative and your are right.</p><p><strong>False Positive: (Type 1 Error):</strong> You predicted positive and you are wrong.</p><p><strong>False Negative: (Type 2 Error): </strong>You predicted negative and you are wrong.</p><p><img src="/2018/06/25/Experiment-Metric/1_OpSYGh2-XE6aE3sVAJAHrw.png" alt="_OpSYGh2-XE6aE3sVAJAHr"></p><p><img src="/2018/06/25/Experiment-Metric/1_uR09zTlPgIj5PvMYJZScVg.png" alt="_uR09zTlPgIj5PvMYJZScV"></p><p><strong>Recall</strong></p><script type="math/tex; mode=display">\operatorname{Recall}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}</script><p><strong>Precision</strong></p><script type="math/tex; mode=display">\text { Precision }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}</script><p><strong>F-measure</strong></p><script type="math/tex; mode=display">F-\text {measure}=\frac{2 * \text { Recall * Precision }}{\text { Recall }+\text { Precision }}</script><p>accuracy常用于分类，比对两个向量，一个是真实向量，一个是预测向量，预测正确加1，最终sum除以向量长度就是准确率。</p><p>精确率(precision)的公式是katex is not defined,它计算的是所有”正确被检索的item(TP)”占所有”实际被检索到的(TP+FP)”的比例.（在所有找对找错里面，找对的概率）</p><p>召回率(recall)的公式是katex is not defined,它计算的是所有”正确被检索的item(TP)”占所有”应该检索到的item(TP+FN)”的比例。（找到正确的，能覆盖目标的所有的概率）</p><p>假如某个班级有男生80人,女生20人,共计100人.目标是找出所有女生. 现在某人挑选出50个人,其中20人是女生,另外还错误的把30个男生也当作女生挑选出来了. 作为评估者的你需要来评估(evaluation)下他的工作。</p><ul><li>很容易，我们可以得到:他把其中70(20女+50男)人判定正确了,而总人数是100人，所以它的accuracy就是70 %(70 / 100).</li><li>在例子中就是希望知道此君得到的所有人中,正确的人(也就是女生)占有的比例.所以其precision也就是40%(20女生/(20女生+30误判为女生的男生)).</li><li>在例子中就是希望知道此君得到的女生占本班中所有女生的比例,所以其recall也就是100%(20女生/(20女生+ 0 误判为男生的女生))</li></ul><h1 id="AUC-ROC-Curve"><a href="#AUC-ROC-Curve" class="headerlink" title="AUC ROC Curve"></a>AUC ROC Curve</h1><h3 id="What-is-ROC"><a href="#What-is-ROC" class="headerlink" title="What is ROC?"></a>What is ROC?</h3><p>ROC (<em>Receiver Operating Characteristic</em>) Curve tells us about how good the model can distinguish between two things (<em>e.g</em> <em>If a patient has a disease or no</em>). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two.</p><p>Let’s assume we have a model which predicts whether the patient has a particular disease or no. The model predicts probabilities for each patient (<em>in python we use the“</em> <strong>predict_proba*</strong>” function*). Using these probabilities, we plot the distribution as shown below:</p><p><img src="/2018/06/25/Experiment-Metric/1_HxNvqTl-Pd63niUIbrD4pg.jpeg" alt="_HxNvqTl-Pd63niUIbrD4p"></p><p>Here, the red distribution represents all the patients who do not have the disease and the green distribution represents all the patients who have the disease.</p><p>Now we got to pick a value where we need to set the cut off i.e. a threshold value, above which we will predict everyone as positive (<em>they have the disease</em>) and below which will predict as negative (<em>they do not have the disease</em>). We will set the threshold at “<strong>0.5</strong>” as shown below:</p><p><img src="/2018/06/25/Experiment-Metric/1_qLjMtrdG3qIcuNBALvsYQA.jpeg" alt="_qLjMtrdG3qIcuNBALvsYQ"></p><p>All the positive values above the threshold will be “<strong>True Positives</strong>” and the negative values above the threshold will be “<strong>False Positives</strong>” as they are predicted incorrectly as positives.</p><p>All the negative values below the threshold will be “<strong>True Negatives</strong>” and the positive values below the threshold will be “<strong>False Negative</strong>” as they are predicted incorrectly as negatives.</p><p><img src="/2018/06/25/Experiment-Metric/1_Bwhr9ots47akHbrgssKXrA.jpeg" alt="_Bwhr9ots47akHbrgssKXr"></p><p>Here, we have got a basic idea of the model predicting correct and incorrect values with respect to the threshold set. Before we move on, let’s go through two important terms: <strong>Sensitivity and Specificity.</strong></p><h3 id="What-is-Sensitivity-and-Specificity"><a href="#What-is-Sensitivity-and-Specificity" class="headerlink" title="What is Sensitivity and Specificity?"></a>What is Sensitivity and Specificity?</h3><p>In simple terms, the proportion of patients that were identified correctly to have the disease (<em>i.e. True Positive</em>) upon the total number of patients who actually have the disease is called as Sensitivity or Recall.</p><p><img src="/2018/06/25/Experiment-Metric/1_aLUZ01GaLPwGDI24jb-uUA.png" alt="_aLUZ01GaLPwGDI24jb-uU"></p><p>Similarly, the proportion of patients that were identified correctly to not have the disease (<em>i.e. True Negative</em>) upon the total number of patients who do not have the disease is called as Specificity.</p><p><img src="/2018/06/25/Experiment-Metric/1_mPEFI9HFEF7GKH5HYQRDzA.png" alt="_mPEFI9HFEF7GKH5HYQRDz"></p><h3 id="Trade-off-between-Sensitivity-and-Specificity"><a href="#Trade-off-between-Sensitivity-and-Specificity" class="headerlink" title="Trade-off between Sensitivity and Specificity"></a>Trade-off between Sensitivity and Specificity</h3><p>When we decrease the threshold, we get more positive values thus increasing the sensitivity. Meanwhile, this will decrease the specificity.</p><p>Similarly, when we increase the threshold, we get more negative values thus increasing the specificity and decreasing sensitivity.</p><p>As <strong>Sensitivity</strong> ⬇️ <strong>Specificity</strong> ⬆️</p><p>As <strong>Specificity</strong> ⬇️ <strong>Sensitivity</strong> ⬆️</p><p><img src="/2018/06/25/Experiment-Metric/1_ceB9hobuBUjnPpRKedA-VA.png" alt="_ceB9hobuBUjnPpRKedA-V"></p><p>But, this is not how we graph the ROC curve. To plot ROC curve, instead of Specificity we use (1 — Specificity) and the graph will look something like this:</p><p><img src="/2018/06/25/Experiment-Metric/1_4Ar_wBQ_xWrFUqwwQGV-8A.png" alt="_4Ar_wBQ_xWrFUqwwQGV-8"></p><p>So now, when the sensitivity increases, (1 — specificity) will also increase. This curve is known as the ROC curve.</p><p><img src="/2018/06/25/Experiment-Metric/1_QqZzGJwzYxnHWZ_axq6ynA.png" alt="_QqZzGJwzYxnHWZ_axq6yn"></p><h3 id="Area-Under-the-Curve"><a href="#Area-Under-the-Curve" class="headerlink" title="Area Under the Curve"></a>Area Under the Curve</h3><p>The AUC is the area under the ROC curve. This score gives us a good idea of how well the model performances.</p><p>Let’s take a few examples</p><p><img src="/2018/06/25/Experiment-Metric/1_AgDJbm6d8qr8ESHNv6VvKg.png" alt="_AgDJbm6d8qr8ESHNv6VvK"></p><p><img src="/2018/06/25/Experiment-Metric/1_KNhNw8BsjbIETPF_BH8Qpg.png" alt="_KNhNw8BsjbIETPF_BH8Qp"></p><p>As we see, the first model does quite a good job of distinguishing the positive and the negative values. Therefore, there the AUC score is 0.9 as the area under the ROC curve is large.</p><p>Whereas, if we see the last model, predictions are completely overlapping each other and we get the AUC score of 0.5. This means that the model is performing poorly and it is predictions are almost random.</p><h3 id="Why-do-we-use-1-—-Specificity"><a href="#Why-do-we-use-1-—-Specificity" class="headerlink" title="Why do we use (1 — Specificity)?"></a>Why do we use (1 — Specificity)?</h3><p>Let’s derive what exactly is (1 — Specificity):</p><script type="math/tex; mode=display">\begin{array}{c}{\text { Specificity }=\frac{T N}{T N+F P}} \\ {1-\text {Specificity}=1-\frac{T N}{T N+F P}} \\ {1-\text {Specificity}=\frac{T N+F P-T N}{T N+F P}} \\ {1-\text {Specificity}=\frac{F P}{T N+F P}}\end{array}</script><p>As we see above, Specificity gives us the True Negative Rate and (1 — Specificity) gives us the False Positive Rate.</p><p>So the sensitivity can be called as the “<strong>True Positive Rate</strong>” and (1 — Specificity) can be called as the “<strong>False Positive Rate</strong>”.</p><p>So now we are just looking at the positives. As we increase the threshold, we decrease the <em>TPR</em> as well as the <em>FPR</em> and when we decrease the threshold, we are increasing the <em>TPR</em> and <em>FPR</em>.</p><p>Thus, AUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes.</p><p><a href="https://medium.com/@sebastiannorena/some-model-tuning-methods-bfef3e6544f0" target="_blank" rel="noopener">ref1</a> </p><p>AOC是衡量一个模型是否有效的参数，比如：我们使用了grid search来暴力寻找最佳的超参数组合，我们就可以使用AOC来比较不同超参数组合模型的效果，从而选择最佳模型的超参数组合。一般将<code>scoring=’roc_auc’</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">rfc = RandomForestClassifier()</div><div class="line">param_grid = &#123;<span class="string">'n_estimators'</span>:[<span class="number">70</span>,<span class="number">100</span>,<span class="number">180</span>],<span class="string">'criterion'</span>:[<span class="string">'gini'</span>,<span class="string">'entropy'</span>],<span class="string">'verbose'</span>:[<span class="number">0</span>,<span class="number">4</span>,<span class="number">10</span>],<span class="string">'warm_start'</span>:[<span class="string">'False'</span>,<span class="string">'True'</span>], <span class="string">'random_state'</span>:[<span class="number">42</span>,<span class="number">72</span>,<span class="number">100</span>,<span class="number">200</span>]&#125;</div><div class="line">CV_rfc = GridSearchCV(estimator=rfc,param_grid=param_grid, scoring=<span class="string">'roc_auc'</span>, cv= <span class="number">5</span>)</div><div class="line">CV_rfc.fit(X, Y)</div><div class="line">print(<span class="string">'BEST PARAMETERS:\n'</span>,CV_rfc.best_params_)</div><div class="line">print(<span class="string">'BEST SCORE:\n'</span>,CV_rfc.best_score_)</div><div class="line"><span class="comment">#&gt;&gt;&gt;BEST PARAMETERS:&#123;'criterion': 'entropy', 'n_estimators': 180, 'random_state': 72, 'verbose': 0, 'warm_start': 'False'&#125;</span></div><div class="line"><span class="comment">#&gt;&gt;&gt;BEST SCORE:0.8326181651784338</span></div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Experiment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Experiment </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Recommendation</title>
      <link href="/2018/06/20/DP-Recommendation/"/>
      <url>/2018/06/20/DP-Recommendation/</url>
      <content type="html"><![CDATA[<p><a href="http://delivery.acm.org/10.1145/2880000/2875483/p9-zhu.pdf?ip=167.96.144.64&amp;id=2875483&amp;acc=ACTIVE%20SERVICE&amp;key=A79D83B43E50B5B8%2E43B53F8E9794E57B%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1529527897_36f29615b56fffafec8352e2eb2de1b8" target="_blank" rel="noopener">[2016-Xue Zhu]</a> </p><p>It employs the differential privacy method in the process of recommendation rather than on the data. An advantage of such choice is that it does not generate accumulative error.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[2016-Xue Zhu] Differential Privacy for Collaborative Filtering Recommender Algorithm</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DM-Collaborative Filtering</title>
      <link href="/2018/06/19/DM-Collaborative-Filtering/"/>
      <url>/2018/06/19/DM-Collaborative-Filtering/</url>
      <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yimingsilence/article/details/54934302" target="_blank" rel="noopener">参考</a></p><p>协同过滤推荐算法主要的功能是预测和推荐。算法通过对用户历史行为数据的挖掘发现用户的偏好，基于不同的偏好对用户进行群组划分并推荐品味相似的商品。协同过滤推荐算法分为两类，分别是基于用户的协同过滤算法(user-based collaboratIve filtering)，和基于物品的协同过滤算法(item-based collaborative filtering)。简单的说就是：人以类聚，物以群分。</p><p><a href="https://blog.csdn.net/qq_20282263/article/details/52692318" target="_blank" rel="noopener">code</a></p><a id="more"></a><h1 id="基于用户"><a href="#基于用户" class="headerlink" title="基于用户"></a>基于用户</h1><p>基于用户的协同过滤算法是通过用户的历史行为数据发现用户对商品或内容的喜欢(如商品购买，收藏，内容评论或分享)，并对这些喜好进行度量和打分。根据不同用户对相同商品或内容的态度和偏好程度计算用户之间的关系。在有相同喜好的用户间进行商品推荐。简单的说就是如果A,B两个用户都购买了x,y,z三本图书，并且给出了5星的好评。那么A和B就属于同一类用户。可以将A看过的图书w也推荐给用户B。</p><h2 id="寻找相似用户"><a href="#寻找相似用户" class="headerlink" title="寻找相似用户"></a>寻找相似用户</h2><p>我们模拟了5个用户对两件商品的评分，来说明如何通过用户对不同商品的态度和偏好寻找相似的用户。在示例中，5个用户分别对两件商品进行了评分。这里的分值可能表示真实的购买，也可以是用户对商品不同行为的量化指标。例如，浏览商品的次数，向朋友推荐商品，收藏，分享，或评论等等。这些行为都可以表示用户对商品的态度和偏好程度。</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">商品1</th><th style="text-align:center">商品2</th><th></th><th></th><th></th></tr></thead><tbody><tr><td style="text-align:center">用户A</td><td style="text-align:center">3.3</td><td style="text-align:center">6.5</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">用户B</td><td style="text-align:center">5.8</td><td style="text-align:center">2.6</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">用户C</td><td style="text-align:center">3.6</td><td style="text-align:center">6.3</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">用户D</td><td style="text-align:center">3.4</td><td style="text-align:center">5.8</td><td></td><td></td><td></td></tr><tr><td style="text-align:center">用户E</td><td style="text-align:center">5.2</td><td style="text-align:center">3.1</td><td></td><td></td></tr></tbody></table></div><p>从表格中很难直观发现5个用户间的联系，我们将5个用户对两件商品的评分用散点图表示出来后，用户间的关系就很容易发现了。在散点图中，Y轴是商品1的评分，X轴是商品2的评分，通过用户的分布情况可以发现，A,C,D三个用户距离较近。用户A(3.3 6.5)和用户C(3.6 6.3)，用户D(3.4 5.8)对两件商品的评分较为接近。而用户E和用户B则形成了另一个群体。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/image-20180619224529974.png" alt="image-20180619224529974"></p><p>散点图虽然直观，但无法投入实际的应用，也不能准确的度量用户间的关系。因此我们需要通过数字对用户的关系进行准确的度量，并依据这些关系完成商品的推荐。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>]</div><div class="line"></div><div class="line">data = [[<span class="number">3.3</span>,<span class="number">6.5</span>],[<span class="number">5.8</span>,<span class="number">2.6</span>],[<span class="number">3.6</span>,<span class="number">6.3</span>],[<span class="number">3.4</span>,<span class="number">5.8</span>],[<span class="number">5.2</span>,<span class="number">3.1</span>]]</div><div class="line">x = [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>,<span class="string">'E'</span>]</div><div class="line">df = pd.DataFrame(data,</div><div class="line">                  columns=[<span class="string">'商品1'</span>,<span class="string">'商品2'</span>],</div><div class="line">                  index=x)</div><div class="line">df.plot.scatter(x=<span class="string">'商品1'</span>,y=<span class="string">'商品2'</span>,c=[<span class="string">'r'</span>,<span class="string">'g'</span>,<span class="string">'y'</span>,<span class="string">'b'</span>,<span class="string">'black'</span>])</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</div><div class="line">    plt.text(data[i][<span class="number">0</span>],data[i][<span class="number">1</span>]+<span class="number">0.05</span>,x[i])</div><div class="line">plt.show()</div></pre></td></tr></table></figure></div></div><h3 id="相似度衡量指标"><a href="#相似度衡量指标" class="headerlink" title="相似度衡量指标"></a>相似度衡量指标</h3><h4 id="欧几里得距离"><a href="#欧几里得距离" class="headerlink" title="欧几里得距离"></a>欧几里得距离</h4><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">欧氏距离</th></tr></thead><tbody><tr><td style="text-align:center">用户A-B</td><td style="text-align:center">4.632493928760188</td></tr><tr><td style="text-align:center">用户A-C</td><td style="text-align:center">0.36055512754639923</td></tr><tr><td style="text-align:center">用户A-D</td><td style="text-align:center">0.7071067811865478</td></tr><tr><td style="text-align:center">用户A-E</td><td style="text-align:center">3.8948684188300895</td></tr><tr><td style="text-align:center">用户B-C</td><td style="text-align:center">4.304648650006176</td></tr><tr><td style="text-align:center">用户B-D</td><td style="text-align:center">3.9999999999999996</td></tr><tr><td style="text-align:center">用户B-E</td><td style="text-align:center">0.7810249675906652</td></tr><tr><td style="text-align:center">用户C-D</td><td style="text-align:center">0.5385164807134505</td></tr><tr><td style="text-align:center">用户C-F</td><td style="text-align:center">3.5777087639996634</td></tr><tr><td style="text-align:center">用户D-F</td><td style="text-align:center">3.24499614791759</td></tr></tbody></table></div><p>通过公式我们获得了5个用户相互间的欧几里德系数，也就是用户间的距离。系数越小表示两个用户间的距离越近。可以发现，用户A&amp;C用户A&amp;D和用户C&amp;D距离较近。同时用户B&amp;E的距离也较为接近。与我们前面在散点图中看到的情况一致。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line"></div><div class="line">data = np.array([[3.3,6.5],[5.8,2.6],[3.6,6.3],[3.4,5.8],[5.2,3.1]])</div><div class="line">l = len(data)</div><div class="line">for i in range(l-1):</div><div class="line">    for j in range(i+1,l):</div><div class="line">        dis = np.linalg.norm(data[i]-data[j])</div><div class="line"></div><div class="line"># 4.632493928760188</div><div class="line"># 0.36055512754639923</div><div class="line"># 0.7071067811865478</div><div class="line"># 3.8948684188300895</div><div class="line"># 4.304648650006176</div><div class="line"># 3.9999999999999996</div><div class="line"># 0.7810249675906652</div><div class="line"># 0.5385164807134505</div><div class="line"># 3.5777087639996634</div><div class="line"># 3.24499614791759</div></pre></td></tr></table></figure></div></div><h4 id="皮尔逊相关度"><a href="#皮尔逊相关度" class="headerlink" title="皮尔逊相关度"></a>皮尔逊相关度</h4><p>皮尔逊相关度评价是另一种计算用户间关系的方法。他比欧几里德距离评价的计算要复杂一些，但对于评分数据不规范时皮尔逊相关度评价能够给出更好的结果。以下是一个多用户对多个商品进行评分的示例。这个示例比之前的两个商品的情况要复杂一些，但也更接近真实的情况。我们通过皮尔逊相关度评价对用户进行分组，并推荐商品。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.43.42 PM.png" alt="Screen Shot 2018-06-19 at 10.43.42 PM"></p><p>皮尔逊相关系数的计算公式如下，结果是一个在-1与1之间的系数。该系数用来说明两个用户间联系的强弱程度。</p><p>　　相关系数的分类</p><ol><li>0.8-1.0 极强相关</li><li>0.6-0.8 强相关</li><li>0.4-0.6 中等程度相关</li><li>0.2-0.4 弱相关</li><li>0.0-0.2 极弱相关或无相关</li></ol><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.41.21 PM.png" alt="Screen Shot 2018-06-19 at 10.41.21 PM"></p><p>通过计算5个用户对5件商品的评分我们获得了用户间的相似度数据。这里可以看到用户A&amp;B，C&amp;D，C&amp;E和D&amp;E之间相似度较高。下一步，我们可以依照相似度对用户进行商品推荐。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">data = np.array([[<span class="number">3.3</span>,<span class="number">6.5</span>,<span class="number">2.8</span>,<span class="number">3.4</span>,<span class="number">5.5</span>],[<span class="number">3.5</span>,<span class="number">5.8</span>,<span class="number">3.1</span>,<span class="number">3.6</span>,<span class="number">5.1</span>],</div><div class="line">        [<span class="number">5.6</span>,<span class="number">3.3</span>,<span class="number">4.5</span>,<span class="number">5.2</span>,<span class="number">3.2</span>],[<span class="number">5.4</span>,<span class="number">2.8</span>,<span class="number">4.1</span>,<span class="number">4.9</span>,<span class="number">2.8</span>],</div><div class="line">        [<span class="number">5.2</span>,<span class="number">3.1</span>,<span class="number">4.7</span>,<span class="number">5.3</span>,<span class="number">3.1</span>]])</div><div class="line"></div><div class="line">cof = np.corrcoef(data)</div><div class="line">print(cof)</div><div class="line"><span class="comment"># [[ 1.          0.99977352 -0.84775831 -0.8418164  -0.91523671]</span></div><div class="line"><span class="comment">#  [ 0.99977352  1.         -0.84174116 -0.83531995 -0.90997534]</span></div><div class="line"><span class="comment">#  [-0.84775831 -0.84174116  1.          0.99898728  0.97627199]</span></div><div class="line"><span class="comment">#  [-0.8418164  -0.83531995  0.99898728  1.          0.96978216]</span></div><div class="line"><span class="comment">#  [-0.91523671 -0.90997534  0.97627199  0.96978216  1.        ]]</span></div></pre></td></tr></table></figure></div></div><h2 id="物品推荐计算"><a href="#物品推荐计算" class="headerlink" title="物品推荐计算"></a>物品推荐计算</h2><p>当我们需要对用户C推荐商品时，首先我们检查之前的相似度列表，发现用户C和用户D和E的相似度较高。换句话说这三个用户是一个群体，拥有相同的偏好。因此，我们可以对用户C推荐D和E的商品。但这里有一个问题。我们不能直接推荐前面商品1-商品5的商品。因为这这些商品用户C以及浏览或者购买过了。不能重复推荐。因此我们要推荐用户C还没有浏览或购买过的商品。</p><h3 id="加权排序推荐"><a href="#加权排序推荐" class="headerlink" title="加权排序推荐"></a>加权排序推荐</h3><p>我们提取了用户D和用户E评价过的另外5件商品A—商品F的商品。并对不同商品的评分进行相似度加权。按加权后的结果对5件商品进行排序，然后推荐给用户C。这样，用户C就获得了与他偏好相似的用户D和E评价的商品。而在具体的推荐顺序和展示上我们依照用户D和用户E与用户C的相似度进行排序。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.48.33 PM.png" alt="Screen Shot 2018-06-19 at 10.48.33 PM"></p><blockquote><p>这个算法依靠用户的历史行为数据来计算相关度。也就是说必须要有一定的数据积累(冷启动问题)。对于新网站或数据量较少的网站，还有一种方法是基于物品的协同过滤算法。</p></blockquote><h1 id="基于物品"><a href="#基于物品" class="headerlink" title="基于物品"></a>基于物品</h1><p>基于物品的协同过滤算法与基于用户的协同过滤算法很像，将商品和用户互换。通过计算不同用户对不同物品的评分获得物品间的关系。基于物品间的关系对用户进行相似物品的推荐。这里的评分代表用户对商品的态度和偏好。简单来说就是如果用户A同时购买了商品1和商品2，那么说明商品1和商品2的相关度较高。当用户B也购买了商品1时，可以推断他也有购买商品2的需求。</p><h2 id="寻找相似物品"><a href="#寻找相似物品" class="headerlink" title="寻找相似物品"></a>寻找相似物品</h2><p>表格中是两个用户对5件商品的评分。在这个表格中我们用户和商品的位置进行了互换，通过两个用户的评分来获得5件商品之间的相似度情况。单从表格中我们依然很难发现其中的联系，因此我们选择通过散点图进行展示。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.51.04 PM.png" alt="Screen Shot 2018-06-19 at 10.51.04 PM"></p><p>在散点图中，X轴和Y轴分别是两个用户的评分。5件商品按照所获的评分值分布在散点图中。我们可以发现，商品1,3,4在用户A和B中有着近似的评分，说明这三件商品的相关度较高。而商品5和2则在另一个群体中。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/image-20180620121631981.png" alt="image-20180620121631981"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">data = [[<span class="number">3.3</span>,<span class="number">6.5</span>],[<span class="number">5.8</span>,<span class="number">2.6</span>],[<span class="number">3.6</span>,<span class="number">6.3</span>],[<span class="number">3.4</span>,<span class="number">5.8</span>],[<span class="number">5.2</span>,<span class="number">3.1</span>]]</div><div class="line">x=[<span class="string">'item1'</span>,<span class="string">'item2'</span>,<span class="string">'item3'</span>,<span class="string">'item4'</span>,<span class="string">'item5'</span>]</div><div class="line">pd_data = pd.DataFrame(data,columns=[<span class="string">'user-A'</span>,<span class="string">'user-B'</span>],index=x)</div><div class="line">pd_data.plot.scatter(x=<span class="string">'user-A'</span>,y=<span class="string">'user-B'</span>,c=[<span class="string">'r'</span>,<span class="string">'g'</span>,<span class="string">'y'</span>,<span class="string">'b'</span>,<span class="string">'black'</span>])</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</div><div class="line">    plt.text(data[i][<span class="number">0</span>],data[i][<span class="number">1</span>]+<span class="number">0.05</span>,x[i])</div><div class="line">plt.show()</div></pre></td></tr></table></figure></div></div><h3 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h3><p>在基于物品的协同过滤算法中，我们依然可以使用欧几里德距离评价来计算不同商品间的距离和关系。以下是计算公式。通过欧几里德系数可以发现，商品间的距离和关系与前面散点图中的表现一致，商品1,3,4距离较近关系密切。商品2和商品5距离较近。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.52.16 PM.png" alt="Screen Shot 2018-06-19 at 10.52.16 PM"></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">data = [[<span class="number">3.3</span>,<span class="number">6.5</span>],[<span class="number">5.8</span>,<span class="number">2.6</span>],[<span class="number">3.6</span>,<span class="number">6.3</span>],[<span class="number">3.4</span>,<span class="number">5.8</span>],[<span class="number">5.2</span>,<span class="number">3.1</span>]]</div><div class="line">data = np.array(data)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)<span class="number">-1</span>):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(data)):</div><div class="line">        print(np.linalg.norm(data[i]-data[j]))</div><div class="line"></div><div class="line"><span class="comment"># 4.632493928760188</span></div><div class="line"><span class="comment"># 0.36055512754639923</span></div><div class="line"><span class="comment"># 0.7071067811865478</span></div><div class="line"><span class="comment"># 3.8948684188300895</span></div><div class="line"><span class="comment"># 4.304648650006176</span></div><div class="line"><span class="comment"># 3.9999999999999996</span></div><div class="line"><span class="comment"># 0.7810249675906652</span></div><div class="line"><span class="comment"># 0.5385164807134505</span></div><div class="line"><span class="comment"># 3.5777087639996634</span></div><div class="line"><span class="comment"># 3.24499614791759</span></div></pre></td></tr></table></figure></div></div><h3 id="皮尔逊相关度-1"><a href="#皮尔逊相关度-1" class="headerlink" title="皮尔逊相关度"></a>皮尔逊相关度</h3><p>我们选择使用皮尔逊相关度评价来计算多用户与多商品的关系计算。下面是5个用户对5件商品的评分表。我们通过这些评分计算出商品间的相关度。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.52.50 PM.png" alt="Screen Shot 2018-06-19 at 10.52.50 PM"></p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.53.19 PM.png" alt="Screen Shot 2018-06-19 at 10.53.19 PM"></p><p>通过计算可以发现，商品1&amp;2，商品3&amp;4，商品3&amp;5和商品4&amp;5相似度较高。下一步我们可以依据这些商品间的相关度对用户进行商品推荐。</p><h2 id="基于相似物品推荐"><a href="#基于相似物品推荐" class="headerlink" title="基于相似物品推荐"></a>基于相似物品推荐</h2><p>这里我们遇到了和基于用户进行商品推荐相同的问题，当需要对用户C基于商品3推荐商品时，需要一张新的商品与已有商品间的相似度列表。在前面的相似度计算中，商品3与商品4和商品5相似度较高，因此我们计算并获得了商品4,5与其他商品的相似度列表。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.54.10 PM.png" alt="Screen Shot 2018-06-19 at 10.54.10 PM"> </p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.54.16 PM.png" alt="Screen Shot 2018-06-19 at 10.54.16 PM"></p><p>这里是用户C已经购买过的商品4,5与新商品A,B,C直接的相似程度。我们将用户C对商品4,5的评分作为权重。对商品A,B,C进行加权排序。用户C评分较高并且与之相似度较高的商品被优先推荐。</p><p><img src="/2018/06/19/DM-Collaborative-Filtering/Screen Shot 2018-06-19 at 10.54.25 PM.png" alt="Screen Shot 2018-06-19 at 10.54.25 PM"></p>]]></content>
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python - NumPy</title>
      <link href="/2018/06/15/ML-NumPy/"/>
      <url>/2018/06/15/ML-NumPy/</url>
      <content type="html"><![CDATA[<p>NumPy 是一个 Python 包。 它代表 “Numeric Python”。 它是一个由多维数组对象和用于处理数组的例程集合组成的库。</p><a id="more"></a><h1 id="混淆速记"><a href="#混淆速记" class="headerlink" title="混淆速记"></a>混淆速记</h1><h2 id="np-max-np-maximum"><a href="#np-max-np-maximum" class="headerlink" title="np.max, np.maximum"></a>np.max, np.maximum</h2><p>np.maximum: compare two arrays and returns a new array containing the element-wise maxima</p><p>np.max: return the maximum of an array or maximum along an axis.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">-1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">-5</span>,<span class="number">6</span>]])</div><div class="line">print(np.max(a,axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [3 6]</span></div><div class="line">print(np.maximum(<span class="number">0</span>,a))</div><div class="line"><span class="comment"># [[0,2,3],[4,0,6]]</span></div></pre></td></tr></table></figure><h2 id="np-random-normal-np-ranodm-normn"><a href="#np-random-normal-np-ranodm-normn" class="headerlink" title="np.random.normal, np.ranodm.normn"></a>np.random.normal, np.ranodm.normn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.random.randn(d0, d1, ..., dn)</div></pre></td></tr></table></figure><p>Return a sample (or samples) from the “<strong>standard normal</strong>” distribution.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p>Draw random samples from a <strong>normal (Gaussian) distribution</strong>.</p><h2 id="dot，outer，"><a href="#dot，outer，" class="headerlink" title="dot，outer，*"></a>dot，outer，*</h2><p>dot：矩阵乘法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]]</span></div><div class="line">b = np.array([<span class="number">5</span>,<span class="number">6</span>])</div><div class="line"><span class="comment"># [5 6] shape = (2,1)</span></div><div class="line">print(np.dot(a,b))</div><div class="line"><span class="comment"># [17 39]</span></div></pre></td></tr></table></figure><p>outer：接受两个数组a和b，分别将a和b展开为一维数组，两个for循环遍历a和b，循环内做乘法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]]</span></div><div class="line">b = np.array([<span class="number">5</span>,<span class="number">6</span>])</div><div class="line"><span class="comment"># [5 6] shape = (2,1)</span></div><div class="line">print(np.outer(a,b))</div><div class="line"><span class="comment"># [[ 5  6]</span></div><div class="line"><span class="comment">#  [10 12]</span></div><div class="line"><span class="comment">#  [15 18]</span></div><div class="line"><span class="comment">#  [20 24]]</span></div><div class="line"></div><div class="line"><span class="comment">#####等价于#####</span></div><div class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line">b = [<span class="number">5</span>,<span class="number">6</span>]</div><div class="line">result = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</div><div class="line">    t = []</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> b:</div><div class="line">        t.append(i*j)</div><div class="line">    result.append(t)</div><div class="line">print(result)</div><div class="line"><span class="comment"># [[5, 6], [10, 12], [15, 18], [20, 24]]</span></div></pre></td></tr></table></figure><p>*：对应元素相乘，要求形状相同。</p><h1 id="Ndarray对象"><a href="#Ndarray对象" class="headerlink" title="Ndarray对象"></a>Ndarray对象</h1><p>NumPy 中定义的最重要的对象是称为 <code>ndarray</code> 的 N 维数组类型。 它描述相同类型的元素集合。 可以使用基于零的索引访问集合中的项目。</p><p><code>ndarray</code>中的每个元素在内存中使用相同大小的块。 <code>ndarray</code>中的每个元素是数据类型对象的对象(称为 <code>dtype</code>)。</p><p> 基本的<code>ndarray</code>是使用 NumPy 中的数组函数创建的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.array(object, dtype = <span class="keyword">None</span>, copy = <span class="keyword">True</span>, order = <span class="keyword">None</span>, subok = <span class="keyword">False</span>, ndmin = <span class="number">0</span>)</div></pre></td></tr></table></figure><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-15 at 4.03.50 PM.png" alt="Screen Shot 2018-06-15 at 4.03.50 PM"></p><h2 id="一维数组"><a href="#一维数组" class="headerlink" title="一维数组"></a>一维数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  </div><div class="line"><span class="keyword">print</span> a </div><div class="line"><span class="comment">#[1, 2, 3]</span></div></pre></td></tr></table></figure><h2 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line">a = np.array([[<span class="number">1</span>,  <span class="number">2</span>],  [<span class="number">3</span>,  <span class="number">4</span>]])  </div><div class="line"><span class="keyword">print</span> a</div><div class="line"><span class="comment"># [[1, 2] </span></div><div class="line"><span class="comment"># [3, 4]]</span></div></pre></td></tr></table></figure><h2 id="数组属性"><a href="#数组属性" class="headerlink" title="数组属性"></a>数组属性</h2><h3 id="ndarray-shape"><a href="#ndarray-shape" class="headerlink" title="ndarray.shape"></a>ndarray.shape</h3><p>这一数组属性返回一个包含数组维度的元组，它也可以用于调整数组大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">print(a.shape)</div><div class="line"><span class="comment"># (2, 3)</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">a.shape=(<span class="number">3</span>,<span class="number">2</span>) <span class="comment"># 调整数组大小 </span></div><div class="line">print(a)</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]</span></div><div class="line"><span class="comment">#  [5 6]]</span></div></pre></td></tr></table></figure><p>NumPy 也提供了<code>reshape</code>函数来调整数组大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">b = a.reshape(<span class="number">3</span>,<span class="number">2</span>)</div><div class="line"><span class="keyword">print</span> (b)</div><div class="line"><span class="comment"># # [[1 2]</span></div><div class="line"><span class="comment"># #  [3 4]</span></div><div class="line"><span class="comment"># #  [5 6]]</span></div></pre></td></tr></table></figure><h1 id="数组创建"><a href="#数组创建" class="headerlink" title="数组创建"></a>数组创建</h1><h2 id="numpy-empty"><a href="#numpy-empty" class="headerlink" title="numpy.empty()"></a>numpy.empty()</h2><p>它创建指定形状和<code>dtype</code>的未初始化数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.empty(shape, dtype = float, order = <span class="string">'C'</span>)</div></pre></td></tr></table></figure><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-15 at 4.14.21 PM.png" alt="Screen Shot 2018-06-15 at 4.14.21 PM"></p><blockquote><p>shape参数以”( )”或者“[ ]”传递整数元组</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.empty([<span class="number">3</span>,<span class="number">2</span>], dtype =  int)</div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [[-9223372036854775808  1152930269942935736]</span></div><div class="line"><span class="comment">#  [ 3401169227572248579  7795500255900756322]</span></div><div class="line"><span class="comment">#  [          4298072681      844429423207176]]</span></div></pre></td></tr></table></figure><blockquote><p>数组元素为随机值，因为它们未初始化。</p></blockquote><h2 id="numpy-zeros"><a href="#numpy-zeros" class="headerlink" title="numpy.zeros()"></a>numpy.zeros()</h2><p>返回特定大小，以 0 填充的新数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.zeros(shape, dtype = float, order = <span class="string">'C'</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 含有 5 个 0 的数组，默认类型为 float  </span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line">x = np.zeros(<span class="number">5</span>)  </div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [0. 0. 0. 0. 0.]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.zeros((<span class="number">5</span>,), dtype = np.int)</div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [0 0 0 0 0]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.zeros((<span class="number">5</span>,<span class="number">2</span>), dtype = np.int)</div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [[0 0]</span></div><div class="line"><span class="comment">#  [0 0]</span></div><div class="line"><span class="comment">#  [0 0]</span></div><div class="line"><span class="comment">#  [0 0]</span></div><div class="line"><span class="comment">#  [0 0]]</span></div></pre></td></tr></table></figure><h2 id="numpy-ones"><a href="#numpy-ones" class="headerlink" title="numpy.ones()"></a>numpy.ones()</h2><p>返回特定大小，以 1 填充的新数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.ones(<span class="number">5</span>)</div><div class="line">print(x)</div><div class="line"><span class="comment"># [1. 1. 1. 1. 1.]</span></div><div class="line"></div><div class="line">x = np.ones([<span class="number">2</span>,<span class="number">2</span>], dtype =  int)</div><div class="line">print(x)</div><div class="line"><span class="comment"># [[1 1]</span></div><div class="line"><span class="comment">#  [1 1]]</span></div></pre></td></tr></table></figure><h2 id="numpy-asarray"><a href="#numpy-asarray" class="headerlink" title="numpy.asarray()"></a>numpy.asarray()</h2><p>此函数类似于<code>numpy.array</code>，除了它有较少的参数。 这个例程对于将 Python 序列转换为<code>ndarray</code>非常有用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.asarray(a, dtype = <span class="keyword">None</span>, order = <span class="keyword">None</span>)</div></pre></td></tr></table></figure><blockquote><p><code>a</code> 任意形式的输入参数，比如列表、列表的元组、元组、元组的元组、元组的列表</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 将列表转换为 ndarray </span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line"></div><div class="line">x =  [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>] </div><div class="line">a = np.asarray(x)  </div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [1 2 3]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置了 dtype</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x =  [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">a = np.asarray(x, dtype =  float)</div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [1. 2. 3.]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 来自元组的 ndarray</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x =  (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</div><div class="line">a = np.asarray(x)</div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [1 2 3]</span></div></pre></td></tr></table></figure><h2 id="numpy-fromiter"><a href="#numpy-fromiter" class="headerlink" title="numpy.fromiter()"></a>numpy.fromiter()</h2><p>此函数从任何可迭代对象构建一个<code>ndarray</code>对象，返回一个新的一维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.fromiter(iterable, dtype, count = <span class="number">-1</span>)</div></pre></td></tr></table></figure><p>以下示例展示了如何使用内置的<code>range()</code>函数返回列表对象。 此列表的迭代器用于形成<code>ndarray</code>对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 使用 range 函数创建列表对象  </span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line">list = range(<span class="number">5</span>)  </div><div class="line"><span class="keyword">print</span> (list)</div><div class="line"><span class="comment"># range(0, 5)</span></div><div class="line"></div><div class="line">it = iter(list)</div><div class="line"><span class="comment"># 使用迭代器创建 ndarray</span></div><div class="line">x = np.fromiter(it, dtype =  float)</div><div class="line">print(x)</div><div class="line"><span class="comment"># [0. 1. 2. 3. 4.]</span></div></pre></td></tr></table></figure><h2 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange()"></a>numpy.arange()</h2><p>这个函数返回<code>ndarray</code>对象，包含给定范围内的等间隔值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.arange(start, stop, step, dtype)</div></pre></td></tr></table></figure><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-17 at 10.23.27 AM.png" alt="Screen Shot 2018-06-17 at 10.23.27 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.arange(<span class="number">5</span>)  </div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [0 1 2 3 4]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># 设置了 dtype</span></div><div class="line">x = np.arange(<span class="number">5</span>, dtype =  float)  </div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [0.  1.  2.  3.  4.]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置了起始值和终止值参数  </span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.arange(<span class="number">10</span>,<span class="number">20</span>,<span class="number">2</span>)  </div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [10  12  14  16  18]</span></div></pre></td></tr></table></figure><h2 id="numpy-linspace"><a href="#numpy-linspace" class="headerlink" title="numpy.linspace"></a>numpy.linspace</h2><p>此函数类似于<code>arange()</code>函数。 在此函数中，指定了范围之间的均匀间隔数量，而不是步长。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.linspace(start, stop, num, endpoint, retstep, dtype)</div></pre></td></tr></table></figure><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-17 at 10.27.52 AM.png" alt="Screen Shot 2018-06-17 at 10.27.52 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.linspace(<span class="number">10</span>,<span class="number">20</span>,<span class="number">5</span>)</div><div class="line"><span class="keyword">print</span> (x)</div><div class="line"><span class="comment"># [10.  12.5 15.  17.5 20. ]</span></div><div class="line"></div><div class="line">x = np.linspace(<span class="number">10</span>,<span class="number">20</span>,  <span class="number">5</span>, endpoint =  <span class="keyword">False</span>)  </div><div class="line"><span class="comment"># [10.   12.   14.   16.   18.]</span></div><div class="line"></div><div class="line">x = np.linspace(<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>, retstep =  <span class="keyword">True</span>)</div><div class="line"><span class="comment"># (array([ 1.  ,  1.25,  1.5 ,  1.75,  2.  ]), 0.25)</span></div></pre></td></tr></table></figure><h2 id="numpy-logspace"><a href="#numpy-logspace" class="headerlink" title="numpy.logspace()"></a>numpy.logspace()</h2><p>此函数返回一个<code>ndarray</code>对象，其中包含在对数刻度上均匀分布的数字。 刻度的开始和结束端点是某个底数的幂，通常为 10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.logscale(start, stop, num, endpoint, base, dtype)</div></pre></td></tr></table></figure><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-17 at 10.31.34 AM.png" alt="Screen Shot 2018-06-17 at 10.31.34 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># 默认底数是 10</span></div><div class="line">a = np.logspace(<span class="number">1.0</span>,  <span class="number">2.0</span>, num =  <span class="number">10</span>)</div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [ 10.          12.91549665  16.68100537  21.5443469   27.82559402</span></div><div class="line"><span class="comment">#   35.93813664  46.41588834  59.94842503  77.42636827 100.        ]</span></div><div class="line"></div><div class="line">a = np.logspace(<span class="number">1</span>,<span class="number">10</span>,num =  <span class="number">10</span>,  base  =  <span class="number">2</span>)  </div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [   2.    4.    8.   16.   32.   64.  128.  256.  512. 1024.]</span></div></pre></td></tr></table></figure><h1 id="切片和索引"><a href="#切片和索引" class="headerlink" title="切片和索引"></a>切片和索引</h1><h2 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h2><p><code>ndarray</code>对象的内容可以通过索引或切片来访问和修改, <code>ndarray</code>对象中的元素遵循基于零的索引。 有三种可用的索引方法类型： <strong>字段访问，基本切片</strong>和<strong>高级索引</strong>。</p><p>基本切片是 Python 中基本切片概念到 n 维的扩展。 通过将<code>start</code>，<code>stop</code>和<code>step</code>参数提供给内置的<code>slice</code>函数来构造一个 Python <code>slice</code>对象。 此<code>slice</code>对象被传递给数组来提取数组的一部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.arange(<span class="number">10</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [0 1 2 3 4 5 6 7 8 9]</span></div><div class="line">s = slice(<span class="number">2</span>,<span class="number">7</span>,<span class="number">2</span>)</div><div class="line"><span class="keyword">print</span> (a[s])</div><div class="line"><span class="comment"># [2 4 6]</span></div></pre></td></tr></table></figure><blockquote><p>在上面的例子中，<code>ndarray</code>对象由<code>arange()</code>函数创建。 然后，分别用起始，终止和步长值<code>2</code>，<code>7</code>和<code>2</code>定义切片对象。 当这个切片对象传递给<code>ndarray</code>时，会对它的一部分进行切片，从索引<code>2</code>到<code>7</code>，步长为<code>2</code>。</p></blockquote><p>通过将由冒号分隔的切片参数(<code>start:stop:step</code>)直接提供给<code>ndarray</code>对象，也可以获得相同的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.arange(<span class="number">10</span>)</div><div class="line">b = a[<span class="number">2</span>:<span class="number">7</span>:<span class="number">2</span>]</div><div class="line"><span class="keyword">print</span> (b)</div><div class="line"><span class="comment"># [2 4 6]</span></div></pre></td></tr></table></figure><p>如果只输入一个参数，则将返回与索引对应的单个项目。 如果使用<code>a:</code>，则从该索引向后的所有项目将被提取。 如果使用两个参数(以<code>:</code>分隔)，则对两个索引(不包括停止索引)之间的元素以默认步骤进行切片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 对单个元素进行切片  </span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">10</span>)</div><div class="line">b = a[<span class="number">5</span>]  </div><div class="line"><span class="keyword">print</span> (b)</div><div class="line"><span class="comment"># 5</span></div><div class="line"></div><div class="line"><span class="comment"># 对始于索引的元素进行切片</span></div><div class="line">a = np.arange(<span class="number">10</span>)</div><div class="line"><span class="keyword">print</span> (a[<span class="number">2</span>:])</div><div class="line"><span class="comment"># [2 3 4 5 6 7 8 9]</span></div><div class="line"></div><div class="line"><span class="comment"># 对索引之间的元素进行切片</span></div><div class="line">a = np.arange(<span class="number">10</span>)</div><div class="line"><span class="keyword">print</span> (a[<span class="number">2</span>:<span class="number">5</span>])</div><div class="line"><span class="comment"># [2  3  4]</span></div></pre></td></tr></table></figure><p>上面的描述也可用于多维<code>ndarray</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  </div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [[1 2 3]</span></div><div class="line"><span class="comment">#  [3 4 5]</span></div><div class="line"><span class="comment">#  [4 5 6]]</span></div><div class="line"></div><div class="line"><span class="comment"># 对始于索引的元素进行切片</span></div><div class="line"><span class="keyword">print</span> (a[<span class="number">1</span>:])</div><div class="line"><span class="comment"># [[3 4 5]</span></div><div class="line"><span class="comment">#  [4 5 6]]</span></div></pre></td></tr></table></figure><p>切片还可以包括省略号(<code>...</code>)，来使选择元组的长度与数组的维度相同。 如果在行位置使用省略号，它将返回包含行中元素的<code>ndarray</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  </div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"><span class="comment"># [[1 2 3]</span></div><div class="line"><span class="comment">#  [3 4 5]</span></div><div class="line"><span class="comment">#  [4 5 6]]</span></div><div class="line"></div><div class="line"><span class="comment"># 这会返回第二列元素的数组：</span></div><div class="line"><span class="keyword">print</span> (<span class="string">'第二列的元素是：'</span>)</div><div class="line"><span class="keyword">print</span> (a[...,<span class="number">1</span>])</div><div class="line"><span class="comment"># [2 4 5]</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 现在我们从第二行切片所有元素：</span></div><div class="line"><span class="keyword">print</span>  (<span class="string">'第二行的元素是：'</span> )</div><div class="line"><span class="keyword">print</span> (a[<span class="number">1</span>,...])</div><div class="line"><span class="comment"># [3 4 5]</span></div><div class="line"></div><div class="line"><span class="comment"># 现在我们从第二列向后切片所有元素：</span></div><div class="line"><span class="keyword">print</span>  (<span class="string">'第二列及其剩余元素是：'</span> )</div><div class="line"><span class="keyword">print</span> (a[...,<span class="number">1</span>:])</div><div class="line"><span class="comment"># [[2 3]</span></div><div class="line"><span class="comment">#  [4 5]</span></div><div class="line"><span class="comment">#  [5 6]]</span></div></pre></td></tr></table></figure><h2 id="高级索引"><a href="#高级索引" class="headerlink" title="高级索引"></a>高级索引</h2><p>高级索引始终返回数据的副本。 与此相反，切片只提供了一个视图。</p><p>有两种类型的高级索引：整数和布尔值。</p><h2 id="整数索引"><a href="#整数索引" class="headerlink" title="整数索引"></a>整数索引</h2><p>这种机制有助于基于 N 维索引来获取数组中任意元素。 每个整数数组表示该维度的下标值。 当索引的元素个数就是目标<code>ndarray</code>的维度时，会变得相当直接。</p><p>以下示例获取了<code>ndarray</code>对象中每一行指定列的一个元素。 因此，行索引包含所有行号，列索引指定要选择的元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x = np.array([[<span class="number">1</span>,  <span class="number">2</span>],  [<span class="number">3</span>,  <span class="number">4</span>],  [<span class="number">5</span>,  <span class="number">6</span>]])</div><div class="line">y = x[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],  [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]</div><div class="line"><span class="keyword">print</span> (y)</div><div class="line"><span class="comment"># [1 4 5]</span></div></pre></td></tr></table></figure><blockquote><p>该结果包括数组中<code>(0,0)</code>，<code>(1,1)</code>和<code>(2,0)</code>位置处的元素。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.array([[  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],[  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],[  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],[  <span class="number">9</span>,  <span class="number">10</span>,  <span class="number">11</span>]])</div><div class="line">print(x)</div><div class="line"><span class="comment"># [[ 0  1  2]</span></div><div class="line"><span class="comment">#  [ 3  4  5]</span></div><div class="line"><span class="comment">#  [ 6  7  8]</span></div><div class="line"><span class="comment">#  [ 9 10 11]]</span></div><div class="line">rows = np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">3</span>]])</div><div class="line">cols = np.array([[<span class="number">0</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">2</span>]])</div><div class="line">z = [rows,cols]</div><div class="line">print(z)</div><div class="line"><span class="comment"># [array([[0, 0],[3, 3]]),</span></div><div class="line"><span class="comment">#  array([[0, 2],[0, 2]])]</span></div><div class="line">y = x[rows,cols]</div><div class="line">print(y)</div><div class="line"><span class="comment"># [[ 0  2]</span></div><div class="line"><span class="comment">#  [ 9 11]]</span></div><div class="line">w = x[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>]]</div><div class="line">print(w)</div><div class="line"><span class="comment"># [ 0  2  9 11]</span></div></pre></td></tr></table></figure><blockquote><p>示例获取了 4X3 数组中的每个角处的元素。 行索引是<code>[0,0]</code>和<code>[3,3]</code>，而列索引是<code>[0,2]</code>和<code>[0,2]</code>。</p></blockquote><p>高级和基本索引可以通过使用切片<code>:</code>或省略号<code>...</code>与索引数组组合。 以下示例使用<code>slice</code>作为列索引和高级索引。 当切片用于两者时，结果是相同的。 但高级索引会导致复制，并且可能有不同的内存布局。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.array([[  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],[  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],[  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],[  <span class="number">9</span>,  <span class="number">10</span>,  <span class="number">11</span>]])</div><div class="line">print(x)</div><div class="line"><span class="comment"># [[ 0  1  2]</span></div><div class="line"><span class="comment">#  [ 3  4  5]</span></div><div class="line"><span class="comment">#  [ 6  7  8]</span></div><div class="line"><span class="comment">#  [ 9 10 11]]</span></div><div class="line"></div><div class="line"><span class="comment"># 切片</span></div><div class="line">z = x[<span class="number">1</span>:<span class="number">4</span>,<span class="number">1</span>:<span class="number">3</span>]</div><div class="line">print(z)</div><div class="line"><span class="comment"># [[ 4  5]</span></div><div class="line"><span class="comment">#  [ 7  8]</span></div><div class="line"><span class="comment">#  [10 11]]</span></div><div class="line"></div><div class="line"><span class="comment"># 对列使用高级索引</span></div><div class="line">y = x[<span class="number">1</span>:<span class="number">4</span>,[<span class="number">1</span>,<span class="number">2</span>]]</div><div class="line">print(y)</div><div class="line"><span class="comment"># [[ 4  5]</span></div><div class="line"><span class="comment">#  [ 7  8]</span></div><div class="line"><span class="comment">#  [10 11]]</span></div></pre></td></tr></table></figure><h2 id="布尔索引"><a href="#布尔索引" class="headerlink" title="布尔索引"></a>布尔索引</h2><p>当结果对象是布尔运算(例如比较运算符)的结果时，将使用此类型的高级索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.array([[  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],[  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],[  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],[  <span class="number">9</span>,  <span class="number">10</span>,  <span class="number">11</span>]])</div><div class="line">print(x)</div><div class="line"><span class="comment"># [[ 0  1  2]</span></div><div class="line"><span class="comment">#  [ 3  4  5]</span></div><div class="line"><span class="comment">#  [ 6  7  8]</span></div><div class="line"><span class="comment">#  [ 9 10 11]]</span></div><div class="line"></div><div class="line"><span class="comment"># 现在我们会打印出大于 5 的元素</span></div><div class="line"><span class="keyword">print</span>  (<span class="string">'大于 5 的元素是：'</span>  )</div><div class="line"><span class="keyword">print</span> (x[x &gt;  <span class="number">5</span>])</div><div class="line"><span class="comment"># 大于 5 的元素是：</span></div><div class="line"><span class="comment"># [ 6  7  8  9 10 11]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([np.nan,  <span class="number">1</span>,<span class="number">2</span>,np.nan,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line"><span class="keyword">print</span> (a[~np.isnan(a)])</div><div class="line"><span class="comment"># [1. 2. 3. 4. 5.]</span></div></pre></td></tr></table></figure><blockquote><p>这个例子使用了<code>~</code>(取补运算符)来过滤<code>NaN</code>。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">1</span>,  <span class="number">2</span>+<span class="number">6j</span>,  <span class="number">5</span>,  <span class="number">3.5</span>+<span class="number">5j</span>])</div><div class="line"><span class="keyword">print</span> (a[np.iscomplex(a)])</div><div class="line"><span class="comment"># [2. +6.j 3.5+5.j]</span></div></pre></td></tr></table></figure><blockquote><p>示例显示如何从数组中过滤掉非复数元素。</p></blockquote><h1 id="NumPy广播"><a href="#NumPy广播" class="headerlink" title="NumPy广播"></a>NumPy广播</h1><p><strong>广播</strong>是指 NumPy 在算术运算期间处理不同形状的数组的能力。 对数组的算术运算通常在相应的元素上进行。 如果两个阵列具有完全相同的形状，则这些操作被无缝执行。</p><p>如果两个数组的维数不相同，则元素到元素的操作是不可能的。 然而，在 NumPy 中仍然可以对形状不相似的数组进行操作，因为它拥有广播功能。 较小的数组会<strong>广播</strong>到较大数组的大小，以便使它们的形状可兼容。</p><h2 id="Scalar-and-One-Dimensional-Array"><a href="#Scalar-and-One-Dimensional-Array" class="headerlink" title="Scalar and One-Dimensional Array"></a>Scalar and One-Dimensional Array</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line">b = <span class="number">2</span></div><div class="line">c = a+b</div><div class="line"><span class="comment"># c = [1+2,2+2,3+2]=[3,4,5]</span></div></pre></td></tr></table></figure><h2 id="Scalar-and-Two-Dimensional-Array"><a href="#Scalar-and-Two-Dimensional-Array" class="headerlink" title="Scalar and Two-Dimensional Array"></a>Scalar and Two-Dimensional Array</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">b = <span class="number">2</span></div><div class="line">c = a+b</div><div class="line"><span class="comment"># [[1+2,2+2,3+2],[4+2,5+2,6+2]]=[[3,4,5],[6,7,8]]</span></div></pre></td></tr></table></figure><h2 id="One-Dimensional-and-Two-Dimensional-Arrays"><a href="#One-Dimensional-and-Two-Dimensional-Arrays" class="headerlink" title="One-Dimensional and Two-Dimensional Arrays"></a>One-Dimensional and Two-Dimensional Arrays</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">b = np.array([<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>])</div><div class="line">c = a+b</div><div class="line"><span class="comment"># [[0,0,0],[3,3,3]]</span></div></pre></td></tr></table></figure><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Element-wise operations on arrays are only valid when the arrays’ shapes are either equal or compatible. The equal shapes case is trivial - this is the stretched array from the example above. What does “compatible” mean, though?</p><p>To determine if two shapes are compatible, Numpy compares their dimensions, starting with the trailing ones and working its way backwards. <strong>If two dimensions are equal, or if one of them equals 1, the comparison continues.</strong> Otherwise, you’ll see a <code>ValueError</code> raised (saying something like “operands could not be broadcast together with shapes …”). When one of the shapes runs out of dimensions (because it has less dimensions than the other shape), Numpy will use 1 in the comparison process until the other shape’s dimensions run out as well. Once Numpy determines that two shapes are compatible, the shape of the result is simply the maximum of the two shapes’ sizes in each dimension.</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>The definition above is precise and complete; to get a feel for it, we’ll need a few examples.</p><p>I’m using the Numpy convention of describing shapes as tuples. <code>macros</code> is a 4-by-3 array, meaning that it has 4 rows with 3 columns each, or 4x3. The Numpy way of describing the shape of <code>macros</code> is <code>(4, 3)</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">80</span>]: macros.shape</div><div class="line">Out[<span class="number">80</span>]: (<span class="number">4</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>When we computed the caloric table using broadcasting, what we did was an operation between <code>macros</code>- a <code>(4, 3)</code> array, and <code>cal_per_macro</code>, a <code>(3,)</code> array <a href="https://eli.thegreenplace.net/2015/broadcasting-arrays-in-numpy/#id9" target="_blank" rel="noopener">[4]</a>. Therefore, following the broadcasting rules outlined above, the shape <code>(3,)</code> is left-padded with 1 to make comparison with <code>(4, 3)</code> possible. The shapes are then deemed compatible and the result shape is <code>(4, 3)</code>, which is exactly what we observed.</p><p>Schematically:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="number">4</span>, <span class="number">3</span>)                   (<span class="number">4</span>, <span class="number">3</span>)</div><div class="line">         == padding ==&gt;          == result ==&gt; (4, 3)</div><div class="line">(<span class="number">3</span>,)                     (<span class="number">1</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>Here’s another example, broadcasting between a 3-D and a 1-D array:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="number">3</span>,)                       (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</div><div class="line">           == padding ==&gt;             == result ==&gt; (5, 4, 3)</div><div class="line">(<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)                  (<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>Note, however, that only left-padding with 1s is allowed. Therefore:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="number">5</span>,)                       (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>)</div><div class="line">           == padding ==&gt;             ==&gt; error (5 != 3)</div><div class="line">(<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)                  (<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>Theoretically, had the broadcasting rules been less rigid - we could say that this broadcasting is valid if we <em>right-pad</em> <code>(5,)</code> with 1s. However, this is not how the rules are defined - therefore these shapes are incompatible.</p><p>Broadcasting is valid between higher-dimensional arrays too:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)                     (<span class="number">1</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</div><div class="line">              == padding ==&gt;                == result ==&gt; (6, 5, 4, 3)</div><div class="line">(<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)                  (<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>Also, in the beginning of the article I mentioned that broadcasting does not necessarily occur between arrays of different number of dimensions. It’s perfectly valid to broadcast arrays with the same number of dimensions, as long as they are compatible:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(<span class="number">5</span>, <span class="number">4</span>, <span class="number">1</span>)</div><div class="line">           == no padding needed ==&gt; result ==&gt; (5, 4, 3)</div><div class="line">(<span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><h1 id="NumPy数组迭代"><a href="#NumPy数组迭代" class="headerlink" title="NumPy数组迭代"></a>NumPy数组迭代</h1><p>NumPy 包包含一个迭代器对象<code>numpy.nditer</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">0</span>,<span class="number">60</span>,<span class="number">5</span>)</div><div class="line">a = a.reshape(<span class="number">3</span>,<span class="number">4</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 0  5 10 15]</span></div><div class="line"><span class="comment">#  [20 25 30 35]</span></div><div class="line"><span class="comment">#  [40 45 50 55]]</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> np.nditer(a):</div><div class="line">    print(ele)</div><div class="line"><span class="comment"># 0 5 10 15 20 25 30 35 40 45 50 55</span></div></pre></td></tr></table></figure><h2 id="迭代顺序"><a href="#迭代顺序" class="headerlink" title="迭代顺序"></a>迭代顺序</h2><p><strong>F/C order</strong></p><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-18 at 3.53.25 PM.png" alt="Screen Shot 2018-06-18 at 3.53.25 PM"></p><p><strong>修改数组的值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">0</span>,<span class="number">60</span>,<span class="number">5</span>)</div><div class="line">a = a.reshape(<span class="number">3</span>,<span class="number">4</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 0  5 10 15]</span></div><div class="line"><span class="comment">#  [20 25 30 35]</span></div><div class="line"><span class="comment">#  [40 45 50 55]]</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> np.nditer(a, op_flags=[<span class="string">'readwrite'</span>]):</div><div class="line">    ele[...] = <span class="number">2</span>*ele</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[  0  10  20  30]</span></div><div class="line"><span class="comment">#  [ 40  50  60  70]</span></div><div class="line"><span class="comment">#  [ 80  90 100 110]]</span></div></pre></td></tr></table></figure><p><strong>外部循环</strong></p><p><code>nditer</code>类的构造器拥有<code>flags</code>参数，如果其值是<code>external_loop</code> 给出的值是具有多个值的一维数组，而不是零维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">0</span>,<span class="number">60</span>,<span class="number">5</span>)</div><div class="line">a = a.reshape(<span class="number">3</span>,<span class="number">4</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 0  5 10 15]</span></div><div class="line"><span class="comment">#  [20 25 30 35]</span></div><div class="line"><span class="comment">#  [40 45 50 55]]</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> np.nditer(a, flags=[<span class="string">'external_loop'</span>],order=<span class="string">'F'</span>):</div><div class="line">    print(ele)</div><div class="line"><span class="comment"># [ 0 20 40]</span></div><div class="line"><span class="comment"># [ 5 25 45]</span></div><div class="line"><span class="comment"># [10 30 50]</span></div><div class="line"><span class="comment"># [15 35 55]</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> np.nditer(a, flags=[<span class="string">'external_loop'</span>]):</div><div class="line">    print(ele)</div><div class="line"><span class="comment"># [ 0  5 10 15 20 25 30 35 40 45 50 55]</span></div></pre></td></tr></table></figure><h1 id="NumPy数组操作"><a href="#NumPy数组操作" class="headerlink" title="NumPy数组操作"></a>NumPy数组操作</h1><p>提供了几个处理<code>ndarray</code>对象中的元素的方法</p><h2 id="修改数组形状"><a href="#修改数组形状" class="headerlink" title="修改数组形状"></a>修改数组形状</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-18 at 4.01.08 PM.png" alt="Screen Shot 2018-06-18 at 4.01.08 PM"></p><h3 id="numpy-reshape"><a href="#numpy-reshape" class="headerlink" title="numpy.reshape"></a>numpy.reshape</h3><p>这个函数在不改变数据的条件下修改形状，它接受如下参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.reshape(arr, newshape, order&apos;)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：要修改形状的数组</li><li><code>newshape</code>：整数或者整数数组，新的形状应当兼容原有形状</li><li><code>order</code>：<code>&#39;C&#39;</code>为 C 风格顺序，<code>&#39;F&#39;</code>为 F 风格顺序，<code>&#39;A&#39;</code>为保留原顺序。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">8</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [0 1 2 3 4 5 6 7]</span></div><div class="line"></div><div class="line">b = a.reshape(<span class="number">4</span>,<span class="number">2</span>)</div><div class="line">print(b)</div><div class="line"><span class="comment"># [[0 1]</span></div><div class="line"><span class="comment">#  [2 3]</span></div><div class="line"><span class="comment">#  [4 5]</span></div><div class="line"><span class="comment">#  [6 7]]</span></div></pre></td></tr></table></figure><h3 id="numpy-ndarray-flat"><a href="#numpy-ndarray-flat" class="headerlink" title="numpy.ndarray.flat"></a>numpy.ndarray.flat</h3><p>该函数返回数组上的一维迭代器，行为类似 Python 内建的迭代器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>]).reshape(<span class="number">4</span>,<span class="number">2</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 2  4]</span></div><div class="line"><span class="comment">#  [ 6  8]</span></div><div class="line"><span class="comment">#  [10 12]</span></div><div class="line"><span class="comment">#  [14 16]]</span></div><div class="line"></div><div class="line">print(a.flat[<span class="number">4</span>])</div><div class="line"><span class="comment"># 10</span></div></pre></td></tr></table></figure><h3 id="numpy-ndarray-flatten"><a href="#numpy-ndarray-flatten" class="headerlink" title="numpy.ndarray.flatten"></a>numpy.ndarray.flatten</h3><p>该函数返回折叠为一维的数组副本，函数接受下列参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ndarray.flatten(order)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><p><code>order</code>：<code>&#39;C&#39;</code> — 按行，<code>&#39;F&#39;</code> — 按列，<code>&#39;A&#39;</code> — 原顺序，<code>&#39;k&#39;</code> — 元素在内存中的出现顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>]).reshape(<span class="number">4</span>,<span class="number">2</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 2  4]</span></div><div class="line"><span class="comment">#  [ 6  8]</span></div><div class="line"><span class="comment">#  [10 12]</span></div><div class="line"><span class="comment">#  [14 16]]</span></div><div class="line"></div><div class="line">print(a.flatten())</div><div class="line"><span class="comment"># [ 2  4  6  8 10 12 14 16]</span></div><div class="line">print(a.flatten(order=<span class="string">'F'</span>))</div><div class="line"><span class="comment"># [ 2  6 10 14  4  8 12 16]</span></div></pre></td></tr></table></figure></li></ul><h3 id="numpy-ravel"><a href="#numpy-ravel" class="headerlink" title="numpy.ravel"></a>numpy.ravel</h3><p>这个函数返回展开的一维数组，并且按需生成副本。返回的数组和输入数组拥有相同数据类型。这个函数接受两个参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.ravel(a, order)</div></pre></td></tr></table></figure><p>构造器接受下列参数：</p><ul><li><code>order</code>：<code>&#39;C&#39;</code> — 按行，<code>&#39;F&#39;</code> — 按列，<code>&#39;A&#39;</code> — 原顺序，<code>&#39;k&#39;</code> — 元素在内存中的出现顺序。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>]).reshape(<span class="number">4</span>,<span class="number">2</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 2  4]</span></div><div class="line"><span class="comment">#  [ 6  8]</span></div><div class="line"><span class="comment">#  [10 12]</span></div><div class="line"><span class="comment">#  [14 16]]</span></div><div class="line"></div><div class="line">print(np.ravel(a))</div><div class="line"><span class="comment"># [ 2  4  6  8 10 12 14 16]</span></div><div class="line">print(np.ravel(a,order=<span class="string">'F'</span>))</div><div class="line"><span class="comment"># [ 2  6 10 14  4  8 12 16]</span></div></pre></td></tr></table></figure><h2 id="数组翻转"><a href="#数组翻转" class="headerlink" title="数组翻转"></a>数组翻转</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-18 at 4.11.26 PM.png" alt="Screen Shot 2018-06-18 at 4.11.26 PM"></p><h3 id="numpy-transpose"><a href="#numpy-transpose" class="headerlink" title="numpy.transpose()"></a>numpy.transpose()</h3><p>这个函数翻转给定数组的维度。如果可能的话它会返回一个视图。函数接受下列参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.transpose(arr, axes)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：要转置的数组</li><li><code>axes</code>：整数的列表，对应维度，通常所有维度都会翻转。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>]).reshape(<span class="number">4</span>,<span class="number">2</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 2  4]</span></div><div class="line"><span class="comment">#  [ 6  8]</span></div><div class="line"><span class="comment">#  [10 12]</span></div><div class="line"><span class="comment">#  [14 16]]</span></div><div class="line"></div><div class="line">print(np.transpose(a))</div><div class="line"><span class="comment"># [[ 2  6 10 14]</span></div><div class="line"><span class="comment">#  [ 4  8 12 16]]</span></div></pre></td></tr></table></figure><h3 id="numpy-ndarray-T"><a href="#numpy-ndarray-T" class="headerlink" title="numpy.ndarray.T"></a>numpy.ndarray.T</h3><p>该函数属于<code>ndarray</code>类，行为类似于<code>numpy.transpose</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>]).reshape(<span class="number">4</span>,<span class="number">2</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 2  4]</span></div><div class="line"><span class="comment">#  [ 6  8]</span></div><div class="line"><span class="comment">#  [10 12]</span></div><div class="line"><span class="comment">#  [14 16]]</span></div><div class="line"></div><div class="line">print(a.T)</div><div class="line"><span class="comment"># [[ 2  6 10 14]</span></div><div class="line"><span class="comment">#  [ 4  8 12 16]]</span></div></pre></td></tr></table></figure><h3 id="numpy-rollaxis"><a href="#numpy-rollaxis" class="headerlink" title="numpy.rollaxis()"></a>numpy.rollaxis()</h3><p>该函数向后滚动特定的轴，直到一个特定位置。这个函数接受三个参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.rollaxis(arr, axis, start)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>axis</code>：要向后滚动的轴，其它轴的相对位置不会改变</li><li><code>start</code>：默认为零，表示完整的滚动。会滚动到特定位置。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#有些数据集的图片默认图片格式是[chanel][height][width], 然而有些地方显示图片是[height][width][chanel], 这就需要改变一下图片轴的次序。</span></div><div class="line"><span class="comment"># old_img是3*32*32的，new_img是32*32*3的</span></div><div class="line">newimg = np.rollaxis(old_img, <span class="number">0</span>, <span class="number">3</span>)  <span class="comment"># 把轴0放到轴3的位置</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">8</span>)</div><div class="line">a = a.reshape(<span class="number">2</span>,<span class="number">4</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[0 1 2 3]</span></div><div class="line"><span class="comment">#  [4 5 6 7]]</span></div><div class="line"></div><div class="line"><span class="comment">#将轴1滚动到轴0，(2,4)-&gt;(4,2)</span></div><div class="line">print(np.rollaxis(a,<span class="number">1</span>))</div><div class="line"><span class="comment"># [[0 4]</span></div><div class="line"><span class="comment">#  [1 5]</span></div><div class="line"><span class="comment">#  [2 6]</span></div><div class="line"><span class="comment">#  [3 7]]</span></div><div class="line"></div><div class="line"><span class="comment">#即轴1滚动到轴1，不变</span></div><div class="line">print(np.rollaxis(a,<span class="number">1</span>,<span class="number">1</span>))</div><div class="line"><span class="comment"># [[0 1 2 3]</span></div><div class="line"><span class="comment">#  [4 5 6 7]]</span></div></pre></td></tr></table></figure><h3 id="numpy-swapaxes"><a href="#numpy-swapaxes" class="headerlink" title="numpy.swapaxes()"></a>numpy.swapaxes()</h3><p>该函数交换数组的两个轴。对于 1.10 之前的 NumPy 版本，会返回交换后数组的试图。这个函数接受下列参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.swapaxes(arr, axis1, axis2)</div></pre></td></tr></table></figure><ul><li><code>arr</code>：要交换其轴的输入数组</li><li><code>axis1</code>：对应第一个轴的整数</li><li><p><code>axis2</code>：对应第二个轴的整数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.arange(<span class="number">16</span>)</div><div class="line">a = a.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[[ 0  1  2  3]</span></div><div class="line"><span class="comment">#   [ 4  5  6  7]]</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#  [[ 8  9 10 11]</span></div><div class="line"><span class="comment">#   [12 13 14 15]]]</span></div><div class="line"></div><div class="line"><span class="comment">#就是将第三个维度和第二个维度交换,</span></div><div class="line"><span class="comment"># 数字7来说，之前的索引是(0,1,3),那么交换之后，就应该是(0,3,1)</span></div><div class="line">print(np.swapaxes(a,<span class="number">2</span>,<span class="number">1</span>))</div><div class="line"><span class="comment"># [[[ 0  4]</span></div><div class="line"><span class="comment">#   [ 1  5]</span></div><div class="line"><span class="comment">#   [ 2  6]</span></div><div class="line"><span class="comment">#   [ 3  7]]</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#  [[ 8 12]</span></div><div class="line"><span class="comment">#   [ 9 13]</span></div><div class="line"><span class="comment">#   [10 14]</span></div><div class="line"><span class="comment">#   [11 15]]]</span></div></pre></td></tr></table></figure></li></ul><h2 id="修改维度"><a href="#修改维度" class="headerlink" title="修改维度"></a>修改维度</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-19 at 3.32.30 PM.png" alt="Screen Shot 2018-06-19 at 3.32.30 PM"></p><h3 id="numpy-expand-dims"><a href="#numpy-expand-dims" class="headerlink" title="numpy.expand_dims()"></a>numpy.expand_dims()</h3><p>函数通过在指定位置插入新的轴来扩展数组形状。该函数需要两个参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.expand_dims(arr, axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>axis</code>：新轴插入的位置</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>])</div><div class="line">print(x,x.shape)</div><div class="line"><span class="comment"># [1 2],(2,)</span></div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line">y = np.expand_dims(x,axis=<span class="number">0</span>)</div><div class="line">print(y,y.shape)</div><div class="line"><span class="comment"># [[1 2]],(1, 2)</span></div><div class="line"></div><div class="line">y = np.expand_dims(x,axis=<span class="number">1</span>)</div><div class="line">print(y,y.shape)</div><div class="line"><span class="comment"># [[1]</span></div><div class="line"><span class="comment">#  [2]]  (2, 1)</span></div></pre></td></tr></table></figure><h3 id="numpy-squeeze"><a href="#numpy-squeeze" class="headerlink" title="numpy.squeeze()"></a>numpy.squeeze()</h3><p>函数从给定数组的形状中删除一维条目,即shape=(x,..,1,…y),那么就会变成（x,.,y）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.squeeze(arr, axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>axis</code>：整数或整数元组，用于选择形状中单一维度条目的子集</li></ul><h2 id="数组连接"><a href="#数组连接" class="headerlink" title="数组连接"></a>数组连接</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-19 at 3.43.35 PM.png" alt="Screen Shot 2018-06-19 at 3.43.35 PM"></p><h3 id="numpy-concatenate"><a href="#numpy-concatenate" class="headerlink" title="numpy.concatenate()"></a>numpy.concatenate()</h3><p>数组的连接是指连接。 此函数用于沿指定轴连接相同形状的两个或多个数组。 该函数接受以下参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.concatenate((a1, a2, ...), axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>a1, a2, ...</code>：相同类型的数组序列</li><li><code>axis</code>：沿着它连接数组的轴，默认为 0，即添加在下方；axis=1使添加在右方</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]]</span></div><div class="line"></div><div class="line">b = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</div><div class="line">print(b)</div><div class="line"><span class="comment"># [[5 6]</span></div><div class="line"><span class="comment">#  [7 8]]</span></div><div class="line"></div><div class="line">print(np.concatenate((a,b)))</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]</span></div><div class="line"><span class="comment">#  [5 6]</span></div><div class="line"><span class="comment">#  [7 8]]</span></div><div class="line"></div><div class="line">print(np.concatenate((a,b),axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [[1 2 5 6]</span></div><div class="line"><span class="comment">#  [3 4 7 8]]</span></div></pre></td></tr></table></figure><h3 id="numpy-stack"><a href="#numpy-stack" class="headerlink" title="numpy.stack()"></a>numpy.stack()</h3><p>此函数沿新轴连接数组序列。 此功能添加自 NumPy 版本 1.10.0。 需要提供以下参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.stack(arrays, axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arrays</code>：相同形状的数组序列</li><li><code>axis</code>：返回数组中的轴，输入数组沿着它来堆叠</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line"></div><div class="line">b = np.array([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</div><div class="line"></div><div class="line">print(np.stack((a,b)))</div><div class="line"><span class="comment"># [[1 2 3]</span></div><div class="line"><span class="comment">#  [4 5 6]]</span></div><div class="line"><span class="comment"># shape = (2,3)</span></div><div class="line"></div><div class="line">print(np.stack((a,b),axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [[1 4]</span></div><div class="line"><span class="comment">#  [2 5]</span></div><div class="line"><span class="comment">#  [3 6]]</span></div><div class="line"><span class="comment"># shape = (3,2)</span></div></pre></td></tr></table></figure><h2 id="数组分割"><a href="#数组分割" class="headerlink" title="数组分割"></a>数组分割</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-19 at 3.50.55 PM.png" alt="Screen Shot 2018-06-19 at 3.50.55 PM"></p><h3 id="numpy-split"><a href="#numpy-split" class="headerlink" title="numpy.split()"></a>numpy.split()</h3><p>该函数沿特定的轴将数组分割为子数组。函数接受三个参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.split(ary, indices_or_sections, axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>ary</code>：被分割的输入数组</li><li><code>indices_or_sections</code>：可以是整数，表明要从输入数组创建的，等大小的子数组的数量。 如果此参数是一维数组，则其元素表明要创建新子数组的点。</li><li><code>axis</code>：默认为 0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.arange(<span class="number">9</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">'第一个数组：'</span>)</div><div class="line"><span class="keyword">print</span> (a)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">'将数组分为三个大小相等的子数组：'</span>)</div><div class="line">b = np.split(a,<span class="number">3</span>)</div><div class="line"><span class="keyword">print</span> (b)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">'将数组在一维数组中表明的位置分割：'</span>)</div><div class="line">b = np.split(a,[<span class="number">4</span>,<span class="number">7</span>])</div><div class="line"><span class="keyword">print</span> (b)</div><div class="line"></div><div class="line"><span class="comment"># 第一个数组：</span></div><div class="line"><span class="comment"># [0 1 2 3 4 5 6 7 8]</span></div><div class="line"><span class="comment"># 将数组分为三个大小相等的子数组：</span></div><div class="line"><span class="comment"># [array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]</span></div><div class="line"><span class="comment"># 将数组在一维数组中表明的位置分割：</span></div><div class="line"><span class="comment"># [array([0, 1, 2, 3]), array([4, 5, 6]), array([7, 8])]</span></div></pre></td></tr></table></figure><blockquote><p>如果之前的shape是(x,y)，那么分割之后是(x/2 , y) 或者 (x , y/2)</p></blockquote><h2 id="元素添加-删除"><a href="#元素添加-删除" class="headerlink" title="元素添加/删除"></a>元素添加/删除</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-20 at 9.10.44 AM.png" alt="Screen Shot 2018-06-20 at 9.10.44 AM"></p><h3 id="numpy-resize"><a href="#numpy-resize" class="headerlink" title="numpy.resize()"></a>numpy.resize()</h3><p>此函数返回指定大小的新数组。 如果新大小大于原始大小，则包含原始数组中的元素的重复副本。 该函数接受以下参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.resize(arr, shape)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：要修改大小的输入数组</li><li><code>shape</code>：返回数组的新形状</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[1 2 3]</span></div><div class="line"><span class="comment">#  [4 5 6]]</span></div><div class="line"></div><div class="line">b = np.resize(a,(<span class="number">3</span>,<span class="number">2</span>))<span class="comment">#效果等于reshape</span></div><div class="line">print(b)</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]</span></div><div class="line"><span class="comment">#  [5 6]]</span></div><div class="line"></div><div class="line">b = np.resize(a,(<span class="number">3</span>,<span class="number">5</span>))<span class="comment">#不能用reshape</span></div><div class="line">print(b)</div><div class="line"><span class="comment"># [[1 2 3 4 5]</span></div><div class="line"><span class="comment">#  [6 1 2 3 4]</span></div><div class="line"><span class="comment">#  [5 6 1 2 3]]</span></div></pre></td></tr></table></figure><h3 id="numpy-append"><a href="#numpy-append" class="headerlink" title="numpy.append()"></a>numpy.append()</h3><p>此函数在输入数组的末尾添加值。 附加操作不是原地的，而是分配新的数组。 此外，输入数组的维度必须匹配否则将生成<code>ValueError</code>。</p><p>函数接受下列函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.append(arr, values, axis)</div></pre></td></tr></table></figure><p> 其中：</p><ul><li><p><code>arr</code>：输入数组</p></li><li><p><code>values</code>：要向<code>arr</code>添加的值，比如和<code>arr</code>形状相同(除了要添加的轴)</p></li><li><p><code>axis</code>：沿着它完成操作的轴。如果没有提供，两个参数都会被展开。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[1 2 3]</span></div><div class="line"><span class="comment">#  [4 5 6]]</span></div><div class="line"></div><div class="line"><span class="comment">#没有提供axis，则展开所有轴</span></div><div class="line">print(np.append(a,[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]))</div><div class="line"><span class="comment"># [1 2 3 4 5 6 7 8 9]</span></div><div class="line"></div><div class="line"><span class="comment">#添加数组维度必须原来的匹配，此例中原来数组是shape=(2,3),则axis=0，那么列的维度相同，shape=(x,3)</span></div><div class="line">print(np.append(a,[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]],axis=<span class="number">0</span>))</div><div class="line"><span class="comment"># [[1 2 3]</span></div><div class="line"><span class="comment">#  [4 5 6]</span></div><div class="line"><span class="comment">#  [7 8 9]</span></div><div class="line"><span class="comment">#  [5 5 5]</span></div><div class="line"><span class="comment">#  [1 2 3]]</span></div><div class="line"></div><div class="line"><span class="comment">#axis=1,则竖向添加，行的维度相同，shape=(2,x)</span></div><div class="line">print(np.append(a,[[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]],axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [[ 1  2  3  5  5  5  5]</span></div><div class="line"><span class="comment">#  [ 4  5  6  7  8  9 10]]</span></div></pre></td></tr></table></figure></li></ul><h3 id="numpy-insert"><a href="#numpy-insert" class="headerlink" title="numpy.insert()"></a>numpy.insert()</h3><p>此函数在给定索引之前，沿给定轴在输入数组中插入值。 如果值的类型转换为要插入，则它与输入数组不同。 插入没有原地的，函数会返回一个新数组。 此外，如果未提供轴，则输入数组会被展开。</p><p><code>insert()</code>函数接受以下参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.insert(arr, obj, values, axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>obj</code>：在其之前插入值的索引</li><li><code>values</code>：要插入的值</li><li><code>axis</code>：沿着它插入的轴，如果未提供，则输入数组会被展开</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[1 2]</span></div><div class="line"><span class="comment">#  [3 4]</span></div><div class="line"><span class="comment">#  [5 6]]</span></div><div class="line"></div><div class="line"><span class="comment">#在index=3插入【11，12】</span></div><div class="line">print(np.insert(a,<span class="number">3</span>,[<span class="number">11</span>,<span class="number">12</span>]))</div><div class="line"><span class="comment"># [ 1  2  3 11 12  4  5  6]</span></div><div class="line"></div><div class="line">print(np.insert(a,<span class="number">1</span>,[<span class="number">11</span>],axis=<span class="number">0</span>))</div><div class="line"><span class="comment"># [[ 1  2]</span></div><div class="line"><span class="comment">#  [11 11]</span></div><div class="line"><span class="comment">#  [ 3  4]</span></div><div class="line"><span class="comment">#  [ 5  6]]</span></div><div class="line"></div><div class="line">print(np.insert(a,<span class="number">1</span>,<span class="number">11</span>,axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [[ 1 11  2]</span></div><div class="line"><span class="comment">#  [ 3 11  4]</span></div><div class="line"><span class="comment">#  [ 5 11  6]]</span></div></pre></td></tr></table></figure><h3 id="numpy-delete"><a href="#numpy-delete" class="headerlink" title="numpy.delete()"></a>numpy.delete()</h3><p>此函数返回从输入数组中删除指定子数组的新数组。 与<code>insert()</code>函数的情况一样，如果未提供轴参数，则输入数组将展开。 该函数接受以下参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Numpy.delete(arr, obj, axis)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组</li><li><code>obj</code>：可以被切片，整数或者整数数组，表明要从输入数组删除的子数组</li><li><code>axis</code>：沿着它删除给定子数组的轴，如果未提供，则输入数组会被展开</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]).reshape(<span class="number">3</span>,<span class="number">4</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[ 1  2  3  4]</span></div><div class="line"><span class="comment">#  [ 5  6  7  8]</span></div><div class="line"><span class="comment">#  [ 9 10 11 12]]</span></div><div class="line"></div><div class="line">print(np.delete(a,<span class="number">5</span>))<span class="comment">#删除index=5的值</span></div><div class="line"><span class="comment"># [ 1  2  3  4  5  7  8  9 10 11 12]</span></div><div class="line"></div><div class="line"><span class="comment">#删除第二行</span></div><div class="line">print(np.delete(a,<span class="number">1</span>,axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [[ 1  3  4]</span></div><div class="line"><span class="comment">#  [ 5  7  8]</span></div><div class="line"><span class="comment">#  [ 9 11 12]]</span></div><div class="line"></div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>])</div><div class="line">print(np.delete(a,np.s_[::<span class="number">2</span>]))</div><div class="line"><span class="comment"># [ 2  4  6  8 10]</span></div></pre></td></tr></table></figure><h3 id="numpy-unique"><a href="#numpy-unique" class="headerlink" title="numpy.unique()"></a>numpy.unique()</h3><p>此函数返回输入数组中的去重元素数组。 该函数能够返回一个元组，包含去重数组和相关索引的数组。 索引的性质取决于函数调用中返回参数的类型。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.unique(arr, return_index, return_inverse, return_counts)</div></pre></td></tr></table></figure><p>其中：</p><ul><li><code>arr</code>：输入数组，如果不是一维数组则会展开</li><li><code>return_index</code>：如果为<code>true</code>，返回输入数组中的元素下标</li><li><code>return_inverse</code>：如果为<code>true</code>，返回去重数组的下标，它可以用于重构输入数组</li><li><code>return_counts</code>：如果为<code>true</code>，返回去重数组中的元素在原数组中的出现次数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">5</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">2</span>,<span class="number">9</span>])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [5 2 6 2 7 5 6 8 2 9]</span></div><div class="line"></div><div class="line"><span class="comment">#返回去重之后的数组</span></div><div class="line">u = np.unique(a)</div><div class="line">print(u)</div><div class="line"><span class="comment"># [2 5 6 7 8 9]</span></div><div class="line"></div><div class="line"><span class="comment"># 去重数组的索引</span></div><div class="line">u,indices = np.unique(a,return_index=<span class="keyword">True</span>)</div><div class="line">print(indices)</div><div class="line"><span class="comment">#[1 0 2 4 7 9]</span></div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line">u,counts = np.unique(a,return_counts=<span class="keyword">True</span>)</div><div class="line">print(counts)</div><div class="line"><span class="comment"># [3 2 2 1 1 1]</span></div></pre></td></tr></table></figure><h1 id="NumPy位操作"><a href="#NumPy位操作" class="headerlink" title="NumPy位操作"></a>NumPy位操作</h1><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-20 at 3.24.42 PM.png" alt="Screen Shot 2018-06-20 at 3.24.42 PM"></p><h2 id="bitwise-and"><a href="#bitwise-and" class="headerlink" title="bitwise_and()"></a>bitwise_and()</h2><p>通过<code>np.bitwise_and()</code>函数对输入数组中的整数的二进制表示的相应位执行位与运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = <span class="number">13</span></div><div class="line">b =<span class="number">17</span></div><div class="line">print(bin(a))</div><div class="line"><span class="comment"># 0b1101</span></div><div class="line">print(bin(b))</div><div class="line"><span class="comment"># 0b10001</span></div><div class="line"></div><div class="line">print(np.bitwise_and(<span class="number">13</span>,<span class="number">17</span>))</div><div class="line"><span class="comment"># 1</span></div></pre></td></tr></table></figure><h2 id="bitwise-or"><a href="#bitwise-or" class="headerlink" title="bitwise_or()"></a>bitwise_or()</h2><p>通过<code>np.bitwise_or()</code>函数对输入数组中的整数的二进制表示的相应位执行位或运算。</p><h2 id="left-shift"><a href="#left-shift" class="headerlink" title="left_shift()"></a>left_shift()</h2><p>将数组元素的二进制表示中的位向左移动到指定位置，右侧附加相等数量的 0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">a = <span class="number">13</span></div><div class="line">print(bin(a))</div><div class="line"><span class="comment"># 0b1101</span></div><div class="line">a_ = np.left_shift(a,<span class="number">2</span>)</div><div class="line">print(bin(a_),a_)</div><div class="line"><span class="comment"># 0b110100 52</span></div></pre></td></tr></table></figure><h2 id="right-shift"><a href="#right-shift" class="headerlink" title="right_shift()"></a>right_shift()</h2><p>将数组元素的二进制表示中的位向右移动到指定位置，左侧附加相等数量的 0。</p><h1 id="NumPy字符串函数"><a href="#NumPy字符串函数" class="headerlink" title="NumPy字符串函数"></a>NumPy字符串函数</h1><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-20 at 3.34.40 PM.png" alt="Screen Shot 2018-06-20 at 3.34.40 PM"></p><p>这些函数在字符数组类(<code>numpy.char</code>)中定义。 <code>numpy.char</code>类中的上述函数在执行向量化字符串操作时非常有用。</p><h2 id="numpy-char-add"><a href="#numpy-char-add" class="headerlink" title="numpy.char.add()"></a>numpy.char.add()</h2><p>函数执行按元素的字符串连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">print(np.char.add([<span class="string">'hello'</span>],[<span class="string">'xyz'</span>]))</div><div class="line"><span class="comment"># ['helloxyz']</span></div><div class="line">print(np.char.add([<span class="string">'hello'</span>,<span class="string">'why'</span>],[<span class="string">'xyz'</span>,<span class="string">'like this'</span>]))</div><div class="line"><span class="comment"># ['helloxyz' 'whylike this']</span></div></pre></td></tr></table></figure><h2 id="numpy-char-multiply"><a href="#numpy-char-multiply" class="headerlink" title="numpy.char.multiply()"></a>numpy.char.multiply()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(np.char.multiply(<span class="string">"Wq"</span>,<span class="number">3</span>))</div><div class="line"><span class="comment"># WqWqWq</span></div></pre></td></tr></table></figure><h2 id="numpy-char-center"><a href="#numpy-char-center" class="headerlink" title="numpy.char.center()"></a>numpy.char.center()</h2><p>此函数返回所需宽度的数组，以便输入字符串位于中心，并使用<code>fillchar</code>在左侧和右侧进行填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(np.char.center(<span class="string">'hello'</span>,<span class="number">10</span>,fillchar=<span class="string">"@"</span>))</div><div class="line"><span class="comment"># @@hello@@@</span></div></pre></td></tr></table></figure><h2 id="numpy-char-capitalize"><a href="#numpy-char-capitalize" class="headerlink" title="numpy.char.capitalize()"></a>numpy.char.capitalize()</h2><p>函数返回字符串的副本，其中第一个字母大写</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> np.char.capitalize(<span class="string">'hello world'</span>)</div><div class="line"><span class="comment"># Hello world</span></div></pre></td></tr></table></figure><h2 id="numpy-char-title"><a href="#numpy-char-title" class="headerlink" title="numpy.char.title()"></a>numpy.char.title()</h2><p>返回输入字符串的按元素标题转换版本，其中每个单词的首字母都大写。</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> np.char.title(<span class="string">'hello how are you?'</span>)</div><div class="line"><span class="comment"># Hello How Are You?</span></div></pre></td></tr></table></figure><h2 id="numpy-char-lower"><a href="#numpy-char-lower" class="headerlink" title="numpy.char.lower()"></a>numpy.char.lower()</h2><p>函数返回一个数组，其元素转换为小写。它对每个元素调用<code>str.lower</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(np.char.lower(<span class="string">'HeLLo'</span>))</div><div class="line"><span class="comment"># hello</span></div><div class="line">print(np.char.lower([<span class="string">"HeLLO"</span>,<span class="string">'WORld'</span>]))</div><div class="line"><span class="comment"># ['hello' 'world']</span></div></pre></td></tr></table></figure><h2 id="numpy-char-upper"><a href="#numpy-char-upper" class="headerlink" title="numpy.char.upper()"></a>numpy.char.upper()</h2><p>函数返回一个数组，其元素转换为大写。它对每个元素调用<code>str.upper</code>。</p><h2 id="numpy-char-split"><a href="#numpy-char-split" class="headerlink" title="numpy.char.split()"></a>numpy.char.split()</h2><p>此函数返回输入字符串中的单词列表。 默认情况下，空格用作分隔符。 否则，指定的分隔符字符用于分割字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (np.char.split (<span class="string">'hello how are you?'</span>) )</div><div class="line"><span class="comment"># ['hello', 'how', 'are', 'you?']</span></div><div class="line"><span class="keyword">print</span> (np.char.split (<span class="string">'YiibaiPoint,Hyderabad,Telangana'</span>, sep = <span class="string">','</span>))</div><div class="line"><span class="comment"># ['YiibaiPoint', 'Hyderabad', 'Telangana']</span></div></pre></td></tr></table></figure><h2 id="numpy-char-splitlines"><a href="#numpy-char-splitlines" class="headerlink" title="numpy.char.splitlines()"></a>numpy.char.splitlines()</h2><p>函数返回数组中元素的单词列表，以换行符分割。<code>&#39;\n&#39;</code>，<code>&#39;\r&#39;</code>，<code>&#39;\r\n&#39;</code>都会用作换行符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (np.char.splitlines(<span class="string">'hello\nhow are you?'</span>) )</div><div class="line"><span class="comment"># ['hello', 'how are you?']</span></div><div class="line"><span class="keyword">print</span> (np.char.splitlines(<span class="string">'hello\rhow are you?'</span>))</div><div class="line"><span class="comment"># ['hello', 'how are you?']</span></div></pre></td></tr></table></figure><h2 id="numpy-char-strip"><a href="#numpy-char-strip" class="headerlink" title="numpy.char.strip()"></a>numpy.char.strip()</h2><p>函数返回数组的副本，其中元素移除了开头或结尾处的特定字符。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> np.char.strip(<span class="string">'ashok arora'</span>,<span class="string">'a'</span>) </div><div class="line"><span class="comment"># shok aror</span></div><div class="line"><span class="keyword">print</span> np.char.strip([<span class="string">'arora'</span>,<span class="string">'admin'</span>,<span class="string">'java'</span>],<span class="string">'a'</span>)</div><div class="line"><span class="comment"># ['ror' 'dmin' 'jav']</span></div></pre></td></tr></table></figure><h2 id="numpy-char-join"><a href="#numpy-char-join" class="headerlink" title="numpy.char.join()"></a>numpy.char.join()</h2><p>这个函数返回一个字符串，其中单个字符由特定的分隔符连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> np.char.join(<span class="string">':'</span>,<span class="string">'dmy'</span>) </div><div class="line"><span class="keyword">print</span> np.char.join([<span class="string">':'</span>,<span class="string">'-'</span>],[<span class="string">'dmy'</span>,<span class="string">'ymd'</span>])</div><div class="line"><span class="comment"># d:m:y</span></div><div class="line"><span class="comment"># ['d:m:y' 'y-m-d']</span></div></pre></td></tr></table></figure><h2 id="numpy-char-replace"><a href="#numpy-char-replace" class="headerlink" title="numpy.char.replace()"></a>numpy.char.replace()</h2><p>这个函数返回字符串副本，其中所有字符序列的出现位置都被另一个给定的字符序列取代。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line"><span class="keyword">print</span> np.char.replace (<span class="string">'He is a good boy'</span>, <span class="string">'is'</span>, <span class="string">'was'</span>)</div><div class="line"><span class="comment"># He was a good boy</span></div></pre></td></tr></table></figure><h1 id="NumPy算数函数"><a href="#NumPy算数函数" class="headerlink" title="NumPy算数函数"></a>NumPy算数函数</h1><p>NumPy 提供标准的三角函数，算术运算的函数，复数处理函数等。</p><h2 id="三角函数"><a href="#三角函数" class="headerlink" title="三角函数"></a>三角函数</h2><p><a href="https://www.yiibai.com/numpy/numpy_mathematical_functions.html#article-start" target="_blank" rel="noopener">参考</a></p><h2 id="舍入函数"><a href="#舍入函数" class="headerlink" title="舍入函数"></a>舍入函数</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-21 at 9.31.30 AM.png" alt="Screen Shot 2018-06-21 at 9.31.30 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([<span class="number">1.0</span>,<span class="number">5.55</span>,  <span class="number">123</span>,  <span class="number">0.567</span>,  <span class="number">25.532</span>])</div><div class="line"></div><div class="line">print(np.around(a))</div><div class="line"><span class="comment"># [1.  6.  123.   1.  26.]</span></div><div class="line">print(np.around(a,decimals=<span class="number">1</span>))</div><div class="line"><span class="comment"># [1.  5.6 123.   0.6  25.5]</span></div><div class="line">print(np.around(a,decimals=<span class="number">-1</span>))</div><div class="line"><span class="comment"># [0.  10. 120.   0.  30.]</span></div></pre></td></tr></table></figure><h2 id="numpy-floor-ceil"><a href="#numpy-floor-ceil" class="headerlink" title="numpy.floor()/.ceil()"></a>numpy.floor()/.ceil()</h2><p>floor() 此函数返回不大于输入参数的最大整数</p><p>ceil() 返回输入值的上限</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">-1.7</span>,  <span class="number">1.5</span>,  <span class="number">-0.2</span>,  <span class="number">0.6</span>,  <span class="number">10</span>])</div><div class="line">print(np.floor(a))</div><div class="line"><span class="comment"># [-2.  1. -1.  0. 10.]</span></div><div class="line">print(np.ceil(a))</div><div class="line"><span class="comment"># [-1.  2. -0.  1. 10.]</span></div></pre></td></tr></table></figure><h1 id="NumPy算数运算"><a href="#NumPy算数运算" class="headerlink" title="NumPy算数运算"></a>NumPy算数运算</h1><p>用于执行算术运算(如<code>add()</code>，<code>subtract()</code>，<code>multiply()</code>和<code>divide()</code>)的输入数组必须具有相同的形状或符合数组广播规则。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[0 1 2]</span></div><div class="line"><span class="comment">#  [3 4 5]</span></div><div class="line"><span class="comment">#  [6 7 8]]</span></div><div class="line">b = np.array([<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>])</div><div class="line">print(a+b) <span class="comment">#等价于np.add(a,b)</span></div><div class="line"><span class="comment"># [[10 11 12]</span></div><div class="line"><span class="comment">#  [13 14 15]</span></div><div class="line"><span class="comment">#  [16 17 18]]</span></div><div class="line">print(a-b) <span class="comment">#等价于np.subtract(a,b)</span></div><div class="line"><span class="comment"># [[-10  -9  -8]</span></div><div class="line"><span class="comment">#  [ -7  -6  -5]</span></div><div class="line"><span class="comment">#  [ -4  -3  -2]]</span></div><div class="line">print(a*b) <span class="comment">#等价于np.multiply(a,b)</span></div><div class="line"><span class="comment"># [[ 0 10 20]</span></div><div class="line"><span class="comment">#  [30 40 50]</span></div><div class="line"><span class="comment">#  [60 70 80]]</span></div><div class="line">print(a/b) <span class="comment">#等价于np.divide(a,b)</span></div><div class="line"><span class="comment"># [[0.  0.1 0.2]</span></div><div class="line"><span class="comment">#  [0.3 0.4 0.5]</span></div><div class="line"><span class="comment">#  [0.6 0.7 0.8]]</span></div></pre></td></tr></table></figure><h2 id="numpy-reciprocal"><a href="#numpy-reciprocal" class="headerlink" title="numpy.reciprocal()"></a>numpy.reciprocal()</h2><p>此函数返回参数逐元素的倒数，。 由于 Python 处理整数除法的方式，对于绝对值大于 1 的整数元素，结果始终为 0， 对于整数 0，则发出溢出警告。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">0.25</span>,  <span class="number">1.33</span>,  <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">100</span>])</div><div class="line">print(np.reciprocal(a))</div><div class="line"><span class="comment"># [4.  0.7518797   1.   inf   0.01 ]</span></div><div class="line">b = np.array([<span class="number">100</span>],dtype=int)</div><div class="line">print(np.reciprocal(b))</div><div class="line"><span class="comment"># [0]</span></div></pre></td></tr></table></figure><h2 id="numpy-power"><a href="#numpy-power" class="headerlink" title="numpy.power()"></a>numpy.power()</h2><p>此函数将第一个输入数组中的元素作为底数，计算它与第二个输入数组中相应元素的幂。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</div><div class="line">print(np.power(a,<span class="number">2</span>))</div><div class="line"><span class="comment"># [ 4  9 16]</span></div><div class="line">b = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line">print(np.power(a,b))</div><div class="line"><span class="comment"># [ 2  9 64]</span></div></pre></td></tr></table></figure><h2 id="numpy-mod"><a href="#numpy-mod" class="headerlink" title="numpy.mod()"></a>numpy.mod()</h2><p>此函数返回输入数组中相应元素的除法余数。 函数<code>numpy.remainder()</code>也产生相同的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>])</div><div class="line">b = np.array([<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>])</div><div class="line">print(np.mod(a,b))</div><div class="line"><span class="comment"># [1 0 2]</span></div><div class="line">print(np.remainder(a,b))</div><div class="line"><span class="comment"># [1 0 2]</span></div></pre></td></tr></table></figure><h1 id="NumPy统计函数"><a href="#NumPy统计函数" class="headerlink" title="NumPy统计函数"></a>NumPy统计函数</h1><p>NumPy 有很多有用的统计函数，用于从数组中给定的元素中查找最小，最大，百分标准差和方差等。</p><h2 id="numpy-amin-amax"><a href="#numpy-amin-amax" class="headerlink" title="numpy.amin()/.amax()"></a>numpy.amin()/.amax()</h2><p>从给定数组中的元素沿指定轴返回最小值和最大值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>],[<span class="number">8</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">4</span>,<span class="number">9</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[3 7 5]</span></div><div class="line"><span class="comment">#  [8 4 3]</span></div><div class="line"><span class="comment">#  [2 4 9]]</span></div><div class="line">print(np.amin(a,<span class="number">1</span>))</div><div class="line"><span class="comment"># [3 3 2]</span></div><div class="line">print(np.amin(a,<span class="number">0</span>))</div><div class="line"><span class="comment"># [2 4 3]</span></div><div class="line">print(np.amin(a))</div><div class="line"><span class="comment"># 2</span></div></pre></td></tr></table></figure><h2 id="numpy-ptp"><a href="#numpy-ptp" class="headerlink" title="numpy.ptp()"></a>numpy.ptp()</h2><p>返回沿轴的值的范围(最大值 - 最小值)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>],[<span class="number">8</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">2</span>,<span class="number">4</span>,<span class="number">9</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[3 7 5]</span></div><div class="line"><span class="comment">#  [8 4 3]</span></div><div class="line"><span class="comment">#  [2 4 9]]</span></div><div class="line">print(np.ptp(a))</div><div class="line"><span class="comment"># 7 (9-2)</span></div><div class="line">print(np.ptp(a,axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [4 5 7]</span></div><div class="line">print(np.ptp(a,<span class="number">0</span>))</div><div class="line"><span class="comment"># [6 3 6]</span></div></pre></td></tr></table></figure><h2 id="numpy-percentile"><a href="#numpy-percentile" class="headerlink" title="numpy.percentile()"></a>numpy.percentile()</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-21 at 9.56.48 AM.png" alt="Screen Shot 2018-06-21 at 9.56.48 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([[<span class="number">30</span>,<span class="number">40</span>,<span class="number">70</span>],[<span class="number">80</span>,<span class="number">20</span>,<span class="number">10</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[30 40 70]</span></div><div class="line"><span class="comment">#  [80 20 10]</span></div><div class="line">print(np.percentile(a,<span class="number">50</span>))<span class="comment"># #50%的分位数，就是a里排序之后的中位数</span></div><div class="line"><span class="comment"># 35.0</span></div><div class="line">print(np.percentile(a,<span class="number">50</span>,axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># [40. 20.]</span></div><div class="line">print(np.percentile(a,<span class="number">50</span>,axis=<span class="number">0</span>))</div><div class="line"><span class="comment"># [55. 30. 40.]</span></div></pre></td></tr></table></figure><h2 id="numpy-median"><a href="#numpy-median" class="headerlink" title="numpy.median()"></a>numpy.median()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.median(a,axis)</div></pre></td></tr></table></figure><h2 id="numpy-mean"><a href="#numpy-mean" class="headerlink" title="numpy.mean()"></a>numpy.mean()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">numpy.mean(a,axis)</div></pre></td></tr></table></figure><h2 id="numpy-average"><a href="#numpy-average" class="headerlink" title="numpy.average()"></a>numpy.average()</h2><p>加权平均值是由每个分量乘以反映其重要性的因子得到的平均值。 <code>numpy.average()</code>函数根据在另一个数组中给出的各自的权重计算数组中元素的加权平均值。 该函数可以接受一个轴参数。 如果没有指定轴，则数组会被展开。</p><p>考虑数组<code>[1,2,3,4]</code>和相应的权重<code>[4,3,2,1]</code>，通过将相应元素的乘积相加，并将和除以权重的和，来计算加权平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">加权平均值 = (<span class="number">1</span>*<span class="number">4</span>+<span class="number">2</span>*<span class="number">3</span>+<span class="number">3</span>*<span class="number">2</span>+<span class="number">4</span>*<span class="number">1</span>)/(<span class="number">4</span>+<span class="number">3</span>+<span class="number">2</span>+<span class="number">1</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</div><div class="line">print(np.mean(a))</div><div class="line"><span class="comment"># 2.5</span></div><div class="line">weight = np.array([<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</div><div class="line">print(np.average(a,weights=weight))</div><div class="line"><span class="comment"># 2.0</span></div><div class="line">print(np.average(a,weights=weight,returned=<span class="keyword">True</span>))  <span class="comment"># 如果 returned 参数设为 true，则返回权重的和</span></div><div class="line"><span class="comment"># (2.0, 10.0)</span></div></pre></td></tr></table></figure><p>在多维数组中，可以使用axis，指定用于计算的轴。</p><h2 id="numpy-std"><a href="#numpy-std" class="headerlink" title="numpy.std()"></a>numpy.std()</h2><p>标准差</p><h2 id="numpy-var"><a href="#numpy-var" class="headerlink" title="numpy.var()"></a>numpy.var()</h2><p>方差</p><h1 id="NumPy排序、搜索和计数"><a href="#NumPy排序、搜索和计数" class="headerlink" title="NumPy排序、搜索和计数"></a>NumPy排序、搜索和计数</h1><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-24 at 8.37.20 PM.png" alt="Screen Shot 2018-06-24 at 8.37.20 PM"></p><h2 id="numpy-sort"><a href="#numpy-sort" class="headerlink" title="numpy.sort()"></a>numpy.sort()</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-24 at 8.38.05 PM.png" alt="Screen Shot 2018-06-24 at 8.38.05 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([[<span class="number">3</span>,<span class="number">7</span>],[<span class="number">9</span>,<span class="number">1</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[3 7]</span></div><div class="line"><span class="comment">#  [9 1]]</span></div><div class="line">print(np.sort(a))</div><div class="line"><span class="comment"># [[3 7]</span></div><div class="line"><span class="comment">#  [1 9]]</span></div><div class="line">print(np.sort(a,axis=<span class="number">0</span>))</div><div class="line"><span class="comment"># [[3 1]</span></div><div class="line"><span class="comment">#  [9 7]]</span></div><div class="line">dt = np.dtype([(<span class="string">'name'</span>,  <span class="string">'S10'</span>),(<span class="string">'age'</span>,  int)])</div><div class="line">a = np.array([(<span class="string">"raju"</span>,<span class="number">21</span>),(<span class="string">"anil"</span>,<span class="number">25</span>),(<span class="string">"ravi"</span>,  <span class="number">17</span>),  (<span class="string">"amar"</span>,<span class="number">27</span>)], dtype = dt)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [(b'raju', 21) (b'anil', 25) (b'ravi', 17) (b'amar', 27)]</span></div><div class="line">print(np.sort(a,order=<span class="string">'name'</span>))</div><div class="line"><span class="comment"># [(b'amar', 27) (b'anil', 25) (b'raju', 21) (b'ravi', 17)]</span></div></pre></td></tr></table></figure><h2 id="numpy-argsort"><a href="#numpy-argsort" class="headerlink" title="numpy.argsort()"></a>numpy.argsort()</h2><p><code>numpy.argsort()</code>函数对输入数组沿给定轴执行间接排序，并使用指定排序类型返回数据的索引数组。 这个索引数组用于构造排序后的数组。</p><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-24 at 8.42.56 PM.png" alt="Screen Shot 2018-06-24 at 8.42.56 PM"></p><h2 id="numpy-lexsort"><a href="#numpy-lexsort" class="headerlink" title="numpy.lexsort()"></a>numpy.lexsort()</h2><p>用于对多个序列进行排序。把它想象成对电子表格进行排序，每一列代表一个序列，排序时优先照顾靠后的列。这里举一个应用场景：小升初考试，重点班录取学生按照总成绩录取。在总成绩相同时，数学成绩高的优先录取，在总成绩和数学成绩都相同时，按照英语成绩录取…… 这里，总成绩排在电子表格的最后一列，数学成绩在倒数第二列，英语成绩在倒数第三列。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">surnames =    (<span class="string">'Hertz'</span>,    <span class="string">'Galilei'</span>, <span class="string">'Hertz'</span>)</div><div class="line">first_names = (<span class="string">'Heinrich'</span>, <span class="string">'Galileo'</span>, <span class="string">'Gustav'</span>)</div><div class="line">ind = np.lexsort((first_names, surnames))</div><div class="line">print(ind)</div><div class="line"><span class="comment"># [1 2 0]</span></div><div class="line">print([surnames[i] + <span class="string">", "</span> + first_names[i] <span class="keyword">for</span> i <span class="keyword">in</span> ind])</div><div class="line"><span class="comment"># ['Galilei, Galileo', 'Hertz, Gustav', 'Hertz, Heinrich']</span></div></pre></td></tr></table></figure><h2 id="numpy-argmax-argmin"><a href="#numpy-argmax-argmin" class="headerlink" title="numpy.argmax()/.argmin()"></a>numpy.argmax()/.argmin()</h2><p>这两个函数分别沿给定轴返回最大和最小元素的<strong>索引</strong>。</p><h2 id="numpy-nonzero"><a href="#numpy-nonzero" class="headerlink" title="numpy.nonzero()"></a>numpy.nonzero()</h2><p>返回输入数组中非零元素的索引。</p><h2 id="numpy-where"><a href="#numpy-where" class="headerlink" title="numpy.where()"></a>numpy.where()</h2><p>函数返回输入数组中满足给定条件的元素的<strong>索引</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.arange(<span class="number">9.</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</div><div class="line">print(x)</div><div class="line"><span class="comment"># [[0. 1. 2.]</span></div><div class="line"><span class="comment">#  [3. 4. 5.]</span></div><div class="line"><span class="comment">#  [6. 7. 8.]]</span></div><div class="line">y = np.where(x&gt;<span class="number">3</span>)</div><div class="line">print(y) <span class="comment">#总共有5个，第一个array是横坐标，第二个array是纵坐标</span></div><div class="line"><span class="comment"># (array([1, 1, 2, 2, 2]), array([1, 2, 0, 1, 2]))</span></div><div class="line">print(x[y])</div><div class="line"><span class="comment"># [4. 5. 6. 7. 8.]</span></div></pre></td></tr></table></figure><h2 id="numpy-extract"><a href="#numpy-extract" class="headerlink" title="numpy.extract()"></a>numpy.extract()</h2><p>返回满足任何条件的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.arange(<span class="number">9.</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</div><div class="line">print(x)</div><div class="line"><span class="comment"># [[0. 1. 2.]</span></div><div class="line"><span class="comment">#  [3. 4. 5.]</span></div><div class="line"><span class="comment">#  [6. 7. 8.]]</span></div><div class="line">con = np.mod(x,<span class="number">2</span>)==<span class="number">0</span></div><div class="line">print(np.extract(con,x))</div><div class="line"><span class="comment"># [0. 2. 4. 6. 8.]</span></div></pre></td></tr></table></figure><h1 id="NumPy副本和视图"><a href="#NumPy副本和视图" class="headerlink" title="NumPy副本和视图"></a>NumPy副本和视图</h1><p>在执行函数时，其中一些返回输入数组的副本，而另一些返回视图。 当内容物理存储在另一个位置时，称为<strong>副本</strong>。 另一方面，如果提供了相同内存内容的不同视图，我们将其称为<strong>视图</strong>。</p><h2 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h2><p>简单的赋值不会创建数组对象的副本，一个数组的任何变化都反映在另一个数组上。 例如，一个数组的形状改变也会改变另一个数组的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.arange(<span class="number">6</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [0 1 2 3 4 5]</span></div><div class="line">b=a</div><div class="line">b[<span class="number">0</span>] = <span class="number">111</span></div><div class="line">print(a)</div><div class="line"><span class="comment"># [111   1   2   3   4   5]</span></div><div class="line">b.shape=<span class="number">3</span>,<span class="number">2</span></div><div class="line">print(a)</div><div class="line"><span class="comment"># [[0 1]</span></div><div class="line"><span class="comment">#  [2 3]</span></div><div class="line"><span class="comment">#  [4 5]]</span></div></pre></td></tr></table></figure><blockquote><p>改变值，改变维度</p></blockquote><h2 id="浅复制"><a href="#浅复制" class="headerlink" title="浅复制"></a>浅复制</h2><p>NumPy 拥有<code>ndarray.view()</code>方法，它是一个新的数组对象，并<strong>可查看原始数组的相同数据</strong>。 与前一种情况不同，新数组的维数更改不会更改原始数据的维数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[0 1 2]</span></div><div class="line"><span class="comment">#  [3 4 5]]</span></div><div class="line">b=a.view()</div><div class="line">b[<span class="number">0</span>] = <span class="number">111</span></div><div class="line">b.shape=<span class="number">3</span>,<span class="number">2</span></div><div class="line">print(b)</div><div class="line"><span class="comment"># [[111 111]</span></div><div class="line"><span class="comment">#  [111   3]</span></div><div class="line"><span class="comment">#  [  4   5]]</span></div><div class="line">print(a)</div><div class="line"><span class="comment"># [[111 111 111]</span></div><div class="line"><span class="comment">#  [  3   4   5]]</span></div></pre></td></tr></table></figure><blockquote><p>改变值，不改变维度</p></blockquote><p>数组的切片也会创建视图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">10</span>,<span class="number">10</span>],  [<span class="number">2</span>,<span class="number">3</span>],  [<span class="number">4</span>,<span class="number">5</span>]])</div><div class="line">print(a)</div><div class="line"><span class="comment"># [[10 10]</span></div><div class="line"><span class="comment">#  [ 2  3]</span></div><div class="line"><span class="comment">#  [ 4  5]]</span></div><div class="line">s = a[:<span class="number">2</span>,:]</div><div class="line">s[<span class="number">0</span>]=<span class="number">-111</span></div><div class="line">print(s)</div><div class="line"><span class="comment"># [[-111 -111]</span></div><div class="line"><span class="comment">#  [   2    3]]</span></div><div class="line">print(a)</div><div class="line"><span class="comment"># [[-111 -111]</span></div><div class="line"><span class="comment">#  [   2    3]</span></div><div class="line"><span class="comment">#  [   4    5]]</span></div></pre></td></tr></table></figure><h2 id="深复制"><a href="#深复制" class="headerlink" title="深复制"></a>深复制</h2><p><code>ndarray.copy()</code>函数创建一个深层副本。 它是数组及其数据的完整副本，不与原始数组共享。</p><p><code>b=a.copy()</code>:不改变数据，不改变维度</p><h1 id="NumPy矩阵库"><a href="#NumPy矩阵库" class="headerlink" title="NumPy矩阵库"></a>NumPy矩阵库</h1><p>NumPy 包包含一个 Matrix库<code>numpy.matlib</code>。此模块的函数返回矩阵而不是返回<code>ndarray</code>对象。</p><h2 id="numpy-matlib-empty"><a href="#numpy-matlib-empty" class="headerlink" title="numpy.matlib.empty()"></a>numpy.matlib.empty()</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-25 at 12.11.39 PM.png" alt="Screen Shot 2018-06-25 at 12.11.39 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> numpy.matlib</div><div class="line">print(np.matlib.empty((<span class="number">2</span>,<span class="number">3</span>)))</div><div class="line"><span class="comment"># [[-1.72723371e-077  3.11109888e+231  1.48219694e-323]</span></div><div class="line"><span class="comment">#  [ 0.00000000e+000  0.00000000e+000  4.17201348e-309]]</span></div></pre></td></tr></table></figure><blockquote><p>填充随机值</p></blockquote><h2 id="numpy-matlib-zeros"><a href="#numpy-matlib-zeros" class="headerlink" title="numpy.matlib.zeros()"></a>numpy.matlib.zeros()</h2><p>以零填充矩阵</p><h2 id="numpy-matlib-ones"><a href="#numpy-matlib-ones" class="headerlink" title="numpy.matlib.ones()"></a>numpy.matlib.ones()</h2><p>以一填充矩阵</p><h2 id="numpy-matlib-eye"><a href="#numpy-matlib-eye" class="headerlink" title="numpy.matlib.eye()"></a>numpy.matlib.eye()</h2><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-26 at 9.35.31 AM.png" alt="Screen Shot 2018-06-26 at 9.35.31 AM"></p><p><code>k</code>:Index of the diagonal: 0 refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal. k的值设置对角元素的位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> numpy.matlib</div><div class="line">print(np.matlib.eye(n =  <span class="number">3</span>, M =  <span class="number">4</span>, k=<span class="number">0</span>,dtype =  float))</div><div class="line"><span class="comment"># [[1. 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 1. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 1. 0.]]</span></div><div class="line">print(np.matlib.eye(n =  <span class="number">3</span>, M =  <span class="number">4</span>, k=<span class="number">1</span>,dtype =  float))</div><div class="line"><span class="comment"># [[0. 1. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 1. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 0. 1.]]</span></div><div class="line">print(np.matlib.eye(n =  <span class="number">3</span>, M =  <span class="number">4</span>, k=<span class="number">-1</span>,dtype =  float))</div><div class="line"><span class="comment"># [[0. 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [1. 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 1. 0. 0.]]</span></div></pre></td></tr></table></figure><h2 id="numpy-matlib-identity"><a href="#numpy-matlib-identity" class="headerlink" title="numpy.matlib.identity()"></a>numpy.matlib.identity()</h2><p>返回给定大小的单位矩阵。单位矩阵是主对角线元素都为 1 的方阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> numpy.matlib</div><div class="line">print(np.matlib.identity(<span class="number">5</span>))</div><div class="line"><span class="comment"># [[1. 0. 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 1. 0. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 1. 0. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 0. 1. 0.]</span></div><div class="line"><span class="comment">#  [0. 0. 0. 0. 1.]]</span></div></pre></td></tr></table></figure><h2 id="numpy-matlib-rand"><a href="#numpy-matlib-rand" class="headerlink" title="numpy.matlib.rand()"></a>numpy.matlib.rand()</h2><p>返回给定大小的填充随机值的矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> numpy.matlib</div><div class="line">print(np.matlib.rand(<span class="number">3</span>,<span class="number">3</span>))</div><div class="line"><span class="comment"># [[0.72845877 0.93067466 0.57828842]</span></div><div class="line"><span class="comment">#  [0.6117656  0.29529873 0.62999305]</span></div><div class="line"><span class="comment">#  [0.76468178 0.18484781 0.96084426]]</span></div></pre></td></tr></table></figure><h2 id="矩阵与数组互换"><a href="#矩阵与数组互换" class="headerlink" title="矩阵与数组互换"></a>矩阵与数组互换</h2><p>np.asarray()和np.asmatrix()</p><h1 id="NumPy线性代数"><a href="#NumPy线性代数" class="headerlink" title="NumPy线性代数"></a>NumPy线性代数</h1><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-26 at 9.50.26 AM.png" alt="Screen Shot 2018-06-26 at 9.50.26 AM"></p><blockquote><p>wise_element乘法：</p><p>向量点积、点乘、內积、数量积：对应位相乘后求和，结果是一个标量</p><p>矩阵乘法：numpy.dot() 、numpy.matmul()</p></blockquote><h2 id="numpy-dot"><a href="#numpy-dot" class="headerlink" title="numpy.dot()"></a>numpy.dot()</h2><p>此函数返回两个数组的点积。 对于二维向量，其等效于矩阵乘法。 对于一维数组，它是向量的内积。 对于 N 维数组，它是<code>a</code>的最后一个轴上的和与<code>b</code>的倒数第二个轴的乘积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">b = np.array([[<span class="number">11</span>,<span class="number">12</span>],[<span class="number">13</span>,<span class="number">14</span>]])</div><div class="line">print(np.dot(a,b))</div><div class="line"><span class="comment"># [[37 40]</span></div><div class="line"><span class="comment">#  [85 92]]</span></div></pre></td></tr></table></figure><h2 id="numpy-vdot"><a href="#numpy-vdot" class="headerlink" title="numpy.vdot()"></a>numpy.vdot()</h2><p>此函数返回两个向量的点积。多维向量会被展开成一维。即 (1 X d) * (d X 1) = [1,1]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">b = np.array([[<span class="number">11</span>,<span class="number">12</span>],[<span class="number">13</span>,<span class="number">14</span>]])</div><div class="line">print(np.vdot(a,b))</div><div class="line"><span class="comment"># 130</span></div></pre></td></tr></table></figure><h2 id="numpy-inner"><a href="#numpy-inner" class="headerlink" title="numpy.inner()"></a>numpy.inner()</h2><p>此函数返回一维数组的向量内积。 对于更高的维度，它返回最后一个轴上的和的乘积。</p><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-26 at 3.40.41 PM.png" alt="Screen Shot 2018-06-26 at 3.40.41 PM"></p><h2 id="numpy-matmul"><a href="#numpy-matmul" class="headerlink" title="numpy.matmul()"></a>numpy.matmul()</h2><p>返回两个数组的矩阵乘积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">a = [[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]]</div><div class="line">b = [[<span class="number">4</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]]</div><div class="line">print(np.matmul(a,b))</div><div class="line"><span class="comment"># [[4 1]</span></div><div class="line"><span class="comment">#  [2 2]]</span></div></pre></td></tr></table></figure><h2 id="numpy-linalg-det"><a href="#numpy-linalg-det" class="headerlink" title="numpy.linalg.det()"></a>numpy.linalg.det()</h2><p>计算输入矩阵的行列式</p><h2 id="numpy-linalg-solve"><a href="#numpy-linalg-solve" class="headerlink" title="numpy.linalg.solve()"></a>numpy.linalg.solve()</h2><p>给出了矩阵形式的线性方程的解。</p><p><img src="/2018/06/15/ML-NumPy/Screen Shot 2018-06-26 at 3.49.39 PM.png" alt="Screen Shot 2018-06-26 at 3.49.39 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">A = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">5</span>,<span class="number">-1</span>]]</div><div class="line">B = [<span class="number">6</span>,<span class="number">-4</span>,<span class="number">27</span>]</div><div class="line">print(np.linalg.solve(A,B))</div><div class="line"><span class="comment"># [ 5.  3. -2.]</span></div></pre></td></tr></table></figure><h2 id="numpy-linalg-inv"><a href="#numpy-linalg-inv" class="headerlink" title="numpy.linalg.inv()"></a>numpy.linalg.inv()</h2><p>求解逆矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">np.linalg.inv(x)</div></pre></td></tr></table></figure><h1 id="NumPy的IO文件操作"><a href="#NumPy的IO文件操作" class="headerlink" title="NumPy的IO文件操作"></a>NumPy的IO文件操作</h1><ul><li><code>load()</code>和<code>save()</code>函数处理 numPy 二进制文件(带<code>npy</code>扩展名)</li><li><code>loadtxt()</code>和<code>savetxt()</code>函数处理正常的文本文件</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">A = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">5</span>,<span class="number">-1</span>]]</div><div class="line">a = np.array(A)</div><div class="line">np.save(<span class="string">'result'</span>,a) <span class="comment">##文件名是"result.npy"</span></div><div class="line">b = np.load(<span class="string">'result.npy'</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </div><div class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]) </div><div class="line">np.savetxt(<span class="string">'out.txt'</span>,a) </div><div class="line">b = np.loadtxt(<span class="string">'out.txt'</span>)</div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-MachineLearning</title>
      <link href="/2018/06/11/DP-MachineLearning/"/>
      <url>/2018/06/11/DP-MachineLearning/</url>
      <content type="html"><![CDATA[<h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><ol><li><a href="https://arxiv.org/pdf/1606.05053.pdf" target="_blank" rel="noopener">2016-Nguyen</a> introduces gradient descent with local dp.</li><li></li></ol><p>[2016-Nguyen] Collecting and Analyzing Data from Smart Device Users with Local Differential Privacy</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP - Related Paper</title>
      <link href="/2018/06/10/DP-Related-Paper/"/>
      <url>/2018/06/10/DP-Related-Paper/</url>
      <content type="html"><![CDATA[<h1 id="survey"><a href="#survey" class="headerlink" title="survey"></a>survey</h1><ul><li>[ ]  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6582713" target="_blank" rel="noopener">Signal Processing and Machine Learning with Differential Privacy</a></li><li>[ ] </li></ul><h1 id="epsilon-delta-differential-privacy"><a href="#epsilon-delta-differential-privacy" class="headerlink" title="($\epsilon,\delta$)-differential privacy"></a>($\epsilon,\delta$)-differential privacy</h1><ul><li>[ ]  <a href="https://link.springer.com/content/pdf/10.1007%2F11761679_29.pdf" target="_blank" rel="noopener">Our Data, Ourselves: Privacy Via Distributed Noise Generation</a></li></ul><h1 id="delta-probability-privacy"><a href="#delta-probability-privacy" class="headerlink" title="$\delta$-probability privacy"></a>$\delta$-probability privacy</h1><ul><li>[ ]  <a href="http://www.cse.psu.edu/~duk17/papers/PrivacyOnTheMap.pdf" target="_blank" rel="noopener">Privacy: Theory meets practice on the map</a></li></ul><h1 id="Sensitivity"><a href="#Sensitivity" class="headerlink" title="Sensitivity"></a>Sensitivity</h1><ul><li>[ ]  <a href="https://link.springer.com/content/pdf/10.1007%2F11681878_14.pdf" target="_blank" rel="noopener">Calibrating Noise to Sensitivity in Private Data Analysis</a></li><li>[ ] </li></ul><h1 id="Composition"><a href="#Composition" class="headerlink" title="Composition"></a>Composition</h1><ul><li>[ ] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5670947" target="_blank" rel="noopener">Boosting and Differential Privacy</a></li><li>[ ] <a href="https://arxiv.org/pdf/0911.1813.pdf" target="_blank" rel="noopener">Interactive Privacy via the Median Mechanism</a></li><li>[ ] [Privacy odometers and filters: Pay-as-you-go composition]</li><li>[ ] [Differential privacy and robust statistics]</li></ul><h1 id="Mechanism"><a href="#Mechanism" class="headerlink" title="Mechanism"></a>Mechanism</h1><ul><li>[ ]  <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-67786-6_6.pdf" target="_blank" rel="noopener">Differentially Private Empirical Risk Minimization with Input Perturbation</a></li><li>[ ] </li></ul><h1 id="Location-DP"><a href="#Location-DP" class="headerlink" title="Location DP"></a>Location DP</h1><ul><li>[ ] <a href="http://www.mdpi.com/1999-5903/10/6/53/htm" target="_blank" rel="noopener">A Privacy Preserving Framework for Worker’s Location in Spatial Crowdsourcing Based on Local Differential Privacy</a></li><li>[ ] <a href="http://delivery.acm.org/10.1145/3000000/2996985/a35-hien.pdf?ip=167.96.144.64&amp;id=2996985&amp;acc=NO%20RULES&amp;key=A79D83B43E50B5B8%2E43B53F8E9794E57B%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1529359613_93c0fbf610c63c38dd11cd8d3082b381" target="_blank" rel="noopener">[Differentially Private Publication of Location Entropy]</a></li></ul><h1 id="Distributed-Lap"><a href="#Distributed-Lap" class="headerlink" title="Distributed Lap"></a>Distributed Lap</h1><ul><li>[ ] <a href="https://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdf" target="_blank" rel="noopener">Our Data, Ourselves: Privacy Via Distributed Noise Generation</a></li><li>[ ] <a href="http://theory.stanford.edu/~tim/papers/priv.pdf" target="_blank" rel="noopener">Universally Utility-Maximizing Privacy Mechanisms</a></li></ul><h1 id="DP-in-SGD"><a href="#DP-in-SGD" class="headerlink" title="DP in SGD"></a>DP in SGD</h1><ul><li>[ ] [Private empirical risk minimization: Efficient algorithms and tight error bounds]</li><li>[ ] [Stochastic gradient descent with differentially private updates]</li></ul><h1 id="DP-in-Machine-Learning"><a href="#DP-in-Machine-Learning" class="headerlink" title="DP in Machine Learning"></a>DP in Machine Learning</h1><ul><li>[ ] [Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis]</li><li>[ ] Related work in [2015-Reza Shokri] Privacy-Preserving Deep Learning</li></ul><h1 id="Error-Bound-in-DP"><a href="#Error-Bound-in-DP" class="headerlink" title="Error Bound in DP"></a>Error Bound in DP</h1><ul><li><p>[ ] [Concentrated differential privacy: Simplifications, extensions, and lower bounds.]</p></li><li><p>[ ] [R’enyi differential privacy]</p></li></ul>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ML - K-Nearest Neighbors</title>
      <link href="/2018/06/10/ML-K-Nearest-Neighbors/"/>
      <url>/2018/06/10/ML-K-Nearest-Neighbors/</url>
      <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=UqYde-LULfs" target="_blank" rel="noopener">How kNN algorithm works</a></p><p><a href="https://www.youtube.com/watch?v=k_7gMp5wh5A" target="_blank" rel="noopener">k nearest neighbor (kNN): how it works</a></p><p><a href="https://www.youtube.com/watch?v=v6278Cjf_qA" target="_blank" rel="noopener">How K-Nearest Neighbors (kNN) Classifier Works</a></p><p>kNN (k-Nearest Neighbors) is a classification algorithm. It is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).</p><a id="more"></a><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>For a new case to be classified, we calculate the distance between this case and each training sample. And we rank the distance in an ascending way and pick the top-k samples with smallest distance with the new case as its k-nearest neighbour. And this case is classified by a majority vote of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors. If K = 1, then the case is simply assigned to the class of its nearest neighbor. </p><h3 id="distance"><a href="#distance" class="headerlink" title="distance"></a>distance</h3><p><img src="/2018/06/10/ML-K-Nearest-Neighbors/Screen Shot 2018-06-10 at 11.48.30 PM.png" alt="Screen Shot 2018-06-10 at 11.48.30 PM"></p><p>It should also be noted that all three distance measures are valid for continuous variables. In the instance of <strong>categorical variables</strong> the Hamming distance must be used. </p><blockquote><p>Say we have a categorical attribute gender and its domain is $\{male, female\}$. And for a new case $x$ and its gender is $male$. For all the training sample, we calculate the gender distance between each sample and the new case. If their gender is same, then the distance is 0; else the distance is 1.  </p></blockquote><h3 id="normalization"><a href="#normalization" class="headerlink" title="normalization"></a>normalization</h3><p>Because the distance of categorical attribute is either 0 or 1, standardization of the numerical variables between 0 and 1 is needed when there is a mixture of numerical and categorical variables in the dataset.</p><h3 id="K-choice"><a href="#K-choice" class="headerlink" title="K choice"></a>K choice</h3><p>Choosing the optimal value for K is best done by first inspecting the data. In general, a large K value is more precise as it reduces the overall noise but there is no guarantee. Cross-validation is another way to retrospectively determine a good K value by using an independent dataset to validate the K value. Historically, the optimal K for most datasets has been between 3-10. That produces much better results than 1NN.</p><h2 id="Example1"><a href="#Example1" class="headerlink" title="Example1"></a>Example1</h2><p>Say we have the training sample database concerning credit default as follows, Age and Loan are two numerical variables (predictors) and Default is the target.        </p><div class="table-container"><table><thead><tr><th style="text-align:center">Age</th><th style="text-align:center">Loan</th><th style="text-align:center">Default</th></tr></thead><tbody><tr><td style="text-align:center">25</td><td style="text-align:center">$40,000</td><td style="text-align:center">N</td></tr><tr><td style="text-align:center">35</td><td style="text-align:center">$60,000</td><td style="text-align:center">N</td></tr><tr><td style="text-align:center">45</td><td style="text-align:center">$80,000</td><td style="text-align:center">N</td></tr><tr><td style="text-align:center">20</td><td style="text-align:center">$20,000</td><td style="text-align:center">N</td></tr><tr><td style="text-align:center">35</td><td style="text-align:center">$120,000</td><td style="text-align:center">N</td></tr><tr><td style="text-align:center">52</td><td style="text-align:center">$18,000</td><td style="text-align:center">N</td></tr><tr><td style="text-align:center">23</td><td style="text-align:center">$95,000</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">40</td><td style="text-align:center">$62,000</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">60</td><td style="text-align:center">$100,000</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">48</td><td style="text-align:center">$220,000</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center">33</td><td style="text-align:center">$150,000</td><td style="text-align:center">Y</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table></div><p>Now we have an unknown case (Age=48 and Loan=$142,000). And we want to predict the credit default for this case using Euclidean distance. So using the following formula to calculate the distance:</p><script type="math/tex; mode=display">d =\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}</script><p>We can get the following:</p><p><img src="/2018/06/10/ML-K-Nearest-Neighbors/Screen Shot 2018-06-11 at 12.29.41 AM.png" alt="Screen Shot 2018-06-11 at 12.29.41 AM"></p><p>With k=3, there are two Default=Y and one Default=N out of the three closest neighbors. So the prediction for this case is Default=Y.</p><h2 id="Example2"><a href="#Example2" class="headerlink" title="Example2"></a>Example2</h2><p>One major drawback in calculating distance measures directly from the training set is in the case where variables have different measurement scales or there is a mixture of numerical and categorical variables. For example, if one variable is based on annual income in dollars, and the other is based on age in years then income will have a much higher influence on the distance calculated. One solution is to standardize the training set as shown below.</p><script type="math/tex; mode=display">X_s=\frac{X-X_{min}}{X_{max}-X_{min}}</script><p><img src="/2018/06/10/ML-K-Nearest-Neighbors/Screen Shot 2018-06-11 at 12.34.40 AM.png" alt="Screen Shot 2018-06-11 at 12.34.40 AM"></p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span><span class="params">()</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self,X,y)</span>:</span></div><div class="line">        self.X_train = X</div><div class="line">        self.y_train = y</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></div><div class="line">        num_test = X.shape[<span class="number">0</span>]</div><div class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">        dists = np.zeros((num_test, num_train))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_train):</div><div class="line">                dists[i, j] = np.linalg.norm(X[i] - self.X_train[j])</div><div class="line">        <span class="keyword">return</span> dists</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></div><div class="line">        num_test = X.shape[<span class="number">0</span>]</div><div class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">        dists = np.zeros((num_test, num_train))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</div><div class="line">            dists[i] = np.linalg.norm(X[i] - self.X_train, axis=<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> dists</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></div><div class="line">        num_test = X.shape[<span class="number">0</span>]</div><div class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</div><div class="line">        dists = np.zeros((num_test, num_train))</div><div class="line">        <span class="comment"># https://medium.com/dataholiks-distillery/l2-distance-matrix-vectorization-trick-26aa3247ac6c</span></div><div class="line">        x = np.sum(X ** <span class="number">2</span>, axis=<span class="number">1</span>).reshape(num_test, <span class="number">1</span>)  <span class="comment"># shape = (num_test,1)</span></div><div class="line">        y = np.sum(self.X_train ** <span class="number">2</span>, axis=<span class="number">1</span>)  <span class="comment"># shape = (1,num_train)</span></div><div class="line">        xy = np.dot(X, self.X_train.T)</div><div class="line">        dists = np.sqrt(x + y - <span class="number">2</span> * xy)  <span class="comment"># shape of (x+y) = (num_test,num_train)</span></div><div class="line">        <span class="keyword">return</span> dists</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></div><div class="line">        num_test = dists.shape[<span class="number">0</span>]</div><div class="line">        y_pred = np.zeros(num_test)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</div><div class="line">            closest_y = []</div><div class="line">            dist = dists[i, :]</div><div class="line">            idx = np.argsort(dist)[:k]  <span class="comment"># choose least distance</span></div><div class="line">            closest_y = self.y_train[idx]</div><div class="line">            b = Counter(sorted(closest_y))</div><div class="line">            c = b.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">            y_pred[i] = c</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span>)</span>:</span></div><div class="line">        <span class="keyword">if</span> num_loops == <span class="number">0</span>:</div><div class="line">            dists = self.compute_distances_no_loops(X)</div><div class="line">        <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</div><div class="line">            dists = self.compute_distances_one_loop(X)</div><div class="line">        <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</div><div class="line">            dists = self.compute_distances_two_loops(X)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Invalid value %d for num_loops'</span> % num_loops)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> self.predict_labels(dists, k=k)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    X_train = []</div><div class="line">    y_train = []</div><div class="line"></div><div class="line">    classifier = KNearestNeighbor()</div><div class="line"></div><div class="line">    num_folds = <span class="number">5</span></div><div class="line">    k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</div><div class="line"></div><div class="line">    X_train_folds = np.array_split(X_train, num_folds)</div><div class="line">    y_train_folds = np.array_split(y_train, num_folds)</div><div class="line"></div><div class="line">    k_to_accuracies = &#123;&#125;</div><div class="line"></div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</div><div class="line">        accuracy = []</div><div class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(num_folds):</div><div class="line">            xtrain = np.concatenate((X_train_folds[:f] + X_train_folds[f + <span class="number">1</span>:]), axis=<span class="number">0</span>)  <span class="comment"># have to use '+'</span></div><div class="line">            ytrain = np.concatenate((y_train_folds[:f] + y_train_folds[f + <span class="number">1</span>:]), axis=<span class="number">0</span>)</div><div class="line">            xval = X_train_folds[f]</div><div class="line">            yval = y_train_folds[f]</div><div class="line"></div><div class="line">            num_val = len(xval)</div><div class="line"></div><div class="line">            classifier.train(xtrain, ytrain)</div><div class="line">            y_predict = classifier.predict(xval, k)</div><div class="line"></div><div class="line">            num_correct = np.sum(y_predict == yval)</div><div class="line">            accuracy.append(float(num_correct) / num_val)</div><div class="line">        k_to_accuracies[k] = accuracy</div><div class="line"></div><div class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></div><div class="line"></div><div class="line">    <span class="comment"># Print out the computed accuracies</span></div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</div><div class="line">        <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</div><div class="line">            print(<span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy))</div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TWICE</title>
      <link href="/2018/06/08/TWICE/"/>
      <url>/2018/06/08/TWICE/</url>
      <content type="html"><![CDATA[<h1 id="综艺"><a href="#综艺" class="headerlink" title="综艺"></a>综艺</h1><h2 id="What-is-Love"><a href="#What-is-Love" class="headerlink" title="What is Love?"></a>What is Love?</h2><ul><li><a href="https://www.bilibili.com/video/av22015572?p=2" target="_blank" rel="noopener">[Ask in a box]</a></li><li></li></ul><h2 id="Heart-Shake"><a href="#Heart-Shake" class="headerlink" title="Heart Shake"></a>Heart Shake</h2><ul><li></li></ul><h2 id="Likey"><a href="#Likey" class="headerlink" title="Likey"></a>Likey</h2><ul><li></li></ul><h2 id="Signal"><a href="#Signal" class="headerlink" title="Signal"></a>Signal</h2><ul><li><a href="https://www.bilibili.com/video/av13991100?from=search&amp;seid=755459151186256462" target="_blank" rel="noopener">[ 认识的哥哥]</a></li></ul><h2 id="Knock-Knock"><a href="#Knock-Knock" class="headerlink" title="Knock Knock"></a>Knock Knock</h2><ul><li></li></ul><h2 id="TT"><a href="#TT" class="headerlink" title="TT"></a>TT</h2><ul><li><a href="https://www.bilibili.com/video/av12257422?p=2" target="_blank" rel="noopener">[Amigo Tv]</a></li><li><a href="">[一周的偶像]</a></li></ul><h2 id="Cheer-Up"><a href="#Cheer-Up" class="headerlink" title="Cheer Up"></a>Cheer Up</h2><ul><li><a href="https://www.bilibili.com/video/av13973897?from=search&amp;seid=10925089392099596062" target="_blank" rel="noopener">[同床异梦]</a></li><li></li></ul><h2 id="Like-Ooh-Ahh"><a href="#Like-Ooh-Ahh" class="headerlink" title="Like Ooh-Ahh"></a>Like Ooh-Ahh</h2><ul><li><a href="https://www.bilibili.com/video/av13358708?from=search&amp;seid=10925089392099596062" target="_blank" rel="noopener">[社长在看]</a></li><li><a href="https://www.bilibili.com/video/av9774482/?p=2" target="_blank" rel="noopener">[Twice Tv Begins]</a></li><li><a href="https://www.bilibili.com/video/av9770455?from=search&amp;seid=13802790461139478911" target="_blank" rel="noopener">[Twice TV 1]</a></li><li><a href="https://www.bilibili.com/video/av21233693?from=search&amp;seid=13802790461139478911" target="_blank" rel="noopener">[Twice Tv 2]</a></li></ul>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Latex</title>
      <link href="/2018/06/08/Latex/"/>
      <url>/2018/06/08/Latex/</url>
      <content type="html"><![CDATA[<ol><li><p>公式换行使等号对齐</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">\begin&#123;equation&#125;</div><div class="line">\begin&#123;aligned&#125;</div><div class="line">Y&amp;=2*(x+3)\\</div><div class="line">&amp;=2*x+6</div><div class="line">\end&#123;aligned&#125;</div><div class="line">\end&#123;equation&#125;</div></pre></td></tr></table></figure></li></ol><script type="math/tex; mode=display">   \begin{equation}   \begin{aligned}   Y&=2*(x+3)\\   &=2*x+6   \end{aligned}   \end{equation}</script><ol><li><p>联立方程组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">t^*=</div><div class="line">\left\&#123;</div><div class="line">\begin&#123;align&#125; </div><div class="line">x&amp;=eq1 &amp; \text&#123;if x=1&#125;\\</div><div class="line">y&amp;=eq2 &amp;\text&#123;if x=2&#125;</div><div class="line">\end&#123;align&#125;</div><div class="line">\right.</div></pre></td></tr></table></figure><script type="math/tex; mode=display">t^*=\left\{\begin{align} x&=eq1 & \text{if x=1}\\y&=eq2 &\text{if x=2}\end{align}\right.</script></li></ol>]]></content>
      
      <categories>
          
          <category> Latex </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python - Pandas</title>
      <link href="/2018/06/08/ML-Pandas/"/>
      <url>/2018/06/08/ML-Pandas/</url>
      <content type="html"><![CDATA[<p>本文参考<a href="https://www.yiibai.com/pandas/python_pandas_data_structures.html" target="_blank" rel="noopener">1</a></p><p>pandas导入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div></pre></td></tr></table></figure><a id="more"></a><h1 id="琐碎"><a href="#琐碎" class="headerlink" title="琐碎"></a>琐碎</h1><ol><li><p>read_csv()默认有表头，会跳过第一行，从第二行开始读起。如果我们要读取的文件，直接就是数据，没有所谓的表头,就需 read_csv(header=None).</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ERROR : pandas.errors.ParserError: Error tokenizing data. C error: Expected <span class="number">1024</span> fields <span class="keyword">in</span> line <span class="number">237</span>, saw <span class="number">1491</span></div><div class="line">data = pd.read_csv(<span class="string">'file1.csv'</span>, error_bad_lines=<span class="keyword">False</span>)</div></pre></td></tr></table></figure></li></ol><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>Pandas处理以下三种数据结构：系列（Series），数据帧（DataFrame）,面板（Panel）</p><p>系列是具有均匀数据的一维数组结构，尺寸大小不变但数据可变</p><p>数据帧(<em>DataFrame</em>)是一个具有异构数据的二维数组，尺寸大小可变且数据可变</p><p>面板是具有异构数据的三维数据结构，尺寸大小可变且数据可变</p><h3 id="系列（Series"><a href="#系列（Series" class="headerlink" title="系列（Series)"></a>系列（Series)</h3><p>系列(<code>Series</code>)是能够保存任何类型的数据(整数，字符串，浮点数，Python对象等)的一维标记数组，轴标签统称为索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Pandas系列可以使用以下构造函数创建</span></div><div class="line">pandas.Series( data, index, dtype, copy)</div><div class="line"><span class="comment"># data - 数据采取各种形式，如：ndarray，list，constants</span></div><div class="line"><span class="comment"># index - 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。</span></div><div class="line"><span class="comment"># dtype - dtype用于数据类型。如果没有，将推断数据类型</span></div><div class="line"><span class="comment"># copy - 复制数据，默认为false</span></div></pre></td></tr></table></figure><h4 id="创建系列"><a href="#创建系列" class="headerlink" title="创建系列"></a>创建系列</h4><h5 id="创建一个空的系列"><a href="#创建一个空的系列" class="headerlink" title="创建一个空的系列"></a>创建一个空的系列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">s = pd.Series()</div><div class="line">print(s)<span class="comment">#Series([], dtype: float64)</span></div></pre></td></tr></table></figure><h5 id="ndarray创建一个系列"><a href="#ndarray创建一个系列" class="headerlink" title="ndarray创建一个系列"></a>ndarray创建一个系列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">data = np.array([<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">s = pd.Series(data)</div><div class="line">print(s)<span class="comment">#索引从0开始</span></div><div class="line"><span class="comment">#0    a</span></div><div class="line"><span class="comment">#1    b</span></div><div class="line"><span class="comment">#2    c</span></div><div class="line"><span class="comment">#3    d</span></div><div class="line"><span class="comment">#dtype: object</span></div><div class="line"></div><div class="line">s = pd.Series(data,index=[<span class="number">100</span>,<span class="number">101</span>,<span class="number">102</span>,<span class="number">103</span>])</div><div class="line">print(s)<span class="comment">#自定义索引</span></div><div class="line"><span class="comment">#100    a</span></div><div class="line"><span class="comment">#101    b</span></div><div class="line"><span class="comment">#102    c</span></div><div class="line"><span class="comment">#103    d</span></div><div class="line"><span class="comment">#dtype: object</span></div></pre></td></tr></table></figure><h5 id="从字典创建一个系列"><a href="#从字典创建一个系列" class="headerlink" title="从字典创建一个系列"></a>从字典创建一个系列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#字典(dict)可以作为输入传递，如果没有指定索引，则按排序顺序取得字典键以构造索引。 如果传递了索引，索引中与标签对应的数据中的值将被拉出。</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">data = &#123;<span class="string">'a'</span>:<span class="number">0</span>,<span class="string">'b'</span>:<span class="number">1</span>,<span class="string">'c'</span>:<span class="number">2</span>&#125;</div><div class="line">s = pd.Series(data)</div><div class="line">print(s)</div><div class="line"><span class="comment"># a    0</span></div><div class="line"><span class="comment"># b    1</span></div><div class="line"><span class="comment"># c    2</span></div><div class="line"><span class="comment"># dtype: int64</span></div><div class="line"></div><div class="line">s = pd.Series(data,index=[<span class="string">'b'</span>,<span class="string">'def1'</span>,<span class="string">'a'</span>,<span class="string">'def2'</span>,<span class="string">'c'</span>])</div><div class="line">print(s) <span class="comment">#索引顺序保持不变，缺少的元素使用NaN(不是数字)填充。</span></div><div class="line"><span class="comment"># b       1.0</span></div><div class="line"><span class="comment"># def1    NaN</span></div><div class="line"><span class="comment"># a       0.0</span></div><div class="line"><span class="comment"># def2    NaN</span></div><div class="line"><span class="comment"># c       2.0</span></div><div class="line"><span class="comment"># dtype: float64</span></div></pre></td></tr></table></figure><h5 id="从标量创建一个系列"><a href="#从标量创建一个系列" class="headerlink" title="从标量创建一个系列"></a>从标量创建一个系列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#如果数据是标量值，则必须提供索引。将重复该值以匹配索引的长度。</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">s = pd.Series(<span class="number">5</span>,index=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</div><div class="line">print(s)</div><div class="line"><span class="comment"># 0    5</span></div><div class="line"><span class="comment"># 1    5</span></div><div class="line"><span class="comment"># 2    5</span></div><div class="line"><span class="comment"># 3    5</span></div><div class="line"><span class="comment"># dtype: int64</span></div></pre></td></tr></table></figure><h4 id="系列访问"><a href="#系列访问" class="headerlink" title="系列访问"></a>系列访问</h4><h5 id="使用位置index访问"><a href="#使用位置index访问" class="headerlink" title="使用位置index访问"></a>使用位置index访问</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">s = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>])</div><div class="line">print(s[<span class="number">0</span>]) <span class="comment">#输出1</span></div><div class="line">print(s[:<span class="number">3</span>])<span class="comment">#检索系列中的前三个元素</span></div><div class="line"><span class="comment"># a    1</span></div><div class="line"><span class="comment"># b    2</span></div><div class="line"><span class="comment"># c    3</span></div><div class="line">print(s[<span class="number">-3</span>:])<span class="comment">#检索最后三个元素</span></div><div class="line"><span class="comment"># c    3</span></div><div class="line"><span class="comment"># d    4</span></div><div class="line"><span class="comment"># e    5</span></div></pre></td></tr></table></figure><h5 id="使用标签检索"><a href="#使用标签检索" class="headerlink" title="使用标签检索"></a>使用标签检索</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">s = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>])</div><div class="line">print(s[<span class="string">'b'</span>]) <span class="comment">#输出2</span></div><div class="line"></div><div class="line"><span class="comment">#使用索引标签值列表检索多个元素。</span></div><div class="line">print(s[[<span class="string">'a'</span>,<span class="string">'c'</span>,<span class="string">'e'</span>]]) </div><div class="line"><span class="comment"># a    1</span></div><div class="line"><span class="comment"># c    3</span></div><div class="line"><span class="comment"># e    5</span></div></pre></td></tr></table></figure><h3 id="数据帧-DataFrame"><a href="#数据帧-DataFrame" class="headerlink" title="数据帧(DataFrame)"></a>数据帧(DataFrame)</h3><p>数据帧(DataFrame)是二维数据结构，即数据以行和列的表格方式排列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#pandas中的DataFrame可以使用以下构造函数创建 </span></div><div class="line">pandas.DataFrame( data, index, columns, dtype, copy)</div><div class="line"><span class="comment"># data - 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。</span></div><div class="line"><span class="comment"># index - 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。</span></div><div class="line"><span class="comment"># columns - 对于列标签，可选的默认语法是 - np.arange(n)</span></div><div class="line"><span class="comment"># dtype - 每列的数据类型。</span></div><div class="line"><span class="comment"># copy - 如果默认值为False，则此命令(或任何它)用于复制数据。</span></div></pre></td></tr></table></figure><h4 id="创建数据帧"><a href="#创建数据帧" class="headerlink" title="创建数据帧"></a>创建数据帧</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">df = pd.DataFrame()</div></pre></td></tr></table></figure><h5 id="通过列表创建"><a href="#通过列表创建" class="headerlink" title="通过列表创建"></a>通过列表创建</h5><p>可以使用单个列表或列表列表创建数据帧(<em>DataFrame</em>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">data = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</div><div class="line">df = pd.DataFrame(data)</div><div class="line">print(df)</div><div class="line"><span class="comment">#    0</span></div><div class="line"><span class="comment"># 0  1</span></div><div class="line"><span class="comment"># 1  2</span></div><div class="line"><span class="comment"># 2  3</span></div><div class="line"><span class="comment"># 3  4</span></div><div class="line"><span class="comment"># 4  5</span></div><div class="line"></div><div class="line">data = [[<span class="string">'Alex'</span>,<span class="number">10</span>],[<span class="string">'Bob'</span>,<span class="number">12</span>],[<span class="string">'Dan'</span>,<span class="number">13</span>]]</div><div class="line">df = pd.DataFrame(data,columns=[<span class="string">'Name'</span>,<span class="string">'Age'</span>])<span class="comment">#dtype=float将参数age类型改为浮点</span></div><div class="line">print(df)</div><div class="line"><span class="comment">#    Name  Age</span></div><div class="line"><span class="comment"># 0  Alex   10</span></div><div class="line"><span class="comment"># 1   Bob   12</span></div><div class="line"><span class="comment"># 2   Dan   13</span></div></pre></td></tr></table></figure><h5 id="通过ndarrays-Lists的字典创建"><a href="#通过ndarrays-Lists的字典创建" class="headerlink" title="通过ndarrays/Lists的字典创建"></a>通过ndarrays/Lists的字典创建</h5><p>所有的<code>ndarrays</code>必须具有相同的长度。如果传递了索引(<code>index</code>)，则索引的长度应等于数组的长度。</p><p>如果没有传递索引，则默认情况下，索引将为<code>range(n)</code>，其中<code>n</code>为数组长度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">data = &#123;<span class="string">'Name'</span>:[<span class="string">'Tom'</span>,<span class="string">'Jack'</span>,<span class="string">'Steve'</span>,<span class="string">'Ricky'</span>],<span class="string">'Age'</span>:[<span class="number">28</span>,<span class="number">34</span>,<span class="number">29</span>,<span class="number">42</span>]&#125;</div><div class="line">df = pd.DataFrame(data)</div><div class="line">print(df) <span class="comment">## 观察值0,1,2,3。它们是分配给每个使用函数range(n)的默认索引。</span></div><div class="line"><span class="comment">#    Age   Name</span></div><div class="line"><span class="comment"># 0   28    Tom</span></div><div class="line"><span class="comment"># 1   34   Jack</span></div><div class="line"><span class="comment"># 2   29  Steve</span></div><div class="line"><span class="comment"># 3   42  Ricky</span></div><div class="line"></div><div class="line">df = pd.DataFrame(data,index=[<span class="string">'rank1'</span>,<span class="string">'rank2'</span>,<span class="string">'rank3'</span>,<span class="string">'rank4'</span>])</div><div class="line">print(df) <span class="comment">## index参数为每行分配一个索引。</span></div><div class="line"><span class="comment">#        Age   Name</span></div><div class="line"><span class="comment"># rank1   28    Tom</span></div><div class="line"><span class="comment"># rank2   34   Jack</span></div><div class="line"><span class="comment"># rank3   29  Steve</span></div><div class="line"><span class="comment"># rank4   42  Ricky</span></div></pre></td></tr></table></figure><h5 id="通过字典创建"><a href="#通过字典创建" class="headerlink" title="通过字典创建"></a>通过字典创建</h5><p>字典列表可作为输入数据传递以用来创建数据帧(<em>DataFrame</em>)，字典键默认为列名。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">data = [&#123;<span class="string">'a'</span>:<span class="number">1</span>,<span class="string">'b'</span>:<span class="number">2</span>&#125;,&#123;<span class="string">'a'</span>:<span class="number">5</span>,<span class="string">'b'</span>:<span class="number">10</span>,<span class="string">'c'</span>:<span class="number">20</span>&#125;]</div><div class="line">df = pd.DataFrame(data)</div><div class="line">print(df)</div><div class="line"><span class="comment">#    a   b     c</span></div><div class="line"><span class="comment"># 0  1   2   NaN</span></div><div class="line"><span class="comment"># 1  5  10  20.0</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">data = [&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;,&#123;<span class="string">'a'</span>: <span class="number">5</span>, <span class="string">'b'</span>: <span class="number">10</span>, <span class="string">'c'</span>: <span class="number">20</span>&#125;]</div><div class="line"></div><div class="line"><span class="comment">#With two column indices, values same as dictionary keys</span></div><div class="line">df1 = pd.DataFrame(data, index=[<span class="string">'first'</span>, <span class="string">'second'</span>], columns=[<span class="string">'a'</span>, <span class="string">'b'</span>])</div><div class="line"></div><div class="line"><span class="comment">#With two column indices with one index with other name</span></div><div class="line">df2 = pd.DataFrame(data, index=[<span class="string">'first'</span>, <span class="string">'second'</span>], columns=[<span class="string">'a'</span>, <span class="string">'b1'</span>])</div><div class="line">print(df1)</div><div class="line"><span class="comment">#         a   b</span></div><div class="line"><span class="comment"># first   1   2</span></div><div class="line"><span class="comment"># second  5  10</span></div><div class="line">print(df2)</div><div class="line"><span class="comment">#         a  b1</span></div><div class="line"><span class="comment"># first   1 NaN</span></div><div class="line"><span class="comment"># second  5 NaN</span></div></pre></td></tr></table></figure><h5 id="通过系列创建"><a href="#通过系列创建" class="headerlink" title="通过系列创建"></a>通过系列创建</h5><p>字典的系列可以传递以形成一个DataFrame。 所得到的索引是通过的所有系列索引的并集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">d = &#123;<span class="string">'one'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]),</div><div class="line">      <span class="string">'two'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])&#125;</div><div class="line"></div><div class="line">df = pd.DataFrame(d)</div><div class="line">print(df)</div><div class="line"><span class="comment">#    one  two</span></div><div class="line"><span class="comment"># a  1.0    1</span></div><div class="line"><span class="comment"># b  2.0    2</span></div><div class="line"><span class="comment"># c  3.0    3</span></div><div class="line"><span class="comment"># d  NaN    4</span></div></pre></td></tr></table></figure><h4 id="数据帧访问"><a href="#数据帧访问" class="headerlink" title="数据帧访问"></a>数据帧访问</h4><h5 id="列选择"><a href="#列选择" class="headerlink" title="列选择"></a>列选择</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">d = &#123;<span class="string">'one'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]),</div><div class="line">      <span class="string">'two'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])&#125;</div><div class="line"></div><div class="line">df = pd.DataFrame(d)</div><div class="line"><span class="keyword">print</span> (df [<span class="string">'one'</span>])</div></pre></td></tr></table></figure><h5 id="列添加"><a href="#列添加" class="headerlink" title="列添加"></a>列添加</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">d = &#123;<span class="string">'one'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]),</div><div class="line">      <span class="string">'two'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])&#125;</div><div class="line"></div><div class="line">df = pd.DataFrame(d)</div><div class="line"></div><div class="line"><span class="comment"># Adding a new column to an existing DataFrame object with column label by passing new series</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"Adding a new column by passing as Series:"</span>)</div><div class="line">df[<span class="string">'three'</span>]=pd.Series([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>],index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment"># Adding a new column by passing as Series:</span></div><div class="line"><span class="comment">#    one  two  three</span></div><div class="line"><span class="comment"># a  1.0    1   10.0</span></div><div class="line"><span class="comment"># b  2.0    2   20.0</span></div><div class="line"><span class="comment"># c  3.0    3   30.0</span></div><div class="line"><span class="comment"># d  NaN    4    NaN</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"Adding a new column using the existing columns in DataFrame:"</span>)</div><div class="line">df[<span class="string">'four'</span>]=df[<span class="string">'one'</span>]+df[<span class="string">'three'</span>]</div><div class="line"></div><div class="line">print(df)</div><div class="line"><span class="comment"># Adding a new column using the existing columns in DataFrame:</span></div><div class="line"><span class="comment">#    one  two  three  four</span></div><div class="line"><span class="comment"># a  1.0    1   10.0  11.0</span></div><div class="line"><span class="comment"># b  2.0    2   20.0  22.0</span></div><div class="line"><span class="comment"># c  3.0    3   30.0  33.0</span></div><div class="line"><span class="comment"># d  NaN    4    NaN   NaN</span></div></pre></td></tr></table></figure><h5 id="列删除"><a href="#列删除" class="headerlink" title="列删除"></a>列删除</h5><p>列可以删除或弹出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">d = &#123;<span class="string">'one'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]),</div><div class="line">     <span class="string">'two'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]),</div><div class="line">     <span class="string">'three'</span> : pd.Series([<span class="number">10</span>,<span class="number">20</span>,<span class="number">30</span>], index=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>])&#125;</div><div class="line"></div><div class="line">df = pd.DataFrame(d)</div><div class="line"><span class="keyword">print</span> (<span class="string">"Our dataframe is:"</span>)</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment"># Our dataframe is:</span></div><div class="line"><span class="comment">#    one  three  two</span></div><div class="line"><span class="comment"># a  1.0   10.0    1</span></div><div class="line"><span class="comment"># b  2.0   20.0    2</span></div><div class="line"><span class="comment"># c  3.0   30.0    3</span></div><div class="line"><span class="comment"># d  NaN    NaN    4</span></div><div class="line"></div><div class="line"><span class="comment"># using del function</span></div><div class="line"><span class="keyword">print</span> (<span class="string">"Deleting the first column using DEL function:"</span>)</div><div class="line"><span class="keyword">del</span> df[<span class="string">'one'</span>]</div><div class="line">print(df)</div><div class="line"><span class="comment"># Deleting the first column using DEL function:</span></div><div class="line"><span class="comment">#    three  two</span></div><div class="line"><span class="comment"># a   10.0    1</span></div><div class="line"><span class="comment"># b   20.0    2</span></div><div class="line"><span class="comment"># c   30.0    3</span></div><div class="line"><span class="comment"># d    NaN    4</span></div><div class="line"></div><div class="line"><span class="comment"># using pop function</span></div><div class="line"><span class="keyword">print</span> (<span class="string">"Deleting another column using POP function:"</span>)</div><div class="line">df.pop(<span class="string">'two'</span>)</div><div class="line">print(df)</div><div class="line"><span class="comment"># Deleting another column using POP function:</span></div><div class="line"><span class="comment">#    three</span></div><div class="line"><span class="comment"># a   10.0</span></div><div class="line"><span class="comment"># b   20.0</span></div><div class="line"><span class="comment"># c   30.0</span></div><div class="line"><span class="comment"># d    NaN</span></div></pre></td></tr></table></figure><h5 id="行选择"><a href="#行选择" class="headerlink" title="行选择"></a>行选择</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#通过将行标签传递给loc()函数来选择行。</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">d = &#123;<span class="string">'one'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]),</div><div class="line">     <span class="string">'two'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])&#125;</div><div class="line"></div><div class="line">df = pd.DataFrame(d)</div><div class="line">print(df.loc[<span class="string">'b'</span>])</div><div class="line"><span class="comment"># one    2.0</span></div><div class="line"><span class="comment"># two    2.0</span></div><div class="line"><span class="comment"># Name: b, dtype: float64</span></div><div class="line"></div><div class="line"><span class="comment">#可以通过将整数位置传递给iloc()函数来选择行</span></div><div class="line">print(df.iloc[<span class="number">2</span>])</div><div class="line"><span class="comment"># one    3.0</span></div><div class="line"><span class="comment"># two    3.0</span></div><div class="line"><span class="comment"># Name: b, dtype: float64</span></div></pre></td></tr></table></figure><h5 id="行切片"><a href="#行切片" class="headerlink" title="行切片"></a>行切片</h5><p>可以使用<code>:</code>运算符选择多行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">d = &#123;<span class="string">'one'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]),</div><div class="line">    <span class="string">'two'</span> : pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])&#125;</div><div class="line"></div><div class="line">df = pd.DataFrame(d)</div><div class="line">print(df[<span class="number">2</span>:<span class="number">4</span>])</div><div class="line"><span class="comment">#    one  two</span></div><div class="line"><span class="comment"># c  3.0    3</span></div><div class="line"><span class="comment"># d  NaN    4</span></div></pre></td></tr></table></figure><h5 id="附加行"><a href="#附加行" class="headerlink" title="附加行"></a>附加行</h5><p>使用<code>append()</code>函数将新行添加到DataFrame</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], columns = [<span class="string">'a'</span>,<span class="string">'b'</span>])</div><div class="line">df2 = pd.DataFrame([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], columns = [<span class="string">'a'</span>,<span class="string">'b'</span>])</div><div class="line"></div><div class="line">df = df.append(df2)</div><div class="line">print(df)</div><div class="line"><span class="comment">#    a  b</span></div><div class="line"><span class="comment"># 0  1  2</span></div><div class="line"><span class="comment"># 1  3  4</span></div><div class="line"><span class="comment"># 0  5  6</span></div><div class="line"><span class="comment"># 1  7  8</span></div></pre></td></tr></table></figure><h5 id="删除行"><a href="#删除行" class="headerlink" title="删除行"></a>删除行</h5><p>使用索引标签从DataFrame中删除或删除行。 如果标签重复，则会删除多行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df = pd.DataFrame([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], columns = [<span class="string">'a'</span>,<span class="string">'b'</span>])</div><div class="line">df2 = pd.DataFrame([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], columns = [<span class="string">'a'</span>,<span class="string">'b'</span>])</div><div class="line"></div><div class="line">df = df.append(df2)</div><div class="line"></div><div class="line"><span class="comment"># Drop rows with label 0</span></div><div class="line">df = df.drop(<span class="number">0</span>)</div><div class="line">print(df)</div><div class="line"><span class="comment">#    a  b</span></div><div class="line"><span class="comment"># 1  3  4</span></div><div class="line"><span class="comment"># 1  7  8</span></div></pre></td></tr></table></figure><h3 id="面板-Panel"><a href="#面板-Panel" class="headerlink" title="面板(Panel)"></a>面板(Panel)</h3><h1 id="Pandas基本功能"><a href="#Pandas基本功能" class="headerlink" title="Pandas基本功能"></a>Pandas基本功能</h1><h2 id="系列基本功能"><a href="#系列基本功能" class="headerlink" title="系列基本功能"></a>系列基本功能</h2><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-08 at 11.56.57 PM.png" alt="Screen Shot 2018-06-08 at 11.56.57 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment">#Create a series with 100 random numbers</span></div><div class="line">s = pd.Series(np.random.randn(<span class="number">4</span>))</div></pre></td></tr></table></figure><h3 id="axes"><a href="#axes" class="headerlink" title="axes"></a>axes</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"This axes are:"</span>)</div><div class="line">print(s.axes)</div><div class="line"><span class="comment"># This axes are:</span></div><div class="line"><span class="comment"># [RangeIndex(start=0, stop=4, step=1)]</span></div></pre></td></tr></table></figure><h3 id="empty"><a href="#empty" class="headerlink" title="empty"></a>empty</h3><p>返回布尔值，表示对象是否为空。返回<code>True</code>则表示对象为空。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Is the Object empty?"</span>)</div><div class="line">print(s.empty)</div><div class="line"><span class="comment"># Is the Object empty?</span></div><div class="line"><span class="comment"># False</span></div></pre></td></tr></table></figure><h3 id="ndim"><a href="#ndim" class="headerlink" title="ndim"></a>ndim</h3><p>返回对象的维数。根据定义，一个系列是一个<code>1D</code>数据结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"The dimensions of the object:"</span>)</div><div class="line">print(s.ndim)</div><div class="line"><span class="comment"># The dimensions of the object:</span></div><div class="line"><span class="comment"># 1</span></div></pre></td></tr></table></figure><h3 id="size"><a href="#size" class="headerlink" title="size"></a>size</h3><p>返回系列的大小(长度)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"The size of the object:"</span>)</div><div class="line"><span class="keyword">print</span> (s.size)</div><div class="line"><span class="comment"># The size of the object:</span></div><div class="line"><span class="comment"># 2</span></div></pre></td></tr></table></figure><h3 id="values"><a href="#values" class="headerlink" title="values"></a>values</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"The actual data series is:"</span>)</div><div class="line"><span class="keyword">print</span> (s.values)</div><div class="line"><span class="comment"># The actual data series is:</span></div><div class="line"><span class="comment"># [-0.15974783  0.75598442 -0.05964617 -0.91200952]</span></div></pre></td></tr></table></figure><h3 id="head-tail"><a href="#head-tail" class="headerlink" title="head-tail"></a>head-tail</h3><p><code>head()</code>返回前<code>n</code>行(观察索引值)。要显示的元素的默认数量为<code>5</code>，但可以传递自定义这个数字值。</p><p><code>tail()</code>返回最后<code>n</code>行(观察索引值)。 要显示的元素的默认数量为<code>5</code>，但可以传递自定义数字值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (<span class="string">"The original series is:"</span>)</div><div class="line"><span class="keyword">print</span> (s)</div><div class="line"><span class="comment"># The original series is:</span></div><div class="line"><span class="comment"># 0    0.209476</span></div><div class="line"><span class="comment"># 1    0.136436</span></div><div class="line"><span class="comment"># 2   -1.387964</span></div><div class="line"><span class="comment"># 3   -0.102560</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"The first two rows of the data series:"</span>)</div><div class="line"><span class="keyword">print</span> (s.head(<span class="number">2</span>))</div><div class="line"><span class="comment"># The first two rows of the data series:</span></div><div class="line"><span class="comment"># 0    0.209476</span></div><div class="line"><span class="comment"># 1    0.136436</span></div></pre></td></tr></table></figure><h2 id="DataFrame基本功能"><a href="#DataFrame基本功能" class="headerlink" title="DataFrame基本功能"></a>DataFrame基本功能</h2><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-09 at 12.15.51 AM.png" alt="Screen Shot 2018-06-09 at 12.15.51 AM"></p><h1 id="Pandas描述性统计"><a href="#Pandas描述性统计" class="headerlink" title="Pandas描述性统计"></a>Pandas描述性统计</h1><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-10 at 10.34.06 AM.png" alt="Screen Shot 2018-06-10 at 10.34.06 AM"></p><blockquote><ul><li>类似于：<code>sum()</code>，<code>cumsum()</code>函数能与数字和字符(或)字符串数据元素一起工作，不会产生任何错误。字符聚合从来都比较少被使用，虽然这些函数不会引发任何异常。</li><li>当DataFrame包含字符或字符串数据时，像<code>abs()</code>，<code>cumprod()</code>这样的函数会抛出异常。</li></ul></blockquote><p>创建一个数据帧</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment">#Create a Dictionary of series</span></div><div class="line">d = &#123;<span class="string">'Name'</span>:pd.Series([<span class="string">'Tom'</span>,<span class="string">'James'</span>,<span class="string">'Ricky'</span>,<span class="string">'Vin'</span>,<span class="string">'Steve'</span>,<span class="string">'Minsu'</span>,<span class="string">'Jack'</span>,</div><div class="line">   <span class="string">'Lee'</span>,<span class="string">'David'</span>,<span class="string">'Gasper'</span>,<span class="string">'Betina'</span>,<span class="string">'Andres'</span>]),</div><div class="line">   <span class="string">'Age'</span>:pd.Series([<span class="number">25</span>,<span class="number">26</span>,<span class="number">25</span>,<span class="number">23</span>,<span class="number">30</span>,<span class="number">29</span>,<span class="number">23</span>,<span class="number">34</span>,<span class="number">40</span>,<span class="number">30</span>,<span class="number">51</span>,<span class="number">46</span>]),</div><div class="line">   <span class="string">'Rating'</span>:pd.Series([<span class="number">4.23</span>,<span class="number">3.24</span>,<span class="number">3.98</span>,<span class="number">2.56</span>,<span class="number">3.20</span>,<span class="number">4.6</span>,<span class="number">3.8</span>,<span class="number">3.78</span>,<span class="number">2.98</span>,<span class="number">4.80</span>,<span class="number">4.10</span>,<span class="number">3.65</span>])&#125;</div><div class="line"></div><div class="line"><span class="comment">#Create a DataFrame</span></div><div class="line">df = pd.DataFrame(d)</div><div class="line">print(df)</div></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Age</th><th style="text-align:center">Name</th><th style="text-align:center">Rating</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">25</td><td style="text-align:center">Tom</td><td style="text-align:center">4.23</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">26</td><td style="text-align:center">James</td><td style="text-align:center">3.24</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">25</td><td style="text-align:center">Ricky</td><td style="text-align:center">3.98</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">23</td><td style="text-align:center">Vin</td><td style="text-align:center">2.56</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">30</td><td style="text-align:center">Steve</td><td style="text-align:center">3.20</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">29</td><td style="text-align:center">Minsu</td><td style="text-align:center">4.60</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">23</td><td style="text-align:center">Jack</td><td style="text-align:center">3.80</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">34</td><td style="text-align:center">Lee</td><td style="text-align:center">3.78</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">40</td><td style="text-align:center">David</td><td style="text-align:center">2.98</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">30</td><td style="text-align:center">Gasper</td><td style="text-align:center">4.80</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">51</td><td style="text-align:center">Betina</td><td style="text-align:center">4.10</td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">46</td><td style="text-align:center">Andres</td><td style="text-align:center">3.65</td></tr></tbody></table></div><h2 id="sum"><a href="#sum" class="headerlink" title="sum()"></a>sum()</h2><p>返回所请求轴的值的总和。 默认情况下，轴为索引(<code>axis=0</code>)，即对列sum</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(df.sum())</div><div class="line"><span class="comment"># Age                                                     382</span></div><div class="line"><span class="comment"># Name      TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe...</span></div><div class="line"><span class="comment"># Rating                                                44.92</span></div><div class="line"><span class="comment"># dtype: object</span></div></pre></td></tr></table></figure><p><strong>axis=1</strong>,则对行sum</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">print(df.sum(<span class="number">1</span>))</div><div class="line"><span class="comment"># 0     29.23</span></div><div class="line"><span class="comment"># 1     29.24</span></div><div class="line"><span class="comment"># 2     28.98</span></div><div class="line"><span class="comment"># 3     25.56</span></div><div class="line"><span class="comment"># 4     33.20</span></div><div class="line"><span class="comment"># 5     33.60</span></div><div class="line"><span class="comment"># 6     26.80</span></div><div class="line"><span class="comment"># 7     37.78</span></div><div class="line"><span class="comment"># 8     42.98</span></div><div class="line"><span class="comment"># 9     34.80</span></div><div class="line"><span class="comment"># 10    55.10</span></div><div class="line"><span class="comment"># 11    49.65</span></div></pre></td></tr></table></figure><h2 id="mean"><a href="#mean" class="headerlink" title="mean()"></a>mean()</h2><p>与sum()用法一样，默认是对列求均值；mean(1)则是对每一行求均值。</p><h2 id="std"><a href="#std" class="headerlink" title="std()"></a>std()</h2><p>返回数字列的Bressel标准偏差。</p><h2 id="数据汇总"><a href="#数据汇总" class="headerlink" title="数据汇总"></a>数据汇总</h2><p><strong>describe()</strong>函数是用来计算有关DataFrame列的统计信息的摘要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">print(df.describe())</div><div class="line"><span class="comment">#              Age     Rating</span></div><div class="line"><span class="comment"># count  12.000000  12.000000</span></div><div class="line"><span class="comment"># mean   31.833333   3.743333</span></div><div class="line"><span class="comment"># std     9.232682   0.661628</span></div><div class="line"><span class="comment"># min    23.000000   2.560000</span></div><div class="line"><span class="comment"># 25%    25.000000   3.230000</span></div><div class="line"><span class="comment"># 50%    29.500000   3.790000</span></div><div class="line"><span class="comment"># 75%    35.500000   4.132500</span></div><div class="line"><span class="comment"># max    51.000000   4.800000</span></div></pre></td></tr></table></figure><p><strong>include</strong>是用于传递关于什么列需要考虑用于总结的必要信息的参数。获取值列表; 默认情况下是”数字值”。</p><ul><li><code>object</code> - 汇总字符串列</li><li><code>number</code> - 汇总数字列</li><li><code>all</code> - 将所有列汇总在一起(不应将其作为列表值传递)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">print(df.describe(include=[<span class="string">'object'</span>]))</div><div class="line"><span class="comment">#          Name</span></div><div class="line"><span class="comment"># count      12</span></div><div class="line"><span class="comment"># unique     12</span></div><div class="line"><span class="comment"># top     James</span></div><div class="line"><span class="comment"># freq        1</span></div></pre></td></tr></table></figure><h1 id="Pandas函数"><a href="#Pandas函数" class="headerlink" title="Pandas函数"></a>Pandas函数</h1><p>以下三种方法是要将自己或其他库的函数应用于Pandas对象，使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。</p><ul><li>pipe() : 表明智函数应用</li><li>apply() : 行或列函数应用</li><li>applymap() : 元素函数应用</li></ul><h2 id="pipe表格函数"><a href="#pipe表格函数" class="headerlink" title="pipe表格函数"></a>pipe表格函数</h2><p>可以通过将函数和适当数量的参数作为管道参数来执行自定义操作。 因此，对整个DataFrame执行操作。例如，为DataFrame中的所有元素相加一个值<code>2</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adder</span><span class="params">(ele1,ele2)</span>:</span></div><div class="line">   <span class="keyword">return</span> ele1+ele2</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">df.pipe(adder,<span class="number">2</span>)</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.298525 -0.950629  0.164956</span></div><div class="line"><span class="comment"># 1  0.518124  0.952160  0.882564</span></div><div class="line"><span class="comment"># 2  1.215512  2.330336 -1.078768</span></div><div class="line"><span class="comment"># 3 -0.672469  0.139257  0.871575</span></div><div class="line"><span class="comment"># 4 -1.038358  1.132721 -0.705976</span></div><div class="line">print(df.pipe(adder,<span class="number">2</span>)) <span class="comment">#对所有元素加2</span></div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  2.298525  1.049371  2.164956</span></div><div class="line"><span class="comment"># 1  2.518124  2.952160  2.882564</span></div><div class="line"><span class="comment"># 2  3.215512  4.330336  0.921232</span></div><div class="line"><span class="comment"># 3  1.327531  2.139257  2.871575</span></div><div class="line"><span class="comment"># 4  0.961642  3.132721  1.294024</span></div></pre></td></tr></table></figure><h2 id="appy行或列智能函数应用"><a href="#appy行或列智能函数应用" class="headerlink" title="appy行或列智能函数应用"></a>appy行或列智能函数应用</h2><p>可以使用<code>apply()</code>方法沿<code>DataFrame</code>或<code>Panel</code>的轴应用任意函数，它与描述性统计方法一样，采用可选的轴参数。 默认情况下，操作按列执行，将每列列为数组。指定axis=1,则按照行执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adder</span><span class="params">(ele1,ele2)</span>:</span></div><div class="line">   <span class="keyword">return</span> ele1+ele2</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.080134 -0.848024 -0.801573</span></div><div class="line"><span class="comment"># 1 -1.464748 -0.257665 -0.799735</span></div><div class="line"><span class="comment"># 2  0.260537 -0.944930 -0.119062</span></div><div class="line"><span class="comment"># 3  2.061183 -0.904605  0.099470</span></div><div class="line"><span class="comment"># 4  0.114762 -0.927484  0.561150</span></div><div class="line">print(df.apply(np.mean)) <span class="comment">#print(df.apply(np.mean,axis=1))</span></div><div class="line"><span class="comment"># col1    0.210374</span></div><div class="line"><span class="comment"># col2   -0.776542</span></div><div class="line"><span class="comment"># col3   -0.211950</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">adder</span><span class="params">(ele1,ele2)</span>:</span></div><div class="line">   <span class="keyword">return</span> ele1+ele2</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.869596  0.284294  1.302871</span></div><div class="line"><span class="comment"># 1  0.761678  1.430786 -0.708223</span></div><div class="line"><span class="comment"># 2 -0.020315  0.643858  1.274853</span></div><div class="line"><span class="comment"># 3  0.124935  0.165798  1.003215</span></div><div class="line"><span class="comment"># 4 -0.201225  1.139868  1.455647</span></div><div class="line">print(df.apply(<span class="keyword">lambda</span> x:x.max()-x.min()))<span class="comment">#lambda x:定义了一个函数</span></div><div class="line"><span class="comment"># col1    1.070822</span></div><div class="line"><span class="comment"># col2    1.264987</span></div><div class="line"><span class="comment"># col3    2.163870</span></div><div class="line"><span class="keyword">print</span> (df) <span class="comment">#apply不改变df的数据</span></div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.869596  0.284294  1.302871</span></div><div class="line"><span class="comment"># 1  0.761678  1.430786 -0.708223</span></div><div class="line"><span class="comment"># 2 -0.020315  0.643858  1.274853</span></div><div class="line"><span class="comment"># 3  0.124935  0.165798  1.003215</span></div><div class="line"><span class="comment"># 4 -0.201225  1.139868  1.455647</span></div></pre></td></tr></table></figure><h2 id="applymap元素智能函数"><a href="#applymap元素智能函数" class="headerlink" title="applymap元素智能函数"></a>applymap元素智能函数</h2><p>并不是所有的函数都可以向量化(也不是返回另一个数组的NumPy数组，也不是任何值)，在DataFrame上的方法<code>applymap()</code>和类似地在Series上的<code>map()</code>接受任何Python函数，并且返回单个值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"></div><div class="line"><span class="comment"># My custom function</span></div><div class="line">print(df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0 -0.645335 -0.014233 -0.228133</span></div><div class="line"><span class="comment"># 1  0.343590  1.726889 -0.303263</span></div><div class="line"><span class="comment"># 2 -0.179465  0.859529  0.736120</span></div><div class="line"><span class="comment"># 3 -0.459130 -0.487665 -0.078175</span></div><div class="line"><span class="comment"># 4  0.256705  1.299887  0.151205</span></div><div class="line">print(df[<span class="string">'col1'</span>].map(<span class="keyword">lambda</span> x:x*<span class="number">100</span>))<span class="comment">#因为是对列操作，所以map</span></div><div class="line"><span class="comment"># 0   -64.533532</span></div><div class="line"><span class="comment"># 1    34.359000</span></div><div class="line"><span class="comment"># 2   -17.946528</span></div><div class="line"><span class="comment"># 3   -45.913013</span></div><div class="line"><span class="comment"># 4    25.670518</span></div><div class="line"></div><div class="line">print(df.applymap(<span class="keyword">lambda</span> x:x*<span class="number">100</span>))</div><div class="line"><span class="comment">#          col1        col2        col3</span></div><div class="line"><span class="comment"># 0  -69.502940  -16.032075  -55.610105</span></div><div class="line"><span class="comment"># 1   28.380491   23.122472  102.260899</span></div><div class="line"><span class="comment"># 2  -71.143064  -12.581149  145.453437</span></div><div class="line"><span class="comment"># 3  145.381364 -128.505845  -58.202885</span></div><div class="line"><span class="comment"># 4   10.271497  -26.350430   56.805339</span></div></pre></td></tr></table></figure><h1 id="Pandas重建索引"><a href="#Pandas重建索引" class="headerlink" title="Pandas重建索引"></a>Pandas重建索引</h1><p>重新索引会更改DataFrame的行标签和列标签。重新索引意味着符合数据以匹配特定轴上的一组给定的标签。</p><p>可以通过索引来实现多个操作 -</p><ul><li>重新排序现有数据以匹配一组新的标签。</li><li>在没有标签数据的标签位置插入缺失值(NA)标记。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">N=<span class="number">20</span></div><div class="line"></div><div class="line">df = pd.DataFrame(&#123;</div><div class="line">   <span class="string">'A'</span>: pd.date_range(start=<span class="string">'2016-01-01'</span>,periods=N,freq=<span class="string">'D'</span>),</div><div class="line">   <span class="string">'x'</span>: np.linspace(<span class="number">0</span>,stop=N<span class="number">-1</span>,num=N),</div><div class="line">   <span class="string">'y'</span>: np.random.rand(N),</div><div class="line">   <span class="string">'C'</span>: np.random.choice([<span class="string">'Low'</span>,<span class="string">'Medium'</span>,<span class="string">'High'</span>],N).tolist(),</div><div class="line">   <span class="string">'D'</span>: np.random.normal(<span class="number">100</span>, <span class="number">10</span>, size=(N)).tolist()</div><div class="line">&#125;)</div><div class="line"></div><div class="line">print(df)</div><div class="line"><span class="comment">#             A       C           D     x         y</span></div><div class="line"><span class="comment"># 0  2016-01-01    High  106.422798   0.0  0.670789</span></div><div class="line"><span class="comment"># 1  2016-01-02  Medium   90.168862   1.0  0.868026</span></div><div class="line"><span class="comment"># 2  2016-01-03    High   96.854027   2.0  0.974715</span></div><div class="line"><span class="comment"># 3  2016-01-04    High   98.622810   3.0  0.944312</span></div><div class="line"><span class="comment"># 4  2016-01-05  Medium  105.046802   4.0  0.922507</span></div><div class="line"><span class="comment"># 5  2016-01-06     Low  115.478319   5.0  0.091310</span></div><div class="line"><span class="comment"># 6  2016-01-07     Low   91.814775   6.0  0.397314</span></div><div class="line"><span class="comment"># 7  2016-01-08    High   95.030925   7.0  0.545544</span></div><div class="line"><span class="comment"># 8  2016-01-09  Medium  102.620742   8.0  0.214454</span></div><div class="line"><span class="comment"># 9  2016-01-10  Medium  102.370108   9.0  0.933056</span></div><div class="line"><span class="comment"># 10 2016-01-11  Medium   91.638636  10.0  0.464718</span></div><div class="line"><span class="comment"># 11 2016-01-12  Medium   83.425189  11.0  0.267500</span></div><div class="line"><span class="comment"># 12 2016-01-13  Medium  113.570416  12.0  0.899810</span></div><div class="line"><span class="comment"># 13 2016-01-14     Low  129.166525  13.0  0.818797</span></div><div class="line"><span class="comment"># 14 2016-01-15  Medium  105.074786  14.0  0.786602</span></div><div class="line"><span class="comment"># 15 2016-01-16    High  102.818441  15.0  0.451534</span></div><div class="line"><span class="comment"># 16 2016-01-17  Medium   95.567230  16.0  0.739450</span></div><div class="line"><span class="comment"># 17 2016-01-18    High  112.581924  17.0  0.550433</span></div><div class="line"><span class="comment"># 18 2016-01-19    High  114.526018  18.0  0.993903</span></div><div class="line"><span class="comment"># 19 2016-01-20     Low  111.224149  19.0  0.328954</span></div><div class="line"></div><div class="line"><span class="comment">#reindex the DataFrame</span></div><div class="line">df_reindexed = df.reindex(index=[<span class="number">0</span>,<span class="number">2</span>,<span class="number">5</span>], columns=[<span class="string">'A'</span>, <span class="string">'C'</span>, <span class="string">'B'</span>])</div><div class="line"></div><div class="line"><span class="keyword">print</span> (df_reindexed)</div><div class="line"><span class="comment">#            A     C   B</span></div><div class="line"><span class="comment"># 0 2016-01-01  High NaN</span></div><div class="line"><span class="comment"># 2 2016-01-03  High NaN</span></div><div class="line"><span class="comment"># 5 2016-01-06   Low NaN</span></div></pre></td></tr></table></figure><h2 id="重建索引与其他对象对齐"><a href="#重建索引与其他对象对齐" class="headerlink" title="重建索引与其他对象对齐"></a>重建索引与其他对象对齐</h2><p>有时可能希望采取一个对象和重新索引，其轴被标记为与另一个对象相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df1 = pd.DataFrame(np.random.randn(<span class="number">10</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">df2 = pd.DataFrame(np.random.randn(<span class="number">7</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"></div><div class="line">print(df1)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  1.690225 -0.237625  0.682889</span></div><div class="line"><span class="comment"># 1  0.529209  0.531100 -1.973715</span></div><div class="line"><span class="comment"># 2  1.363789 -0.070481  1.149173</span></div><div class="line"><span class="comment"># 3  0.963786  0.378348  0.543242</span></div><div class="line"><span class="comment"># 4 -1.181315  0.453184  1.335767</span></div><div class="line"><span class="comment"># 5 -1.260020 -0.042963  0.210953</span></div><div class="line"><span class="comment"># 6 -0.218576 -1.195482 -1.600476</span></div><div class="line"><span class="comment"># 7  0.347295  0.677118 -2.225262</span></div><div class="line"><span class="comment"># 8  1.349135  0.672671  1.647481</span></div><div class="line"><span class="comment"># 9  2.080021  1.322867  1.502295</span></div><div class="line">print(df2)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0 -0.500110  1.379281 -0.323381</span></div><div class="line"><span class="comment"># 1 -0.498182  0.365029  0.161133</span></div><div class="line"><span class="comment"># 2 -0.598007 -0.010500 -1.304982</span></div><div class="line"><span class="comment"># 3  0.701804 -0.097031 -0.770933</span></div><div class="line"><span class="comment"># 4  1.406598 -0.765083  0.557169</span></div><div class="line"><span class="comment"># 5  0.171932  0.518117 -0.263640</span></div><div class="line"><span class="comment"># 6  0.792754  0.424461  0.602631</span></div><div class="line">df1 = df1.reindex_like(df2)</div><div class="line"><span class="keyword">print</span> (df1)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  1.690225 -0.237625  0.682889</span></div><div class="line"><span class="comment"># 1  0.529209  0.531100 -1.973715</span></div><div class="line"><span class="comment"># 2  1.363789 -0.070481  1.149173</span></div><div class="line"><span class="comment"># 3  0.963786  0.378348  0.543242</span></div><div class="line"><span class="comment"># 4 -1.181315  0.453184  1.335767</span></div><div class="line"><span class="comment"># 5 -1.260020 -0.042963  0.210953</span></div><div class="line"><span class="comment"># 6 -0.218576 -1.195482 -1.600476</span></div></pre></td></tr></table></figure><blockquote><p>注意 - 在这里，<code>df1</code>数据帧(<em>DataFrame</em>)被更改并重新编号，如<code>df2</code>。 列名称应该匹配，否则将为整个列标签添加<code>NAN</code>。</p></blockquote><h2 id="填充时重新加注"><a href="#填充时重新加注" class="headerlink" title="填充时重新加注"></a>填充时重新加注</h2><p><code>reindex()</code>采用可选参数方法，它是一个填充方法，其值如下：</p><ul><li><code>pad/ffill</code> - 向前填充值</li><li><code>bfill/backfill</code> - 向后填充值</li><li><code>nearest</code> - 从最近的索引值填充</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df1 = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">df2 = pd.DataFrame(np.random.randn(<span class="number">2</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"></div><div class="line"><span class="comment"># Padding NAN's</span></div><div class="line"><span class="keyword">print</span> (df2.reindex_like(df1))</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  1.598405  1.451217  1.383048</span></div><div class="line"><span class="comment"># 1 -0.695038 -1.105388 -0.774673</span></div><div class="line"><span class="comment"># 2       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 3       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 4       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 5       NaN       NaN       NaN</span></div><div class="line"></div><div class="line"><span class="comment"># Now Fill the NAN's with preceding Values</span></div><div class="line"><span class="keyword">print</span> (<span class="string">"Data Frame with Forward Fill:"</span>)</div><div class="line"><span class="keyword">print</span> (df2.reindex_like(df1,method=<span class="string">'ffill'</span>))</div><div class="line"><span class="comment"># Data Frame with Forward Fill:</span></div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  1.598405  1.451217  1.383048</span></div><div class="line"><span class="comment"># 1 -0.695038 -1.105388 -0.774673</span></div><div class="line"><span class="comment"># 2 -0.695038 -1.105388 -0.774673</span></div><div class="line"><span class="comment"># 3 -0.695038 -1.105388 -0.774673</span></div><div class="line"><span class="comment"># 4 -0.695038 -1.105388 -0.774673</span></div><div class="line"><span class="comment"># 5 -0.695038 -1.105388 -0.774673</span></div></pre></td></tr></table></figure><blockquote><p>注 - 最后四行被填充了。</p></blockquote><h3 id="重建索引时的填充限制"><a href="#重建索引时的填充限制" class="headerlink" title="重建索引时的填充限制"></a>重建索引时的填充限制</h3><p>限制参数在重建索引时提供对填充的额外控制。限制指定连续匹配的最大计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df1 = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">df2 = pd.DataFrame(np.random.randn(<span class="number">2</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"></div><div class="line"><span class="comment"># Padding NAN's</span></div><div class="line"><span class="keyword">print</span> (df2.reindex_like(df1))</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.424747 -0.060944 -1.374431</span></div><div class="line"><span class="comment"># 1  0.897544  0.922728 -0.951170</span></div><div class="line"><span class="comment"># 2       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 3       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 4       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 5       NaN       NaN       NaN</span></div><div class="line"></div><div class="line"><span class="comment"># Now Fill the NAN's with preceding Values</span></div><div class="line"><span class="keyword">print</span> (<span class="string">"Data Frame with Forward Fill limiting to 1:"</span>)</div><div class="line"><span class="keyword">print</span> (df2.reindex_like(df1,method=<span class="string">'ffill'</span>,limit=<span class="number">1</span>))</div><div class="line"><span class="comment"># Data Frame with Forward Fill limiting to 1:</span></div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.424747 -0.060944 -1.374431</span></div><div class="line"><span class="comment"># 1  0.897544  0.922728 -0.951170</span></div><div class="line"><span class="comment"># 2  0.897544  0.922728 -0.951170</span></div><div class="line"><span class="comment"># 3       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 4       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 5       NaN       NaN       NaN</span></div></pre></td></tr></table></figure><blockquote><p>注意 - 只有第<code>7</code>行由前<code>6</code>行填充。 然后，其它行按原样保留。</p></blockquote><h2 id="重命名"><a href="#重命名" class="headerlink" title="重命名"></a>重命名</h2><p><code>rename()</code>方法允许基于一些映射(字典或者系列)或任意函数来重新标记一个轴。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df1 = pd.DataFrame(np.random.randn(<span class="number">6</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line"><span class="keyword">print</span> (df1)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0 -0.162633  0.487238  0.376467</span></div><div class="line"><span class="comment"># 1 -0.370538 -1.132116  0.320140</span></div><div class="line"><span class="comment"># 2 -1.611416  1.934140  0.223959</span></div><div class="line"><span class="comment"># 3  0.220631  0.303568 -1.636330</span></div><div class="line"><span class="comment"># 4  1.412771 -0.428163 -1.034903</span></div><div class="line"><span class="comment"># 5 -0.804933 -1.000358  0.886100</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (<span class="string">"After renaming the rows and columns:"</span>)</div><div class="line"><span class="keyword">print</span> (df1.rename(columns=&#123;<span class="string">'col1'</span> : <span class="string">'c1'</span>, <span class="string">'col2'</span> : <span class="string">'c2'</span>&#125;,index = &#123;<span class="number">0</span> : <span class="string">'apple'</span>, <span class="number">1</span> : <span class="string">'banana'</span>, <span class="number">2</span> : <span class="string">'durian'</span>&#125;))</div><div class="line"><span class="comment"># After renaming the rows and columns:</span></div><div class="line"><span class="comment">#               c1        c2      col3</span></div><div class="line"><span class="comment"># apple  -0.162633  0.487238  0.376467</span></div><div class="line"><span class="comment"># banana -0.370538 -1.132116  0.320140</span></div><div class="line"><span class="comment"># durian -1.611416  1.934140  0.223959</span></div><div class="line"><span class="comment"># 3       0.220631  0.303568 -1.636330</span></div><div class="line"><span class="comment"># 4       1.412771 -0.428163 -1.034903</span></div><div class="line"><span class="comment"># 5      -0.804933 -1.000358  0.886100</span></div></pre></td></tr></table></figure><h1 id="Pandas迭代"><a href="#Pandas迭代" class="headerlink" title="Pandas迭代"></a>Pandas迭代</h1><p><code>Pandas</code>对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：<code>DataFrame</code>和<code>Panel</code>，遵循类似惯例迭代对象的键。</p><p>简而言之，基本迭代(对于<code>i</code>在对象中)产生 -</p><ul><li><em>Series</em> - 值</li><li><em>DataFrame</em> - 列标签</li><li><em>Pannel</em> - 项目标签</li></ul><h2 id="DataFrame迭代"><a href="#DataFrame迭代" class="headerlink" title="DataFrame迭代"></a>DataFrame迭代</h2><p>要遍历数据帧(DataFrame)中的行，可以使用以下函数 -</p><ul><li><code>iteritems()</code> - 迭代<code>(key，value)</code>对</li><li><code>iterrows()</code> - 将行迭代为(索引，系列)对</li><li><code>itertuples()</code> - 以<code>namedtuples</code>的形式迭代行</li></ul><blockquote><p>注意 - 不要尝试在迭代时修改任何对象。迭代是用于读取，迭代器返回原始对象(视图)的副本，因此更改将不会反映在原始对象上。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">N=<span class="number">5</span></div><div class="line"></div><div class="line">df = pd.DataFrame(&#123;</div><div class="line">    <span class="string">'A'</span>: pd.date_range(start=<span class="string">'2016-01-01'</span>,periods=N,freq=<span class="string">'D'</span>),</div><div class="line">    <span class="string">'x'</span>: np.linspace(<span class="number">0</span>,stop=N<span class="number">-1</span>,num=N),</div><div class="line">    <span class="string">'y'</span>: np.random.rand(N),</div><div class="line">    <span class="string">'C'</span>: np.random.choice([<span class="string">'Low'</span>,<span class="string">'Medium'</span>,<span class="string">'High'</span>],N).tolist(),</div><div class="line">    <span class="string">'D'</span>: np.random.normal(<span class="number">100</span>, <span class="number">10</span>, size=(N)).tolist()</div><div class="line">    &#125;)</div><div class="line">print(df)</div><div class="line"><span class="comment">#            A       C           D    x         y</span></div><div class="line"><span class="comment"># 0 2016-01-01  Medium   68.074794  0.0  0.155689</span></div><div class="line"><span class="comment"># 1 2016-01-02     Low  102.783422  1.0  0.499028</span></div><div class="line"><span class="comment"># 2 2016-01-03     Low   99.767849  2.0  0.072982</span></div><div class="line"><span class="comment"># 3 2016-01-04     Low  121.020377  3.0  0.912389</span></div><div class="line"><span class="comment"># 4 2016-01-05  Medium  108.772650  4.0  0.624642</span></div><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df:</div><div class="line">   <span class="keyword">print</span> (col)</div><div class="line"><span class="comment"># A</span></div><div class="line"><span class="comment"># C</span></div><div class="line"><span class="comment"># D</span></div><div class="line"><span class="comment"># x</span></div><div class="line"><span class="comment"># y</span></div></pre></td></tr></table></figure><h3 id="iteritems"><a href="#iteritems" class="headerlink" title="iteritems()"></a>iteritems()</h3><p>将每个列作为键，将值与值作为键和列值迭代为Series对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">4</span>,<span class="number">3</span>),columns=[<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0 -0.087990 -0.049726  0.572913</span></div><div class="line"><span class="comment"># 1  0.859409  0.695247 -1.382812</span></div><div class="line"><span class="comment"># 2  0.275735 -0.528393 -0.814816</span></div><div class="line"><span class="comment"># 3  0.020638 -0.448942 -1.757958</span></div><div class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> df.iteritems():</div><div class="line">   <span class="keyword">print</span> (key,<span class="string">"---"</span>,value)</div><div class="line"><span class="comment"># col1 --- 0   -0.087990</span></div><div class="line"><span class="comment"># 1    0.859409</span></div><div class="line"><span class="comment"># 2    0.275735</span></div><div class="line"><span class="comment"># 3    0.020638</span></div><div class="line"><span class="comment"># Name: col1, dtype: float64</span></div><div class="line"><span class="comment"># col2 --- 0   -0.049726</span></div><div class="line"><span class="comment"># 1    0.695247</span></div><div class="line"><span class="comment"># 2   -0.528393</span></div><div class="line"><span class="comment"># 3   -0.448942</span></div><div class="line"><span class="comment"># Name: col2, dtype: float64</span></div><div class="line"><span class="comment"># col3 --- 0    0.572913</span></div><div class="line"><span class="comment"># 1   -1.382812</span></div><div class="line"><span class="comment"># 2   -0.814816</span></div><div class="line"><span class="comment"># 3   -1.757958</span></div><div class="line"><span class="comment"># Name: col3, dtype: float64</span></div></pre></td></tr></table></figure><h3 id="iterrows"><a href="#iterrows" class="headerlink" title="iterrows()"></a>iterrows()</h3><p><code>iterrows()</code>返回迭代器，产生每个索引值以及包含每行数据的序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">4</span>,<span class="number">3</span>),columns = [<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0  0.674005  0.042299  0.211592</span></div><div class="line"><span class="comment"># 1  1.080131  1.020547 -1.596309</span></div><div class="line"><span class="comment"># 2 -0.717016 -0.323693  0.697079</span></div><div class="line"><span class="comment"># 3  0.158146  0.936253 -0.103222</span></div><div class="line"><span class="keyword">for</span> row_index,row <span class="keyword">in</span> df.iterrows():</div><div class="line">   <span class="keyword">print</span> (row_index,<span class="string">"---"</span>,row)</div><div class="line"><span class="comment"># 0 --- col1    0.674005</span></div><div class="line"><span class="comment"># col2    0.042299</span></div><div class="line"><span class="comment"># col3    0.211592</span></div><div class="line"><span class="comment"># Name: 0, dtype: float64</span></div><div class="line"><span class="comment"># 1 --- col1    1.080131</span></div><div class="line"><span class="comment"># col2    1.020547</span></div><div class="line"><span class="comment"># col3   -1.596309</span></div><div class="line"><span class="comment"># Name: 1, dtype: float64</span></div><div class="line"><span class="comment"># 2 --- col1   -0.717016</span></div><div class="line"><span class="comment"># col2   -0.323693</span></div><div class="line"><span class="comment"># col3    0.697079</span></div><div class="line"><span class="comment"># Name: 2, dtype: float64</span></div><div class="line"><span class="comment"># 3 --- col1    0.158146</span></div><div class="line"><span class="comment"># col2    0.936253</span></div><div class="line"><span class="comment"># col3   -0.103222</span></div><div class="line"><span class="comment"># Name: 3, dtype: float64</span></div></pre></td></tr></table></figure><h3 id="itertuples"><a href="#itertuples" class="headerlink" title="itertuples()"></a>itertuples()</h3><p><code>itertuples()</code>方法将为<code>DataFrame</code>中的每一行返回一个产生一个命名元组的迭代器。元组的第一个元素将是行的相应索引值，而剩余的值是行值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">4</span>,<span class="number">3</span>),columns = [<span class="string">'col1'</span>,<span class="string">'col2'</span>,<span class="string">'col3'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#        col1      col2      col3</span></div><div class="line"><span class="comment"># 0 -3.525917  0.557613 -0.740162</span></div><div class="line"><span class="comment"># 1 -0.672476 -0.932059  0.638660</span></div><div class="line"><span class="comment"># 2 -0.138301  0.783534 -0.699881</span></div><div class="line"><span class="comment"># 3 -1.003695 -0.485473 -0.651522</span></div><div class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples():</div><div class="line">    <span class="keyword">print</span> (row)</div><div class="line"><span class="comment"># Pandas(Index=0, col1=-3.525917244517192, col2=0.5576131753319682, col3=-0.7401618735482571)</span></div><div class="line"><span class="comment"># Pandas(Index=1, col1=-0.6724761731790204, col2=-0.9320585818750085, col3=0.6386597998390104)</span></div><div class="line"><span class="comment"># Pandas(Index=2, col1=-0.13830132314512975, col2=0.7835337994714413, col3=-0.6998806104325761)</span></div><div class="line"><span class="comment"># Pandas(Index=3, col1=-1.003694634971743, col2=-0.4854728772237874, col3=-0.651522414077405)</span></div></pre></td></tr></table></figure><h1 id="Pandas排序"><a href="#Pandas排序" class="headerlink" title="Pandas排序"></a>Pandas排序</h1><p><em>Pandas</em>有两种排序方式，它们分别是 -</p><ul><li>按标签</li><li>按实际值</li></ul><h2 id="以标签排序"><a href="#以标签排序" class="headerlink" title="以标签排序"></a>以标签排序</h2><p>使用<code>sort_index()</code>方法，通过传递<code>axis</code>参数和排序顺序，可以对<code>DataFrame</code>进行排序。 默认情况下，按照升序对行标签进行排序。</p><p>通过传递<code>axis</code>参数值为<code>0</code>或<code>1</code>，可以对列标签进行排序。 默认情况下，<code>axis = 0</code>，逐行排列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">unsorted_df=pd.DataFrame(np.random.randn(<span class="number">10</span>,<span class="number">2</span>),index=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">7</span>],columns=[<span class="string">'col2'</span>,<span class="string">'col1'</span>])</div><div class="line"><span class="keyword">print</span> (unsorted_df)</div><div class="line"><span class="comment">#        col2      col1</span></div><div class="line"><span class="comment"># 1  1.013623  0.889024</span></div><div class="line"><span class="comment"># 4  1.058798 -0.076676</span></div><div class="line"><span class="comment"># 6 -0.277161  0.665921</span></div><div class="line"><span class="comment"># 2  0.019196  0.636835</span></div><div class="line"><span class="comment"># 3  0.776214 -0.178358</span></div><div class="line"><span class="comment"># 5  0.112524  3.321190</span></div><div class="line"><span class="comment"># 9 -1.172484 -1.609542</span></div><div class="line"><span class="comment"># 8  0.081354  0.878184</span></div><div class="line"><span class="comment"># 0 -1.336118 -0.085982</span></div><div class="line"><span class="comment"># 7  1.398648  0.750015</span></div><div class="line"></div><div class="line">sorted_df=unsorted_df.sort_index() <span class="comment"># sorted_df = unsorted_df.sort_index(ascending=False)降序排序</span></div><div class="line"><span class="keyword">print</span> (sorted_df)</div><div class="line"><span class="comment">#        col2      col1</span></div><div class="line"><span class="comment"># 0 -1.336118 -0.085982</span></div><div class="line"><span class="comment"># 1  1.013623  0.889024</span></div><div class="line"><span class="comment"># 2  0.019196  0.636835</span></div><div class="line"><span class="comment"># 3  0.776214 -0.178358</span></div><div class="line"><span class="comment"># 4  1.058798 -0.076676</span></div><div class="line"><span class="comment"># 5  0.112524  3.321190</span></div><div class="line"><span class="comment"># 6 -0.277161  0.665921</span></div><div class="line"><span class="comment"># 7  1.398648  0.750015</span></div><div class="line"><span class="comment"># 8  0.081354  0.878184</span></div><div class="line"><span class="comment"># 9 -1.172484 -1.609542</span></div></pre></td></tr></table></figure><h2 id="以值排序"><a href="#以值排序" class="headerlink" title="以值排序"></a>以值排序</h2><p>像索引排序一样，<code>sort_values()</code>是按值排序的方法。它接受一个<code>by</code>参数，它将使用要与其排序值的<code>DataFrame</code>的列名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">unsorted_df = pd.DataFrame(&#123;<span class="string">'col1'</span>:[<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],<span class="string">'col2'</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>]&#125;)</div><div class="line">print(unsorted_df)</div><div class="line"><span class="comment">#    col1  col2</span></div><div class="line"><span class="comment"># 0     2     1</span></div><div class="line"><span class="comment"># 1     1     3</span></div><div class="line"><span class="comment"># 2     1     2</span></div><div class="line"><span class="comment"># 3     1     4</span></div><div class="line">sorted_df = unsorted_df.sort_values(by=<span class="string">'col1'</span>)</div><div class="line"><span class="keyword">print</span> (sorted_df) <span class="comment"># col1值被排序，相应的col2值和行索引将随col1一起改变。</span></div><div class="line"><span class="comment">#    col1  col2</span></div><div class="line"><span class="comment"># 1     1     3</span></div><div class="line"><span class="comment"># 2     1     2</span></div><div class="line"><span class="comment"># 3     1     4</span></div><div class="line"><span class="comment"># 0     2     1</span></div></pre></td></tr></table></figure><p>通过<code>by</code>参数指定需要列值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sorted_df = unsorted_df.sort_values(by=[<span class="string">'col1'</span>,<span class="string">'col2'</span>])</div><div class="line"><span class="keyword">print</span> (sorted_df)</div><div class="line"><span class="comment">#    col1  col2</span></div><div class="line"><span class="comment"># 2     1     2</span></div><div class="line"><span class="comment"># 1     1     3</span></div><div class="line"><span class="comment"># 3     1     4</span></div><div class="line"><span class="comment"># 0     2     1</span></div></pre></td></tr></table></figure><blockquote><p>先对col1列排序，使该列有序，同时col2也对应变化为3，2，4，1.接着对col2列排序，这时对前一列相同的值情况下再排序，即3，2，4排序为2，3，4.这样前一列还是有序的</p></blockquote><h1 id="Pandas索引和数据选择"><a href="#Pandas索引和数据选择" class="headerlink" title="Pandas索引和数据选择"></a>Pandas索引和数据选择</h1><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-11 at 3.17.18 PM.png" alt="Screen Shot 2018-06-11 at 3.17.18 PM"></p><h2 id="loc"><a href="#loc" class="headerlink" title=".loc()"></a>.loc()</h2><p><code>.loc()</code>具有多种访问方式，如 -</p><ul><li>单个标量标签</li><li>标签列表</li><li>切片对象</li><li>一个布尔数组</li></ul><p><code>loc</code>需要两个单/列表/范围运算符，用<code>&quot;,&quot;</code>分隔。第一个表示行，第二个表示列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#import the pandas library and aliasing as pd</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">8</span>, <span class="number">4</span>),</div><div class="line">index = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>,<span class="string">'f'</span>,<span class="string">'g'</span>,<span class="string">'h'</span>], columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># a  1.411390  0.307114 -0.988168 -1.757327</span></div><div class="line"><span class="comment"># b  1.723277 -0.494447  1.372176  0.486670</span></div><div class="line"><span class="comment"># c  1.511763  0.132625 -1.242458  0.457793</span></div><div class="line"><span class="comment"># d  0.709051  0.361528  0.366901 -0.565920</span></div><div class="line"><span class="comment"># e -0.030279  1.160672  0.275871 -1.063416</span></div><div class="line"><span class="comment"># f -0.265881 -0.087183  0.313044  1.348987</span></div><div class="line"><span class="comment"># g  1.049137 -1.490819  1.085902  0.857283</span></div><div class="line"><span class="comment"># h -2.894211  0.086263  0.474841 -0.876164</span></div><div class="line"></div><div class="line"><span class="comment">#select all rows for a specific column</span></div><div class="line"><span class="keyword">print</span> (df.loc[:,<span class="string">'A'</span>])</div><div class="line"><span class="comment"># a    1.411390</span></div><div class="line"><span class="comment"># b    1.723277</span></div><div class="line"><span class="comment"># c    1.511763</span></div><div class="line"><span class="comment"># d    0.709051</span></div><div class="line"><span class="comment"># e   -0.030279</span></div><div class="line"><span class="comment"># f   -0.265881</span></div><div class="line"><span class="comment"># g    1.049137</span></div><div class="line"><span class="comment"># h   -2.894211</span></div><div class="line"></div><div class="line"><span class="comment"># Select all rows for multiple columns, say list[]</span></div><div class="line"><span class="keyword">print</span> (df.loc[:,[<span class="string">'A'</span>,<span class="string">'C'</span>]])</div><div class="line"><span class="comment">#           A         C</span></div><div class="line"><span class="comment"># a  1.411390 -0.988168</span></div><div class="line"><span class="comment"># b  1.723277  1.372176</span></div><div class="line"><span class="comment"># c  1.511763 -1.242458</span></div><div class="line"><span class="comment"># d  0.709051  0.366901</span></div><div class="line"><span class="comment"># e -0.030279  0.275871</span></div><div class="line"><span class="comment"># f -0.265881  0.313044</span></div><div class="line"><span class="comment"># g  1.049137  1.085902</span></div><div class="line"><span class="comment"># h -2.894211  0.474841</span></div><div class="line"></div><div class="line"><span class="comment"># Select few rows for multiple columns, say list[]</span></div><div class="line"><span class="keyword">print</span> (df.loc[[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'f'</span>,<span class="string">'h'</span>],[<span class="string">'A'</span>,<span class="string">'C'</span>]])</div><div class="line"><span class="comment">#           A         C</span></div><div class="line"><span class="comment"># a  1.411390 -0.988168</span></div><div class="line"><span class="comment"># b  1.723277  1.372176</span></div><div class="line"><span class="comment"># f -0.265881  0.313044</span></div><div class="line"><span class="comment"># h -2.894211  0.474841</span></div><div class="line"></div><div class="line"><span class="comment"># Select range of rows for all columns</span></div><div class="line"><span class="keyword">print</span> (df.loc[<span class="string">'a'</span>:<span class="string">'c'</span>])</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># a  1.411390  0.307114 -0.988168 -1.757327</span></div><div class="line"><span class="comment"># b  1.723277 -0.494447  1.372176  0.486670</span></div><div class="line"><span class="comment"># c  1.511763  0.132625 -1.242458  0.457793</span></div><div class="line"></div><div class="line"><span class="comment"># for getting values with a boolean array</span></div><div class="line"><span class="keyword">print</span> (df.loc[<span class="string">'a'</span>]&gt;<span class="number">0</span>) <span class="comment">#选择行‘a’，并判断这一行的每一个值是否大于0</span></div><div class="line"><span class="comment"># A     True</span></div><div class="line"><span class="comment"># B     True</span></div><div class="line"><span class="comment"># C    False</span></div><div class="line"><span class="comment"># D    False</span></div></pre></td></tr></table></figure><h2 id="iloc"><a href="#iloc" class="headerlink" title=".iloc()"></a>.iloc()</h2><p><em>Pandas</em>提供了各种方法，以获得纯整数索引。像python和numpy一样，第一个位置是基于<code>0</code>的索引。</p><p>各种访问方式如下 -</p><ul><li>整数</li><li>整数列表</li><li>系列值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">8</span>, <span class="number">4</span>), columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0 -1.097794 -0.343903 -0.545227  0.693677</span></div><div class="line"><span class="comment"># 1  0.748052 -0.800368  0.391323  0.079908</span></div><div class="line"><span class="comment"># 2  1.395456 -0.622070  0.431188  0.321310</span></div><div class="line"><span class="comment"># 3 -0.133916  0.723562 -2.708785  1.397269</span></div><div class="line"><span class="comment"># 4  0.998216  0.229914  1.551281 -0.279701</span></div><div class="line"><span class="comment"># 5 -0.747833 -0.557234 -0.309676 -0.222850</span></div><div class="line"><span class="comment"># 6  1.034332  0.240854  0.730528 -0.825282</span></div><div class="line"><span class="comment"># 7 -0.095764 -0.899946 -0.616187  2.121193</span></div><div class="line"></div><div class="line"><span class="comment"># select all rows for a specific column</span></div><div class="line"><span class="keyword">print</span> (df.iloc[:<span class="number">4</span>])</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0 -1.097794 -0.343903 -0.545227  0.693677</span></div><div class="line"><span class="comment"># 1  0.748052 -0.800368  0.391323  0.079908</span></div><div class="line"><span class="comment"># 2  1.395456 -0.622070  0.431188  0.321310</span></div><div class="line"><span class="comment"># 3 -0.133916  0.723562 -2.708785  1.397269</span></div><div class="line"></div><div class="line"><span class="comment"># Integer slicing</span></div><div class="line"><span class="keyword">print</span> (df.iloc[:<span class="number">4</span>])</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0 -1.097794 -0.343903 -0.545227  0.693677</span></div><div class="line"><span class="comment"># 1  0.748052 -0.800368  0.391323  0.079908</span></div><div class="line"><span class="comment"># 2  1.395456 -0.622070  0.431188  0.321310</span></div><div class="line"><span class="comment"># 3 -0.133916  0.723562 -2.708785  1.397269</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.iloc[<span class="number">1</span>:<span class="number">5</span>, <span class="number">2</span>:<span class="number">4</span>])</div><div class="line"><span class="comment">#           C         D</span></div><div class="line"><span class="comment"># 1  0.391323  0.079908</span></div><div class="line"><span class="comment"># 2  0.431188  0.321310</span></div><div class="line"><span class="comment"># 3 -2.708785  1.397269</span></div><div class="line"><span class="comment"># 4  1.551281 -0.279701</span></div><div class="line"></div><div class="line"><span class="comment"># Slicing through list of values</span></div><div class="line"><span class="keyword">print</span> (df.iloc[[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">3</span>]])</div><div class="line"><span class="comment">#           B         D</span></div><div class="line"><span class="comment"># 1 -0.800368  0.079908</span></div><div class="line"><span class="comment"># 3  0.723562  1.397269</span></div><div class="line"><span class="comment"># 5 -0.557234 -0.222850</span></div><div class="line"><span class="keyword">print</span> (df.iloc[<span class="number">1</span>:<span class="number">3</span>, :])</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 1  0.748052 -0.800368  0.391323  0.079908</span></div><div class="line"><span class="comment"># 2  1.395456 -0.622070  0.431188  0.321310</span></div><div class="line"><span class="keyword">print</span> (df.iloc[:,<span class="number">1</span>:<span class="number">3</span>])</div><div class="line"><span class="comment">#           B         C</span></div><div class="line"><span class="comment"># 0 -0.343903 -0.545227</span></div><div class="line"><span class="comment"># 1 -0.800368  0.391323</span></div><div class="line"><span class="comment"># 2 -0.622070  0.431188</span></div><div class="line"><span class="comment"># 3  0.723562 -2.708785</span></div><div class="line"><span class="comment"># 4  0.229914  1.551281</span></div><div class="line"><span class="comment"># 5 -0.557234 -0.309676</span></div><div class="line"><span class="comment"># 6  0.240854  0.730528</span></div><div class="line"><span class="comment"># 7 -0.899946 -0.616187</span></div></pre></td></tr></table></figure><h2 id="ix"><a href="#ix" class="headerlink" title=".ix()"></a>.ix()</h2><p>除了基于纯标签和整数之外，<em>Pandas</em>还提供了一种使用<code>.ix()</code>运算符进行选择和子集化对象的混合方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">4</span>, <span class="number">4</span>), columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0  1.992586 -1.018359 -0.726185 -0.602579</span></div><div class="line"><span class="comment"># 1  0.112121  0.634097 -1.000867 -0.196700</span></div><div class="line"><span class="comment"># 2 -1.727799  0.033016 -0.250457 -0.009763</span></div><div class="line"><span class="comment"># 3 -0.253342  0.512558 -0.284954 -0.775973</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.ix[:<span class="number">2</span>]) <span class="comment">#前3行</span></div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0  1.992586 -1.018359 -0.726185 -0.602579</span></div><div class="line"><span class="comment"># 1  0.112121  0.634097 -1.000867 -0.196700</span></div><div class="line"><span class="comment"># 2 -1.727799  0.033016 -0.250457 -0.009763</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.ix[:,<span class="string">'A'</span>]) <span class="comment">#'A'列</span></div><div class="line"><span class="comment"># 0    1.992586</span></div><div class="line"><span class="comment"># 1    0.112121</span></div><div class="line"><span class="comment"># 2   -1.727799</span></div><div class="line"><span class="comment"># 3   -0.253342</span></div></pre></td></tr></table></figure><h2 id="多轴索引"><a href="#多轴索引" class="headerlink" title="多轴索引"></a>多轴索引</h2><p>使用基本索引运算符<code>[]</code> </p><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-12 at 9.18.14 AM.png" alt="Screen Shot 2018-06-12 at 9.18.14 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">4</span>, <span class="number">4</span>), columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0 -0.118619 -0.180957  1.119985  0.786177</span></div><div class="line"><span class="comment"># 1  1.948019  2.303557 -1.179559 -0.068304</span></div><div class="line"><span class="comment"># 2  2.138096  1.280755  2.486576 -0.089437</span></div><div class="line"><span class="comment"># 3 -0.998829  0.371025  0.644868  0.166721</span></div><div class="line"></div><div class="line">print(df[<span class="string">'A'</span>])</div><div class="line"><span class="comment"># 0   -0.118619</span></div><div class="line"><span class="comment"># 1    1.948019</span></div><div class="line"><span class="comment"># 2    2.138096</span></div><div class="line"><span class="comment"># 3   -0.998829</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df[[<span class="string">'A'</span>,<span class="string">'B'</span>]])</div><div class="line"><span class="comment">#           A         B</span></div><div class="line"><span class="comment"># 0 -0.118619 -0.180957</span></div><div class="line"><span class="comment"># 1  1.948019  2.303557</span></div><div class="line"><span class="comment"># 2  2.138096  1.280755</span></div><div class="line"><span class="comment"># 3 -0.998829  0.371025</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df[<span class="number">1</span>:<span class="number">2</span>])</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 1  1.948019  2.303557 -1.179559 -0.068304</span></div></pre></td></tr></table></figure><p>可以使用属性运算符<code>.</code>来选择列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">4</span>, <span class="number">4</span>), columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#           A         B         C         D</span></div><div class="line"><span class="comment"># 0  3.115677 -0.164456  0.404479 -0.612033</span></div><div class="line"><span class="comment"># 1 -0.820360 -1.395105  0.189704 -0.449740</span></div><div class="line"><span class="comment"># 2  1.354399  1.575434 -1.080091 -0.219215</span></div><div class="line"><span class="comment"># 3 -0.435956 -0.751293  0.828191  1.198294</span></div><div class="line"></div><div class="line">print(df.A)</div><div class="line"><span class="comment"># 0    3.115677</span></div><div class="line"><span class="comment"># 1   -0.820360</span></div><div class="line"><span class="comment"># 2    1.354399</span></div><div class="line"><span class="comment"># 3   -0.435956</span></div></pre></td></tr></table></figure><h1 id="统计函数"><a href="#统计函数" class="headerlink" title="统计函数"></a>统计函数</h1><h2 id="pct-change-函数"><a href="#pct-change-函数" class="headerlink" title="pct_change()函数"></a>pct_change()函数</h2><p>系列，DatFrames和Panel都有<code>pct_change()</code>函数。此函数将每个元素与其前一个元素进行比较，并计算变化百分比。</p><p>认情况下，<code>pct_change()</code>对列进行操作; 如果想应用到行上，那么可使用<code>axis = 1</code>参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">s = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>])<span class="comment"># [x,y]:(y-x)/x </span></div><div class="line"><span class="keyword">print</span> (s.pct_change())</div><div class="line"><span class="comment"># 0         NaN</span></div><div class="line"><span class="comment"># 1    1.000000</span></div><div class="line"><span class="comment"># 2    0.500000</span></div><div class="line"><span class="comment"># 3    0.333333</span></div><div class="line"><span class="comment"># 4    0.250000</span></div><div class="line"><span class="comment"># 5   -0.200000</span></div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>, <span class="number">2</span>))</div><div class="line"><span class="keyword">print</span> (df.pct_change())</div><div class="line"><span class="comment">#            0         1</span></div><div class="line"><span class="comment"># 0        NaN       NaN</span></div><div class="line"><span class="comment"># 1  -1.475974 -0.719708</span></div><div class="line"><span class="comment"># 2  -0.533308 -2.573370</span></div><div class="line"><span class="comment"># 3  -0.119521 -2.599591</span></div><div class="line"><span class="comment"># 4  14.990426 -1.470710</span></div></pre></td></tr></table></figure><h2 id="cov-协方差"><a href="#cov-协方差" class="headerlink" title="cov()协方差"></a>cov()协方差</h2><p>协方差适用于系列数据。Series对象有一个方法<code>cov</code>用来计算序列对象之间的协方差。<code>NA</code>将被自动排除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">s1 = pd.Series(np.random.randn(<span class="number">10</span>))</div><div class="line">s2 = pd.Series(np.random.randn(<span class="number">10</span>))</div><div class="line"><span class="keyword">print</span> (s1.cov(s2))<span class="comment"># -0.6094391964528769</span></div></pre></td></tr></table></figure><p>当应用于<code>DataFrame</code>时，协方差方法计算所有列之间的协方差(<code>cov</code>)值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">frame = pd.DataFrame(np.random.randn(<span class="number">10</span>, <span class="number">5</span>), columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>])</div><div class="line"><span class="keyword">print</span> (frame[<span class="string">'a'</span>].cov(frame[<span class="string">'b'</span>]))</div><div class="line"><span class="comment"># -0.05532965605044696</span></div><div class="line"><span class="keyword">print</span> (frame.cov()) <span class="comment">#协方差矩阵</span></div><div class="line"><span class="comment">#           a         b         c         d         e</span></div><div class="line"><span class="comment"># a  0.605689 -0.055330 -0.449482  0.110269 -0.093332</span></div><div class="line"><span class="comment"># b -0.055330  0.910118 -0.330857  0.207272 -0.415138</span></div><div class="line"><span class="comment"># c -0.449482 -0.330857  0.881411 -0.347100  0.251697</span></div><div class="line"><span class="comment"># d  0.110269  0.207272 -0.347100  0.437272  0.021039</span></div><div class="line"><span class="comment"># e -0.093332 -0.415138  0.251697  0.021039  0.561143</span></div></pre></td></tr></table></figure><blockquote><p>注 - 观察第一个语句中<code>a</code>和<code>b</code>列之间的<code>cov</code>结果值，与由DataFrame上的<code>cov</code>返回的值相同。</p></blockquote><h2 id="相关性函数"><a href="#相关性函数" class="headerlink" title="相关性函数"></a>相关性函数</h2><p>相关性显示了任何两个数值(系列)之间的线性关系。有多种方法来计算<code>pearson</code>(默认)，<code>spearman</code>和<code>kendall</code>之间的相关性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">frame = pd.DataFrame(np.random.randn(<span class="number">10</span>, <span class="number">5</span>), columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>])</div><div class="line"></div><div class="line"><span class="keyword">print</span> (frame[<span class="string">'a'</span>].corr(frame[<span class="string">'b'</span>]))</div><div class="line"><span class="comment"># -0.10411289414403013</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (frame.corr())</div><div class="line"><span class="comment">#           a         b         c         d         e</span></div><div class="line"><span class="comment"># a  1.000000 -0.104113 -0.136458  0.732508  0.372643</span></div><div class="line"><span class="comment"># b -0.104113  1.000000  0.098084 -0.074208  0.275227</span></div><div class="line"><span class="comment"># c -0.136458  0.098084  1.000000  0.280941  0.592759</span></div><div class="line"><span class="comment"># d  0.732508 -0.074208  0.280941  1.000000  0.572732</span></div><div class="line"><span class="comment"># e  0.372643  0.275227  0.592759  0.572732  1.000000</span></div></pre></td></tr></table></figure><blockquote><p>如果DataFrame中存在任何非数字列，则会自动排除。</p></blockquote><h2 id="数据排名"><a href="#数据排名" class="headerlink" title="数据排名"></a>数据排名</h2><p>数据排名为元素数组中的每个元素生成排名，不是比大小哦。在关系的情况下，分配平均等级。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">s = pd.Series(np.random.np.random.randn(<span class="number">5</span>), index=list(<span class="string">'abcde'</span>))</div><div class="line">print(s)</div><div class="line"><span class="comment"># a    0.198131</span></div><div class="line"><span class="comment"># b    0.544257</span></div><div class="line"><span class="comment"># c   -0.253626</span></div><div class="line"><span class="comment"># d    0.163365</span></div><div class="line"><span class="comment"># e    0.105286</span></div><div class="line">s[<span class="string">'d'</span>] = s[<span class="string">'b'</span>] <span class="comment"># so there's a tie</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (s.rank()) <span class="comment">#根据值得东西分配等级，因为有两个值相等，分别是（5级+4级）/2 = 4.5级</span></div><div class="line"><span class="comment"># a    3.0</span></div><div class="line"><span class="comment"># b    4.5</span></div><div class="line"><span class="comment"># c    1.0</span></div><div class="line"><span class="comment"># d    4.5</span></div><div class="line"><span class="comment"># e    2.0</span></div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-12 at 9.40.29 AM.png" alt="Screen Shot 2018-06-12 at 9.40.29 AM"></p><h1 id="Pandas窗口函数"><a href="#Pandas窗口函数" class="headerlink" title="Pandas窗口函数"></a>Pandas窗口函数</h1><p>为了处理数字数据，Pandas提供了几个变体，如滚动，展开和指数移动窗口统计的权重。 其中包括总和，均值，中位数，方差，协方差，相关性等。</p><h2 id="rolling"><a href="#rolling" class="headerlink" title=".rolling()"></a>.rolling()</h2><p>这个函数可以应用于Series数据。指定<code>window=n</code>参数并在其上应用适当的统计函数。</p><p>窗口大小指定几行数据进行运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>],[<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>]],</div><div class="line">index = pd.date_range(<span class="string">'1/1/2020'</span>, periods=<span class="number">4</span>), columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line"></div><div class="line">print(df)</div><div class="line"><span class="comment">#              A   B   C   D</span></div><div class="line"><span class="comment"># 2020-01-01   1   2   3   4</span></div><div class="line"><span class="comment"># 2020-01-02   4   5   6   7</span></div><div class="line"><span class="comment"># 2020-01-03   8   9  10  11</span></div><div class="line"><span class="comment"># 2020-01-04  12  13  14  15</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.rolling(window=<span class="number">2</span>).mean())</div><div class="line"><span class="comment">#                A     B     C     D</span></div><div class="line"><span class="comment"># 2020-01-01   NaN   NaN   NaN   NaN</span></div><div class="line"><span class="comment"># 2020-01-02   2.5   3.5   4.5   5.5</span></div><div class="line"><span class="comment"># 2020-01-03   6.0   7.0   8.0   9.0</span></div><div class="line"><span class="comment"># 2020-01-04  10.0  11.0  12.0  13.0</span></div></pre></td></tr></table></figure><h2 id="expanding"><a href="#expanding" class="headerlink" title=".expanding()"></a>.expanding()</h2><p>这个函数可以应用于Series数据。 指定<code>min_periods = n</code>参数并在其上应用适当的统计函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (df.expanding(min_periods=<span class="number">2</span>).mean())</div><div class="line"><span class="comment">#                    A         B         C         D</span></div><div class="line"><span class="comment"># 2020-01-01       NaN       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># 2020-01-02  2.500000  3.500000  4.500000  5.500000 (行1和行2的平均值)</span></div><div class="line"><span class="comment"># 2020-01-03  4.333333  5.333333  6.333333  7.333333（行1、2、3的3行平均值）</span></div><div class="line"><span class="comment"># 2020-01-04  6.250000  7.250000  8.250000  9.250000（4行的平均值）</span></div></pre></td></tr></table></figure><h2 id="ewm"><a href="#ewm" class="headerlink" title=".ewm()"></a>.ewm()</h2><p><code>ewm()</code>可应用于系列数据。指定<code>com</code>，<code>span</code>，<code>halflife</code>参数，并在其上应用适当的统计函数。它以指数形式分配权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (df.ewm(com=<span class="number">0.5</span>).mean())</div><div class="line"><span class="comment">#                     A          B          C          D</span></div><div class="line"><span class="comment"># 2020-01-01   1.000000   2.000000   3.000000   4.000000</span></div><div class="line"><span class="comment"># 2020-01-02   3.250000   4.250000   5.250000   6.250000</span></div><div class="line"><span class="comment"># 2020-01-03   6.538462   7.538462   8.538462   9.538462</span></div><div class="line"><span class="comment"># 2020-01-04  10.225000  11.225000  12.225000  13.225000</span></div></pre></td></tr></table></figure><h1 id="Pandas聚合"><a href="#Pandas聚合" class="headerlink" title="Pandas聚合"></a>Pandas聚合</h1><p>当有了滚动，扩展和<code>ewm</code>对象创建了以后，就有几种方法可以对数据执行聚合。</p><h2 id="在整个数据框上应用聚合"><a href="#在整个数据框上应用聚合" class="headerlink" title="在整个数据框上应用聚合"></a>在整个数据框上应用聚合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>],[<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>]],</div><div class="line">index = pd.date_range(<span class="string">'1/1/2020'</span>, periods=<span class="number">4</span>), columns = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>])</div><div class="line"></div><div class="line">print(df)</div><div class="line"><span class="comment">#              A   B   C   D</span></div><div class="line"><span class="comment"># 2020-01-01   1   2   3   4</span></div><div class="line"><span class="comment"># 2020-01-02   4   5   6   7</span></div><div class="line"><span class="comment"># 2020-01-03   8   9  10  11</span></div><div class="line"><span class="comment"># 2020-01-04  12  13  14  15</span></div><div class="line"></div><div class="line">r = df.rolling(window=<span class="number">2</span>,min_periods=<span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> (r) <span class="comment">#Rolling [window=2,min_periods=1,center=False,axis=0]</span></div><div class="line">print(r.aggregate(np.sum))</div><div class="line"><span class="comment">#                A     B     C     D</span></div><div class="line"><span class="comment"># 2020-01-01   1.0   2.0   3.0   4.0</span></div><div class="line"><span class="comment"># 2020-01-02   5.0   7.0   9.0  11.0</span></div><div class="line"><span class="comment"># 2020-01-03  12.0  14.0  16.0  18.0</span></div><div class="line"><span class="comment"># 2020-01-04  20.0  22.0  24.0  26.0</span></div></pre></td></tr></table></figure><h2 id="在数据框的单个列上应用聚合"><a href="#在数据框的单个列上应用聚合" class="headerlink" title="在数据框的单个列上应用聚合"></a><strong>在数据框的单个列上应用聚合</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (r[<span class="string">'A'</span>].aggregate(np.sum))</div><div class="line"><span class="comment"># 2020-01-01     1.0</span></div><div class="line"><span class="comment"># 2020-01-02     5.0</span></div><div class="line"><span class="comment"># 2020-01-03    12.0</span></div><div class="line"><span class="comment"># 2020-01-04    20.0</span></div></pre></td></tr></table></figure><h2 id="在DataFrame的多列上应用聚合"><a href="#在DataFrame的多列上应用聚合" class="headerlink" title="在DataFrame的多列上应用聚合"></a>在DataFrame的多列上应用聚合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (r[[<span class="string">'A'</span>,<span class="string">'B'</span>]].aggregate(np.sum))</div><div class="line"><span class="comment">#                A     B</span></div><div class="line"><span class="comment"># 2020-01-01   1.0   2.0</span></div><div class="line"><span class="comment"># 2020-01-02   5.0   7.0</span></div><div class="line"><span class="comment"># 2020-01-03  12.0  14.0</span></div><div class="line"><span class="comment"># 2020-01-04  20.0  22.0</span></div></pre></td></tr></table></figure><h2 id="在DataFrame的单个列上应用多个函数"><a href="#在DataFrame的单个列上应用多个函数" class="headerlink" title="在DataFrame的单个列上应用多个函数"></a>在DataFrame的单个列上应用多个函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (r[<span class="string">'A'</span>].aggregate([np.sum,np.mean]))</div><div class="line"><span class="comment">#              sum  mean</span></div><div class="line"><span class="comment"># 2020-01-01   1.0   1.0</span></div><div class="line"><span class="comment"># 2020-01-02   5.0   2.5</span></div><div class="line"><span class="comment"># 2020-01-03  12.0   6.0</span></div><div class="line"><span class="comment"># 2020-01-04  20.0  10.0</span></div></pre></td></tr></table></figure><h2 id="在DataFrame的多列上应用多个函数"><a href="#在DataFrame的多列上应用多个函数" class="headerlink" title="在DataFrame的多列上应用多个函数"></a>在DataFrame的多列上应用多个函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (r[[<span class="string">'A'</span>,<span class="string">'B'</span>]].aggregate([np.sum,np.mean]))</div><div class="line"><span class="comment">#                A           B      </span></div><div class="line"><span class="comment">#              sum  mean   sum  mean</span></div><div class="line"><span class="comment"># 2020-01-01   1.0   1.0   2.0   2.0</span></div><div class="line"><span class="comment"># 2020-01-02   5.0   2.5   7.0   3.5</span></div><div class="line"><span class="comment"># 2020-01-03  12.0   6.0  14.0   7.0</span></div><div class="line"><span class="comment"># 2020-01-04  20.0  10.0  22.0  11.0</span></div></pre></td></tr></table></figure><h2 id="将不同的函数应用于DataFrame的不同列"><a href="#将不同的函数应用于DataFrame的不同列" class="headerlink" title="将不同的函数应用于DataFrame的不同列"></a>将不同的函数应用于DataFrame的不同列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (r.aggregate(&#123;<span class="string">'A'</span> : np.sum,<span class="string">'B'</span> : np.mean&#125;))</div><div class="line"><span class="comment">#                A     B</span></div><div class="line"><span class="comment"># 2020-01-01   1.0   2.0</span></div><div class="line"><span class="comment"># 2020-01-02   5.0   3.5</span></div><div class="line"><span class="comment"># 2020-01-03  12.0   7.0</span></div><div class="line"><span class="comment"># 2020-01-04  20.0  11.0</span></div></pre></td></tr></table></figure><h1 id="Pandas缺失数据"><a href="#Pandas缺失数据" class="headerlink" title="Pandas缺失数据"></a>Pandas缺失数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>, <span class="number">3</span>), index=[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>,</div><div class="line"><span class="string">'h'</span>],columns=[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -1.188673  0.575815 -0.720743</span></div><div class="line"><span class="comment"># c -0.044184 -0.581790 -1.967263</span></div><div class="line"><span class="comment"># e  0.969510  0.183313 -0.311744</span></div><div class="line"><span class="comment"># f  0.358851  0.212901  0.849545</span></div><div class="line"><span class="comment"># h  0.302852 -1.235476 -0.113741</span></div><div class="line"></div><div class="line">df = df.reindex([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>])</div><div class="line"></div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -1.188673  0.575815 -0.720743</span></div><div class="line"><span class="comment"># b       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># c -0.044184 -0.581790 -1.967263</span></div><div class="line"><span class="comment"># d       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># e  0.969510  0.183313 -0.311744</span></div><div class="line"><span class="comment"># f  0.358851  0.212901  0.849545</span></div><div class="line"><span class="comment"># g       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># h  0.302852 -1.235476 -0.113741</span></div></pre></td></tr></table></figure><blockquote><p>使用重构索引(reindexing)，创建了一个缺少值的DataFrame。 在输出中，<code>NaN</code>表示不是数字的值。</p></blockquote><h2 id="检查缺失值"><a href="#检查缺失值" class="headerlink" title="检查缺失值"></a>检查缺失值</h2><p>为了更容易地检测缺失值(以及跨越不同的数组<code>dtype</code>)，Pandas提供了<code>isnull()</code>和<code>notnull()</code>函数，它们也是Series和DataFrame对象的方法 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>, <span class="number">3</span>), index=[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>,</div><div class="line"><span class="string">'h'</span>],columns=[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -1.188673  0.575815 -0.720743</span></div><div class="line"><span class="comment"># c -0.044184 -0.581790 -1.967263</span></div><div class="line"><span class="comment"># e  0.969510  0.183313 -0.311744</span></div><div class="line"><span class="comment"># f  0.358851  0.212901  0.849545</span></div><div class="line"><span class="comment"># h  0.302852 -1.235476 -0.113741</span></div><div class="line"></div><div class="line">df = df.reindex([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.116164 -0.209310 -0.695126</span></div><div class="line"><span class="comment"># c -0.638017  0.303101  0.614645</span></div><div class="line"><span class="comment"># e  0.839940 -0.693997  0.213362</span></div><div class="line"><span class="comment"># f -0.405377  0.633845  0.196080</span></div><div class="line"><span class="comment"># h -0.448376  1.459465  0.051836</span></div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.116164 -0.209310 -0.695126</span></div><div class="line"><span class="comment"># b       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># c -0.638017  0.303101  0.614645</span></div><div class="line"><span class="comment"># d       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># e  0.839940 -0.693997  0.213362</span></div><div class="line"><span class="comment"># f -0.405377  0.633845  0.196080</span></div><div class="line"><span class="comment"># g       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># h -0.448376  1.459465  0.051836</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df[<span class="string">'one'</span>].isnull())</div><div class="line"><span class="comment"># a    False</span></div><div class="line"><span class="comment"># b     True</span></div><div class="line"><span class="comment"># c    False</span></div><div class="line"><span class="comment"># d     True</span></div><div class="line"><span class="comment"># e    False</span></div><div class="line"><span class="comment"># f    False</span></div><div class="line"><span class="comment"># g     True</span></div><div class="line"><span class="comment"># h    False</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df[<span class="string">'one'</span>].notnull())</div><div class="line"><span class="comment"># a     True</span></div><div class="line"><span class="comment"># b    False</span></div><div class="line"><span class="comment"># c     True</span></div><div class="line"><span class="comment"># d    False</span></div><div class="line"><span class="comment"># e     True</span></div><div class="line"><span class="comment"># f     True</span></div><div class="line"><span class="comment"># g    False</span></div><div class="line"><span class="comment"># h     True</span></div></pre></td></tr></table></figure><h2 id="缺失数据的计算"><a href="#缺失数据的计算" class="headerlink" title="缺失数据的计算"></a>缺失数据的计算</h2><ul><li>在数据计算时，<code>NA</code>将被视为<code>0</code></li><li>如果数据全部是<code>NA</code>，那么结果将是0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]], index=[<span class="string">'a'</span>, <span class="string">'c'</span>,</div><div class="line"><span class="string">'h'</span>],columns=[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#    one  two  three</span></div><div class="line"><span class="comment"># a    1    2      3</span></div><div class="line"><span class="comment"># c    4    5      6</span></div><div class="line"><span class="comment"># h    7    8      9</span></div><div class="line"></div><div class="line">df = df.reindex([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#    one  two  three</span></div><div class="line"><span class="comment"># a  1.0  2.0    3.0</span></div><div class="line"><span class="comment"># b  NaN  NaN    NaN</span></div><div class="line"><span class="comment"># c  4.0  5.0    6.0</span></div><div class="line"><span class="comment"># d  NaN  NaN    NaN</span></div><div class="line"><span class="comment"># e  NaN  NaN    NaN</span></div><div class="line"><span class="comment"># f  NaN  NaN    NaN</span></div><div class="line"><span class="comment"># g  NaN  NaN    NaN</span></div><div class="line"><span class="comment"># h  7.0  8.0    9.0</span></div><div class="line"><span class="keyword">print</span> (df[<span class="string">'one'</span>].sum()) <span class="comment">#12</span></div><div class="line"></div><div class="line">df = pd.DataFrame(index=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],columns=[<span class="string">'one'</span>,<span class="string">'two'</span>])</div><div class="line"><span class="keyword">print</span> (df[<span class="string">'one'</span>].sum()) <span class="comment">#0</span></div></pre></td></tr></table></figure><h2 id="缺失数据清理-填充"><a href="#缺失数据清理-填充" class="headerlink" title="缺失数据清理/填充"></a>缺失数据清理/填充</h2><p><em>Pandas</em>提供了各种方法来清除缺失的值。<code>fillna()</code>函数可以通过几种方法用非空数据“填充”<code>NA</code>值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">3</span>, <span class="number">3</span>), index=[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>],columns=[<span class="string">'one'</span>,</div><div class="line"><span class="string">'two'</span>, <span class="string">'three'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.158275 -1.280046 -0.670825</span></div><div class="line"><span class="comment"># c  0.826673  0.510478 -1.862733</span></div><div class="line"><span class="comment"># e  1.237025 -1.371099  0.104532</span></div><div class="line"></div><div class="line">df = df.reindex([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.158275 -1.280046 -0.670825</span></div><div class="line"><span class="comment"># b       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># c  0.826673  0.510478 -1.862733</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.fillna(<span class="number">0</span>))</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.158275 -1.280046 -0.670825</span></div><div class="line"><span class="comment"># b  0.000000  0.000000  0.000000</span></div><div class="line"><span class="comment"># c  0.826673  0.510478 -1.862733</span></div></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">行为</th></tr></thead><tbody><tr><td style="text-align:center">pad/fill</td><td style="text-align:center">填充的方法向前</td></tr><tr><td style="text-align:center">bfill/backfill</td><td style="text-align:center">填充的方法向后</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">3</span>, <span class="number">3</span>), index=[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>],columns=[<span class="string">'one'</span>,</div><div class="line"><span class="string">'two'</span>, <span class="string">'three'</span>])</div><div class="line"></div><div class="line">df = df.reindex([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.634633 -1.146325  0.684612</span></div><div class="line"><span class="comment"># b       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># c  0.378554  0.018536  0.472808</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.fillna(method=<span class="string">'pad'</span>))</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.634633 -1.146325  0.684612</span></div><div class="line"><span class="comment"># b -0.634633 -1.146325  0.684612</span></div><div class="line"><span class="comment"># c  0.378554  0.018536  0.472808</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.fillna(method=<span class="string">'backfill'</span>))</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.634633 -1.146325  0.684612</span></div><div class="line"><span class="comment"># b  0.378554  0.018536  0.472808</span></div><div class="line"><span class="comment"># c  0.378554  0.018536  0.472808</span></div></pre></td></tr></table></figure><h2 id="放弃缺失值"><a href="#放弃缺失值" class="headerlink" title="放弃缺失值"></a>放弃缺失值</h2><p>如果只想排除缺少的值，则使用<code>dropna</code>函数和<code>axis</code>参数。 默认情况下，<code>axis = 0</code>，即在行上应用，这意味着如果行内的任何值是<code>NA</code>，那么整个行被排除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>, <span class="number">3</span>), index=[<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>,</div><div class="line"><span class="string">'h'</span>],columns=[<span class="string">'one'</span>, <span class="string">'two'</span>, <span class="string">'three'</span>])</div><div class="line"></div><div class="line">df = df.reindex([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>])</div><div class="line">print(df)</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.147621 -3.030745 -0.633145</span></div><div class="line"><span class="comment"># b       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># c -1.595044 -0.100556  1.246761</span></div><div class="line"><span class="comment"># d       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># e -0.990907 -0.416591  0.763210</span></div><div class="line"><span class="comment"># f  0.908294  1.061932 -0.889028</span></div><div class="line"><span class="comment"># g       NaN       NaN       NaN</span></div><div class="line"><span class="comment"># h  1.558081 -0.399321 -0.272056</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.dropna())</div><div class="line"><span class="comment">#         one       two     three</span></div><div class="line"><span class="comment"># a -0.147621 -3.030745 -0.633145</span></div><div class="line"><span class="comment"># c -1.595044 -0.100556  1.246761</span></div><div class="line"><span class="comment"># e -0.990907 -0.416591  0.763210</span></div><div class="line"><span class="comment"># f  0.908294  1.061932 -0.889028</span></div><div class="line"><span class="comment"># h  1.558081 -0.399321 -0.272056</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.dropna(axis=<span class="number">1</span>))</div><div class="line"><span class="comment"># Empty DataFrame</span></div><div class="line"><span class="comment"># Columns: []</span></div><div class="line"><span class="comment"># Index: [a, b, c, d, e, f, g, h]</span></div></pre></td></tr></table></figure><h2 id="值替换"><a href="#值替换" class="headerlink" title="值替换"></a>值替换</h2><p>很多时候，必须用一些具体的值取代一个通用的值。可以通过应用替换方法来实现这一点。</p><p>用标量值替换<code>NA</code>是<code>fillna()</code>函数的等效行为。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">df = pd.DataFrame(&#123;<span class="string">'one'</span>:[<span class="number">1000</span>,<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">2000</span>],</div><div class="line"><span class="string">'two'</span>:[<span class="number">1000</span>,<span class="number">0</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>]&#125;)</div><div class="line">print(df)</div><div class="line"><span class="comment">#     one   two</span></div><div class="line"><span class="comment"># 0    1000  1000</span></div><div class="line"><span class="comment"># 1    20     0</span></div><div class="line"><span class="comment"># 2    30    30</span></div><div class="line"><span class="comment"># 3    40    40</span></div><div class="line"><span class="comment"># 4    50    50</span></div><div class="line"><span class="comment"># 5  2000    60</span></div><div class="line"><span class="keyword">print</span> (df.replace(&#123;<span class="number">1000</span>:<span class="number">10</span>,<span class="number">2000</span>:<span class="number">60</span>&#125;))</div><div class="line"><span class="comment">#    one  two</span></div><div class="line"><span class="comment"># 0   10   10</span></div><div class="line"><span class="comment"># 1   20    0</span></div><div class="line"><span class="comment"># 2   30   30</span></div><div class="line"><span class="comment"># 3   40   40</span></div><div class="line"><span class="comment"># 4   50   50</span></div><div class="line"><span class="comment"># 5   60   60</span></div></pre></td></tr></table></figure><h1 id="Pandas分组"><a href="#Pandas分组" class="headerlink" title="Pandas分组"></a>Pandas分组</h1><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-12 at 3.45.43 PM.png" alt="Screen Shot 2018-06-12 at 3.45.43 PM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">ipl_data = &#123;<span class="string">'Team'</span>: [<span class="string">'Riders'</span>, <span class="string">'Riders'</span>, <span class="string">'Devils'</span>, <span class="string">'Devils'</span>, <span class="string">'Kings'</span>,</div><div class="line">         <span class="string">'kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Riders'</span>, <span class="string">'Royals'</span>, <span class="string">'Royals'</span>, <span class="string">'Riders'</span>],</div><div class="line">         <span class="string">'Rank'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">4</span> ,<span class="number">1</span> ,<span class="number">1</span>,<span class="number">2</span> , <span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">         <span class="string">'Year'</span>: [<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2016</span>,<span class="number">2017</span>,<span class="number">2016</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2017</span>],</div><div class="line">         <span class="string">'Points'</span>:[<span class="number">876</span>,<span class="number">789</span>,<span class="number">863</span>,<span class="number">673</span>,<span class="number">741</span>,<span class="number">812</span>,<span class="number">756</span>,<span class="number">788</span>,<span class="number">694</span>,<span class="number">701</span>,<span class="number">804</span>,<span class="number">690</span>]&#125;</div><div class="line">df = pd.DataFrame(ipl_data)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (df)</div><div class="line"><span class="comment">#     Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 0      876     1  Riders  2014</span></div><div class="line"><span class="comment"># 1      789     2  Riders  2015</span></div><div class="line"><span class="comment"># 2      863     2  Devils  2014</span></div><div class="line"><span class="comment"># 3      673     3  Devils  2015</span></div><div class="line"><span class="comment"># 4      741     3   Kings  2014</span></div><div class="line"><span class="comment"># 5      812     4   kings  2015</span></div><div class="line"><span class="comment"># 6      756     1   Kings  2016</span></div><div class="line"><span class="comment"># 7      788     1   Kings  2017</span></div><div class="line"><span class="comment"># 8      694     2  Riders  2016</span></div><div class="line"><span class="comment"># 9      701     4  Royals  2014</span></div><div class="line"><span class="comment"># 10     804     1  Royals  2015</span></div><div class="line"><span class="comment"># 11     690     2  Riders  2017</span></div></pre></td></tr></table></figure><h2 id="数据拆分成组"><a href="#数据拆分成组" class="headerlink" title="数据拆分成组"></a>数据拆分成组</h2><p>Pandas对象可以分成任何对象。有多种方式来拆分对象，如 -</p><ul><li>obj.groupby(‘key’)</li><li>obj.groupby([‘key1’,’key2’])</li><li>obj.groupby(key,axis=1)</li></ul><p><strong>单列分组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (df.groupby(<span class="string">'Team'</span>)) <span class="comment">#&lt;pandas.core.groupby.DataFrameGroupBy object at 0x110c0d588&gt;</span></div><div class="line"><span class="keyword">print</span> (df.groupby(<span class="string">'Team'</span>).groups)<span class="comment"># 查看分组</span></div><div class="line"><span class="comment"># &#123;</span></div><div class="line"><span class="comment"># 'Devils': Int64Index([2, 3], dtype='int64'),</span></div><div class="line"><span class="comment"># 'Kings': Int64Index([4, 6, 7], dtype='int64'),</span></div><div class="line"><span class="comment"># 'Riders': Int64Index([0, 1, 8, 11], dtype='int64'),</span></div><div class="line"><span class="comment"># 'Royals': Int64Index([9, 10], dtype='int64'),</span></div><div class="line"><span class="comment"># 'kings': Int64Index([5], dtype='int64')</span></div><div class="line"><span class="comment"># &#125;</span></div></pre></td></tr></table></figure><p><strong>多列分组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> (df.groupby([<span class="string">'Team'</span>,<span class="string">'Year'</span>]).groups)</div><div class="line"><span class="comment"># &#123;</span></div><div class="line"><span class="comment"># ('Devils', 2014): Int64Index([2], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Devils', 2015): Int64Index([3], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Kings', 2014): Int64Index([4], dtype='int64'),</span></div><div class="line"><span class="comment"># ('Kings', 2016): Int64Index([6], dtype='int64'),</span></div><div class="line"><span class="comment"># ('Kings', 2017): Int64Index([7], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Riders', 2014): Int64Index([0], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Riders', 2015): Int64Index([1], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Riders', 2016): Int64Index([8], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Riders', 2017): Int64Index([11], dtype='int64'),</span></div><div class="line"><span class="comment"># ('Royals', 2014): Int64Index([9], dtype='int64'), </span></div><div class="line"><span class="comment"># ('Royals', 2015): Int64Index([10], dtype='int64'), </span></div><div class="line"><span class="comment"># ('kings', 2015): Int64Index([5], dtype='int64')</span></div><div class="line"><span class="comment"># &#125;</span></div></pre></td></tr></table></figure><h3 id="迭代遍历分组"><a href="#迭代遍历分组" class="headerlink" title="迭代遍历分组"></a>迭代遍历分组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">grouped = df.groupby(<span class="string">'Year'</span>)</div><div class="line"><span class="keyword">for</span> name,group <span class="keyword">in</span> grouped:</div><div class="line">    <span class="keyword">print</span> (name)</div><div class="line">    <span class="keyword">print</span> (group)</div><div class="line"><span class="comment"># 2014</span></div><div class="line"><span class="comment">#    Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 0     876     1  Riders  2014</span></div><div class="line"><span class="comment"># 2     863     2  Devils  2014</span></div><div class="line"><span class="comment"># 4     741     3   Kings  2014</span></div><div class="line"><span class="comment"># 9     701     4  Royals  2014</span></div><div class="line"><span class="comment"># 2015</span></div><div class="line"><span class="comment">#     Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 1      789     2  Riders  2015</span></div><div class="line"><span class="comment"># 3      673     3  Devils  2015</span></div><div class="line"><span class="comment"># 5      812     4   kings  2015</span></div><div class="line"><span class="comment"># 10     804     1  Royals  2015</span></div><div class="line"><span class="comment"># 2016</span></div><div class="line"><span class="comment">#    Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 6     756     1   Kings  2016</span></div><div class="line"><span class="comment"># 8     694     2  Riders  2016</span></div><div class="line"><span class="comment"># 2017</span></div><div class="line"><span class="comment">#     Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 7      788     1   Kings  2017</span></div><div class="line"><span class="comment"># 11     690     2  Riders  2017</span></div></pre></td></tr></table></figure><h3 id="选择一个分组"><a href="#选择一个分组" class="headerlink" title="选择一个分组"></a>选择一个分组</h3><p>使用<code>get_group()</code>方法，可以选择一个组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">grouped = df.groupby(<span class="string">'Year'</span>)</div><div class="line"><span class="keyword">print</span> (grouped.get_group(<span class="number">2014</span>))</div><div class="line"><span class="comment">#    Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 0     876     1  Riders  2014</span></div><div class="line"><span class="comment"># 2     863     2  Devils  2014</span></div><div class="line"><span class="comment"># 4     741     3   Kings  2014</span></div><div class="line"><span class="comment"># 9     701     4  Royals  2014</span></div></pre></td></tr></table></figure><h2 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h2><p>聚合函数为每个组返回单个聚合值。当创建了分组(<em>group by</em>)对象，就可以对分组数据执行多个聚合操作。一个比较常用的是通过聚合或等效的<code>agg</code>方法聚合 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">ipl_data = &#123;<span class="string">'Team'</span>: [<span class="string">'Riders'</span>, <span class="string">'Riders'</span>, <span class="string">'Devils'</span>, <span class="string">'Devils'</span>, <span class="string">'Kings'</span>,</div><div class="line">         <span class="string">'kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Riders'</span>, <span class="string">'Royals'</span>, <span class="string">'Royals'</span>, <span class="string">'Riders'</span>],</div><div class="line">         <span class="string">'Rank'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">4</span> ,<span class="number">1</span> ,<span class="number">1</span>,<span class="number">2</span> , <span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">         <span class="string">'Year'</span>: [<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2016</span>,<span class="number">2017</span>,<span class="number">2016</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2017</span>],</div><div class="line">         <span class="string">'Points'</span>:[<span class="number">876</span>,<span class="number">789</span>,<span class="number">863</span>,<span class="number">673</span>,<span class="number">741</span>,<span class="number">812</span>,<span class="number">756</span>,<span class="number">788</span>,<span class="number">694</span>,<span class="number">701</span>,<span class="number">804</span>,<span class="number">690</span>]&#125;</div><div class="line">df = pd.DataFrame(ipl_data)</div><div class="line">print(df)</div><div class="line"><span class="comment">#     Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 0      876     1  Riders  2014</span></div><div class="line"><span class="comment"># 1      789     2  Riders  2015</span></div><div class="line"><span class="comment"># 2      863     2  Devils  2014</span></div><div class="line"><span class="comment"># 3      673     3  Devils  2015</span></div><div class="line"><span class="comment"># 4      741     3   Kings  2014</span></div><div class="line"><span class="comment"># 5      812     4   kings  2015</span></div><div class="line"><span class="comment"># 6      756     1   Kings  2016</span></div><div class="line"><span class="comment"># 7      788     1   Kings  2017</span></div><div class="line"><span class="comment"># 8      694     2  Riders  2016</span></div><div class="line"><span class="comment"># 9      701     4  Royals  2014</span></div><div class="line"><span class="comment"># 10     804     1  Royals  2015</span></div><div class="line"><span class="comment"># 11     690     2  Riders  2017</span></div><div class="line">grouped = df.groupby(<span class="string">'Year'</span>)</div><div class="line"><span class="keyword">print</span> (grouped[<span class="string">'Points'</span>].agg(np.mean))</div><div class="line"><span class="comment"># Year</span></div><div class="line"><span class="comment"># 2014    795.25</span></div><div class="line"><span class="comment"># 2015    769.50</span></div><div class="line"><span class="comment"># 2016    725.00</span></div><div class="line"><span class="comment"># 2017    739.00</span></div></pre></td></tr></table></figure><p>另一种查看每个分组的大小的方法是应用<code>size()</code>函数 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">ipl_data = &#123;<span class="string">'Team'</span>: [<span class="string">'Riders'</span>, <span class="string">'Riders'</span>, <span class="string">'Devils'</span>, <span class="string">'Devils'</span>, <span class="string">'Kings'</span>,</div><div class="line">         <span class="string">'kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Riders'</span>, <span class="string">'Royals'</span>, <span class="string">'Royals'</span>, <span class="string">'Riders'</span>],</div><div class="line">         <span class="string">'Rank'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">4</span> ,<span class="number">1</span> ,<span class="number">1</span>,<span class="number">2</span> , <span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">         <span class="string">'Year'</span>: [<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2016</span>,<span class="number">2017</span>,<span class="number">2016</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2017</span>],</div><div class="line">         <span class="string">'Points'</span>:[<span class="number">876</span>,<span class="number">789</span>,<span class="number">863</span>,<span class="number">673</span>,<span class="number">741</span>,<span class="number">812</span>,<span class="number">756</span>,<span class="number">788</span>,<span class="number">694</span>,<span class="number">701</span>,<span class="number">804</span>,<span class="number">690</span>]&#125;</div><div class="line">df = pd.DataFrame(ipl_data)</div><div class="line">grouped = df.groupby(<span class="string">'Team'</span>)</div><div class="line"><span class="keyword">print</span> (grouped.agg(np.size))</div><div class="line"><span class="comment">#         Points  Rank  Year</span></div><div class="line"><span class="comment"># Team                      </span></div><div class="line"><span class="comment"># Devils       2     2     2</span></div><div class="line"><span class="comment"># Kings        3     3     3</span></div><div class="line"><span class="comment"># Riders       4     4     4</span></div><div class="line"><span class="comment"># Royals       2     2     2</span></div><div class="line"><span class="comment"># kings        1     1     1</span></div></pre></td></tr></table></figure><p>一次应用多个聚合函数, 通过分组系列，还可以传递函数的列表或字典来进行聚合，并生成<code>DataFrame</code>作为输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">ipl_data = &#123;<span class="string">'Team'</span>: [<span class="string">'Riders'</span>, <span class="string">'Riders'</span>, <span class="string">'Devils'</span>, <span class="string">'Devils'</span>, <span class="string">'Kings'</span>,</div><div class="line">         <span class="string">'kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Riders'</span>, <span class="string">'Royals'</span>, <span class="string">'Royals'</span>, <span class="string">'Riders'</span>],</div><div class="line">         <span class="string">'Rank'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">4</span> ,<span class="number">1</span> ,<span class="number">1</span>,<span class="number">2</span> , <span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">         <span class="string">'Year'</span>: [<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2016</span>,<span class="number">2017</span>,<span class="number">2016</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2017</span>],</div><div class="line">         <span class="string">'Points'</span>:[<span class="number">876</span>,<span class="number">789</span>,<span class="number">863</span>,<span class="number">673</span>,<span class="number">741</span>,<span class="number">812</span>,<span class="number">756</span>,<span class="number">788</span>,<span class="number">694</span>,<span class="number">701</span>,<span class="number">804</span>,<span class="number">690</span>]&#125;</div><div class="line">df = pd.DataFrame(ipl_data)</div><div class="line"></div><div class="line">grouped = df.groupby(<span class="string">'Team'</span>)</div><div class="line">agg = grouped[<span class="string">'Points'</span>].agg([np.sum, np.mean, np.std])</div><div class="line"><span class="keyword">print</span> (agg)</div><div class="line"><span class="comment">#          sum        mean         std</span></div><div class="line"><span class="comment"># Team                                </span></div><div class="line"><span class="comment"># Devils  1536  768.000000  134.350288</span></div><div class="line"><span class="comment"># Kings   2285  761.666667   24.006943</span></div><div class="line"><span class="comment"># Riders  3049  762.250000   88.567771</span></div><div class="line"><span class="comment"># Royals  1505  752.500000   72.831998</span></div><div class="line"><span class="comment"># kings    812  812.000000         NaN</span></div></pre></td></tr></table></figure><p>转换：分组或列上的转换返回索引大小与被分组的索引相同的对象。因此，转换应该返回与组块大小相同的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">ipl_data = &#123;<span class="string">'Team'</span>: [<span class="string">'Riders'</span>, <span class="string">'Riders'</span>, <span class="string">'Devils'</span>, <span class="string">'Devils'</span>, <span class="string">'Kings'</span>,</div><div class="line">         <span class="string">'kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Riders'</span>, <span class="string">'Royals'</span>, <span class="string">'Royals'</span>, <span class="string">'Riders'</span>],</div><div class="line">         <span class="string">'Rank'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">4</span> ,<span class="number">1</span> ,<span class="number">1</span>,<span class="number">2</span> , <span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">         <span class="string">'Year'</span>: [<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2016</span>,<span class="number">2017</span>,<span class="number">2016</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2017</span>],</div><div class="line">         <span class="string">'Points'</span>:[<span class="number">876</span>,<span class="number">789</span>,<span class="number">863</span>,<span class="number">673</span>,<span class="number">741</span>,<span class="number">812</span>,<span class="number">756</span>,<span class="number">788</span>,<span class="number">694</span>,<span class="number">701</span>,<span class="number">804</span>,<span class="number">690</span>]&#125;</div><div class="line">df = pd.DataFrame(ipl_data)</div><div class="line">print(df)</div><div class="line"><span class="comment">#     Points  Rank    Team  Year</span></div><div class="line"><span class="comment"># 0      876     1  Riders  2014</span></div><div class="line"><span class="comment"># 1      789     2  Riders  2015</span></div><div class="line"><span class="comment"># 2      863     2  Devils  2014</span></div><div class="line"><span class="comment"># 3      673     3  Devils  2015</span></div><div class="line"><span class="comment"># 4      741     3   Kings  2014</span></div><div class="line"><span class="comment"># 5      812     4   kings  2015</span></div><div class="line"><span class="comment"># 6      756     1   Kings  2016</span></div><div class="line"><span class="comment"># 7      788     1   Kings  2017</span></div><div class="line"><span class="comment"># 8      694     2  Riders  2016</span></div><div class="line"><span class="comment"># 9      701     4  Royals  2014</span></div><div class="line"><span class="comment"># 10     804     1  Royals  2015</span></div><div class="line"><span class="comment"># 11     690     2  Riders  2017</span></div><div class="line"></div><div class="line">grouped = df.groupby(<span class="string">'Team'</span>)</div><div class="line">score = <span class="keyword">lambda</span> x: x+<span class="number">10</span></div><div class="line"><span class="keyword">print</span> (grouped.transform(score))</div><div class="line"><span class="comment">#     Points  Rank  Year</span></div><div class="line"><span class="comment"># 0      886    11  2024</span></div><div class="line"><span class="comment"># 1      799    12  2025</span></div><div class="line"><span class="comment"># 2      873    12  2024</span></div><div class="line"><span class="comment"># 3      683    13  2025</span></div><div class="line"><span class="comment"># 4      751    13  2024</span></div><div class="line"><span class="comment"># 5      822    14  2025</span></div><div class="line"><span class="comment"># 6      766    11  2026</span></div><div class="line"><span class="comment"># 7      798    11  2027</span></div><div class="line"><span class="comment"># 8      704    12  2026</span></div><div class="line"><span class="comment"># 9      711    14  2024</span></div><div class="line"><span class="comment"># 10     814    11  2025</span></div><div class="line"><span class="comment"># 11     700    12  2027</span></div></pre></td></tr></table></figure><p>过滤：过滤根据定义的标准过滤数据并返回数据的子集。<code>filter()</code>函数用于过滤数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">ipl_data = &#123;<span class="string">'Team'</span>: [<span class="string">'Riders'</span>, <span class="string">'Riders'</span>, <span class="string">'Devils'</span>, <span class="string">'Devils'</span>, <span class="string">'Kings'</span>,</div><div class="line">         <span class="string">'kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Kings'</span>, <span class="string">'Riders'</span>, <span class="string">'Royals'</span>, <span class="string">'Royals'</span>, <span class="string">'Riders'</span>],</div><div class="line">         <span class="string">'Rank'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>,<span class="number">4</span> ,<span class="number">1</span> ,<span class="number">1</span>,<span class="number">2</span> , <span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>],</div><div class="line">         <span class="string">'Year'</span>: [<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2016</span>,<span class="number">2017</span>,<span class="number">2016</span>,<span class="number">2014</span>,<span class="number">2015</span>,<span class="number">2017</span>],</div><div class="line">         <span class="string">'Points'</span>:[<span class="number">876</span>,<span class="number">789</span>,<span class="number">863</span>,<span class="number">673</span>,<span class="number">741</span>,<span class="number">812</span>,<span class="number">756</span>,<span class="number">788</span>,<span class="number">694</span>,<span class="number">701</span>,<span class="number">804</span>,<span class="number">690</span>]&#125;</div><div class="line">df = pd.DataFrame(ipl_data)</div><div class="line">filter = df.groupby(<span class="string">'Team'</span>).filter(<span class="keyword">lambda</span> x: len(x) &gt;= <span class="number">3</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> (filter) <span class="comment"># 在上述过滤条件下，要求返回三次以上参加IPL的队伍。</span></div><div class="line"><span class="comment">#  Points  Rank    Team  Year</span></div><div class="line"><span class="number">0</span>      <span class="number">876</span>     <span class="number">1</span>  Riders  <span class="number">2014</span></div><div class="line"><span class="number">1</span>      <span class="number">789</span>     <span class="number">2</span>  Riders  <span class="number">2015</span></div><div class="line"><span class="number">4</span>      <span class="number">741</span>     <span class="number">3</span>   Kings  <span class="number">2014</span></div><div class="line"><span class="number">6</span>      <span class="number">756</span>     <span class="number">1</span>   Kings  <span class="number">2016</span></div><div class="line"><span class="number">7</span>      <span class="number">788</span>     <span class="number">1</span>   Kings  <span class="number">2017</span></div><div class="line"><span class="number">8</span>      <span class="number">694</span>     <span class="number">2</span>  Riders  <span class="number">2016</span></div><div class="line"><span class="number">11</span>     <span class="number">690</span>     <span class="number">2</span>  Riders  <span class="number">2017</span></div></pre></td></tr></table></figure><h1 id="Pandas合并-连接"><a href="#Pandas合并-连接" class="headerlink" title="Pandas合并/连接"></a>Pandas合并/连接</h1><p>Pandas具有功能全面的高性能内存中连接操作，与SQL等关系数据库非常相似。 Pandas提供了一个单独的<code>merge()</code>函数，作为DataFrame对象之间所有标准数据库连接操作的入口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pd.merge(left, right, how=<span class="string">'inner'</span>, on=<span class="keyword">None</span>, left_on=<span class="keyword">None</span>, right_on=<span class="keyword">None</span>,</div><div class="line">left_index=<span class="keyword">False</span>, right_index=<span class="keyword">False</span>, sort=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>在这里，有以下几个参数可以使用 -</p><ul><li><em>left</em> - 一个DataFrame对象。</li><li><em>right</em> - 另一个DataFrame对象。</li><li><em>on</em> - 列(名称)连接，必须在左和右DataFrame对象中存在(找到)。</li><li><em>left_on</em> - 左侧DataFrame中的列用作键，可以是列名或长度等于DataFrame长度的数组。</li><li><em>right_on</em> - 来自右的DataFrame的列作为键，可以是列名或长度等于DataFrame长度的数组。</li><li><em>left_index</em> - 如果为<code>True</code>，则使用左侧DataFrame中的索引(行标签)作为其连接键。 在具有MultiIndex(分层)的DataFrame的情况下，级别的数量必须与来自右DataFrame的连接键的数量相匹配。</li><li><em>right_index</em> - 与右DataFrame的<em>left_index</em>具有相同的用法。</li><li><em>how</em> - 它是<em>left</em>, <em>right</em>, <em>outer</em>以及<em>inner</em>之中的一个，默认为内<em>inner</em>。 下面将介绍每种方法的用法。</li><li><em>sort</em> - 按照字典顺序通过连接键对结果DataFrame进行排序。默认为<code>True</code>，设置为<code>False</code>时，在很多情况下大大提高性能。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">left = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Alex'</span>, <span class="string">'Amy'</span>, <span class="string">'Allen'</span>, <span class="string">'Alice'</span>, <span class="string">'Ayoung'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub1'</span>,<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>]&#125;)</div><div class="line">right = pd.DataFrame(</div><div class="line">         &#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Billy'</span>, <span class="string">'Brian'</span>, <span class="string">'Bran'</span>, <span class="string">'Bryce'</span>, <span class="string">'Betty'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub3'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>]&#125;)</div><div class="line"><span class="keyword">print</span> (left)</div><div class="line">print(<span class="string">"========================================"</span>)</div><div class="line"><span class="keyword">print</span> (right)</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">     Name  id subject_id</div><div class="line">0    Alex   1       sub1</div><div class="line">1     Amy   2       sub2</div><div class="line">2   Allen   3       sub4</div><div class="line">3   Alice   4       sub6</div><div class="line">4  Ayoung   5       sub5</div><div class="line">========================================</div><div class="line">    Name  id subject_id</div><div class="line">0  Billy   1       sub2</div><div class="line">1  Brian   2       sub4</div><div class="line">2   Bran   3       sub3</div><div class="line">3  Bryce   4       sub6</div><div class="line">4  Betty   5       sub5</div></pre></td></tr></table></figure><h2 id="on参数"><a href="#on参数" class="headerlink" title="on参数"></a>on参数</h2><p><strong>在一个键上合并两个数据帧</strong></p> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">left = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Alex'</span>, <span class="string">'Amy'</span>, <span class="string">'Allen'</span>, <span class="string">'Alice'</span>, <span class="string">'Ayoung'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub1'</span>,<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>]&#125;)</div><div class="line">right = pd.DataFrame(</div><div class="line">         &#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Billy'</span>, <span class="string">'Brian'</span>, <span class="string">'Bran'</span>, <span class="string">'Bryce'</span>, <span class="string">'Betty'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub3'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>]&#125;)</div><div class="line">rs = pd.merge(left,right,on=<span class="string">'id'</span>)</div><div class="line">print(rs)</div><div class="line"><span class="comment">#    Name_x  id subject_id_x Name_y subject_id_y</span></div><div class="line"><span class="comment"># 0    Alex   1         sub1  Billy         sub2</span></div><div class="line"><span class="comment"># 1     Amy   2         sub2  Brian         sub4</span></div><div class="line"><span class="comment"># 2   Allen   3         sub4   Bran         sub3</span></div><div class="line"><span class="comment"># 3   Alice   4         sub6  Bryce         sub6</span></div><div class="line"><span class="comment"># 4  Ayoung   5         sub5  Betty         sub5</span></div></pre></td></tr></table></figure><p><strong>合并多个键上的两个数据框</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">rs = pd.merge(left,right,on=[<span class="string">'id'</span>,<span class="string">'subject_id'</span>])</div><div class="line">print(rs)</div><div class="line"><span class="comment">#    Name_x  id subject_id Name_y</span></div><div class="line"><span class="comment"># 0   Alice   4       sub6  Bryce</span></div><div class="line"><span class="comment"># 1  Ayoung   5       sub5  Betty</span></div></pre></td></tr></table></figure><h2 id="how参数"><a href="#how参数" class="headerlink" title="how参数"></a>how参数</h2><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-13 at 9.46.32 AM.png" alt="Screen Shot 2018-06-13 at 9.46.32 AM"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">rs = pd.merge(left, right, on=<span class="string">'subject_id'</span>, how=<span class="string">'left'</span>)</div><div class="line"><span class="keyword">print</span> (rs)</div><div class="line"><span class="comment">#    Name_x  id_x subject_id Name_y  id_y</span></div><div class="line"><span class="comment"># 0    Alex     1       sub1    NaN   NaN</span></div><div class="line"><span class="comment"># 1     Amy     2       sub2  Billy   1.0</span></div><div class="line"><span class="comment"># 2   Allen     3       sub4  Brian   2.0</span></div><div class="line"><span class="comment"># 3   Alice     4       sub6  Bryce   4.0</span></div><div class="line"><span class="comment"># 4  Ayoung     5       sub5  Betty   5.0</span></div><div class="line"></div><div class="line">rs = pd.merge(left, right, on=<span class="string">'subject_id'</span>, how=<span class="string">'right'</span>)</div><div class="line"><span class="keyword">print</span> (rs)</div><div class="line"><span class="comment">#    Name_x  id_x subject_id Name_y  id_y</span></div><div class="line"><span class="comment"># 0     Amy   2.0       sub2  Billy     1</span></div><div class="line"><span class="comment"># 1   Allen   3.0       sub4  Brian     2</span></div><div class="line"><span class="comment"># 2   Alice   4.0       sub6  Bryce     4</span></div><div class="line"><span class="comment"># 3  Ayoung   5.0       sub5  Betty     5</span></div><div class="line"><span class="comment"># 4     NaN   NaN       sub3   Bran     3</span></div><div class="line"></div><div class="line">rs = pd.merge(left, right, how=<span class="string">'outer'</span>, on=<span class="string">'subject_id'</span>)</div><div class="line"><span class="keyword">print</span> (rs)</div><div class="line"><span class="comment">#    Name_x  id_x subject_id Name_y  id_y</span></div><div class="line"><span class="comment"># 0    Alex   1.0       sub1    NaN   NaN</span></div><div class="line"><span class="comment"># 1     Amy   2.0       sub2  Billy   1.0</span></div><div class="line"><span class="comment"># 2   Allen   3.0       sub4  Brian   2.0</span></div><div class="line"><span class="comment"># 3   Alice   4.0       sub6  Bryce   4.0</span></div><div class="line"><span class="comment"># 4  Ayoung   5.0       sub5  Betty   5.0</span></div><div class="line"><span class="comment"># 5     NaN   NaN       sub3   Bran   3.0</span></div><div class="line"></div><div class="line">rs = pd.merge(left, right, on=<span class="string">'subject_id'</span>, how=<span class="string">'inner'</span>)</div><div class="line"><span class="keyword">print</span> (rs)</div><div class="line"><span class="comment">#    Name_x  id_x subject_id Name_y  id_y</span></div><div class="line"><span class="comment"># 0     Amy     2       sub2  Billy     1</span></div><div class="line"><span class="comment"># 1   Allen     3       sub4  Brian     2</span></div><div class="line"><span class="comment"># 2   Alice     4       sub6  Bryce     4</span></div><div class="line"><span class="comment"># 3  Ayoung     5       sub5  Betty     5</span></div></pre></td></tr></table></figure><h1 id="Pandas级联"><a href="#Pandas级联" class="headerlink" title="Pandas级联"></a>Pandas级联</h1><p><strong>Pandas</strong>提供了各种工具(功能)，可以轻松地将<code>Series</code>，<code>DataFrame</code>和<code>Panel</code>对象组合在一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pd.concat(objs,axis=<span class="number">0</span>,join=<span class="string">'outer'</span>,join_axes=<span class="keyword">None</span>,</div><div class="line">ignore_index=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><ul><li><em>objs</em> - 这是Series，DataFrame或Panel对象的序列或映射。</li><li><em>axis</em> - <code>{0，1，...}</code>，默认为<code>0</code>，这是连接的轴。</li><li><em>join</em> - <code>{&#39;inner&#39;, &#39;outer&#39;}</code>，默认<code>inner</code>。如何处理其他轴上的索引。联合的外部和交叉的内部。</li><li><em>ignore_index</em> − 布尔值，默认为<code>False</code>。如果指定为<code>True</code>，则不要使用连接轴上的索引值。结果轴将被标记为：<code>0，...，n-1</code>。</li><li><em>join_axes</em> - 这是Index对象的列表。用于其他<code>(n-1)</code>轴的特定索引，而不是执行内部/外部集逻辑。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">one = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Alex'</span>, <span class="string">'Amy'</span>, <span class="string">'Allen'</span>, <span class="string">'Alice'</span>, <span class="string">'Ayoung'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub1'</span>,<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>],</div><div class="line">         <span class="string">'Marks_scored'</span>:[<span class="number">98</span>,<span class="number">90</span>,<span class="number">87</span>,<span class="number">69</span>,<span class="number">78</span>]&#125;,</div><div class="line">         index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line">two = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Billy'</span>, <span class="string">'Brian'</span>, <span class="string">'Bran'</span>, <span class="string">'Bryce'</span>, <span class="string">'Betty'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub3'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>],</div><div class="line">         <span class="string">'Marks_scored'</span>:[<span class="number">89</span>,<span class="number">80</span>,<span class="number">79</span>,<span class="number">97</span>,<span class="number">88</span>]&#125;,</div><div class="line">         index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line">print(one)</div><div class="line"><span class="comment">#    Marks_scored    Name subject_id</span></div><div class="line"><span class="comment"># 1            98    Alex       sub1</span></div><div class="line"><span class="comment"># 2            90     Amy       sub2</span></div><div class="line"><span class="comment"># 3            87   Allen       sub4</span></div><div class="line"><span class="comment"># 4            69   Alice       sub6</span></div><div class="line"><span class="comment"># 5            78  Ayoung       sub5</span></div><div class="line"></div><div class="line">print(two)</div><div class="line"><span class="comment">#    Marks_scored   Name subject_id</span></div><div class="line"><span class="comment"># 1            89  Billy       sub2</span></div><div class="line"><span class="comment"># 2            80  Brian       sub4</span></div><div class="line"><span class="comment"># 3            79   Bran       sub3</span></div><div class="line"><span class="comment"># 4            97  Bryce       sub6</span></div><div class="line"><span class="comment"># 5            88  Betty       sub5</span></div><div class="line"></div><div class="line">rs = pd.concat([one,two])</div><div class="line">print(rs)</div><div class="line"><span class="comment">#    Marks_scored    Name subject_id</span></div><div class="line"><span class="comment"># 1            98    Alex       sub1</span></div><div class="line"><span class="comment"># 2            90     Amy       sub2</span></div><div class="line"><span class="comment"># 3            87   Allen       sub4</span></div><div class="line"><span class="comment"># 4            69   Alice       sub6</span></div><div class="line"><span class="comment"># 5            78  Ayoung       sub5</span></div><div class="line"><span class="comment"># 1            89   Billy       sub2</span></div><div class="line"><span class="comment"># 2            80   Brian       sub4</span></div><div class="line"><span class="comment"># 3            79    Bran       sub3</span></div><div class="line"><span class="comment"># 4            97   Bryce       sub6</span></div><div class="line"><span class="comment"># 5            88   Betty       sub</span></div></pre></td></tr></table></figure><p>结果的索引是重复的; 每个索引重复。如果想要生成的对象必须遵循自己的索引，请将<code>ignore_index</code>设置为<code>True</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">rs = pd.concat([one,two],ignore_index=<span class="keyword">True</span>)</div><div class="line">print(rs)</div><div class="line"><span class="comment">#    Marks_scored    Name subject_id</span></div><div class="line"><span class="comment"># 0            98    Alex       sub1</span></div><div class="line"><span class="comment"># 1            90     Amy       sub2</span></div><div class="line"><span class="comment"># 2            87   Allen       sub4</span></div><div class="line"><span class="comment"># 3            69   Alice       sub6</span></div><div class="line"><span class="comment"># 4            78  Ayoung       sub5</span></div><div class="line"><span class="comment"># 5            89   Billy       sub2</span></div><div class="line"><span class="comment"># 6            80   Brian       sub4</span></div><div class="line"><span class="comment"># 7            79    Bran       sub3</span></div><div class="line"><span class="comment"># 8            97   Bryce       sub6</span></div><div class="line"><span class="comment"># 9            88   Betty       sub5</span></div></pre></td></tr></table></figure><p>如果需要沿<code>axis=1</code>添加两个对象，则会添加新列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">rs = pd.concat([one,two],axis=<span class="number">1</span>)</div><div class="line">print(rs)</div><div class="line"></div><div class="line"><span class="comment">#    Marks_scored    Name subject_id  Marks_scored   Name subject_id</span></div><div class="line"><span class="comment"># 1            98    Alex       sub1            89  Billy       sub2</span></div><div class="line"><span class="comment"># 2            90     Amy       sub2            80  Brian       sub4</span></div><div class="line"><span class="comment"># 3            87   Allen       sub4            79   Bran       sub3</span></div><div class="line"><span class="comment"># 4            69   Alice       sub6            97  Bryce       sub6</span></div><div class="line"><span class="comment"># 5            78  Ayoung       sub5            88  Betty       sub5</span></div></pre></td></tr></table></figure><p>连接的一个有用的快捷方式是在Series和DataFrame实例的<code>append</code>方法。这些方法实际上早于<code>concat()</code>方法。 它们沿<code>axis=0</code>连接，即索引 -</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">one = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Alex'</span>, <span class="string">'Amy'</span>, <span class="string">'Allen'</span>, <span class="string">'Alice'</span>, <span class="string">'Ayoung'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub1'</span>,<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>],</div><div class="line">         <span class="string">'Marks_scored'</span>:[<span class="number">98</span>,<span class="number">90</span>,<span class="number">87</span>,<span class="number">69</span>,<span class="number">78</span>]&#125;,</div><div class="line">         index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line">two = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Billy'</span>, <span class="string">'Brian'</span>, <span class="string">'Bran'</span>, <span class="string">'Bryce'</span>, <span class="string">'Betty'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub3'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>],</div><div class="line">         <span class="string">'Marks_scored'</span>:[<span class="number">89</span>,<span class="number">80</span>,<span class="number">79</span>,<span class="number">97</span>,<span class="number">88</span>]&#125;,</div><div class="line">         index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line">rs = one.append(two)</div><div class="line">print(rs)</div></pre></td></tr></table></figure><p><code>append()</code>函数也可以带多个对象 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">one = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Alex'</span>, <span class="string">'Amy'</span>, <span class="string">'Allen'</span>, <span class="string">'Alice'</span>, <span class="string">'Ayoung'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub1'</span>,<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>],</div><div class="line">         <span class="string">'Marks_scored'</span>:[<span class="number">98</span>,<span class="number">90</span>,<span class="number">87</span>,<span class="number">69</span>,<span class="number">78</span>]&#125;,</div><div class="line">         index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line"></div><div class="line">two = pd.DataFrame(&#123;</div><div class="line">         <span class="string">'Name'</span>: [<span class="string">'Billy'</span>, <span class="string">'Brian'</span>, <span class="string">'Bran'</span>, <span class="string">'Bryce'</span>, <span class="string">'Betty'</span>],</div><div class="line">         <span class="string">'subject_id'</span>:[<span class="string">'sub2'</span>,<span class="string">'sub4'</span>,<span class="string">'sub3'</span>,<span class="string">'sub6'</span>,<span class="string">'sub5'</span>],</div><div class="line">         <span class="string">'Marks_scored'</span>:[<span class="number">89</span>,<span class="number">80</span>,<span class="number">79</span>,<span class="number">97</span>,<span class="number">88</span>]&#125;,</div><div class="line">         index=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</div><div class="line">rs = one.append([two,one,two])</div><div class="line">print(rs)</div></pre></td></tr></table></figure><h1 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a>时间序列</h1><p><em>Pandas</em>为时间序列数据的工作时间提供了一个强大的工具，尤其是在金融领域。在处理时间序列数据时，我们经常遇到以下情况 -</p><ul><li>生成时间序列</li><li>将时间序列转换为不同的频率</li></ul><h2 id="获取当前时间"><a href="#获取当前时间" class="headerlink" title="获取当前时间"></a>获取当前时间</h2><p><code>datetime.now()</code>用于获取当前的日期和时间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">print</span> pd.datetime.now()</div></pre></td></tr></table></figure><h2 id="创建时间戳"><a href="#创建时间戳" class="headerlink" title="创建时间戳"></a>创建时间戳</h2><p>时间戳数据是时间序列数据的最基本类型，它将数值与时间点相关联。 对于<em>Pandas</em>对象来说，意味着使用时间点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">time = pd.Timestamp(<span class="string">'2018-10-01'</span>)</div><div class="line">print(time)</div><div class="line"><span class="comment"># 2018-10-01 00:00:00</span></div></pre></td></tr></table></figure><h2 id="创建时间范围"><a href="#创建时间范围" class="headerlink" title="创建时间范围"></a>创建时间范围</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">time = pd.date_range(<span class="string">"12:00"</span>, <span class="string">"23:59"</span>, freq=<span class="string">"30min"</span>).time</div><div class="line">print(time)</div><div class="line"><span class="comment"># [datetime.time(12, 0) datetime.time(12, 30) datetime.time(13, 0)</span></div><div class="line"><span class="comment">#  datetime.time(13, 30) datetime.time(14, 0) datetime.time(14, 30)</span></div><div class="line"><span class="comment">#  datetime.time(15, 0) datetime.time(15, 30) datetime.time(16, 0)</span></div><div class="line"><span class="comment">#  datetime.time(16, 30) datetime.time(17, 0) datetime.time(17, 30)</span></div><div class="line"><span class="comment">#  datetime.time(18, 0) datetime.time(18, 30) datetime.time(19, 0)</span></div><div class="line"><span class="comment">#  datetime.time(19, 30) datetime.time(20, 0) datetime.time(20, 30)</span></div><div class="line"><span class="comment">#  datetime.time(21, 0) datetime.time(21, 30) datetime.time(22, 0)</span></div><div class="line"><span class="comment">#  datetime.time(22, 30) datetime.time(23, 0) datetime.time(23, 30)]</span></div></pre></td></tr></table></figure><p>改变时间频率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">time = pd.date_range(<span class="string">"12:00"</span>, <span class="string">"23:59"</span>, freq=<span class="string">"H"</span>).time</div><div class="line">print(time)</div><div class="line"><span class="comment"># [datetime.time(12, 0) datetime.time(13, 0) datetime.time(14, 0)</span></div><div class="line"><span class="comment">#  datetime.time(15, 0) datetime.time(16, 0) datetime.time(17, 0)</span></div><div class="line"><span class="comment">#  datetime.time(18, 0) datetime.time(19, 0) datetime.time(20, 0)</span></div><div class="line"><span class="comment">#  datetime.time(21, 0) datetime.time(22, 0) datetime.time(23, 0)]</span></div></pre></td></tr></table></figure><h1 id="Pandas分类数据"><a href="#Pandas分类数据" class="headerlink" title="Pandas分类数据"></a>Pandas分类数据</h1><h2 id="分类对象创建"><a href="#分类对象创建" class="headerlink" title="分类对象创建"></a>分类对象创建</h2><p>通过在<code>pandas</code>对象创建中将<code>dtype</code>指定为<code>“category”</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">s = pd.Series([<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"a"</span>])</div><div class="line">print(s)</div><div class="line"><span class="comment"># 0    a</span></div><div class="line"><span class="comment"># 1    b</span></div><div class="line"><span class="comment"># 2    c</span></div><div class="line"><span class="comment"># 3    a</span></div><div class="line"><span class="comment"># dtype: object</span></div><div class="line"></div><div class="line">s = pd.Series([<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"a"</span>], dtype=<span class="string">"category"</span>)</div><div class="line"><span class="keyword">print</span> (s)</div><div class="line"><span class="comment"># 0    a</span></div><div class="line"><span class="comment"># 1    b</span></div><div class="line"><span class="comment"># 2    c</span></div><div class="line"><span class="comment"># 3    a</span></div><div class="line"><span class="comment"># dtype: category</span></div><div class="line"><span class="comment"># Categories (3, object): [a, b, c]</span></div></pre></td></tr></table></figure><blockquote><p>传递给系列对象的元素数量是四个，但类别只有三个。观察相同的输出类别。</p></blockquote><p>使用标准<em>Pandas</em>分类构造函数，我们可以创建一个类别对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pandas.Categorical(values, categories, ordered)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">cat = pd.Categorical([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</div><div class="line"><span class="keyword">print</span> (cat)</div><div class="line"><span class="comment"># [a, b, c, a, b, c]</span></div><div class="line"><span class="comment"># Categories (3, object): [a, b, c]</span></div><div class="line"></div><div class="line">cat = cat=pd.Categorical([<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>], [<span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>])</div><div class="line"><span class="keyword">print</span> (cat)</div><div class="line"><span class="comment"># [a, b, c, a, b, c, NaN]</span></div><div class="line"><span class="comment"># Categories (3, object): [c, b, a]</span></div><div class="line"></div><div class="line">cat = cat=pd.Categorical([<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>], [<span class="string">'c'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>],ordered=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">print</span> (cat)</div><div class="line"><span class="comment"># [a, b, c, a, b, c, NaN]</span></div><div class="line"><span class="comment"># Categories (3, object): [c &lt; b &lt; a]</span></div></pre></td></tr></table></figure><blockquote><ul><li>第二个参数表示类别。因此，在类别中不存在的任何值将被视为<code>NaN</code></li><li>从逻辑上讲，排序(<em>ordered</em>)意味着，<code>a</code>大于<code>b</code>，<code>b</code>大于<code>c</code>。</li></ul></blockquote><h2 id="Describe"><a href="#Describe" class="headerlink" title="Describe()"></a>Describe()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">cat = pd.Categorical([<span class="string">"a"</span>, <span class="string">"c"</span>, <span class="string">"c"</span>, np.nan], categories=[<span class="string">"b"</span>, <span class="string">"a"</span>, <span class="string">"c"</span>])</div><div class="line">df = pd.DataFrame(&#123;<span class="string">"cat"</span>:cat, <span class="string">"s"</span>:[<span class="string">"a"</span>, <span class="string">"c"</span>, <span class="string">"c"</span>, np.nan]&#125;)</div><div class="line">print(df)</div><div class="line"><span class="comment">#    cat    s</span></div><div class="line"><span class="comment"># 0    a    a</span></div><div class="line"><span class="comment"># 1    c    c</span></div><div class="line"><span class="comment"># 2    c    c</span></div><div class="line"><span class="comment"># 3  NaN  NaN</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df.describe())</div><div class="line"><span class="comment">#        cat  s</span></div><div class="line"><span class="comment"># count    3  3</span></div><div class="line"><span class="comment"># unique   2  2</span></div><div class="line"><span class="comment"># top      c  c</span></div><div class="line"><span class="comment"># freq     2  2</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (df[<span class="string">"cat"</span>].describe())</div><div class="line"><span class="comment"># count     3</span></div><div class="line"><span class="comment"># unique    2</span></div><div class="line"><span class="comment"># top       c</span></div><div class="line"><span class="comment"># freq      2</span></div><div class="line"><span class="comment"># Name: cat, dtype: object</span></div></pre></td></tr></table></figure><h1 id="Pandas可视化"><a href="#Pandas可视化" class="headerlink" title="Pandas可视化"></a>Pandas可视化</h1><h2 id="线形图"><a href="#线形图" class="headerlink" title="线形图"></a>线形图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">10</span>,<span class="number">4</span>),index=pd.date_range(<span class="string">'2018/12/18'</span>,</div><div class="line">   periods=<span class="number">10</span>), columns=list(<span class="string">'ABCD'</span>))</div><div class="line"></div><div class="line">print(df)</div><div class="line"><span class="comment">#                    A         B         C         D</span></div><div class="line"><span class="comment"># 2018-12-18  0.141680  0.512593 -1.162793  0.780302</span></div><div class="line"><span class="comment"># 2018-12-19 -0.692091 -0.315033 -2.442913  1.066298</span></div><div class="line"><span class="comment"># 2018-12-20  1.025117  1.543544  1.111169 -0.143340</span></div><div class="line"><span class="comment"># 2018-12-21  0.156190  0.312793 -1.776588 -0.191348</span></div><div class="line"><span class="comment"># 2018-12-22  0.075410  0.628234 -0.611224 -0.191468</span></div><div class="line"><span class="comment"># 2018-12-23 -0.864251 -0.357473 -0.144735 -0.261345</span></div><div class="line"><span class="comment"># 2018-12-24  0.162799 -0.869081 -0.377572  0.333409</span></div><div class="line"><span class="comment"># 2018-12-25 -0.186183 -2.437047  0.441362  0.859709</span></div><div class="line"><span class="comment"># 2018-12-26  0.332621 -1.022226  0.170011 -0.197164</span></div><div class="line"><span class="comment"># 2018-12-27 -0.409928  1.305865 -0.435077 -0.816023</span></div><div class="line">df.plot()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180614155744022.png" alt="image-20180614155744022"></p><blockquote><p>如果索引由日期组成，则调用<code>gct().autofmt_xdate()</code>来格式化<code>x</code>轴，如上图所示。</p><p>我们可以使用<code>x</code>和<code>y</code>关键字绘制一列与另一列。</p></blockquote><h2 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(<span class="number">10</span>,<span class="number">4</span>),columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">df.plot.bar()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180614160506651.png" alt="image-20180614160506651"></p><p>要生成一个堆积条形图，通过指定：<em>pass stacked=True</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(<span class="number">10</span>,<span class="number">4</span>),columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">df.plot.bar(stacked=<span class="keyword">True</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180614160626454.png" alt="image-20180614160626454"></p><p>要获得水平条形图，使用<code>barh()</code>方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(<span class="number">10</span>,<span class="number">4</span>),columns=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</div><div class="line">df.plot.barh(stacked=<span class="keyword">True</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180614160738761.png" alt="image-20180614160738761"></p><h2 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h2><p>可以使用<code>plot.hist()</code>方法绘制直方图。我们可以指定<code>bins</code>的数量值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(&#123;<span class="string">'a'</span>:np.random.randn(<span class="number">1000</span>)+<span class="number">1</span>,<span class="string">'b'</span>:np.random.randn(<span class="number">1000</span>),<span class="string">'c'</span>:</div><div class="line">np.random.randn(<span class="number">1000</span>) - <span class="number">1</span>&#125;, columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</div><div class="line"></div><div class="line">df.plot.hist(bins=<span class="number">20</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180615094442572.png" alt="image-20180615094442572"></p><p>要为每列绘制不同的直方图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.hist(bins=<span class="number">20</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180615094630408.png" alt="image-20180615094630408"></p><h2 id="箱线图"><a href="#箱线图" class="headerlink" title="箱线图"></a>箱线图</h2><p><strong>基本绘图：绘图</strong></p><p>Series和DataFrame上的这个功能只是使用<code>matplotlib</code>库的<code>plot()</code>方法的简单包装实现。参考以下示例代码 -</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(10,4),index=pd.date_range(&apos;2018/12/18&apos;,</div><div class="line">   periods=10), columns=list(&apos;ABCD&apos;))</div><div class="line"></div><div class="line">df.plot()</div><div class="line"></div><div class="line"></div><div class="line">Python</div></pre></td></tr></table></figure><p>执行上面示例代码，得到以下结果 -</p><p><img src="http://www.yiibai.com/uploads/images/201711/0511/385181122_97686.png" alt="img"></p><p>如果索引由日期组成，则调用<code>gct().autofmt_xdate()</code>来格式化<code>x</code>轴，如上图所示。</p><p>我们可以使用<code>x</code>和<code>y</code>关键字绘制一列与另一列。</p><p>绘图方法允许除默认线图之外的少数绘图样式。 这些方法可以作为<code>plot()</code>的<code>kind</code>关键字参数提供。这些包括 -</p><ul><li><code>bar</code>或<code>barh</code>为条形</li><li><code>hist</code>为直方图</li><li><code>boxplot</code>为盒型图</li><li><code>area</code>为“面积”</li><li><code>scatter</code>为散点图</li></ul><h2 id="条形图-1"><a href="#条形图-1" class="headerlink" title="条形图"></a>条形图</h2><p>现在通过创建一个条形图来看看条形图是什么。条形图可以通过以下方式来创建 -</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(10,4),columns=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</div><div class="line">df.plot.bar()</div><div class="line"></div><div class="line"></div><div class="line">Python</div></pre></td></tr></table></figure><p>执行上面示例代码，得到以下结果 -</p><p><img src="http://www.yiibai.com/uploads/images/201711/0511/220181133_35165.png" alt="img"></p><p>要生成一个堆积条形图，通过指定：<em>pass stacked=True</em> -</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">df = pd.DataFrame(np.random.rand(10,4),columns=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</div><div class="line">df.plot.bar(stacked=True)</div><div class="line"></div><div class="line"></div><div class="line">Python</div></pre></td></tr></table></figure><p>执行上面示例代码，得到以下结果 -</p><p><img src="http://www.yiibai.com/uploads/images/201711/0511/726181135_51813.png" alt="img"></p><p>要获得水平条形图，使用<code>barh()</code>方法 -</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(10,4),columns=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;])</div><div class="line"></div><div class="line">df.plot.barh(stacked=True)</div><div class="line"></div><div class="line"></div><div class="line">Python</div></pre></td></tr></table></figure><p>执行上面示例代码，得到以下结果 -</p><p><img src="http://www.yiibai.com/uploads/images/201711/0511/596181136_49400.png" alt="img"></p><h2 id="直方图-1"><a href="#直方图-1" class="headerlink" title="直方图"></a>直方图</h2><p>可以使用<code>plot.hist()</code>方法绘制直方图。我们可以指定<code>bins</code>的数量值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">df = pd.DataFrame(&#123;&apos;a&apos;:np.random.randn(1000)+1,&apos;b&apos;:np.random.randn(1000),&apos;c&apos;:</div><div class="line">np.random.randn(1000) - 1&#125;, columns=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</div><div class="line"></div><div class="line">df.plot.hist(bins=20)</div><div class="line"></div><div class="line"></div><div class="line">Python</div></pre></td></tr></table></figure><p>执行上面示例代码，得到以下结果 -</p><p><img src="http://www.yiibai.com/uploads/images/201711/0511/354181140_72017.png" alt="img"></p><p>要为每列绘制不同的直方图，请使用以下代码 -</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">df=pd.DataFrame(&#123;&apos;a&apos;:np.random.randn(1000)+1,&apos;b&apos;:np.random.randn(1000),&apos;c&apos;:</div><div class="line">np.random.randn(1000) - 1&#125;, columns=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;])</div><div class="line"></div><div class="line">df.hist(bins=20)</div><div class="line"></div><div class="line"></div><div class="line">Python</div></pre></td></tr></table></figure><p>执行上面示例代码，得到以下结果 -</p><p><img src="http://www.yiibai.com/uploads/images/201711/0511/532181142_60937.png" alt="img"></p><h2 id="箱形图"><a href="#箱形图" class="headerlink" title="箱形图"></a>箱形图</h2><p>Boxplot可以绘制调用<code>Series.box.plot()</code>和<code>DataFrame.box.plot()</code>或<code>DataFrame.boxplot()</code>来可视化每列中值的分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(<span class="number">10</span>, <span class="number">5</span>), columns=[<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>])</div><div class="line">df.plot.box()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180615094905015.png" alt="image-20180615094905015"></p><blockquote><p>这里是一个箱形图，表示对<code>[0,1)</code>上的统一随机变量的<code>10</code>次观察的五次试验。</p></blockquote><h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.rand(<span class="number">50</span>, <span class="number">4</span>), columns=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])</div><div class="line">df.plot.scatter(x=<span class="string">'a'</span>, y=<span class="string">'b'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180615095026828.png" alt="image-20180615095026828"></p><h2 id="饼状图"><a href="#饼状图" class="headerlink" title="饼状图"></a>饼状图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">df = pd.DataFrame(<span class="number">3</span> * np.random.rand(<span class="number">4</span>), index=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>], columns=[<span class="string">'x'</span>])</div><div class="line">df.plot.pie(subplots=<span class="keyword">True</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/image-20180615095118032.png" alt="image-20180615095118032"></p><h1 id="IO工具"><a href="#IO工具" class="headerlink" title="IO工具"></a>IO工具</h1><p>Pandas I/O API是一套像<code>pd.read_csv()</code>一样返回<code>Pandas</code>对象的顶级读取器函数。</p><p>读取文本文件(或平面文件)的两个主要功能是<code>read_csv()</code>和<code>read_table()</code>。它们都使用相同的解析代码来智能地将表格数据转换为<code>DataFrame</code>对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">pandas.read_csv(filepath_or_buffer, sep=<span class="string">','</span>, delimiter=<span class="keyword">None</span>, header=<span class="string">'infer'</span>,</div><div class="line">names=<span class="keyword">None</span>, index_col=<span class="keyword">None</span>, usecols=<span class="keyword">None</span>)</div><div class="line"><span class="comment">########################################################</span></div><div class="line">pandas.read_csv(filepath_or_buffer, sep=<span class="string">'\t'</span>, delimiter=<span class="keyword">None</span>, header=<span class="string">'infer'</span>,</div><div class="line">names=<span class="keyword">None</span>, index_col=<span class="keyword">None</span>, usecols=<span class="keyword">None</span>)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 9.54.01 AM.png" alt="Screen Shot 2018-06-15 at 9.54.01 AM"></p><h2 id="read-csv"><a href="#read-csv" class="headerlink" title="read_csv()"></a>read_csv()</h2><p><code>read.csv</code>从csv文件中读取数据并创建一个<code>DataFrame</code>对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line">df=pd.read_csv(<span class="string">"temp.csv"</span>)</div><div class="line"><span class="keyword">print</span> (df)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 9.55.43 AM.png" alt="Screen Shot 2018-06-15 at 9.55.43 AM"></p><h3 id="自定义索引"><a href="#自定义索引" class="headerlink" title="自定义索引"></a>自定义索引</h3><p>可以指定csv文件中的一列来使用<code>index_col</code>定制索引。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df=pd.read_csv(<span class="string">"temp.csv"</span>,index_col=[<span class="string">'S.No'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 9.57.17 AM.png" alt="Screen Shot 2018-06-15 at 9.57.17 AM"></p><h3 id="类型转换器"><a href="#类型转换器" class="headerlink" title="类型转换器"></a>类型转换器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">df = pd.read_csv(<span class="string">"temp.csv"</span>, dtype=&#123;<span class="string">'Salary'</span>: np.float64&#125;)</div><div class="line"><span class="keyword">print</span> (df.dtypes)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 9.58.33 AM.png" alt="Screen Shot 2018-06-15 at 9.58.33 AM"></p><h3 id="指定列名"><a href="#指定列名" class="headerlink" title="指定列名"></a>指定列名</h3><p>使用<code>names</code>参数指定标题的名称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df=pd.read_csv(<span class="string">"temp.csv"</span>, names=[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>])</div><div class="line"><span class="keyword">print</span> (df)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 10.00.18 AM.png" alt="Screen Shot 2018-06-15 at 10.00.18 AM"></p><p>观察可以看到，标题名称附加了自定义名称，但文件中的标题还没有被消除。 现在，使用<code>header</code>参数来删除它。</p><p>如果标题不是第一行，则将行号传递给标题。这将跳过前面的行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df=pd.read_csv(<span class="string">"temp.csv"</span>,names=[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>],header=<span class="number">0</span>)</div><div class="line"><span class="keyword">print</span> (df)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 10.01.15 AM.png" alt="Screen Shot 2018-06-15 at 10.01.15 AM"></p><h3 id="skiprows"><a href="#skiprows" class="headerlink" title="skiprows"></a>skiprows</h3><p><code>skiprows</code>跳过指定的行数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df=pd.read_csv(<span class="string">"temp.csv"</span>, skiprows=<span class="number">2</span>)</div><div class="line"><span class="keyword">print</span> (df)</div></pre></td></tr></table></figure><p><img src="/2018/06/08/ML-Pandas/Screen Shot 2018-06-15 at 10.02.13 AM.png" alt="Screen Shot 2018-06-15 at 10.02.13 AM"></p><h1 id="Pandas稀疏数据"><a href="#Pandas稀疏数据" class="headerlink" title="Pandas稀疏数据"></a>Pandas稀疏数据</h1><p>当任何匹配特定值的数据(NaN/缺失值，尽管可以选择任何值)被省略时，稀疏对象被“压缩”。 一个特殊的SparseIndex对象跟踪数据被“稀疏”的地方。 这将在一个例子中更有意义。 所有的标准Pandas数据结构都应用了<code>to_sparse</code>方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">ts = pd.Series(np.random.randn(<span class="number">10</span>))</div><div class="line">ts[<span class="number">2</span>:<span class="number">-2</span>] = np.nan</div><div class="line">sts = ts.to_sparse()</div><div class="line"><span class="keyword">print</span> (sts)</div><div class="line"><span class="comment"># 0   -0.554670</span></div><div class="line"><span class="comment"># 1    1.738188</span></div><div class="line"><span class="comment"># 2         NaN</span></div><div class="line"><span class="comment"># 3         NaN</span></div><div class="line"><span class="comment"># 4         NaN</span></div><div class="line"><span class="comment"># 5         NaN</span></div><div class="line"><span class="comment"># 6         NaN</span></div><div class="line"><span class="comment"># 7         NaN</span></div><div class="line"><span class="comment"># 8   -1.363170</span></div><div class="line"><span class="comment"># 9   -0.142478</span></div></pre></td></tr></table></figure><p>为了内存效率的原因，所以需要稀疏对象的存在。</p><p>现在假设有一个大的NA DataFrame并执行下面的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.DataFrame(np.random.randn(<span class="number">10000</span>, <span class="number">4</span>))</div><div class="line">df.ix[:<span class="number">9998</span>] = np.nan</div><div class="line">sdf = df.to_sparse()</div><div class="line"></div><div class="line"><span class="keyword">print</span> (sdf.density)</div><div class="line"><span class="comment"># 0.0001</span></div></pre></td></tr></table></figure><p>通过调用<code>to_dense</code>可以将任何稀疏对象转换回标准密集形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">ts = pd.Series(np.random.randn(<span class="number">10</span>))</div><div class="line">ts[<span class="number">2</span>:<span class="number">-2</span>] = np.nan</div><div class="line">sts = ts.to_sparse()</div><div class="line">print(sts)</div><div class="line"><span class="comment"># 0   -2.222412</span></div><div class="line"><span class="comment"># 1   -0.074302</span></div><div class="line"><span class="comment"># 2         NaN</span></div><div class="line"><span class="comment"># 3         NaN</span></div><div class="line"><span class="comment"># 4         NaN</span></div><div class="line"><span class="comment"># 5         NaN</span></div><div class="line"><span class="comment"># 6         NaN</span></div><div class="line"><span class="comment"># 7         NaN</span></div><div class="line"><span class="comment"># 8   -0.289743</span></div><div class="line"><span class="comment"># 9    0.256266</span></div><div class="line"><span class="comment"># dtype: float64</span></div><div class="line"><span class="comment"># BlockIndex</span></div><div class="line"><span class="comment"># Block locations: array([0, 8], dtype=int32)</span></div><div class="line"><span class="comment"># Block lengths: array([2, 2], dtype=int32)</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> (sts.to_dense())</div><div class="line"><span class="comment"># 0   -2.222412</span></div><div class="line"><span class="comment"># 1   -0.074302</span></div><div class="line"><span class="comment"># 2         NaN</span></div><div class="line"><span class="comment"># 3         NaN</span></div><div class="line"><span class="comment"># 4         NaN</span></div><div class="line"><span class="comment"># 5         NaN</span></div><div class="line"><span class="comment"># 6         NaN</span></div><div class="line"><span class="comment"># 7         NaN</span></div><div class="line"><span class="comment"># 8   -0.289743</span></div><div class="line"><span class="comment"># 9    0.256266</span></div><div class="line"><span class="comment"># dtype: float64</span></div></pre></td></tr></table></figure><h2 id="稀疏Dtypes"><a href="#稀疏Dtypes" class="headerlink" title="稀疏Dtypes"></a>稀疏Dtypes</h2><p>稀疏数据应该具有与其密集表示相同的dtype。 目前，支持<code>float64</code>，<code>int64</code>和<code>booldtypes</code>。 取决于原始的<code>dtype</code>，<code>fill_value</code>默认值的更改 -</p><ul><li><code>float64</code> − <code>np.nan</code></li><li><code>int64</code> − <code>0</code></li><li><code>bool</code> − <code>False</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">s = pd.Series([<span class="number">1</span>, np.nan, np.nan])</div><div class="line"><span class="keyword">print</span> (s)</div><div class="line"><span class="keyword">print</span> (<span class="string">"============================="</span>)</div><div class="line">s.to_sparse()</div><div class="line"><span class="keyword">print</span> (s)</div><div class="line"></div><div class="line"><span class="comment"># 0    1.0</span></div><div class="line"><span class="comment"># 1    NaN</span></div><div class="line"><span class="comment"># 2    NaN</span></div><div class="line"><span class="comment"># dtype: float64</span></div><div class="line"><span class="comment"># =============================</span></div><div class="line"><span class="comment"># 0    1.0</span></div><div class="line"><span class="comment"># 1    NaN</span></div><div class="line"><span class="comment"># 2    NaN</span></div><div class="line"><span class="comment"># dtype: float64</span></div></pre></td></tr></table></figure><h2 id="按位布尔"><a href="#按位布尔" class="headerlink" title="按位布尔"></a>按位布尔</h2><p>按位布尔运算符(如<code>==</code>和<code>!=</code>)将返回一个布尔系列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">s = pd.Series(range(<span class="number">5</span>))</div><div class="line"><span class="keyword">print</span> (s==<span class="number">4</span>) <span class="comment">#不等于4的是false</span></div><div class="line"></div><div class="line"><span class="comment"># 0    False</span></div><div class="line"><span class="comment"># 1    False</span></div><div class="line"><span class="comment"># 2    False</span></div><div class="line"><span class="comment"># 3    False</span></div><div class="line"><span class="comment"># 4     True</span></div></pre></td></tr></table></figure><h2 id="isin"><a href="#isin" class="headerlink" title="isin()"></a>isin()</h2><p>这将返回一个布尔序列，显示系列中的每个元素是否完全包含在传递的值序列中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">s = pd.Series(list(<span class="string">'abc'</span>))</div><div class="line">s = s.isin([<span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>])</div><div class="line"><span class="keyword">print</span> (s)</div><div class="line"><span class="comment"># 0     True</span></div><div class="line"><span class="comment"># 1    False</span></div><div class="line"><span class="comment"># 2     True</span></div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python-SciPy</title>
      <link href="/2018/06/07/ML-SciPy/"/>
      <url>/2018/06/07/ML-SciPy/</url>
      <content type="html"><![CDATA[<p>Scipy基于Numpy，提供了大量科学算法，它的不同子模块相应于不同的应用，本文参考了<a href="https://www.jianshu.com/p/1a3db06e786d" target="_blank" rel="noopener">1</a> <a href="https://www.yiibai.com/scipy/" target="_blank" rel="noopener">2</a></p><ol><li>文件IO（scipy.io）：数据输入输出</li><li>特殊函数（scipy.special）：特殊函数是先验函数，常用的有伽马函数scipy.special.gamma()</li><li>线性代数运算（scipy.linalg）</li><li>快速傅里叶变化（scipy.fftpack）</li><li>优化和拟合（scipy.optimize）：提供了函数最小值(标量或多维)、曲线拟合和寻找等式的根的有用算法。</li><li>统计和随机数（scipy.stats）</li><li>数值积分（scipy.integrate Fusy）</li></ol><p>模块导入的标准方式是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> io <span class="comment">#其他模块类似</span></div></pre></td></tr></table></figure><a id="more"></a><h2 id="文件输入-输出：scipy-io"><a href="#文件输入-输出：scipy-io" class="headerlink" title="文件输入/输出：scipy.io"></a>文件输入/输出：scipy.io</h2><h3 id="导入和保存matlab文件"><a href="#导入和保存matlab文件" class="headerlink" title="导入和保存matlab文件"></a>导入和保存matlab文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> io <span class="keyword">as</span> spio</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.ones((<span class="number">3</span>,<span class="number">3</span>)) <span class="comment">#3*3的全1矩阵</span></div><div class="line">spio.savemat(<span class="string">'file.mat'</span>,&#123;<span class="string">'a'</span>:a&#125;) <span class="comment">#以字典形式保存</span></div><div class="line">data = spio.loadmat(<span class="string">'file.mat'</span>,struct_as_record=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><h3 id="图片读取"><a href="#图片读取" class="headerlink" title="图片读取"></a>图片读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> misc</div><div class="line"></div><div class="line">x = misc.imread(<span class="string">"scipy.png"</span>)</div></pre></td></tr></table></figure><h3 id="txt-csv"><a href="#txt-csv" class="headerlink" title="txt/csv"></a>txt/csv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">numpy.loadtxt()</div><div class="line">numpy.savetxt()</div><div class="line">numpy.genfromtxt() <span class="comment">#智能导入文本</span></div><div class="line">numpy.recfromcsv() <span class="comment">#智能导入csv文件</span></div><div class="line"></div><div class="line"><span class="comment">#高速，有效率但numpy特有的二进制格式：</span></div><div class="line">numpy.save() </div><div class="line">numpy.load()</div></pre></td></tr></table></figure><h2 id="线性代数运算：scipy-linalg"><a href="#线性代数运算：scipy-linalg" class="headerlink" title="线性代数运算：scipy.linalg"></a>线性代数运算：scipy.linalg</h2><h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> linalg</div><div class="line"></div><div class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">re = linalg.det(x) <span class="comment">#-2.0</span></div></pre></td></tr></table></figure><h3 id="逆矩阵"><a href="#逆矩阵" class="headerlink" title="逆矩阵"></a>逆矩阵</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> linalg</div><div class="line"></div><div class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">re = linalg.inv(x)</div></pre></td></tr></table></figure><h3 id="奇异值分解SVD"><a href="#奇异值分解SVD" class="headerlink" title="奇异值分解SVD"></a>奇异值分解SVD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> linalg</div><div class="line"></div><div class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">uarr,spec,vharr = linalg.svd(x)</div><div class="line"></div><div class="line"><span class="comment">###原始矩阵可以由svd的输出用np.dot点乘重新组合得到</span></div><div class="line">sarr = np.diag(spec)</div><div class="line">svd_mat = uarr.dot(sarr).dot(vharr)</div><div class="line">re = np.allclose(svd_mat,x) <span class="comment">#True</span></div></pre></td></tr></table></figure><h2 id="快速傅里叶变换：scipy-fftpack"><a href="#快速傅里叶变换：scipy-fftpack" class="headerlink" title="快速傅里叶变换：scipy.fftpack"></a>快速傅里叶变换：scipy.fftpack</h2><h2 id="优化和拟合：scipy-optimize"><a href="#优化和拟合：scipy-optimize" class="headerlink" title="优化和拟合：scipy.optimize"></a>优化和拟合：scipy.optimize</h2><h3 id="标量函数最小值"><a href="#标量函数最小值" class="headerlink" title="标量函数最小值"></a>标量函数最小值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + <span class="number">10</span>*np.sin(x)</div><div class="line">x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</div><div class="line">plt.plot(x,f(x))</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-SciPy/image-20180608001714621.png" alt="image-20180608001714621"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">####从上面图像可以看出x在-1.3附近有个全局最小值，在3.8附近有个局部最小</span></div><div class="line"><span class="comment">####从初始点使用梯度下降法求解，BFGS算法</span></div><div class="line">re = optimize.fmin_bfgs(f,<span class="number">0</span>) <span class="comment">##必须定义成函数</span></div><div class="line">print(re)</div><div class="line"><span class="comment">#Optimization terminated successfully.</span></div><div class="line"><span class="comment">#Current function value: -7.945823</span></div><div class="line"><span class="comment">#Iterations: 5</span></div><div class="line"><span class="comment">#Function evaluations: 18</span></div><div class="line"><span class="comment">#Gradient evaluations: 6</span></div><div class="line"><span class="comment">#[-1.30644012]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#这个方法一个可能的问题在于，如果函数有局部最小值，算法会因初始点不同找到这些局部最小而不是全局最小:</span></div><div class="line">re = optimize.fmin_bfgs(f,<span class="number">3</span>) <span class="comment">#[3.83746709]</span></div><div class="line"><span class="comment">#如果我们不知道全局最小值的邻近值来选定初始点，我们需要借助于耗费资源些的全局优化。为了找到全局最小点，最简单的算法是蛮力算法^2，该算法求出给定格点的每个函数值。</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + <span class="number">10</span>*np.sin(x)</div><div class="line">x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</div><div class="line"></div><div class="line">grid = (<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</div><div class="line">re = optimize.brute(f,(grid,)) <span class="comment">##[-1.30641113]</span></div><div class="line"></div><div class="line"><span class="comment">##为了找到局部最小，我们把变量限制在(0, 10)之间，使用scipy.optimize.fminbound():</span></div><div class="line">re_local = optimize.fminbound(f,<span class="number">0</span>,<span class="number">10</span>) <span class="comment">##3.8374671194983834</span></div></pre></td></tr></table></figure><h3 id="标量函数的根"><a href="#标量函数的根" class="headerlink" title="标量函数的根"></a>标量函数的根</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#为了寻找根，例如令f(x)=0的点，对以上的用来示例的函数f我们可以使用scipy.optimize.fsolve()</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + <span class="number">10</span>*np.sin(x)</div><div class="line">x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</div><div class="line"></div><div class="line">root = optimize.fsolve(f,<span class="number">1</span>)<span class="comment">#初始猜测是1</span></div><div class="line">print(root) <span class="comment">#[0.]</span></div><div class="line"></div><div class="line"><span class="comment">#注意仅仅一个根被找到。检查f的图像在-2.5附近有第二个根。我们可以通过调整我们的初始猜测找到这一确切值</span></div><div class="line">root = optimize.fsolve(f,<span class="number">-2.5</span>)</div><div class="line">print(root)<span class="comment">#[-2.47948183]</span></div></pre></td></tr></table></figure><h3 id="曲线拟合"><a href="#曲线拟合" class="headerlink" title="曲线拟合"></a>曲线拟合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + <span class="number">10</span>*np.sin(x)</div><div class="line">xdata = np.linspace(<span class="number">-10</span>,<span class="number">10</span>,num=<span class="number">20</span>)</div><div class="line"><span class="comment">#得到污染的数据</span></div><div class="line">ydata = f(xdata) + np.random.randn(xdata.size)</div><div class="line"><span class="comment">#我们知道xdata-ydata大致服从y=x^2 + sin(x),但是噪声的引入而又偏差，我们需要拟合找出系数a和b，即y=a*x^2 + b*sin(x)</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">(x,a,b)</span>:</span></div><div class="line">    <span class="keyword">return</span> a*x**<span class="number">2</span>+b*np.sin(x)</div><div class="line">guess = [<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">params, params_covariance = optimize.curve_fit(f2,xdata,ydata,guess)</div><div class="line"><span class="comment"># params = [0.98996991 9.87613168]</span></div><div class="line"><span class="comment"># params_covariance = [[1.23238891e-05 1.30651701e-11] [1.30651701e-11 6.29408152e-02]]</span></div></pre></td></tr></table></figure><h2 id="统计和随机数：-scipy-stats"><a href="#统计和随机数：-scipy-stats" class="headerlink" title="统计和随机数： scipy.stats"></a>统计和随机数： scipy.stats</h2><h3 id="直方图和概率密度函数"><a href="#直方图和概率密度函数" class="headerlink" title="直方图和概率密度函数"></a>直方图和概率密度函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#给定一个随机过程的观察值，它们的直方图是随机过程的pdf(概率密度函数)的估计器</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">a = np.random.normal(size=<span class="number">1000</span>)</div><div class="line">bins = np.arange(<span class="number">-4</span>,<span class="number">5</span>) <span class="comment"># bins=[-4, -3, -2, -1,  0,  1,  2,  3,  4]</span></div><div class="line"></div><div class="line">histogram = np.histogram(a,bins=bins,normed=<span class="keyword">True</span>)[<span class="number">0</span>]</div><div class="line">bins = <span class="number">0.5</span>*(bins[<span class="number">1</span>:]+bins[:<span class="number">-1</span>]) <span class="comment">#bins=[-3.5, -2.5, -1.5, -0.5,  0.5,  1.5,  2.5,  3.5]</span></div><div class="line"></div><div class="line">b = stats.norm.pdf(bins)</div><div class="line"></div><div class="line">plt.plot(bins,histogram,color=<span class="string">'g'</span>)</div><div class="line">plt.plot(bins,b,color=<span class="string">'b'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-SciPy/image-20180608005630507.png" alt="image-20180608005630507"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#如果我们知道随机过程属于给定的随机过程族，比如正态过程。我们可以对观测值进行最大似然拟合来估计基本分布参数。这里我们对观测值拟合一个正态过程：</span></div><div class="line">loc,std = stats.norm.fit(a) <span class="comment">#loc= 0.007411672576925874 std= 1.005072454096087</span></div></pre></td></tr></table></figure><h3 id="百分位"><a href="#百分位" class="headerlink" title="百分位"></a>百分位</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#中位数,又叫作50百分位点，因为有50%的观测值在它之下,是来观测值之下一半之上一半的值。</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">a = np.random.normal(size=<span class="number">1000</span>)</div><div class="line"></div><div class="line">med = np.median(a)</div><div class="line">med = stats.scoreatpercentile(a,<span class="number">50</span>) <span class="comment">#百分位是CDF的一个估计器</span></div></pre></td></tr></table></figure><h2 id="数值积分：scipy-integrate-Fusy"><a href="#数值积分：scipy-integrate-Fusy" class="headerlink" title="数值积分：scipy.integrate Fusy"></a>数值积分：scipy.integrate Fusy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy.integrate <span class="keyword">import</span> quad</div><div class="line"></div><div class="line">re,err = quad(np.sin,<span class="number">0</span>,np.pi/<span class="number">2</span>)</div><div class="line"><span class="comment">#re=0.9999999999999999, err=1.1102230246251564e-14</span></div></pre></td></tr></table></figure><p>上述计算$\int_{0}^{\frac{\pi}{2}}sin(x)dx=1$</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python - Sklearn</title>
      <link href="/2018/06/07/ML-Sklearn/"/>
      <url>/2018/06/07/ML-Sklearn/</url>
      <content type="html"><![CDATA[<p>本文参考了<a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/" target="_blank" rel="noopener">1</a></p><a id="more"></a><h1 id="通用学习模式"><a href="#通用学习模式" class="headerlink" title="通用学习模式"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/2-2-general-pattern/" target="_blank" rel="noopener">通用学习模式</a></h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line">iris = datasets.load_iris()</div><div class="line">iris_x = iris.data</div><div class="line">iris_y = iris.target</div><div class="line"><span class="comment"># print(iris_x[:2,:]) #输出前面两个样本</span></div><div class="line"><span class="comment"># print(iris_y[2])</span></div><div class="line">x_train,x_test,y_train,y_test= train_test_split(iris_x,iris_y,test_size=<span class="number">0.3</span>)</div><div class="line">knn = KNeighborsClassifier()</div><div class="line">knn.fit(x_train,y_train)</div><div class="line">result = knn.predict(x_train)</div></pre></td></tr></table></figure><h1 id="Sklearn中数据集"><a href="#Sklearn中数据集" class="headerlink" title="Sklearn中数据集"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/2-3-database/" target="_blank" rel="noopener">Sklearn中数据集</a></h1><p>sklearn的<a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets" target="_blank" rel="noopener">[数据集]</a> <a href="http://scikit-learn.org/stable/datasets/index.html" target="_blank" rel="noopener">[官方说明]</a> </p><p><img src="/2018/06/07/ML-Sklearn/Screen Shot 2018-06-28 at 9.34.12 AM.png" alt="Screen Shot 2018-06-28 at 9.34.12 AM"></p><p>生成样本数据，按照函数的形式，输入 <code>sample，feature，target</code> 的个数等等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sklearn.datasets.make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">100</span>, n_informative=<span class="number">10</span>, n_targets=<span class="number">1</span>, bias=<span class="number">0.0</span>, effective_rank=<span class="keyword">None</span>, tail_strength=<span class="number">0.5</span>, noise=<span class="number">0.0</span>, shuffle=<span class="keyword">True</span>, coef=<span class="keyword">False</span>, random_state=<span class="keyword">None</span>)[source]</div></pre></td></tr></table></figure><p><strong>Example</strong>: 房价预测</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment">#数据导入</span></div><div class="line">loaded_data = datasets.load_boston()</div><div class="line">data_x = loaded_data.data</div><div class="line">data_y = loaded_data.target</div><div class="line"><span class="comment">#定义模型 and 训练模型</span></div><div class="line">model = LinearRegression()</div><div class="line">model.fit(data_x,data_y)</div><div class="line"><span class="comment">#预测</span></div><div class="line">result = model.predict(data_x[:<span class="number">4</span>,:])<span class="comment">#预测前四个样本</span></div><div class="line">print(result)</div><div class="line"><span class="comment"># [30.00821269 25.0298606  30.5702317  28.60814055]</span></div><div class="line">print(data_y[:<span class="number">4</span>]) <span class="comment"># 打印真实值，作为对比，可以看到是有些误差的。</span></div><div class="line"><span class="comment"># [24.  21.6 34.7 33.4]</span></div></pre></td></tr></table></figure><p><strong>Example</strong>：生成样本数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment"># 建立 100 个 sample，有一个 feature，和一个 target的样本数据集</span></div><div class="line">x,y = datasets.make_regression(n_samples=<span class="number">100</span>,n_features=<span class="number">1</span>,n_targets=<span class="number">1</span>,noise=<span class="number">10</span>)</div><div class="line">plt.scatter(x,y)</div><div class="line">plt.show()</div><div class="line"><span class="comment"># noise 越大的话，点就会越来越离散，例如 noise 由 10 变为 50.</span></div><div class="line">x,y = datasets.make_regression(n_samples=<span class="number">100</span>,n_features=<span class="number">1</span>,n_targets=<span class="number">1</span>,noise=<span class="number">50</span>)</div><div class="line">plt.scatter(x,y)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180628094729828.png" alt="image-20180628094729828"></p><p><img src="/2018/06/07/ML-Sklearn/image-20180628094801799.png" alt="image-20180628094801799"></p><h1 id="Sklearn常用属性和功能"><a href="#Sklearn常用属性和功能" class="headerlink" title="Sklearn常用属性和功能"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/2-4-model-attributes/" target="_blank" rel="noopener">Sklearn常用属性和功能</a></h1><p>以<strong><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" rel="noopener">LinearRegression</a></strong>方法为例</p><ol><li><p>导入：包、数据和模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</div><div class="line"></div><div class="line">loaded_data = datasets.load_boston()</div><div class="line">data_X = loaded_data.data</div><div class="line">data_y = loaded_data.target</div><div class="line"></div><div class="line">model = LinearRegression()</div></pre></td></tr></table></figure></li><li><p>模型训练与预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">model.fit(data_X, data_y)</div><div class="line"></div><div class="line">print(model.predict(data_X[:<span class="number">4</span>, :]))</div></pre></td></tr></table></figure></li><li><p>模型参数</p><p>模型：$f(x) = w_1x_1+w_2x_2+…+x_nw_n+w_0$</p><p><code>model.coef_</code> 和 <code>model.intercept_</code>属于 Model 的属性</p><p><code>model.coef_</code>：模型权重，$(w_1,w_2,…,w_n)$</p><p><code>model.intercept_</code>：$w_0$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">print(model.coef_)</div><div class="line">print(model.intercept_)</div><div class="line"></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00</span></div><div class="line"><span class="string">  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00</span></div><div class="line"><span class="string">   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03</span></div><div class="line"><span class="string">  -5.25466633e-01]</span></div><div class="line"><span class="string">36.4911032804</span></div><div class="line"><span class="string">"""</span></div></pre></td></tr></table></figure></li><li><p>预测效果评分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">print(model.score(data_X, data_y)) <span class="comment"># R^2 coefficient of determination</span></div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">0.740607742865</span></div><div class="line"><span class="string">"""</span></div></pre></td></tr></table></figure><blockquote><p>$R^2$计算方法：</p><p><img src="/2018/06/07/ML-Sklearn/Screen Shot 2018-06-28 at 3.53.28 PM.png" alt="Screen Shot 2018-06-28 at 3.53.28 PM"></p></blockquote></li></ol><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/3-1-normalization/" target="_blank" rel="noopener">正则化</a></h1><p><img src="/2018/06/07/ML-Sklearn/Screen Shot 2018-06-28 at 4.05.38 PM.png" alt="Screen Shot 2018-06-28 at 4.05.38 PM"></p><p>sklearn中模块preprocessing提供了scale功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing <span class="comment">#标准化数据模块</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment">#建立Array</span></div><div class="line">a = np.array([[<span class="number">10</span>, <span class="number">2.7</span>, <span class="number">3.6</span>],</div><div class="line">              [<span class="number">-100</span>, <span class="number">5</span>, <span class="number">-2</span>],</div><div class="line">              [<span class="number">120</span>, <span class="number">20</span>, <span class="number">40</span>]], dtype=np.float64)</div><div class="line"></div><div class="line"><span class="comment">#将normalized后的a打印出</span></div><div class="line">print(preprocessing.scale(a)) <span class="comment">#对每列进行单独的正则化</span></div><div class="line"><span class="comment"># [[ 0.         -0.85170713 -0.55138018]</span></div><div class="line"><span class="comment">#  [-1.22474487 -0.55187146 -0.852133  ]</span></div><div class="line"><span class="comment">#  [ 1.22474487  1.40357859  1.40351318]]</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn. model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_classification</div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment">##分类数据生成</span></div><div class="line">x,y = make_classification(n_samples=<span class="number">300</span>,n_features=<span class="number">2</span>,</div><div class="line">                          n_redundant=<span class="number">0</span>,n_informative=<span class="number">2</span>,</div><div class="line">                          random_state=<span class="number">2</span>,scale=<span class="number">100</span>,</div><div class="line">                          n_clusters_per_class=<span class="number">1</span>)</div><div class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180628212803165.png" alt="image-20180628212803165"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.3</span>)</div><div class="line">clf = SVC()</div><div class="line">clf.fit(x_train,y_train)</div><div class="line">print(clf.score(x_test,y_test))</div><div class="line"><span class="comment"># 0.5111111111111111</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x = preprocessing.scale(x)</div><div class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y)</div><div class="line">plt.show() <span class="comment">#可以看到坐标范围缩小了</span></div><div class="line">x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.3</span>)</div><div class="line">clf = SVC()</div><div class="line">clf.fit(x_train,y_train)</div><div class="line">print(clf.score(x_test,y_test))</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180628213057808.png" alt="image-20180628213057808"></p><h1 id="交叉验证cross-validation"><a href="#交叉验证cross-validation" class="headerlink" title="交叉验证cross-validation"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/3-2-cross-validation1/" target="_blank" rel="noopener">交叉验证cross-validation</a></h1><p><a href="https://blog.csdn.net/evillist/article/details/61912827" target="_blank" rel="noopener">[Ref1]</a> <a href="https://www.cnblogs.com/pinard/p/5992719.html" target="_blank" rel="noopener">[Ref2]</a> </p><p><img src="/2018/06/07/ML-Sklearn/Screen Shot 2018-07-12 at 10.12.09 AM.png" alt="Screen Shot 2018-07-12 at 10.12.09 AM"></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[M,N]=<span class="built_in">size</span>(data);<span class="comment">%数据集为一个M*N的矩阵，其中每一行代表一个样本</span></div><div class="line">    indices=crossvalind(<span class="string">'Kfold'</span>,data(<span class="number">1</span>:M,N),<span class="number">10</span>);<span class="comment">%进行随机分包</span></div><div class="line">    <span class="keyword">for</span> k=<span class="number">1</span>:<span class="number">10</span><span class="comment">%交叉验证k=10，10个包轮流作为测试集</span></div><div class="line">        test = (indices == k); <span class="comment">%获得test集元素在数据集中对应的单元编号</span></div><div class="line">        train = ~test;<span class="comment">%train集元素的编号为非test元素的编号</span></div><div class="line">        train_data=data(train,:);<span class="comment">%从数据集中划分出train样本的数据</span></div><div class="line">        train_target=target(:,train);<span class="comment">%获得样本集的测试目标，在本例中是train样本的实际分类情况</span></div><div class="line">        test_data=data(test,:);<span class="comment">%test样本集</span></div><div class="line">        test_target=target(:,test);<span class="comment">%test的实际分类情况</span></div><div class="line"></div><div class="line">        ...........</div><div class="line">    <span class="keyword">end</span></div></pre></td></tr></table></figure><blockquote><p>有一个<a href="https://blog.csdn.net/sq_sh1987/article/details/52671824" target="_blank" rel="noopener">问题</a>，k次训练会得到k个模型，也就是说每个模型的系数可能都不相同，那最终的模型是如何得到的呢？这个问题困扰了我很久。今天，通过阅读网上资料和matlab源码，解决这个问题，其实原理很简单。交叉验证只是一种模型验证方法，而不是一个模型优化方法，即用于评估一种模型在实际情况下的预测能力。比如，现在有若干个分类模型（决策树、SVM、KNN等），通过k-fold cross validation，可得到不同模型的误差值，进而选择最合适的模型，但是无法确定每个模型中的最优参数。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"></div><div class="line">iris = load_iris()</div><div class="line">x = iris.data</div><div class="line">y = iris.target</div><div class="line"></div><div class="line">knn = KNeighborsClassifier()</div><div class="line">scores = cross_val_score(knn,x,y,cv=<span class="number">5</span>,scoring=<span class="string">'accuracy'</span>)</div><div class="line">print(scores)</div><div class="line"><span class="comment"># [0.96666667    1.    0.93333333   0.96666667   1.]</span></div><div class="line">print(scores.mean())</div><div class="line"><span class="comment"># 0.9733333333333334</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">iris = load_iris()</div><div class="line">x = iris.data</div><div class="line">y = iris.target</div><div class="line"></div><div class="line">neighbour_num = range(<span class="number">1</span>,<span class="number">31</span>)</div><div class="line">k_score = []</div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> neighbour_num:</div><div class="line">    knn = KNeighborsClassifier(n_neighbors=k)</div><div class="line">    scores = cross_val_score(knn, x, y, cv=<span class="number">10</span>, scoring=<span class="string">'accuracy'</span>)</div><div class="line">    k_score.append(scores.mean())</div><div class="line">plt.plot(neighbour_num,k_score)</div><div class="line">plt.xlabel(<span class="string">'neighbour number'</span>)</div><div class="line">plt.ylabel(<span class="string">'cross-validated accuracy'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180629095455584.png" alt="image-20180629095455584"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">iris = load_iris()</div><div class="line">x = iris.data</div><div class="line">y = iris.target</div><div class="line"></div><div class="line">neighbour_num = range(<span class="number">1</span>,<span class="number">31</span>)</div><div class="line">k_score = []</div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> neighbour_num:</div><div class="line">    knn = KNeighborsClassifier(n_neighbors=k)</div><div class="line">    loss = -cross_val_score(knn, x, y, cv=<span class="number">10</span>, scoring=<span class="string">'mean_squared_error'</span>)</div><div class="line">    k_score.append(loss.mean())</div><div class="line">plt.plot(neighbour_num,k_score)</div><div class="line">plt.xlabel(<span class="string">'neighbour number'</span>)</div><div class="line">plt.ylabel(<span class="string">'cross-validated MSE'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180629095709851.png" alt="image-20180629095709851"></p><blockquote><p>一般来说<code>准确率(accuracy)</code>会用于判断分类(Classification)模型的好坏。</p><p><code>平均方差(Mean squared error)</code>会用于判断回归(Regression)模型的好坏。</p></blockquote><h1 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/3-3-cross-validation2/" target="_blank" rel="noopener">Overfitting</a></h1><ol><li><p>overfitting检查</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">digits = load_digits()</div><div class="line">x = digits.data</div><div class="line">y = digits.target</div><div class="line"></div><div class="line">train_sizes,train_loss,test_loss = learning_curve(</div><div class="line">    SVC(gamma=<span class="number">0.001</span>),x,y,cv=<span class="number">10</span>,</div><div class="line">    scoring=<span class="string">'mean_squared_error'</span>,</div><div class="line">    train_sizes=[<span class="number">0.1</span>,<span class="number">0.25</span>,<span class="number">0.5</span>,<span class="number">0.75</span>,<span class="number">1</span>])</div><div class="line">train_loss_mean = -np.mean(train_loss,axis=<span class="number">1</span>)</div><div class="line">test_loss_mean = -np.mean(test_loss,axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">plt.plot(train_sizes, train_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</div><div class="line">         label=<span class="string">"Training"</span>)</div><div class="line">plt.plot(train_sizes, test_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"g"</span>,</div><div class="line">        label=<span class="string">"Cross-validation"</span>)</div><div class="line"></div><div class="line">plt.xlabel(<span class="string">"Training examples"</span>)</div><div class="line">plt.ylabel(<span class="string">"Loss"</span>)</div><div class="line">plt.legend(loc=<span class="string">"best"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180701102341647.png" alt="image-20180701102341647"></p><blockquote><ul><li>加载digits数据集，其包含的是手写体的数字，从0到9。数据集总共有1797个样本，每个样本由64个特征组成， 分别为其手写体对应的8×8像素表示，每个特征取值0~16。</li><li>采用K折交叉验证 <code>cv=10</code>, 选择平均方差检视模型效能 <code>scoring=&#39;mean_squared_error&#39;</code>, 样本由小到大分成5轮检视学习曲线<code>(10%, 25%, 50%, 75%, 100%)</code></li><li>从图中可以看出，随着训练样本的变大，训练误差和测试误差同时变小，说明模型的准确度因为样本增多而增高；</li></ul></blockquote></li><li><p>这次我们来验证<code>SVC</code>中的一个参数 <code>gamma</code> 在什么范围内能使 model 产生好的结果. 以及过拟合和 <code>gamma</code> 取值的关系.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> validation_curve</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">digits = load_digits()</div><div class="line">x = digits.data</div><div class="line">y = digits.target</div><div class="line"></div><div class="line">param_range = np.logspace(<span class="number">-6</span>,<span class="number">-2.3</span>,<span class="number">5</span>)</div><div class="line"></div><div class="line">train_loss,test_loss = validation_curve(</div><div class="line">    SVC(),x,y,param_name=<span class="string">'gamma'</span>,</div><div class="line">    param_range=param_range,cv=<span class="number">10</span>,</div><div class="line">    scoring=<span class="string">'mean_squared_error'</span>)</div><div class="line">train_loss_mean = -np.mean(train_loss,axis=<span class="number">1</span>)</div><div class="line">test_loss_mean = -np.mean(test_loss,axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">plt.plot(param_range, train_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</div><div class="line">         label=<span class="string">"Training"</span>)</div><div class="line">plt.plot(param_range, test_loss_mean, <span class="string">'o-'</span>, color=<span class="string">"g"</span>,</div><div class="line">        label=<span class="string">"Cross-validation"</span>)</div><div class="line"></div><div class="line">plt.xlabel(<span class="string">"gamma"</span>)</div><div class="line">plt.ylabel(<span class="string">"Loss"</span>)</div><div class="line">plt.legend(loc=<span class="string">"best"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/07/ML-Sklearn/image-20180701104019906.png" alt="image-20180701104019906"></p><blockquote><p>从图中可以看到，当gamma从0到0.0005左右，training error和test error都在减小，说明模型效果随着gamma的最大而变好；但是当gamma继续增大，test error 却开始增加，说明gamma大于0.001时模型过拟合了。</p></blockquote></li></ol><h1 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a><a href="https://morvanzhou.github.io/tutorials/machine-learning/sklearn/3-5-save/" target="_blank" rel="noopener">模型保存</a></h1><ol><li><p>pickle</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"></div><div class="line">clf = svm.SVC()</div><div class="line">iris = datasets.load_iris()</div><div class="line">X, y = iris.data, iris.target</div><div class="line">clf.fit(X,y)</div><div class="line"></div><div class="line"><span class="keyword">import</span> pickle <span class="comment">#pickle模块</span></div><div class="line"></div><div class="line"><span class="comment">#保存Model(注:save文件夹要预先建立，否则会报错)</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'save/clf.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    pickle.dump(clf, f)</div><div class="line"></div><div class="line"><span class="comment">#读取Model</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'save/clf.pickle'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</div><div class="line">    clf2 = pickle.load(f)</div><div class="line">    <span class="comment">#测试读取后的Model</span></div><div class="line">    print(clf2.predict(X[<span class="number">0</span>:<span class="number">1</span>]))</div><div class="line"></div><div class="line"><span class="comment"># [0]</span></div></pre></td></tr></table></figure></li><li><p>joblib</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib <span class="comment">#jbolib模块</span></div><div class="line"></div><div class="line"><span class="comment">#保存Model(注:save文件夹要预先建立，否则会报错)</span></div><div class="line">joblib.dump(clf, <span class="string">'save/clf.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment">#读取Model</span></div><div class="line">clf3 = joblib.load(<span class="string">'save/clf.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment">#测试读取后的Model</span></div><div class="line">print(clf3.predict(X[<span class="number">0</span>:<span class="number">1</span>]))</div><div class="line"></div><div class="line"><span class="comment"># [0]</span></div></pre></td></tr></table></figure></li></ol>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ML - Frequent Itemset Mining</title>
      <link href="/2018/06/05/ML%20-%20Frequent%20Itemset%20Mining/"/>
      <url>/2018/06/05/ML%20-%20Frequent%20Itemset%20Mining/</url>
      <content type="html"><![CDATA[<p>An association rule is a pattern that states when an event occurs, another event occurs with certain probability. Association relus find all sets of items that have support count greater than the mimimum support; then using the large itemsets to generate the desired rules that have confidence greater than the minimum confidence. For frequent itemset mining, we use Apriori algorithm.</p><p>The following details are from <a href="https://www.youtube.com/watch?v=Hk1zFOMLTrw" target="_blank" rel="noopener">The Apriori Algorithm … How The Apriori Algorithm Works</a>.</p><a id="more"></a><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><blockquote><ol><li><p>A set of all items in a store $I=\{i_1,i_2,…,i_m\}$</p></li><li><p>A set of all transactions (Transaction Database T)</p><p>$T={t_1,t_2,…,t_n}$</p></li><li><p>Each $t_i$ is a set of items s.t. $t\subseteq I$</p></li><li><p>Each transaction $t_i$ has a transaction ID(TID)</p></li></ol></blockquote><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><img src="/2018/06/05/ML - Frequent Itemset Mining/Screen Shot 2018-06-05 at 4.44.19 PM.png" alt="Screen Shot 2018-06-05 at 4.44.19 PM"></p><h2 id="Apriori-Algorithm"><a href="#Apriori-Algorithm" class="headerlink" title="Apriori Algorithm"></a>Apriori Algorithm</h2><p><img src="/2018/06/05/ML - Frequent Itemset Mining/Screen Shot 2018-06-09 at 11.38.24 PM.png" alt="Screen Shot 2018-06-09 at 11.38.24 PM"></p><p><img src="/2018/06/05/ML - Frequent Itemset Mining/Screen Shot 2018-06-09 at 11.39.28 PM.png" alt="Screen Shot 2018-06-09 at 11.39.28 PM"></p><p><img src="/2018/06/05/ML - Frequent Itemset Mining/Screen Shot 2018-06-09 at 11.40.39 PM.png" alt="Screen Shot 2018-06-09 at 11.40.39 PM"></p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Say we have a transaction database and the minimum support count is $sc=2$.</p><div class="table-container"><table><thead><tr><th style="text-align:center">TID</th><th style="text-align:center">Items</th></tr></thead><tbody><tr><td style="text-align:center">100</td><td style="text-align:center">1 , 3 , 4</td></tr><tr><td style="text-align:center">200</td><td style="text-align:center">2 , 3 , 5</td></tr><tr><td style="text-align:center">300</td><td style="text-align:center">1, 2, 3, 5</td></tr><tr><td style="text-align:center">400</td><td style="text-align:center">2, 5</td></tr><tr><td style="text-align:center">500</td><td style="text-align:center">1, 3, 5</td></tr></tbody></table></div><p><strong>Step One : 1-itemset</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">Itemset</th><th style="text-align:center">Support</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">4</td></tr></tbody></table></div><p>so we delete itemset = 4 because its support is less than 2, after prunning:</p><div class="table-container"><table><thead><tr><th style="text-align:center">Itemset</th><th style="text-align:center">Support</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">4</td></tr></tbody></table></div><p><strong>Step 2 : 2-itemset</strong></p><p>we build 2-itemset based on 1-itemset, where we get {1,2},{1,3},{1,5},{2,3},{2,5},}{3,5}</p><div class="table-container"><table><thead><tr><th style="text-align:center">Itemset</th><th style="text-align:center">Support</th></tr></thead><tbody><tr><td style="text-align:center">{1,2}</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">{1,3}</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">{1,5}</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">{2,3}</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">{2,5}</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">{3,5}</td><td style="text-align:center">3</td></tr></tbody></table></div><p>delete itemset { 1,2 } because its support is 1.</p><div class="table-container"><table><thead><tr><th style="text-align:center">Itemset</th><th style="text-align:center">Support</th></tr></thead><tbody><tr><td style="text-align:center">{1,3}</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">{1,5}</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">{2,3}</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">{2,5}</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">{3,5}</td><td style="text-align:center">3</td></tr></tbody></table></div><p><strong>Step 3 : 3-itemset</strong></p><p>we build 3-itemset based on 2-itemset, where we get{1,3,5},{1,2,3},{1,2,5},{2,3,5}</p><p>Before counting each itemset’s support, we do some prunning.</p><p>For {1,3,5} : {1,3},{1,5},{3,5}, and according to the fact that any frequent itemsets’ subset must be frequent itemset, so {1,3,5} may be frequent itemset because all of its subsets are frequent itemset.</p><p>For {1,2,3} : {1,2},{1,3},{2,3}. Because {1,2} is not frequent itemset, {1,2,3} is not frequent itemset and we don’t need to calculate its support.</p><p>Same rule to {1,2,5} and {2,3,5}, we can say that {1,2,5} is not frequent itemset but {2,3,5} may be frequent itemset.</p><div class="table-container"><table><thead><tr><th style="text-align:center">Itemset</th><th style="text-align:center">Support</th></tr></thead><tbody><tr><td style="text-align:center">{1,3,5}</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">{2,3,5}</td><td style="text-align:center">2</td></tr></tbody></table></div><p><strong>Step 4 : 4-itemset</strong></p><p>we get 4-itemset : {1,2,3,5}.</p><p>Prune ：{1,2,3},{1,2,5},{1,3,5},{2,3,5}. because {1,2,5} and {1,2,3} are not frequent itemsets, {1,2,3,5} is not frequent itemset.</p><p>Then we end up with empty. We return 3-itemset {1,3,5} and {2,3,5} for rule generation.</p><h2 id="Association-rule-generation"><a href="#Association-rule-generation" class="headerlink" title="Association rule generation"></a>Association rule generation</h2><p>Now we have the list of frequent itemsets</p><div class="table-container"><table><thead><tr><th style="text-align:center">Itemset</th><th style="text-align:center">Support</th></tr></thead><tbody><tr><td style="text-align:center">{1,3,5}</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">{2,3,5}</td><td style="text-align:center">2</td></tr></tbody></table></div><p>Generate all nonempty subsets for each frequent itemset $I$</p><blockquote><ul><li>For {1,3,5}, we have {1},{3},{5},{1,3},{1,5},{3,5}</li><li>For {2,3,5}, we have {2},{3},{5},{2,3},{2,5},{3,5}</li></ul></blockquote><p>For every subset $s$ of frequent itemset $I$, with the minimum confidence threshold, we generate the rule:</p><script type="math/tex; mode=display">s\to(I-s) \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if \ \ \ \  \ \frac{sc(I)}{sc(s)}\ge min\_conf</script><blockquote><p>Let us assume the minimum confidence threshold is 60%,</p><p>For frequent itemset {1,3,5}:</p><ul><li>Rule1: $\{1\}\to \{3,5\}$, confidence = $\frac{sc\{1,3,5\}}{sc\{1\}}=\frac{2}{3}=66.66\%$ <strong>Selected</strong></li><li>Rule2: $\{3\}\to \{1,5\}$, confidence = $\frac{sc\{1,3,5\}}{sc\{3\}}=\frac{2}{4}=50\%$ <strong>Rejected</strong></li><li>Rule3: $\{5\}\to \{1,3\}$, confidence = $\frac{sc\{1,3,5\}}{sc\{5\}}=\frac{2}{4}=50\%$ <strong>Rejected</strong></li><li>Rule4: $\{1,3\}\to \{5\}$, confidence = $\frac{sc\{1,3,5\}}{sc\{1,3\}}=\frac{2}{3}=66.66\%$ <strong>Selected</strong></li><li>Rule5: $\{1,5\}\to \{3\}$, confidence = $\frac{sc\{1,3,5\}}{sc\{1,5\}}=\frac{2}{2}=100\%$ <strong>Selected</strong></li><li>Rule6: $\{3,5\}\to \{1\}$, confidence = $\frac{sc\{1,3,5\}}{sc\{3,5\}}=\frac{2}{3}=66.66\%$ <strong>Selected</strong></li></ul><p>Same operations on {2,3,5}.</p></blockquote>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Algorithm - Bloom Filter</title>
      <link href="/2018/06/04/Algorithm-Bloom-Filter/"/>
      <url>/2018/06/04/Algorithm-Bloom-Filter/</url>
      <content type="html"><![CDATA[<p>文字参考自<a href="https://blog.csdn.net/hguisu/article/details/7866173" target="_blank" rel="noopener">海量数据处理算法—Bloom Filter</a>.</p><p>Bloom Filter（BF）是一种空间效率很高的随机数据结构，它是一个判断元素是否存在集合的快速的概率算法。Bloom Filter有可能会出现错误判断，但不会漏掉判断。也就是Bloom Filter判断元素不再集合，那肯定不在。如果判断元素存在集合中，有一定的概率判断错误。因此，Bloom Filter不适合那些“零错误”的应用场合。</p><a id="more"></a><h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><blockquote><p>计算某元素x是否在一个集合中，首先能想到的方法就是将所有的已知元素保存起来构成一个集合R，然后用元素x跟这些R中的元素一一比较来判断是否存在于集合R中；我们可以采用链表等数据结构来实现。但是，随着集合R中元素的增加，其占用的内存将越来越大。</p><p>于是，我们会想到用Hash table的数据结构，运用一个足够好的Hash函数将一个值映射到二进制位数组（位图数组）中的某一位。如果该位已经被置为1，那么表示该值已经存在。</p><p>Hash存在一个冲突（碰撞）的问题，两个值用同一个Hash得到的可能有相同的index。为了减少冲突，我们可以多引入几个Hash，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这便是Bloom-Filter的基本思想。</p></blockquote><h2 id="算法模块"><a href="#算法模块" class="headerlink" title="算法模块"></a>算法模块</h2><ol><li><p><strong>数据结构</strong></p><p>Bloom Filter使用一个<strong>m</strong>比特的数组来保存信息，初始状态时，Bloom Filter是一个包含m位的位数组，每一位都置为0，即BF整个数组的元素都设置为0。</p><p><img src="/2018/06/04/Algorithm-Bloom-Filter/Screen Shot 2018-06-04 at 4.43.28 PM.png" alt="Screen Shot 2018-06-04 at 4.43.28 PM"></p></li><li><p><strong>元素添加</strong></p><p>为了表达S={x1, x2,…,xn}这样一个n个元素的集合，Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。</p><p>当我们往Bloom Filter中增加任意一个元素x时候，我们使用k个哈希函数得到k个哈希值，也会是在数组中的下标index，然后将数组中对应的比特位设置为1。</p><p>如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。在下图中，k=3，且有两个哈希函数选中同一个位置（从左边数第五位，即第二个“1“处）。   </p><p><img src="/2018/06/04/Algorithm-Bloom-Filter/Screen Shot 2018-06-04 at 4.45.44 PM.png" alt="Screen Shot 2018-06-04 at 4.45.44 PM"></p></li><li><p><strong>元素查找</strong></p><p>在判断y是否属于这个集合时，我们只需要对y使用k个哈希函数得到k个哈希值，如果所有hashi(y)的位置都是1（1≤i≤k），即k个位置都被设置为1了，那么我们就认为y是集合中的元素，否则就认为y不是集合中的元素。下图中y1就不是集合中的元素（因为y1有一处指向了“0”位）。y2或者属于这个集合，或者刚好是一个false positive误判。</p><p><img src="/2018/06/04/Algorithm-Bloom-Filter/Screen Shot 2018-06-04 at 4.45.51 PM.png" alt="Screen Shot 2018-06-04 at 4.45.51 PM"></p></li></ol><h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a><a href="http://huangjacky.github.io/post/python-bloom-filter/" target="_blank" rel="noopener">Python实现</a></h2><ol><li><p>安装</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install pybloom</div></pre></td></tr></table></figure></li><li><p>使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pybloom <span class="keyword">import</span> BloomFilter, ScalableBloomFilter</div><div class="line">bf = BloomFilter(capacity=<span class="number">10000</span>, error_rate=<span class="number">0.001</span>)</div><div class="line">bf.add(<span class="string">'test'</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'test'</span> <span class="keyword">in</span> bf</div><div class="line">sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)</div><div class="line">sbf.add(<span class="string">'dddd'</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'ddd'</span> <span class="keyword">in</span> sbf</div></pre></td></tr></table></figure><p>BloomFilter是一个定容的过滤器，<strong>error_rate</strong>是指最大的误报率是0.1%，而ScalableBloomFilter是一个不定容量的布隆过滤器，它可以不断添加元素。<strong>add</strong>方法是添加元素，如果元素已经在布隆过滤器中，就返回true，如果不在返回fasle并将该元素添加到过滤器中。判断一个元素是否在过滤器中，只需要使用<strong>in</strong>运算符即可了。</p></li></ol>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ML - Expectation Maximization Algorithm</title>
      <link href="/2018/06/03/ML-Expectation-Maximization-Algorithm/"/>
      <url>/2018/06/03/ML-Expectation-Maximization-Algorithm/</url>
      <content type="html"><![CDATA[<p>下文资料参考了<a href="https://www.cnblogs.com/zlslch/p/6965374.html" target="_blank" rel="noopener">EM(期望最大化)算法初步认识</a></p><p><strong>极大似然估计</strong>，是参数估计的方法之一。其基本思想是已知样本符合某种概率分布，但是分布的参数未知，于是通过采样的随机样本估计参数。其基本步骤是：</p><ol><li>求出似然函数：该样本集的概率，即每个样本出现的概率连积</li><li>对似然函数取对数：将连乘变连加</li><li>求导：使对数似然函数取最大值的参数便是结果</li><li>求解方程：得到的参数即为所求。</li></ol><p><strong>期望最大算法</strong>，是一种从不完全数据或有数据丢失的数据集（存在隐含变量）中求解概率模型参数的最大似然估计方法。在每一次的迭代过程中，主要分为两步：即求期望(Expectation)步骤和最大化(Maximization)步骤。</p><a id="more"></a><h1 id="极大似然估计-Maximum-Likelihood"><a href="#极大似然估计-Maximum-Likelihood" class="headerlink" title="极大似然估计(Maximum Likelihood)"></a>极大似然估计(Maximum Likelihood)</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><blockquote><p>假设需要调查学校的男生身高分布，于是采用采样的方法在校园里随机的访问了100个男生。于是统计抽样得到了100个男生的身高，假设他们的身高服从正态分布，但是这个分布的均值$\mu$和方差$\delta^2$是未知的，即这两个参数是需要估计的，记作$\theta=[\mu,\delta]$。</p><p>用数学的语言来说就是：在学校的男生中，独立地按照概率密度函数$p(x|\theta)$抽样了100个人的身高，组成样本集$X$,我们想通过样本集$X$来估计未知参数$\theta$。</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><blockquote><p>由于每个样本都是独立地从$p(x|\theta)$中抽取的，换句话说这100个男生中的任何一个，都是随机抽取的，从我的角度来看这些男生之间是没有关系的。那么，我从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？因为这些男生（的身高）是服从同一个高斯分布$p(x|\theta)$的。那么我抽到男生A（的身高）的概率是$p(x_A|\theta)$，抽到男生B的概率是$p(x_B|\theta)$，那因为他们是独立的，所以很明显，我同时抽到男生A和男生B的概率是$p(x_A|\theta)\times{p(x_B|\theta)}$，同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。用数学家的口吻说就是从分布是$p(x|\theta)$的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：</p><p><img src="/2018/06/03/ML-Expectation-Maximization-Algorithm/Screen Shot 2018-06-03 at 4.37.55 PM.png" alt="Screen Shot 2018-06-03 at 4.37.55 PM"></p><p>这个概率反映了，在概率密度函数的参数是$theta$时，得到X这组样本的概率。因为这里X是已知的，也就是说我抽取到的这100个人的身高可以测出来，则每一种身高在该100个样本中可以计算$p(x_i;\theta)=\frac{身高为x_i的人数$}{100}$。而$\theta$是未知了，则上面这个公式只有θ是未知数，所以它是$\theta$的函数。这个函数放映的是在不同的参数$\theta$取值下，取得当前这个样本集的可能性，因此称为参数$\theta$相对于样本集X的似然函数（likehood function）。记为$L(\theta)$。</p></blockquote><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><blockquote><ol><li><p>似然函数思想：抽取的样本的概率最大</p><p>下课了，一群男女同学分别去厕所了。然后，你想知道课间是男生上厕所的人多还是女生上厕所的人比较多，然后你就跑去蹲在男厕和女厕的门口。蹲了五分钟，你跑过来告诉我，课间女生上厕所的人比较多。我问你是怎么知道的。你说：“5分钟了，出来的是女生，女生啊，那么女生出来的概率肯定是最大的了，或者说比男生要大，那么女厕所的人肯定比男厕所的人多”。看到了没，你已经运用最大似然估计了。你通过观察到女生先出来，那么什么情况下，女生会先出来呢？肯定是女生出来的概率最大的时候了，那什么时候女生出来的概率最大啊，那肯定是女厕所比男厕所多人的时候了，这个就是你估计到的参数了。</p><p>回到男生身高那个例子。在学校那么男生中，我一抽就抽到这100个男生（表示身高），而不是其他人，那是不是表示在整个学校中，这100个人（的身高）出现的概率最大啊。那么这个概率怎么表示？就是上面那个似然函数$L(\theta)$。所以，我们就只需要找到一个参数θ，其对应的似然函数$L(\theta)$最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做$\theta$的最大似然估计量，记为：</p><script type="math/tex; mode=display">\hat{\theta}=arg maxL(\theta)</script><p>可以看到$L(\theta)$是连乘的，所以为了便于分析，还可以定义对数似然函数，将其变成连加的：</p><p><img src="/2018/06/03/ML-Expectation-Maximization-Algorithm/Screen Shot 2018-06-03 at 4.44.37 PM.png" alt="Screen Shot 2018-06-03 at 4.44.37 PM"></p><p>这样使似然函数取得最大值的$\theta$便是我们的估计。这里就回到了求最值的问题了。怎么求一个函数的最值？当然是求导，然后让导数为0，那么解这个方程得到的$\theta$就是了（当然，前提是函数$L(\theta)$连续可微）。那如果θ是包含多个参数的向量那怎么处理啊？当然是求$L(\theta)$对所有参数的偏导数，也就是梯度了，那么n个未知的参数，就有n个方程，方程组的解就是似然函数的极值点了，当然就得到这n个参数了。</p></li></ol></blockquote><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><a href="https://www.youtube.com/watch?v=cXHvC_FGx24" target="_blank" rel="noopener">SciPy.minimize</a></p><ol><li><p><a href="https://cosx.org/2009/07/maximum-likelihood-estimation-in-r" target="_blank" rel="noopener">题目</a></p><p>采用geyser 数据，该数据采集自美国黄石公园内的一个名叫 Old Faithful 的喷泉。“waiting”就是喷泉两次喷发的间隔时间，“duration”当然就是指每次喷发的持续时间。在这里，我们只用到 “waiting” 数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">waiting  duration</div><div class="line"><span class="number">1</span>        <span class="number">80</span> <span class="number">4.0166667</span></div><div class="line"><span class="number">2</span>        <span class="number">71</span> <span class="number">2.1500000</span></div><div class="line"><span class="number">3</span>        <span class="number">57</span> <span class="number">4.0000000</span></div><div class="line"><span class="number">4</span>        <span class="number">80</span> <span class="number">4.0000000</span></div><div class="line"><span class="number">5</span>        <span class="number">75</span> <span class="number">4.0000000</span></div><div class="line">......</div></pre></td></tr></table></figure><p><img src="/2018/06/03/ML-Expectation-Maximization-Algorithm/Screen Shot 2018-06-03 at 8.29.59 PM.png" alt="Screen Shot 2018-06-03 at 8.29.59 PM"></p><p><img src="/2018/06/03/ML-Expectation-Maximization-Algorithm/Screen Shot 2018-06-03 at 8.30.27 PM.png" alt="Screen Shot 2018-06-03 at 8.30.27 PM"></p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> stats <span class="comment">#获取正态分布</span></div><div class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize <span class="comment">#目标优化</span></div><div class="line"></div><div class="line"><span class="comment">#####数据存储于.txt文件中</span></div><div class="line">data = np.loadtxt(<span class="string">'data'</span>)</div><div class="line">values = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</div><div class="line">    values.append(i[<span class="number">2</span>])</div><div class="line">values = np.array(values)</div><div class="line"></div><div class="line"><span class="comment">#####定义极大似然函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective_func</span><span class="params">(params)</span>:</span><span class="comment">#优化函数，总共优化5个参数</span></div><div class="line">    prob = params[<span class="number">0</span>]</div><div class="line">    mu1 = params[<span class="number">1</span>]</div><div class="line">    sigma1 = params[<span class="number">2</span>]</div><div class="line">    mu2 = params[<span class="number">3</span>]</div><div class="line">    sigma2 = params[<span class="number">4</span>]</div><div class="line">    norm_dis1 = stats.norm.pdf(values,loc=mu1,scale=sigma1) <span class="comment">#将x轴数据输入，输出该样本的正态分布的概率密度</span></div><div class="line">    norm_dis2 = stats.norm.pdf(values,loc=mu2,scale=sigma2)</div><div class="line">    f = prob*norm_dis1+(<span class="number">1</span>-prob)*norm_dis2</div><div class="line">    L = np.sum(np.log(f))</div><div class="line">    <span class="keyword">return</span> -L <span class="comment">#minimize函数求解最小值，但是我们要最大化，所以加一个“-”</span></div><div class="line"></div><div class="line"><span class="comment">#####求解</span></div><div class="line">param = np.array([<span class="number">0.5</span>,<span class="number">50</span>,<span class="number">10</span>,<span class="number">80</span>,<span class="number">10</span>]) <span class="comment">#参数初始化,根据数据直方分布图猜测</span></div><div class="line">results = minimize(objective_func, param, method=<span class="string">'nelder-mead'</span>)</div><div class="line">paramss = results.x <span class="comment">#[0.36088303 54.61479245  5.87121392 80.09109663  5.86777145]</span></div><div class="line">prob = paramss[<span class="number">0</span>]</div><div class="line">mu1 = paramss[<span class="number">1</span>]</div><div class="line">sigma1 = paramss[<span class="number">2</span>]</div><div class="line">mu2 = paramss[<span class="number">3</span>]</div><div class="line">sigma2 = paramss[<span class="number">4</span>]</div><div class="line"></div><div class="line"><span class="comment">#####数据可视化</span></div><div class="line">x = np.linspace(<span class="number">40</span>,<span class="number">120</span>,<span class="number">100</span>)</div><div class="line">f = prob*stats.norm.pdf(x,loc=mu1,scale=sigma1) + (<span class="number">1</span>-prob)*stats.norm.pdf(x,loc=mu2,scale=sigma2)</div><div class="line">plt.plot(x,f,color=<span class="string">'r'</span>) <span class="comment">#将估计的参数函数代入原密度函数。</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="/2018/06/03/ML-Expectation-Maximization-Algorithm/Screen Shot 2018-06-03 at 8.44.22 PM.png" alt="Screen Shot 2018-06-03 at 8.44.22 PM"></p></li></ol><h1 id="期望最大算法"><a href="#期望最大算法" class="headerlink" title="期望最大算法()"></a>期望最大算法()</h1><h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h2><blockquote><p>回到上面那个身高分布估计的问题。现在，通过抽取得到的那100个男生的身高和已知的其身高服从高斯分布，我们通过最大化其似然函数，就可以得到了对应高斯分布的参数$\theta=[\mu,\sigma]^T$了。那么，对于我们学校的女生的身高分布也可以用同样的方法得到了。</p><p>如果对学生抽样时，男生和女生混合在一起，200个人（的身高）里面随便给我指一个人（的身高），我都无法确定这个人（的身高）是男生（的身高）还是女生（的身高）。也就是说你不知道抽取的那200个人里面的每一个人到底是从男生的那个身高分布里面抽取的，还是女生的那个身高分布抽取的。用数学的语言就是，抽取得到的每个样本都不知道是从哪个分布抽取的。这个时候，对于每一个样本或者你抽取到的人，就有两个东西需要猜测或者估计的了，一是这个人是男的还是女的？二是男生和女生对应的身高的高斯分布的参数是多少？只有当我们知道了哪些人属于同一个高斯分布的时候，我们才能够对这个分布的参数作出靠谱的预测，例如刚开始的最大似然所说的，但现在两种高斯分布的人混在一块了，我们又不知道哪些人属于第一个高斯分布，哪些属于第二个，所以就没法估计这两个分布的参数。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些人属于第一个分布，那些人属于第二个分布。</p></blockquote><h2 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h2><blockquote><p>EM的意思是“Expectation Maximization”，在我们上面这个问题里面，我们是先随便猜一下男生（身高）的正态分布的参数：如均值和方差是多少。例如男生的均值是1米7，方差是0.1米（当然了，刚开始肯定没那么准），然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是1米8，那很明显，他最大可能属于男生的那个分布），这个是属于Expectation一步。有了每个人的归属，或者说我们已经大概地按上面的方法将这200个人分为男生和女生两部分，我们就可以根据之前说的最大似然那样，通过这些被大概分为男生的n个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是Maximization。然后，当我们更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么我们就再需要调整E步……如此往复，直到参数基本不再发生变化为止。</p><p>这里把每个人（样本）的完整描述看做是三元组yi={xi,zi1,zi2}，其中，xi是第i个样本的观测值，也就是对应的这个人的身高，是可以观测到的值。zi1和zi2表示男生和女生这两个高斯分布中哪个被用来产生值xi，就是说这两个值标记这个人到底是男生还是女生（的身高分布产生的）。这两个值我们是不知道的，是隐含变量。确切的说，zij在xi由第j个高斯分布产生时值为1，否则为0。例如一个样本的观测值为1.8，然后他来自男生的那个高斯分布，那么我们可以将这个样本表示为{1.8, 1, 0}。如果zi1和zi2的值已知，也就是说每个人我已经标记为男生或者女生了，那么我们就可以利用上面说的最大似然算法来估计他们各自高斯分布的参数。但是它们未知，因此我们只能用EM算法。</p></blockquote><h2 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a><a href="https://www.youtube.com/watch?v=REypj2sy_5U" target="_blank" rel="noopener">EM算法推导</a></h2><blockquote><p>假设我们知道了样本$X=\{x_1,x_2,…,x_n\}$的分布，比如正态分布，那么我们需要估计的参数就是均值$\mu$和方差$\sigma^2$，即：</p><script type="math/tex; mode=display">\mu=\frac{x_1+x_2+...+x_n}{n}\\\sigma^2=\frac{(x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2}{n}</script><p>这样我们就可以估计样本分布参数了。</p><p>但是对于给定的样本$X=\{x_1,x_2,…,x_n\}$,假设它们来自于两个正态分布$N_1(\mu_1,\sigma_1^2)$和$N_2(\mu_2,\sigma_2^2)$，但是我们不知道每一个样本$x_i$具体来自于哪一个分布，所以我们的额外需要估计的一个参数$\pi$，即每一个样本点来自于分布1的概率向量，那么每一个样本点来自于样本2的概率就是$1-\pi$。</p><p>假设我们知道了两个分布，即两个正态分布的参数已知，那么对于每一个样本$x_i$，我们假设这个样本来自于$N_1$，那么我们可以计算出该样本的概率$\pi(i)=P_{N_1}(x_i)$；同理，我们也可以计算出该样本属于$N_2$的概率$P_{N_2}(x_i)$。这样，直觉上，概率越大，样本点越可能属于对应的那个分布。对所有的样本点进行这样的概率比较，我们就可以将样本点分成两类，一个是更可能属于分布1的样本点$X_{N_1}$，一个是更可能属于分布2的样本点$X_{N_2}$。这样，我们就可以使用<script type="math/tex">X_{N_1}</script>中的点去更新$N_1$中的参数：</p><script type="math/tex; mode=display">\mu_1=\frac{\pi(1)x_1+\pi(2)x_2+...+\pi(n)x_n}{\pi(1)+\pi(2)+...+\pi(n)}\\\sigma_1^2=\frac{\pi(1)(x_1-\mu)^2+\pi(2)(x_2-\mu)^2+...+\pi(n)(x_n-\mu)^2}{\pi(1)+\pi(2)+...+\pi(n)}</script><p>同理，对于分布2的参数估计：</p><script type="math/tex; mode=display">\mu_2=\frac{(1-pi(1))x_1+(1-\pi(2))x_2+...+(1-\pi(n))x_n}{(1-\pi(1))+(1-\pi(2))+...+(1-\pi(n))}\\\sigma_2^2=\frac{(1-\pi(1))(x_1-\mu)^2+(1-\pi(2))(x_2-\mu)^2+...+(1-\pi(n))(x_n-\mu)^2}{(1-\pi(1))+(1-\pi(2))+...+(1-\pi(n))}</script></blockquote><h2 id="例子1：双硬币问题"><a href="#例子1：双硬币问题" class="headerlink" title="例子1：双硬币问题"></a><a href="https://blog.csdn.net/lilynothing/article/details/64443563" target="_blank" rel="noopener">例子1：双硬币问题</a></h2><ol><li><p><strong>问题</strong></p><p>假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的抛硬币实验：共做5次实验，每次实验独立的抛十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H，H代表正面朝上。  假设试验数据记录员可能是实习生，业务不一定熟悉，造成a和b两种情况  a表示实习生记录了详细的试验数据，我们可以观测到试验数据中每次选择的是A还是B  b表示实习生忘了记录每次试验选择的是A还是B，我们无法观测实验数据中选择的硬币是哪个  问在两种情况下分别如何估计两个硬币正面出现的概率？</p><p>对于已知是A硬币还是B硬币抛出的结果的时候，可以直接采用概率的求法来进行求解。对于含有隐变量的情况，也就是不知道到底是A硬币抛出的结果还是B硬币抛出的结果的时候，就需要采用EM算法进行求解了。如下图：</p><p><img src="/2018/06/03/ML-Expectation-Maximization-Algorithm/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/ML-Expectation-Maximization-Algorithm/20170321142254904.jpeg" alt="20170321142254904"></p></li><li><p><strong>分析</strong></p><p>对于已知样本来源，那么我们需要估计的参数，对于硬币A，其正面概率$\theta_A$，那么其反面概率即$1-\theta_A$；对于硬币B， 其正面概率$\theta_B$，那么其反面概率即$1-\theta_B$；那么$\theta_A=\frac{正面样本数}{硬币A的样本数}=\frac{24}{24+6}=0.8$</p><p>那么对于不知道样本来源，我们先假设$\theta_A=0.6，\theta_B=0.5$,这样我们需要去估计每一个样本来自于硬币A的概率分布$\pi$，即由贝叶斯法则：</p><script type="math/tex; mode=display">\pi(A|i)=\frac{p(A)p(i|A)}{p(i)}</script><p>比如对于第一行的第一个硬币结果是H，那么$p(i|A)=0.6$，即硬币A产生正面的概率是0.6；$p(A)=\frac{1}{2}$，即选择硬币A的概率是0.5；$p(i)=p(A)p(i|A)+p(B)p(i|B)$，即硬币A和B均可以产生正面。</p><p>那么$\pi(1)=0.55$，即如果是正面向上的样本点，它有0.55可能性来自于样本A。同理可以计算出反面向上的样本点来自于A的概率是0.44，则所有样本点的$\pi=(H,T,T,T,H,…,H,T,H)=(0.55,0.44,0.44,0.44,0.55,…,0.55,0.44,0.55)$</p><p>那么更新$\theta_A,\theta_B$:</p><script type="math/tex; mode=display">\theta_A=\frac{正面次数}{总次数}=\frac{0.55*(5+9+8+4+7)}{0.55*33+0.44*17}=0.71\\\theta_B=\frac{(1-0.55)*(33)}{(1-0.55)*33+(1-0.44)*17}=0.61</script></li><li><p><strong>代码</strong></p></li></ol><h2 id="例子2：混合高斯模型"><a href="#例子2：混合高斯模型" class="headerlink" title="例子2：混合高斯模型"></a>例子2：混合高斯模型</h2><ol><li><p>问题</p><p>假设我们有一观测样本$X=\{X_1,X_2,..,X_n\}$, 已知$X$ 是由$k$ 个高斯分布均匀混合而成，这$k$个高斯分布的均值不同，但是有相同的方差。简单起见，$k=2$。</p></li><li><p>思路</p></li><li><p>代码</p></li></ol>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP Application - Local Privacy</title>
      <link href="/2018/06/02/DP-Application%20-%20Local-Privacy/"/>
      <url>/2018/06/02/DP-Application%20-%20Local-Privacy/</url>
      <content type="html"><![CDATA[<h2 id="Q"><a href="#Q" class="headerlink" title="Q?"></a>Q?</h2><ol><li>the communication cost of $n$ users for Bassily and Smith is $log_2(n)$</li><li>the communication cost of $n$ users for k-rr is $log_2d$</li><li><a href="https://arxiv.org/pdf/1705.04421.pdf" target="_blank" rel="noopener">[Optimizing Locally Differentially Private Protocols]</a> remains : what is pure? THE protocol? BLH? Threshold $T_s$ in experiment and true/false positive?</li></ol><h2 id="T"><a href="#T" class="headerlink" title="T!"></a>T!</h2><ol><li>in order to optimize ldp protocol, tends to optimize encoding step,</li><li>using domain knowledge to elimate impossible candidates to narrow down the size of encoded input.<a href="https://arxiv.org/pdf/1708.06674.pdf" target="_blank" rel="noopener">Locally Differentially Private Heavy Hitter Identification</a></li><li></li></ol><p>Local differential privacy (LDP) techniques collects randomized answers from each user, with guarantees of plausible <a href="https://arxiv.org/pdf/1712.01524.pdf" target="_blank" rel="noopener">deniability</a>; meanwhile, the aggregator can still build accurate models and predictors by analyzing large amounts of such randomized data.</p><p>Unlike other models of differential privacy, which publish randomized aggregates but still collect the exact sensitive data, LDP avoids collecting exact personal information in the first place, thus providing a stronger assurance to the users and to the aggregator.</p><p>The well-established Laplace mechanism and exponential mechanism are no longer suitable to the local setting in which a user may have only a single element to release and laplace noise may introduce too much noise.  But unlike the laplace mechanism where we can use the noisy output directly, we need to do some estimation on the randomized output to get the estimator in LDP.<a href="https://davidyinyang.weebly.com/uploads/9/8/6/2/9862052/ccs16-ldp.pdf" target="_blank" rel="noopener">2016-Zhan</a></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-11 at 10.16.20 PM.png" alt="Screen Shot 2018-06-11 at 10.16.20 PM"></p><h2 id="Development-Track"><a href="#Development-Track" class="headerlink" title="Development Track"></a>Development Track</h2><p>Local differential privacy starts from randomized response. At first, it collects <strong>binary categorical data</strong> from clients with <u>W-RR</u> algorithm. Then, polybasic categorical data is considered with solutions like <u>K-RR</u> and <u>K-RAPPOR</u> methods. After that, <u>Random Matrix Projection</u> is proposed for dealling with extremely large categorical data in the practical setting. </p><p>Since the method above is limited to categorical data, so <a href="https://arxiv.org/pdf/1302.3203.pdf" target="_blank" rel="noopener">[Duchi et al.]</a> proposed for <strong>numeric data</strong> and details algorithm is in <a href="https://arxiv.org/pdf/1606.05053.pdf" target="_blank" rel="noopener">[Collecting and Analyzing Data from Smart Device Users with Local Differential Privacy]</a> and based on Duchi, <u>harmony algorithm</u> is developed. </p><p>The most methods above assmue that each client only has one value, so the mechanism for <u>set-value</u> setting is proposed <a href="https://davidyinyang.weebly.com/uploads/9/8/6/2/9862052/ccs16-ldp.pdf" target="_blank" rel="noopener">[Heavy Hitter Estimation over Set-Valued Data with Local Differential Privacy]</a>.</p><p>And in real world, the collecting process in the client end happens frequently, like daily or every few hours. So methods for <strong>data collecting regularly</strong> is proposed like <u>RAPPOR</u> and <a href="https://arxiv.org/pdf/1712.01524.pdf" target="_blank" rel="noopener">[Collecting Telemetry Data Privately]</a></p><p>Recently, a mechanism for computing <strong>joint distribution</strong> of data attributes collected by LDP is proposed in <a href="https://arxiv.org/pdf/1503.01214.pdf" target="_blank" rel="noopener">[Building a RAPPOR with the Unknown- Privacy-Preserving Learning of Associations and Data Dictionaries 12.45.24 PM]</a> and <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8306916" target="_blank" rel="noopener">[LoPub: High-Dimensional Crowdsourced Data Publication With Local Differential Privacy]</a></p><a id="more"></a><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-04 at 11.16.07 AM.png" alt="Screen Shot 2018-06-04 at 11.16.07 AM"></p><blockquote><ol><li>According to the above definition, the aggregator, who receives the perturbed tuple $t*$ , cannot distinguish whether the true tuple is $t$ or another tuple $t$ with high confidence (controlled by parameter $\epsilon$), regardless of the background information of the aggregator.</li><li>There is no neighbour databases in the local-DP; instead, for a local user, all pairs of his values should be undistinguished by using the mechanism.</li><li>While LDP provides rigorous privacy protection in the local setting, its drawback is on the utility side. In real-world applications, where the universe size $Dom(f)$ is large, achieving a reasonable trade-off between privacy and utility under LDP is almost a chimera. For example, in location protection, $Dom(f)$ is all possible locations of a user, which could be extremely large.</li></ol></blockquote><h2 id="Comparison-With-Standard-DP"><a href="#Comparison-With-Standard-DP" class="headerlink" title="Comparison With Standard DP"></a><a href="https://arxiv.org/pdf/1805.01456.pdf" target="_blank" rel="noopener">Comparison With Standard DP</a></h2><h3 id="Standard-DP"><a href="#Standard-DP" class="headerlink" title="Standard DP"></a>Standard DP</h3><p>More precisely, let $M$ be a (noisy) mechanism for answering a certain query on a generic database $D$, and let $P[M(D) \in S]$ denote the probability that the answer given by $M$ to the query, on $D$, is in the (measurable) set of values $S$. We say that $M$ is $\epsilon$-differentially private if for every pairs of adjacent databases $D$ and $D’$ (i.e., differing only for the value of a single individual), and for every measurable set $S$, we have $P[M(D) \in S] ≤ e^{\epsilon}P[M(D’) \in S]$. </p><h3 id="LDP"><a href="#LDP" class="headerlink" title="LDP"></a>LDP</h3><p>Technically, an obfuscation mechanism $M$ is locally differentially private with privacy level $\epsilon$ if for every pair of input values $x$, $x’$ $\in X$ (the set of possible values for the data of a generic user), and for every measurable set $S$, we have $P[M(x) \in S] ≤ e^{\epsilon} P[M(x 0 ) \in S]$. The idea is that the user provides $M(x)$ to the data collector, and not $x$.</p><h2 id="Mechanism"><a href="#Mechanism" class="headerlink" title="Mechanism"></a>Mechanism</h2><p><a href="https://arxiv.org/pdf/1602.07387.pdf" target="_blank" rel="noopener">2016-Peter</a></p><h3 id="Binary-Values"><a href="#Binary-Values" class="headerlink" title="Binary Values"></a>Binary Values</h3><h4 id="W-RR-Categorical-data"><a href="#W-RR-Categorical-data" class="headerlink" title="W-RR-Categorical data"></a>W-RR-Categorical data</h4><ol><li><p><strong>Probelm setting</strong></p><p>In the binary attribute setting, say each user has a data $x, x\in\{0,1\}$。Now the user ranodmly sends his data $x=0 \ or \  x=1$ with a probability.  </p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen%20Shot%202018-06-05%20at%209.06.11%20PM.png" alt="Screen Shot 2018-06-05 at 9.06.11 PM"></p><p>Now we want $P$ to ensure Local-DP and at the same time achieve a good utility.</p></li><li><p><strong>Analysis</strong></p><p>Without loss of generality, we assume the randomized response still favors the true value, i.e., $p_{00},p_{11}\gt{0.5}$；At the same time, $\frac{p_{00}}{p_{01}}\le{e^\epsilon}$ to ensure differential privacy.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-05 at 9.12.44 PM 2.png" alt="Screen Shot 2018-06-05 at 9.12.44 PM 2"></p></li><li><p><strong>Conclusion</strong></p><p>So based on above proof, we can get the transit matrix:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen%20Shot%202018-06-05%20at%209.15.49%20PM.png" alt="Screen Shot 2018-06-05 at 9.15.49 PM"></p><blockquote><ol><li>W-RR (Warner’s randomized response model) is optimal for binary distribution minimax estimation.</li><li>For all binary distributions $p$, all loss functions $l$, and all privacy levels $\epsilon$, $Q_{WRR}$ is the optimal solution to the private minimax distribution estimation problem.</li></ol></blockquote></li><li><p><strong>Utility</strong></p><blockquote><ul><li><p>Each user reports her true answer $t$ with probability $p$, and a random answer with probability $1 − p$. The latter has the same probability to be −1 and +1, whose expected value is zero. Therefore, the expected value for ui’s reported value is $p \times t$; thus, we can obtain an unbiased estimate of true answer by multiplying the reported value by a scaling factor $c = 1/p$. </p><p>$p_{00}=p+(1-p)*(1/2)\ \to \ p=\frac{e^\epsilon -1}{e^\epsilon +1}$.</p><p>So each user reports his random value in the following way to get an unbiased estimator:</p><script type="math/tex; mode=display">t^*=\left\{\begin{align} &c\times t=\frac{e^\epsilon +1}{e^\epsilon -1}\times t  & \textrm{Pr=p+(1-p)/2}=\frac{e^\epsilon }{e^\epsilon +1} \\&-c\times t=-\frac{e^\epsilon +1}{e^\epsilon -1}\times t  & \textrm{Pr=(1-p)/2}=\frac{1 }{e^\epsilon +1}\end{align}\right.</script></li><li><p>Once the aggregator receives all reported values for attribute $t$ , it computes the average over all users, which is an estimate of the mean value $\mathbb{E}[t]$ for $t$. Since $t$ can be either $+1$ or $-1$, the the percentage of users with $+1$ is $\frac{1+\mathbb{E}[t]}{2}$ and +1 is $\frac{1-\mathbb{E}[t]}{2}$</p></li></ul></blockquote></li></ol><h3 id="Polychotomous-Values"><a href="#Polychotomous-Values" class="headerlink" title="Polychotomous Values"></a>Polychotomous Values</h3><ol><li><p><strong>Problem setting</strong></p><p>Now we extend the question to multiple attributes and each user has a data $x, x\in\{0,1,2,…,t\}$。And the transit matrix is  </p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-05 at 9.22.05 PM.png" alt="Screen Shot 2018-06-05 at 9.22.05 PM"></p></li><li><p><strong>Solution</strong></p><ul><li><p><strong>K-ary Randomized Response</strong>-Categorical data</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-05 at 4.51.03 PM.png" alt="Screen Shot 2018-06-05 at 4.51.03 PM"></p><blockquote><p>Remarks:</p><ul><li>k-RR mechanism is optimal in the low privacy regime while suboptimal in high privacy regime.</li></ul></blockquote></li><li><p><strong>K-RAPPOR</strong>-Categorical data</p><p>The simplest version of RAPPOR, , called the basic one-time RAPPOR and referred to herein as k-RAPPO. There are several steps here.</p><blockquote><ol><li><p>data representaion</p><p>K-RAPPOR maps the input $X$ of size $k$ to an output $Y=\{0,1\}^k$ of size $2^k$. </p><p>Firstly, we map $X$ deterministically to $\widetilde{X}=R^k$, the $k$-dimensional Euclidean space, where $X=x_i \ (1\le{x_i}\le{k})$ and $\widetilde{X}=e_i\in\{0,1\}^k$. Specifically, it deterministically maps $X=x_i$ to $\widetilde{X}=e_i$, where $e_i$ is the $i$-th standard basis vector.</p><p>For example, $k=3$, and the input $X=\{x_1,x_2,x_3\}$, then $\widetilde{X}$  is $\{001,010,100\}$. And $x_1$ is represented as 1st data in $\widetilde{X}$, which is 001, $x_2$ is 2ed one 010. $x_3$ is 3rd one 100.</p></li><li><p>bit perturbation</p><p>In order to achieve $\epsilon$-locally differential privacy, we randomize the coordinates of $\widetilde{X}$ independently to obtain the private vector $Y\in\{0,1\}^k$. Formally, the $j^{th}$ coordinate of $Y$ is given by :</p><script type="math/tex; mode=display">Y(i)=\begin{cases}\widetilde{X}(i) \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ p=\frac{e^{\epsilon/2}}{1+e^{\epsilon/2}}\\1-\widetilde{X}(i) \ \ \ \ \ \ \ \ \ \ \ \ q=1-\frac{e^{\epsilon/2}}{1+e^{\epsilon/2}}\end{cases}</script></li><li><p>frequency estimation<a href="https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-wang-tianhao.pdf" target="_blank" rel="noopener">[2017-Wang]</a></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-13 at 10.32.15 PM.png" alt="Screen Shot 2018-06-13 at 10.32.15 PM"></p></li></ol><p>Remarks:</p><ul><li>for two input, take $\{001.010,100\}$ as the example, the sensitivity is 2. Because for any two different input, the difference is 2 bits.</li><li>using w-rr as bit perturbation method</li><li>k-RAPPOR is a optimal privacy-preserving mechanism in high privacy regime while suboptimal in low privacy regime.</li></ul><p>Pros:</p><p>Cons:</p><ol><li>A main drawback of RAPPOR is its high communication overhead, i.e., each user needs to transmit d bits to the data collector, which can be expensive when the number of possible items d is large.</li></ol></blockquote></li><li><p>Random Matrix Projection<a href="https://arxiv.org/pdf/1504.04686.pdf" target="_blank" rel="noopener">[2015-Bassily]</a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498248" target="_blank" rel="noopener">[2016-Chen]</a><a href="https://arxiv.org/pdf/1705.04421.pdf" target="_blank" rel="noopener">[2017-Wang]</a><a href="https://arxiv.org/pdf/1606.05053.pdf" target="_blank" rel="noopener">[2016-Nguyen]</a>-Categorical data</p><p>Here, each attribute contains $k\ge2$ possible values and the aggregator aims to build a histogram that contains the estimated frequency for each of the $k$ possible values.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 9.59.32 PM.png" alt="Screen Shot 2018-06-09 at 9.59.32 PM"></p><blockquote><p>Algorithm R: $\epsilon$-Basic Randomizer</p><ol><li><p>Assume that the number of possible values $d$ in the categorical attribute is far larger than the number of users $n$; hence, the method applies random projection to reduce the dimensionality from $d$ to $m$</p></li><li><p><strong>Encoding</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-07 at 5.52.05 PM.png" alt="Screen Shot 2018-06-07 at 5.52.05 PM"></p><p>Encode($x$)=$\Phi[r,j]$, where $r$ r is selected uniformly at random from $\{1,2,…,m\}$ and $j$ is the index of $x$ in original dataset $\{x_1,x_2,…,x_d\}$.</p><p>So for each user, the return vector is  $m$-dimensional vector where $s^{th} $ entry is $z_j$ and others are 0, which means the true loc in $i^{th} $ pos is maped to $s^{th}$ pos.</p></li><li><p><strong>Aggregation</strong></p><p>After receive all $z_i$ from users, we compute $\hat{z}=\frac{1}{n}\sum_{i=1}^{n}z_i$, where $\hat{z}$ is a $m$-dimentional vector.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 10.07.38 PM.png" alt="Screen Shot 2018-06-09 at 10.07.38 PM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-13 at 10.58.58 PM.png" alt="Screen Shot 2018-06-13 at 10.58.58 PM"></p></li></ol><p>Remarks:</p><ol><li>We map the input domain with size $d$ to output domain with size $m$, where $m$ is determined through Johnson-Lindenstrauss Lemma.</li><li>$R$ is $\epsilon$-LDP for every choice of the index $s$</li><li>$R(x)=z_j$ is an unbiased estimator of $x$. That is, $\mathbb{E}[R(x)]=x$</li><li>when $d\ge n$, SH applies random projection on the data to reduce its dimensionality from d to $m = O(n)$ in a preprocessing step.</li><li>Assuming each user only has one item.</li></ol><p>Pros:</p><ol><li>the communication cost of SH between each user and the data collector is $O(1)$ rather than $O(d)$ as in RAPPOR.</li></ol><p>Cons:</p></blockquote></li><li><p><a href="https://arxiv.org/pdf/1606.05053.pdf" target="_blank" rel="noopener">Harmony-for-numeric</a>-Numeric data</p><blockquote><p>This method is to handle multiple numeric attributes. Say a user has a private tuple $t$, which contains $d$ attributes $A_1,A_2,…,A_d$. And assuming that each numeric attribute has a domain $[-1,1]$. And for numeric data, we try to calculate the mean of data.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 6.11.42 PM.png" alt="Screen Shot 2018-06-09 at 6.11.42 PM"></p><p>Remarks:</p><ol><li><p>The return tuple $t^*_{i}$ has non-zero value on only one attribute $A_j(j\in [d])$ and this value is binary.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 6.14.45 PM.png" alt="Screen Shot 2018-06-09 at 6.14.45 PM"></p></li><li><p>Algorithm 2 is $\epsilon$-LDP.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 6.16.58 PM.png" alt="Screen Shot 2018-06-09 at 6.16.58 PM"></p></li><li><p>the estimator $\frac{1}{n}\sum^{n}_{i=1}t^*[A_j]$ is an unbiased estimator of the mean of $A_j$.</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 6.20.05 PM.png" alt="Screen Shot 2018-06-09 at 6.20.05 PM"></p></li></ol><ol><li>xx</li></ol></blockquote></li></ul></li></ol><h2 id="Summary-of-Mechanism"><a href="#Summary-of-Mechanism" class="headerlink" title="Summary of Mechanism"></a>Summary of Mechanism</h2><p><a href="https://arxiv.org/pdf/1705.04421.pdf" target="_blank" rel="noopener">[Locally Differentially Private Protocols for Frequency Estimation]</a> gives a comprehensive introduction of all kinds of mechanism. Here is the comparision among all methods.</p><h3 id="Problem-setting"><a href="#Problem-setting" class="headerlink" title="Problem setting"></a>Problem setting</h3><p>It generalizes all methods as the same protocol. We have a input domain $x\in\{1,2,…,d\}$. For each user holding data $x_i$ and he send his data as $x_i$ with probability </p><p>$p^*$ </p><p>and as others with probability as </p><p>$q^*$. </p><p>Intuitively, we want</p><p> $p^*$ </p><p>to be as large as possible, and </p><p>$q^*$</p><p> to be as small as possible. However, satisfying </p><p>$\epsilon$</p><p>-LDP requires that</p><p> $\frac{p^<em>}{q^</em>}\le e^{\epsilon }$.</p><p>And we want to know the number of people whose data is $x_i$ and we only observe $y\in\{y_1,y_2,…y_n\}$ where $y_i$ the number of people holding data $x_i$ and is counted based on the collected samples. </p><h3 id="Utility"><a href="#Utility" class="headerlink" title="Utility"></a>Utility</h3><h4 id="Estimator"><a href="#Estimator" class="headerlink" title="Estimator"></a>Estimator</h4><p>Assuming the numebr of participated users is $n$ and the true number of people holding data $x_i$ is $n_i$. For people holding $x_i$, his support to $y_i$ should be 1 and those holding other data contribute 0 to $y_i$. However, due to randomized operation, people having data $x_i$ contribute only $p^<em>\le1$ to $y_i$ while those not holding $x_i$ contribute $q^</em>\ge0$ to $y_i$. So for $n_i$ people who have $x_i$, the contribution to $y_i$ is $np<em>$ while for $(n-n_i)$ people who don’t have $x_i$, their contribution to $y_i$ is $(n-n_i)q^</em>$. So the $y_i$ can be :</p><script type="math/tex; mode=display">y_i=n_ip^*+(n-n_i)q^*</script><p>since $y_i$ is known, the estimator for $n_i$ is:</p><script type="math/tex; mode=display">\hat{n}_i=\frac{y_i-n*q^*}{p^* - q^*}</script><p>and this estimator is unbiased.</p><blockquote><p> To prove unbiased estimator, we need to prove $E(\hat n_i)=n_i$.</p><script type="math/tex; mode=display">E(\hat n_i)=E(\frac{y_i-n*q^*}{p^* - q^*})</script><p>except $y_i$, all variables are constant, so </p><script type="math/tex; mode=display">E(\hat n_i)=E(\frac{y_i-n*q^*}{p^* - q^*})=\frac{1}{p^*-q^*}(E(y_i)-n*q^*)</script><p>all we need to calculate is:</p><script type="math/tex; mode=display">E(y_i)=E(n_ip^*+(n-n_i)q^*)=n_ip^*+(n-n_i)q^*=n_i(p^*-q^*)+nq^*</script><p>So, based on all above, we have</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}E(\hat n_i)=&E(\frac{y_i-n*q^*}{p^* - q^*})\\=&\frac{1}{p^*-q^*}(E(y_i)-n*q^*)\\=&\frac{1}{p^*-q^*}(  n_ip^*+(n-n_i)q^*   -n*q^*)\\=&n^i\end{aligned}\end{equation}</script></blockquote><h4 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h4><p>The variance of the estimator is a valuable indicator of an LDP protocol’s accuracy.</p><script type="math/tex; mode=display">Var(\hat n_i)=\frac{nq^*(1-q^*)}{(p^*-q^*)^2}+\frac{n_i(1-p^*-q^*)}{p^*-q^*}</script><p>where $n_i$ is the true number of people who have data $x_i$</p><blockquote><p>For each user whose data is $x_i$, his sending data process is a Bernoulli process, sending data $x_i$ if $X_{Bernoulli}=1$ with probability $p^<em>$ and not sending $x_i$ if $X_{Bernoulli}=0$ with probability $1-p^</em>$. So his variance contributing to $n_i$ is $p<em>(1-p</em>)$. For users whose data is not $x_i$, it is also a Bernoulli process and the variance is $q^<em>(1-q^</em>)$. So $n_i$ users contribute $n_ip^<em>(1-p^</em>)$ and $(n-n_i)$ users contribute $(n-n_i)q^<em>(1-q^</em>)$</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}Var(\hat n_i)&=Var(\frac{y_i-n*q^*}{p^* - q^*})\\&=\frac{1}{(p^*-q^*)^2}Var(y_i)\\&=\frac{1}{(p^*-q^*)^2}(Var(user_1)+Var(user_2)+...+Var(user_n))\\&=\frac{nq^*(1-q^*)}{(p^*-q^*)^2}+\frac{n_i(1-p^*-q^*)}{p^*-q^*}\end{aligned}\end{equation}</script></blockquote><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 5.46.48 PM.png" alt="Screen Shot 2018-06-14 at 5.46.48 PM"></p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><ol><li><p><strong>DE</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 8.53.53 PM.png" alt="Screen Shot 2018-06-14 at 8.53.53 PM"></p></li><li><p><strong>HE</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 8.54.36 PM.png" alt="Screen Shot 2018-06-14 at 8.54.36 PM"></p></li><li><p><strong>UE</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 8.55.44 PM.png" alt="Screen Shot 2018-06-14 at 8.55.44 PM"></p></li><li><p><strong>LH</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 8.56.40 PM.png" alt="Screen Shot 2018-06-14 at 8.56.40 PM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 8.57.14 PM.png" alt="Screen Shot 2018-06-14 at 8.57.14 PM"></p></li></ol><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 12.17.33 PM.png" alt="Screen Shot 2018-06-14 at 12.17.33 PM"></p><ol><li><p><strong>DE</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 9.01.38 PM.png" alt="Screen Shot 2018-06-14 at 9.01.38 PM"></p></li><li><p><strong>SHE</strong></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-14 at 9.04.19 PM.png" alt="Screen Shot 2018-06-14 at 9.04.19 PM"></p></li><li><p><strong>OLH||BLH</strong></p><p>The estimator for $n_i$ is </p><script type="math/tex; mode=display">\hat n_i=\frac{y_i-n/d'}{p*-1/d'}</script><p>where $d’$ is the mapping domain size.</p></li></ol><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><h3 id="Metric-based-local-differential-privacy-2018-Mario-S-Alvim"><a href="#Metric-based-local-differential-privacy-2018-Mario-S-Alvim" class="headerlink" title="Metric-based local differential privacy[2018-Mario S. Alvim]"></a>Metric-based local differential privacy<a href="https://arxiv.org/pdf/1805.01456.pdf" target="_blank" rel="noopener">[2018-Mario S. Alvim]</a></h3><h3 id="Heavy-Hitter-Estimation-for-Set-Valued-Data-2016-Zhan"><a href="#Heavy-Hitter-Estimation-for-Set-Valued-Data-2016-Zhan" class="headerlink" title="Heavy Hitter Estimation for Set-Valued Data[2016-Zhan]"></a>Heavy Hitter Estimation for Set-Valued Data<a href="https://davidyinyang.weebly.com/uploads/9/8/6/2/9862052/ccs16-ldp.pdf" target="_blank" rel="noopener">[2016-Zhan]</a></h3><h4 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h4><blockquote><p>The basic LDP frequent oracle protocol enables the aggregator to estimate the frequency of any value. But when the domain of input values is large, finding the most frequent values, also known as the heavy hitters, by estimating the frequencies of all possible values, is computationally infeasible.</p><p>In this paper, they aims to solve the problem of heavy hitters over set-valued, where each user has a set of up to l items and they try to identify the top-k most frequent items among all users. There are $d$ items and $n$ users, and each uesr has $l$ items and try to find  top-$k$ frequent items, where $k\ll\ l$ and $k\ll d$. </p><p>And each user has his own data which we call a record and every record is a set of items.</p></blockquote><h4 id="Naive-solutions"><a href="#Naive-solutions" class="headerlink" title="Naive solutions"></a>Naive solutions</h4><h5 id="Rappor-for-set-valued-data"><a href="#Rappor-for-set-valued-data" class="headerlink" title="Rappor for set-valued data"></a>Rappor for set-valued data</h5><blockquote><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-12 at 5.46.13 PM.png" alt="Screen Shot 2018-06-12 at 5.46.13 PM"></p></blockquote><h5 id="Bassily-and-Smith-Method-for-set-valued-data"><a href="#Bassily-and-Smith-Method-for-set-valued-data" class="headerlink" title="Bassily and Smith Method for set-valued data"></a>Bassily and Smith Method for set-valued data</h5><blockquote><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-12 at 6.09.06 PM.png" alt="Screen Shot 2018-06-12 at 6.09.06 PM"></p><p>REMARKS:</p><p>For the $\frac{\epsilon}{l}$, because each user only have a record although it has many items, we want $\epsilon$ privacy for this record. And we run algorithm $l$ times. So for overall $\epsilon$ privacy, each run should have only $\epsilon / l$ privacy budget.</p></blockquote><h4 id="Two-Phase-Framework"><a href="#Two-Phase-Framework" class="headerlink" title="Two-Phase Framework"></a>Two-Phase Framework</h4><h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h5><blockquote><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-12 at 6.14.59 PM.png" alt="Screen Shot 2018-06-12 at 6.14.59 PM"></p></blockquote><h5 id="Phase-I"><a href="#Phase-I" class="headerlink" title="Phase I"></a>Phase I</h5><blockquote><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-13 at 10.56.36 AM.png" alt="Screen Shot 2018-06-13 at 10.56.36 AM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-13 at 10.58.24 AM.png" alt="Screen Shot 2018-06-13 at 10.58.24 AM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-13 at 10.59.18 AM.png" alt="Screen Shot 2018-06-13 at 10.59.18 AM"></p><p>Remarks:</p><ul><li></li></ul></blockquote><h5 id="Phase-II"><a href="#Phase-II" class="headerlink" title="Phase II"></a>Phase II</h5><blockquote></blockquote><h3 id="Multiple-Attributes-Mean-or-Frequencies-Estimation-2016-Nguyen-Code"><a href="#Multiple-Attributes-Mean-or-Frequencies-Estimation-2016-Nguyen-Code" class="headerlink" title="Multiple Attributes Mean or Frequencies Estimation [2016-Nguyen] [Code]"></a>Multiple Attributes Mean or Frequencies Estimation <a href="https://arxiv.org/pdf/1606.05053.pdf" target="_blank" rel="noopener">[2016-Nguyen]</a> <a href="https://github.com/pokhym/LDP" target="_blank" rel="noopener">[Code]</a></h3><blockquote><p>They propose the Harmony method, which can handle multiple numeric or categorical attributes. </p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-09 at 10.21.40 PM.png" alt="Screen Shot 2018-06-09 at 10.21.40 PM"></p><p>Remarks:</p><ol><li>Algorithm2 is [Harmony-for-numeric] algorithm.</li><li>Algorithm3 is [Bassily and Smith’s method]</li><li>Each user randomly selects one attribute to submit.</li></ol></blockquote><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><blockquote><p>Since the gradient is the numeric number, so the author user <strong>[Harmony-for-numeric]</strong> algorithm for randomized operation. But the error by this method is $\sqrt{dlog(d)/\epsilon}$, where $d$ is the number of attributes (the length of input vector $x$).</p><p>For logistic regression and SVM, they use min-batch gradient descent with the batch size $k$, reducing error to $\sqrt{dlog(d)/(\epsilon k)}$.</p><p>For linear regression, they introduce <strong>Dimension reduction</strong>, which introdcues error $\sqrt{rlog(r)/(\epsilon k)}$</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-11 at 11.28.05 AM.png" alt="Screen Shot 2018-06-11 at 11.28.05 AM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-11 at 11.29.58 AM.png" alt="Screen Shot 2018-06-11 at 11.29.58 AM"></p></blockquote><h3 id="Locations-Density-Estimation-2018-Kim"><a href="#Locations-Density-Estimation-2018-Kim" class="headerlink" title="Locations Density Estimation [2018-Kim]"></a>Locations Density Estimation <a href="Application of Local Differential Privacy to Collection of Indoor Positioning Data">[2018-Kim]</a></h3><p>A number of studies have recently been made on discrete distribution estimation in the local model, in which users obfuscate their personal data (e.g., location, response in a survey) by themselves and a data collector estimates a distribution of the original personal data from the obfuscated data.</p><p>The purpose of this research is to approximately compute the distribution of users in the indoor space. </p><h4 id="Pricacy-model"><a href="#Pricacy-model" class="headerlink" title="Pricacy model"></a>Pricacy model</h4><ol><li><p><strong>data representation</strong></p><p>There are $n$ locations $B=\{b_1,b_2,…,b_n\}$, and the location of a user is in the form of a $n$-bit array, denoted as $L$, where the bit corresponding to the user’s location is set to 1 while the others are set to 0. So the input space size is $n$, the output space size is $2^n$.</p></li><li><p><strong>RAPPOR</strong></p><p>The next step is to perturb L, where each bit in L is first perturbed by randomized response as follows:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 9.41.00 PM-8080101.png" alt="Screen Shot 2018-06-03 at 9.41.00 PM-8080101"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 9.42.49 PM.png" alt="Screen Shot 2018-06-03 at 9.42.49 PM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 9.44.09 PM.png" alt="Screen Shot 2018-06-03 at 9.44.09 PM"></p><p>The instantaneous randomized response, S, is transmitted to the data collector server.</p></li></ol><h4 id="Estimation"><a href="#Estimation" class="headerlink" title="Estimation"></a>Estimation</h4><h5 id="STATISTIC-BASED-APPROACH"><a href="#STATISTIC-BASED-APPROACH" class="headerlink" title="STATISTIC-BASED APPROACH"></a>STATISTIC-BASED APPROACH</h5><blockquote><ol><li><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 9.49.04 PM.png" alt="Screen Shot 2018-06-03 at 9.49.04 PM"></p></li><li><p>the number of the instantaneous randomized responses of which the i-th bit is expected to be set to 1, $num(S_i)$, is estimated as follows:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen%20Shot%202018-06-03%20at%209.55.59%20PM.png" alt="Screen Shot 2018-06-03 at 9.55.59 PM"></p><p>$num(U_i)$ is estimated as follows:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen%20Shot%202018-06-03%20at%209.57.20%20PM.png" alt="Screen Shot 2018-06-03 at 9.57.20 PM"></p><p>then the number of location i where users have been to can be calculated by combining above two formulas:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen%20Shot%202018-06-03%20at%2010.04.58%20PM.png" alt="Screen Shot 2018-06-03 at 10.04.58 PM"></p><p>Let $N_i$ denotes the total number of instantaneous randomized responses, S, of which the i-bit is set to 1 as well as which is received between $ts_{start}$ and $ts_{end}$. And $N_{total}$ denotes the total number of instantaneous randomized responses, S, that the data collection server received from the data contributors in the time interval between  $ts_{start}$ and $ts_{end}$. </p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.08.06 PM.png" alt="Screen Shot 2018-06-03 at 10.08.06 PM"></p></li><li><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.08.45 PM.png" alt="Screen Shot 2018-06-03 at 10.08.45 PM"></p></li></ol></blockquote><h5 id="EM-BASED-APPROACH"><a href="#EM-BASED-APPROACH" class="headerlink" title="EM-BASED APPROACH"></a>EM-BASED APPROACH</h5><blockquote><p>Here we intend to estimate  $P(L=x_i)$.</p><ol><li><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.13.26 PM.png" alt="Screen Shot 2018-06-03 at 10.13.26 PM"></p></li><li><p>Then we want to calculate $P(pos_r|L=x_y)$</p><p>​    </p><p>Given a $n$-bit array $L$, the probabilities that the k-th bit of the corresponding permanent randomized response, U, sets to 1 and 0 are respectively computed as follows:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.21.19 PM.png" alt="Screen Shot 2018-06-03 at 10.21.19 PM"></p><p>Then, Given $L_k=1$, the probabilities that the k-th bit of the instantaneous randomized response, S, sets to 1 and 0 are respectively computed as follows:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.22.12 PM.png" alt="Screen Shot 2018-06-03 at 10.22.12 PM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.22.22 PM.png" alt="Screen Shot 2018-06-03 at 10.22.22 PM"></p></li></ol><p>   <img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.26.21 PM.png" alt="Screen Shot 2018-06-03 at 10.26.21 PM"></p><ol><li><p>The EM algorithm that computes $P(L =  x_i)$, 1 ≤ i ≤ n proceeds as follows:</p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.27.39 PM.png" alt="Screen Shot 2018-06-03 at 10.27.39 PM"></p><p><img src="/2018/06/02/DP-Application - Local-Privacy/Screen Shot 2018-06-03 at 10.28.02 PM.png" alt="Screen Shot 2018-06-03 at 10.28.02 PM"></p></li></ol></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p> [2018-Kim] Application of Local Differential Privacy to Collection of Indoor Positioning Data</p><p>[2016-Peter]  Discrete Distribution Estimation under Local Privacy</p><p>[2015-Bassily] Local, Private, Efficient Protocols for Succinct Histograms</p><p>[2016-Chen] Private Spatial Data Aggregation in the Local Setting</p><p>[2017-Wang] Optimizing Locally Differentially Private Protocols</p><p>[2016-Nguyen] Collecting and Analyzing Data from Smart Device Users with Local Differential Privacy</p><p>[2016-Zhan] Heavy Hitter Estimation over Set-Valued Data with Local Differential Privacy</p><p><a href="https://pdfs.semanticscholar.org/presentation/e610/b59b829bec396eab4b49fa748fd33b88d5c2.pdf" target="_blank" rel="noopener">[2017-PPT]</a> Local differential privacy</p><p>[2018-Mario S. Alvim] Metric-based local differential privacy for statistical applications</p><p>​          </p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Mechanism of Differential Privacy</title>
      <link href="/2018/05/30/DP-Mechanism/"/>
      <url>/2018/05/30/DP-Mechanism/</url>
      <content type="html"><![CDATA[<h1 id="Laplace-Distribution"><a href="#Laplace-Distribution" class="headerlink" title="Laplace Distribution"></a>Laplace Distribution</h1><p>The following details are from <a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83" target="_blank" rel="noopener">Wikipedia</a>. And the python version is <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.laplace.html" target="_blank" rel="noopener">here</a></p><h2 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h2><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-05-30 at 6.17.03 PM.png" alt="Screen Shot 2018-05-30 at 6.17.03 PM"></p><p><img src="/2018/05/30/DP-Mechanism/Laplace_distribution_pdf-7722463.png" alt="Laplace_distribution_pdf-7722463"></p><blockquote><ol><li>概率密度函数反映了概率在$ x$点处的密集程度。</li><li>x轴表示每一种取值，而y轴表示概率。</li><li>the variance of this distribution is $\sigma^2=2b^2$, while its scale is $b$.</li></ol></blockquote><h2 id="CDF"><a href="#CDF" class="headerlink" title="CDF"></a>CDF</h2><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-05-30 at 6.18.29 PM.png" alt="Screen Shot 2018-05-30 at 6.18.29 PM"></p><p><img src="/2018/05/30/DP-Mechanism/Laplace_distribution_cdf.png" alt="Laplace_distribution_cdf"></p><blockquote><ol><li>CDF表示连续累积概率，即$F(0)=P(x\le{0})$，即所有非正值的概率和。</li><li>PDF 表示某一点取值的概率$f(0)=P(x=0)=0.5$</li></ol></blockquote><h1 id="Sensitivity"><a href="#Sensitivity" class="headerlink" title="Sensitivity"></a>Sensitivity</h1><h2 id="L1-sensitivity"><a href="#L1-sensitivity" class="headerlink" title="L1-sensitivity"></a>L1-sensitivity</h2><blockquote><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-06 at 3.41.25 PM.png" alt="Screen Shot 2018-06-06 at 3.41.25 PM"></p><p><strong>Remarks</strong>:</p><ol><li>$N^{|X|}$ 表示数据库的大小是$|X|$, 即有$X$条记录；而$\mathbb{R}$表示real number，$\mathbb{R}^{k}$则表示$k$维空间，即数据是$k$维的，$k$维向量。</li><li></li></ol></blockquote><h1 id="Laplace-Mechanism"><a href="#Laplace-Mechanism" class="headerlink" title="Laplace Mechanism"></a><a href="https://link.springer.com/content/pdf/10.1007%2F11681878_14.pdf" target="_blank" rel="noopener">Laplace Mechanism</a></h1><p>TO DO QUESTION??</p><ol><li><p>WHY LAPLACE DISTRIBUTION????</p></li><li><p>WHY SENSITIVITY IN THE SCALE????</p></li></ol><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><blockquote><p>the Laplace mechanism will simply compute $f$, and perturb each coordinate with noise drawn from the Laplace distribution.</p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-06 at 3.48.56 PM.png" alt="Screen Shot 2018-06-06 at 3.48.56 PM"></p><p>About the proof, the key is that :</p><ol><li><p>neighbourhood database:</p><p> $x\in{N^{|X|}}$ and $y\in{N^{|X|}}$, such that $||x-y||_1=\le1$. </p></li><li><p>noise result:</p><p>$\frac{P(f(x)=z)}{P(f(y)=z)}\le{e^{\epsilon}}$</p></li></ol><p>Remarks:</p><ol><li><p>Let $\hat{f(x)}$ be the output of Laplace mechanism with zero mean given an input $x$. Then, for ant $x$, $\mathbb{E}[\hat{f(x)}]=f(x)$, where $f(x)$ is true query result without any perturbation.</p><p>$\mathbb{E}[\hat{f(x)}]=\int{P(x)}\hat{f(x)}dx=\int{P(x)(f(x)+n(x))}dx=f(x)\int{P(x)}dx+\int{P(x)n(x)}dx$</p><p>Obviously, $\int{P(x)}dx=1$, since the distribution of $n$ has zero mean so it is symmetric. For any $Pr(x)$, there are always two variables $n_1$ and $n_2$ such that $n_2=-n_1$, resulting in $P(n_1)n_1+P(n_2)n_2=0$. So $\int{P(x)n(x)}dx=0$.</p><p>SO $\mathbb{E}[\hat{f(x)}]=f(x)$</p></li><li><p>The error introduced by $Lap(0,\frac 1 \epsilon) \ is\  \frac 1 \epsilon​$</p><p>Assume $X$ is true answer and $Y=X+n$ is the noise output. $Error^2=E((Y-X)^2)=E(n^2)=E((n-E(n))^2)=E((n-0)^2)=Var(n)$</p><p>so the variance of noise distribution is the error, which is $\sqrt 2 b=\sqrt 2 \frac{1}{\epsilon}$ </p></li></ol></blockquote><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><h3 id="Divisibility-of-Laplace-Distribution"><a href="#Divisibility-of-Laplace-Distribution" class="headerlink" title="Divisibility of Laplace Distribution"></a><a href="https://link.springer.com/content/pdf/10.1007%2Fs00450-016-0310-y.pdf" target="_blank" rel="noopener">Divisibility of Laplace Distribution</a></h3><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-25 at 9.46.44 PM.png" alt="Screen Shot 2018-06-25 at 9.46.44 PM"></p><h2 id="Python-Implement"><a href="#Python-Implement" class="headerlink" title="Python Implement"></a>Python Implement</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">noise</span><span class="params">()</span>:</span></div><div class="line">    loc = <span class="number">0</span></div><div class="line">    scale = <span class="number">1</span></div><div class="line">    noise = np.random.laplace(loc,scale,<span class="number">1</span>) <span class="comment">#返回一个list</span></div><div class="line">    <span class="keyword">return</span> noise[<span class="number">0</span>]</div></pre></td></tr></table></figure><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><blockquote><p>We sometimes refer to the problem of responding to large numbers of (possibly arbitrary) queries as the query release problem.</p><ol><li><p>Counting Queries</p><p>Since the sensitivity of a counting query is 1 (the addition or deletion of a single individual can change a count by at most 1), so $(\epsilon,0)$-differential privacy can be achieved for counting queries by the addition of noise scaled to $\frac{1}{\epsilon}$, that is, by adding noise drawn from Lap($\frac{1}{\epsilon}$). The expected distortion, or error, is $\frac{1}{\epsilon}$, independent of the size of the database.</p><p>A fixed but arbitrary list of m counting queries can be viewed as a vector-valued query. Absent any further information about the set of queries a worst-case bound on the sensitivity of this vector-valued query is m, as a single individual might change every count. In this case $(\epsilon,0)$-differential privacy can be achieved by adding noise scaled to $\frac{m}{\epsilon}$ to the true answer to each query.</p></li><li><p>Histogram Queries</p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-06 at 4.04.09 PM.png" alt="Screen Shot 2018-06-06 at 4.04.09 PM"></p></li></ol></blockquote><h2 id="Utility"><a href="#Utility" class="headerlink" title="Utility"></a>Utility</h2><blockquote><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-06 at 4.05.35 PM.png" alt="Screen Shot 2018-06-06 at 4.05.35 PM"></p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-06 at 4.12.18 PM.png" alt="Screen Shot 2018-06-06 at 4.12.18 PM"></p><p>Remarks:</p><ol><li>Theorem3.8中，Pr[]里面，左边表示Laplace mechanism产生的error，右边中的两项，$t=In(\frac{k}{\delta})$，$k$是query result的维度，$b=\frac{\Delta{f}}{\epsilon}$。</li><li><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-06 at 4.18.19 PM.png" alt="Screen Shot 2018-06-06 at 4.18.19 PM"></li></ol></blockquote><h1 id="Exponential-mechanism"><a href="#Exponential-mechanism" class="headerlink" title="Exponential mechanism"></a>Exponential mechanism</h1><p>The exponential mechanism was designed for situations in which we wish to choose the “best” response but adding noise directly to the computed quantity can completely destroy its value, such as setting a price in an auction, where the goal is to maximize revenue, and adding a small amount of positive noise to the optimal price (in order to protect the privacy of a bid) could dramatically reduce the resulting revenue.</p><h2 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h2><p>The sensitivity of score function $q$ tells us the maximum change in the scoring function for any pair of datasets $d$ and $d’$ such that $|d\otimes d’|=1$:</p><script type="math/tex; mode=display">\Delta q = max_{r,d,d' where |d\otimes d'|=1 }|q(d,r)-q(d',r)|</script><blockquote><p>Say we have a discrete candidate output in a range $R$, and assume that the probability distribution of output is $u$, commonly uniform. The general mechanism is to design a query function $q:D^n(input)\times{R(output)}\to{\mathbb{R}}$, which assign a real valued score to any pair $(d,r)$ from $D^n\times{R}$. With a prior distribution of candidate output $u$, we amplify the probability associated with each output by a factor of $e^{\epsilon{q(d,r)}}$:</p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-07 at 12.43.02 PM.png" alt="Screen Shot 2018-06-07 at 12.43.02 PM"></p><script type="math/tex; mode=display">\Delta{q}=max_{r\in{R}}\ max_{x,y:||x-y||_1\le{1}}|q(x,r)-q(y-r)|</script><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-07 at 12.47.15 PM.png" alt="Screen Shot 2018-06-07 at 12.47.15 PM"></p></blockquote><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-08 at 4.21.27 PM.png" alt="Screen Shot 2018-06-08 at 4.21.27 PM"></p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-08 at 4.21.48 PM.png" alt="Screen Shot 2018-06-08 at 4.21.48 PM"></p><blockquote><p>Remarks:</p><ol><li><p>by amplifying the probability, the sum of all output probability will not equal 1. So in order to bound the probability, we need a normalization term.</p></li><li><p>from the denifition, if we set the probability factor as $\epsilon$, then it will provide us $2\epsilon \Delta q$, which amptifies with a factor $2\Delta q$. SO if we want $\epsilon$-DP, then the probability factor should be $\frac{\epsilon}{2\Delta q}$</p></li><li><p>the exponential mechanism defines a distribution over the output domain and samples from this distribution.</p></li><li><p>now we want to know how a small change in the database can affect the output. Say we have database $d$ and $d’$, where $||d-d’||=1$. And we assume $q(d,r)=q(d’,r)+\Delta{q}$. we want to bound the probability ratio $\frac{P(r|d)}{P(r|d’)}$</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\frac{P(r|d)}{P(r|d')}&=\frac{e^{\epsilon{q(d,r)}}}{\int{e^{\epsilon{q(d,r)}}}}\div\frac{e^{\epsilon{q(d',r)}}}{\int{e^{\epsilon{q(d',r)}}}}\\&=\frac{e^{\epsilon{q(d,r)}}}{e^{\epsilon{q(d',r)}}}\times\frac{\int{e^{\epsilon{q(d',r)}}}}{\int{e^{\epsilon{q(d,r)}}}}\\for\ the\ left\ one,\ we\ replace\ q(d,r)=q(d',r)+\Delta{q} ,\\ for\ right\ since\ q(d',r)= q(d',r)-q(d,r)+q(d,r)\\&={\frac{e^{\epsilon{(q(d',r)+\Delta{q})}}}{e^{\epsilon{q(d',r)}}}}\times\frac{\int{e^{\epsilon{(q(d',r)}-\epsilon{(q(d,r)+\epsilon{(q(d,r)}})}}}{\int{e^{\epsilon{q(d,r)}}}}\\since\ |q(d',r)-q(d,r)|\le \Delta q,\ so\ we\ have \\&\le  e^{\epsilon \Delta q} \times \int{e^{\epsilon \Delta q}}\\&=e^{\epsilon \Delta q}\times{e^{\epsilon{\Delta{q}}}}\\&=e^{2\epsilon \Delta q}\end{aligned}\end{equation}</script></li></ol></blockquote><h2 id="Utility-1"><a href="#Utility-1" class="headerlink" title="Utility"></a>Utility</h2><p>The exponential mechanism can often give strong utility guarantees, because it discounts outcomes exponentially quickly as their quality score falls off.</p><p>Let $OPT_{q}(d)=max_{r\in R}q(d,r)$ denote the maximum score of any candidate output $r\in R$ with respect to database $d$. </p><p>Fixing a database $d$, let $R_{OPT}=\{r\in R:q(d,r)=OPT_{q}(x)\}$ denote the set of candidates in $R$ which attain maximum utility score $OPT_q(x)$. Then:</p><p><strong>Theorem</strong>:</p><script type="math/tex; mode=display">Pr[q(M_{E(d,q,R)}\le OPT_q(d)-\frac{2\Delta q}{\epsilon}(ln(\frac{|R|}{|R_{OPT}|})+t))]\le e^{-t}</script><p><strong>Corollary</strong>:</p><script type="math/tex; mode=display">Pr[q(M_{E(d,q,R)}\le OPT_q(d)-\frac{2\Delta q}{\epsilon}(ln(|R|)+t))]\le e^{-t}</script><blockquote><p>Remarks:</p><ol><li>We bound the probability that the exponential mechanism returns a “good” output of $R$, where good is measured in terms of $OPT_q(d)$.  The result is that it will be highly unlikely that the returned output $r$ has a utility score that is inferior to $OPT_q(x)$ by more than an additive factoc of $O(\frac{\Delta q}{\epsilon}log|R|)$</li><li>from the utility theorem, we can find that utility of the mechanism only depends on log(|Outputs|), leading to the fact that sampling an output may not be computationally efficient if output space is large. </li><li><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-08 at 11.08.54 AM.png" alt="Screen Shot 2018-06-08 at 11.08.54 AM"></li><li><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-08 at 3.37.36 PM.png" alt="Screen Shot 2018-06-08 at 3.37.36 PM"></li><li></li></ol></blockquote><h2 id="Laplace-vs-Exponential"><a href="#Laplace-vs-Exponential" class="headerlink" title="Laplace vs Exponential"></a><a href="http://www.cs.bu.edu/~goldbe/teaching/HW55812/exponential.pdf" target="_blank" rel="noopener">Laplace vs Exponential</a></h2><p>Exponential mechanism is an instance of the exponential mechanism.</p><ol><li><p><strong>candidate output domain</strong></p><p>Say we have the output domian $r\in R$, and the query function is $f$ which works over the database $d$ and $d’$ such that $|d\otimes{d’}|=1$.</p></li><li><p><strong>utility function</strong></p><script type="math/tex; mode=display">q(d,r)=-|f(d)-r|</script><p>To see how this works, notice first that with this scoring function, the exponential mechanism becomes:</p><p>$M(d,q,\epsilon)$=output $r$ with probability proportional to $e^{\epsilon q(d,r)}$</p><p>which provides $2\epsilon \Delta q$-DP.</p></li><li><p><strong>sensitivity</strong></p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\Delta q &=max|q(d,r)-q(d',r)|\\&=max||f(d')-r|-|f(d,r)||\\&\le max|f(d',r)-f(d,r)| \ \ \ \ \ \ \ \ \ \ \ \ \ \ (triangle inequlity)\\\end{aligned}\end{equation}</script><blockquote><p>So based on above details, for query function with sensitivity 1 and with privacy parameter $\epsilon$, exponential mechanism provides $2\epsilon$-DP, while Laplace mechanism provides $\epsilon$-DP. This is because we’ve used the general analysis of the exponential mechanism, so our result is less “tight”.</p><p>This means that to provide the same privacy level $\epsilon$, laplace mechanism is $\epsilon-$DP while exponential mechanism is  $\frac{\epsilon}{2}$-DP, meaning more noise.</p></blockquote></li></ol><h2 id="Examples-1"><a href="#Examples-1" class="headerlink" title="Examples"></a>Examples</h2><p><strong>The median mechanism</strong></p><p>Recall that the median of a list numbers $d=\{1,2,4,5,6\}$ is 4; so we write $med(d)=4$</p><ol><li><p><strong>candidate ooutputs</strong></p><p>every element in $d$ is the possilble output of the median query.</p></li><li><p><strong>utility function</strong></p><p>For the median example, our scoring function will be:</p><p>$q(d,r)=-min|d\otimes d’|$ such that $med(d’)=r$</p><blockquote><p>What does it mean? The idea is that we take in a database $d$ and a candidate median $r$, and we look for a $d’$ that is as similar as possible to $d$, such that $med(d’)=r$.</p><p>For example, $d=\{1,2,3,4,5\}$ and candidate output $r=4$. What we want to do is to change $d$ to $d’$ so that  $med(d’)=4$. To do so, we can remove $1$ and $2$ from the $d$ to get $d’=\{3,4,5\}$. so $q(\{1,2,3,4,5\},4)=-2$. If $r=3$, $q(\{1,2,3,4,5\},4)=0$ becasue $3$ is the true median and we needn’t change anything about database $d$. </p></blockquote></li><li><p><strong>sensitivity</strong> </p><p>We can get $|q(d,r)-q(d’,r)|\le 1$. It is obvious because the input database is added or removed one element, for the unchanged database, the change you make to make $r$ median is $x$ step.  Now we have already help you make a step. So the best situation is you only need to maek (x-1) step for x as median. The utility change is 1. While the worst situation is the addition or removal in the original database don’t help you at all, so you still need $x$ step. So the utility change is 0.</p><p><strong>There is the mathametical proof.</strong></p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-09 at 12.02.40 PM.png" alt="Screen Shot 2018-06-09 at 12.02.40 PM"></p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-09 at 12.02.49 PM.png" alt="Screen Shot 2018-06-09 at 12.02.49 PM"></p></li></ol><h1 id="Composition-Theorems"><a href="#Composition-Theorems" class="headerlink" title="Composition Theorems"></a>Composition Theorems</h1><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2009/06/sigmod115-mcsherry.pdf" target="_blank" rel="noopener">[2009-McSherry]</a> Any approach to privacy must address issues of composition: that several outputs may be taken together, and should still provide privacy guarantees even when subjected to joint analysis</p><p><a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf" target="_blank" rel="noopener">[2014-Dwork]</a> The combination of two differentially private algorithms is differentially private itself. But the parameters $\epsilon$ and $\delta$ will necessarily degrade —- consider repeatedly computing the same statistic using the Laplace mechanism. The average of the answer given by each instance of the mechanism will eventually converge to the true value of the statistic, and so we cannot avoid that the strength of our privacy guarantee will degrade with repeated use.</p><h2 id="Sequential-composition"><a href="#Sequential-composition" class="headerlink" title="Sequential composition"></a>Sequential composition</h2><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-11 at 9.55.46 AM.png" alt="Screen Shot 2018-06-11 at 9.55.46 AM"></p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-11 at 10.31.41 AM.png" alt="Screen Shot 2018-06-11 at 10.31.41 AM"></p><blockquote><p>Proof:</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}Pr(M(X)=O)&=\prod_{i=1}^{m}Pr(M_i(X)=O_i) \\&\le \prod_{i=1}^{m} e^{\epsilon_i}Pr(M_i{(X')=O_i})\\&=e^{\sum_{i=1}^{m}\epsilon_i}\prod_{i=1}^{m} Pr(M_i(X')=O_i)\\&=e^{\sum_{i=1}^{m}\epsilon_i} Pr(M_i(X)=O)\end{aligned}\end{equation}</script><p>x</p></blockquote><h2 id="Parallel-composition"><a href="#Parallel-composition" class="headerlink" title="Parallel composition"></a>Parallel composition</h2><p>While general sequences of queries accumulate privacy costs additively, when the queries are applied to disjoint subsets of the data we can improve the bound. Specifically, if the domain of input records is partitioned into disjoint sets, independent of the actual data, and the restrictions of the input data to each part are subjected to differentially-private analysis, the ultimate privacy guarantee depends only on the worst of the guarantees of each analysis, not the sum.</p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-11 at 9.58.34 AM.png" alt="Screen Shot 2018-06-11 at 9.58.34 AM"></p><h2 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL-Divergence"></a>KL-Divergence</h2><blockquote><p>又叫相对熵，是衡量分布间距离的一个度量。</p></blockquote><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><blockquote><p>设$P(i)​$和$Q(i)​$是$X​$取值的两个概率分布，则$P​$相对$Q​$的KL-Divergence是：</p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-06-05 at 10.10.43 AM.png" alt="Screen Shot 2018-06-05 at 10.10.43 AM"></p></blockquote><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><blockquote><ol><li><p>非对称性</p><script type="math/tex; mode=display">D(p||q)\ne{D(q||p)}</script></li><li><p>非负性</p><script type="math/tex; mode=display">D(p||q)\ge0</script></li></ol></blockquote><p><a href="http://www.cis.upenn.edu/~aaroth/courses/slides/Lecture4.pdf" target="_blank" rel="noopener">[2011-Aaron]</a></p><p><img src="/2018/05/30/DP-Mechanism/Screen Shot 2018-05-31 at 3.10.30 PM.png" alt="Screen Shot 2018-05-31 at 3.10.30 PM"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[2014-Dwork] The Algorithmic Foundations of Differential Privacy</p><p>[2006-Dwork] Calibrating Noise to Sensitivity in Private Data Analysis</p><p>[2009-McSherry] Privacy Integrated Queries</p><p>[2016-Gunther Eibl] Differential privacy for real smart metering data</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Statistic 110</title>
      <link href="/2018/05/30/Statistic-110/"/>
      <url>/2018/05/30/Statistic-110/</url>
      <content type="html"><![CDATA[<p>This post is about my learning process of probability and statistics.</p><a id="more"></a><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/unit-i/" target="_blank" rel="noopener">Probabilistic Systems Analysis and Applied Probability</a></p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/unit-iii/lecture-13/" target="_blank" rel="noopener">last time</a></p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/index.htm" target="_blank" rel="noopener">[Fundamentals of Probability]</a></p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/index.htm" target="_blank" rel="noopener">[Discrete Stochastic Processes]</a></p><p><strong>Mathematics for Computer Science</strong></p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/index.htm" target="_blank" rel="noopener">open course</a> </p><p><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2005/assignments/" target="_blank" rel="noopener">homework with solutions</a> </p><h1 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h1><p>This chapter is based on the book “Mathematics for Computer Science”. </p><h2 id="Event-and-Probability"><a href="#Event-and-Probability" class="headerlink" title="Event and Probability"></a>Event and Probability</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>A countable sample space $S$ is a nonempty countable set. An element $w \in S$ is called an outcome. A subset of $S$ is called an event.</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-28 at 2.09.35 PM.png" alt="creen Shot 2019-03-28 at 2.09.35 P"></p><h2 id="The-Four-Step-Method"><a href="#The-Four-Step-Method" class="headerlink" title="The Four Step Method"></a>The Four Step Method</h2><p>In order to understand what event and probability are, we are going to introduce a problem and solve with a four-step method.</p><blockquote><p>Suppose you’re on a game show, and you’re given the choice of three doors. Behind one door is a car, behind the others, goats. You pick a door, say number 1, and the host, who knows what’s behind the doors, opens another door, say number 3, which has a goat. He says to you, “Do you want to pick door number 2?” Is it to your advantage to switch your choice of doors?</p></blockquote><p>In order to answer this question, we want to know whether we can increase the possibility of winning by switching the door. Therefore, our goal becomes probability computation and The-Four-Step-Method is commonly used to perform this task.</p><h3 id="Step-1-Find-the-Sample-Space"><a href="#Step-1-Find-the-Sample-Space" class="headerlink" title="Step 1: Find the Sample Space"></a>Step 1: Find the Sample Space</h3><p>In this step, our goal is to determine the all the possible outcomes of the experiments. A typical experiment usually involves several steps. For example, in our problem setting, our experiment has the following steps:</p><ol><li>Put a car in a random door.</li><li>Pick a door.</li><li>Open another door with a goat behind it. </li></ol><p>Every possible combination of these randomly-determined quantities is called an<br><strong>outcome</strong>. The set of all possible outcomes is called the <strong>sample space</strong> for the experiment.</p><p>Considering the small sample space in this experiment, we are going to use a tree structure to represent the whole space. </p><p>Specifically, for the first step: put a car in a random door. Since there are three available door, so we have three possibilities:</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.04.53 AM.png" alt="creen Shot 2019-03-26 at 11.04.53 A"></p><p>Here, A, B and C are door names.</p><p>In the second step, for each car possible location, a user have three possible way to choose a door which he thinks the car is behind:</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.09.03 AM.png" alt="creen Shot 2019-03-26 at 11.09.03 A"></p><p>In the final step, we try to reveal a door with a goat behind it. For example, say the car is behind door A and the user choose the door A, then we can reveal either of the doors, i.e., B or C, since goats are both behind these two doors. But if the car is in location A and the user choose the door B, then we can only reveal the door C. The full tree structure is as following:</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.15.37 AM.png" alt="creen Shot 2019-03-26 at 11.15.37 A"></p><p>Now let’s relate this picture to the terms we introduced earlier: the leaves of the<br>tree represent outcomes of the experiment, and the set of all leaves represents the<br>sample space.</p><p>Thus in this experiment, there are 12 outcomes. If we represent each outcome in the format: (door with car, door chosen, door opened with a goat behind), we have the sample space :</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.18.44 AM.png" alt="creen Shot 2019-03-26 at 11.18.44 A"></p><h3 id="Step-3-Define-Events-of-Interest"><a href="#Step-3-Define-Events-of-Interest" class="headerlink" title="Step 3: Define Events of Interest"></a>Step 3: Define Events of Interest</h3><p>A set of outcomes is called an <strong>event</strong>. Each of the preceding phrases characterizes<br>an event. For example, the event [prize is behind door C] refers to the set:</p><script type="math/tex; mode=display">\{(C,A,B),(C,B,A),(C,C,A),(C,C,B)\}</script><p>and the event [prize is behind the door first picked by the player] is:</p><script type="math/tex; mode=display">\{(A,A,B),(A,A,C),(B,B,A),(B,B,C),(C,C,A),(C,C,B)\}</script><p>Remember our goal is the probability of wining if the user switches the door, so the door with prize is different from the door opened, i.e., the 1st element is different from the 3rd element of an outcome; and since we are going to switch the door, so the door chosen is differnt the door opened, i.e., the three elements of an outcome should be different. Therefore, this event can be represented by:</p><script type="math/tex; mode=display">\{(A,B,C),(A,C,B),(B,A,C),(B,C,A),(C,A,B),(C,B,A)\}</script><p>We can mark the outcomes in the tree diagram:</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.36.57 AM.png" alt="creen Shot 2019-03-26 at 11.36.57 A"></p><h3 id="Step-3-Determine-Outcome-Probabilities"><a href="#Step-3-Determine-Outcome-Probabilities" class="headerlink" title="Step 3: Determine Outcome Probabilities"></a>Step 3: Determine Outcome Probabilities</h3><p>The goal of this step is to assign each outcome a probability, indicating the fraction of the time this outcome is expected to occur. And the sum of all the outcome probabilities must equal one, reflecting the fact that there must be an outcome. We are going to break the task of determining outcome probabilities into two stages.</p><ol><li><p>Assign Edge Probabilities</p><p>For example, for the car location field, probability of car behind door A is $\frac{1}{3}$, so we assign $\frac{1}{3}$ to that edge. Then for the player’s initial guess, the probability of each choice is also $\frac{1}{3}$. Say the player choose door A, then for door revealed filed, each possibility is $\frac{1}{2}$. But if the player choose the door B, then we can only reveal door C, so the probability is 1.</p></li><li><p>Compute Outcome Probabilities</p><p>For each outcome, we multiply the edge probability and get the probability of this outcome.</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.51.26 AM.png" alt="creen Shot 2019-03-26 at 11.51.26 A"></p></li></ol><h3 id="Step-4-Compute-Event-Probabilities"><a href="#Step-4-Compute-Event-Probabilities" class="headerlink" title="Step 4: Compute Event Probabilities"></a>Step 4: Compute Event Probabilities</h3><p>Now we have the probability of each outcome, so we can compute an event probability by adding all the outcome probability of that event.</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-03-26 at 11.50.15 AM.png" alt="creen Shot 2019-03-26 at 11.50.15 A"></p><p>It means that we have the $\frac{2}{3}$ probability to win if we switch the doors.</p><h1 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h1><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-02 at 1.08.45 PM.png" alt="creen Shot 2019-04-02 at 1.08.45 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-02 at 1.09.54 PM.png" alt="creen Shot 2019-04-02 at 1.09.54 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-02 at 1.10.16 PM.png" alt="creen Shot 2019-04-02 at 1.10.16 P"></p><p><img src="/2018/05/30/Statistic-110/Screen%20Shot%202018-06-10%20at%2011.04.20%20PM.png" alt="Screen Shot 2018-06-10 at 11.04.20 PM"></p><p>Based on the pic, $P(AB)\ne P(A)P(B)$, so event A and event B are not independent event.</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-02 at 1.11.25 PM.png" alt="creen Shot 2019-04-02 at 1.11.25 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-13 at 9.26.43 PM.png" alt="Screen Shot 2018-06-13 at 9.26.43 PM"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-13 at 9.26.49 PM.png" alt="Screen Shot 2018-06-13 at 9.26.49 PM"></p><h1 id="Random-Variables"><a href="#Random-Variables" class="headerlink" title="Random Variables"></a>Random Variables</h1><p><strong>Definition</strong>  A random variable $R$ is a function, mapping the sample space to a number.</p><p><strong>Example</strong> </p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.00.15 PM.png" alt="creen Shot 2019-04-03 at 4.00.15 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.00.26 PM.png" alt="creen Shot 2019-04-03 at 4.00.26 P"></p><h2 id="Indicator-Random-Variables"><a href="#Indicator-Random-Variables" class="headerlink" title="Indicator Random Variables"></a>Indicator Random Variables</h2><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.02.40 PM.png" alt="creen Shot 2019-04-03 at 4.02.40 P"></p><h2 id="Distribution-Functions"><a href="#Distribution-Functions" class="headerlink" title="Distribution Functions"></a>Distribution Functions</h2><h3 id="Bernoulli-Distributions"><a href="#Bernoulli-Distributions" class="headerlink" title="Bernoulli Distributions"></a>Bernoulli Distributions</h3><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.14.35 PM.png" alt="creen Shot 2019-04-03 at 4.14.35 P"></p><h3 id="Uniform-Distributions"><a href="#Uniform-Distributions" class="headerlink" title="Uniform Distributions"></a>Uniform Distributions</h3><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.15.19 PM.png" alt="creen Shot 2019-04-03 at 4.15.19 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.15.29 PM.png" alt="creen Shot 2019-04-03 at 4.15.29 P"></p><h3 id="Binomial-Distributions"><a href="#Binomial-Distributions" class="headerlink" title="Binomial Distributions"></a>Binomial Distributions</h3><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.17.00 PM.png" alt="creen Shot 2019-04-03 at 4.17.00 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.17.48 PM.png" alt="creen Shot 2019-04-03 at 4.17.48 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.17.55 PM.png" alt="creen Shot 2019-04-03 at 4.17.55 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.19.24 PM.png" alt="creen Shot 2019-04-03 at 4.19.24 P"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.19.30 PM.png" alt="creen Shot 2019-04-03 at 4.19.30 P"></p><h2 id="Expectations"><a href="#Expectations" class="headerlink" title="Expectations"></a>Expectations</h2><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.22.10 PM.png" alt="creen Shot 2019-04-03 at 4.22.10 P"></p><h3 id="The-Expected-Value-of-a-Uniform-Random-Variable"><a href="#The-Expected-Value-of-a-Uniform-Random-Variable" class="headerlink" title="The Expected Value of a Uniform Random Variable"></a>The Expected Value of a Uniform Random Variable</h3><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.26.57 PM.png" alt="creen Shot 2019-04-03 at 4.26.57 P"></p><h3 id="The-Expected-Value-of-a-Reciprocal-Random-Variable"><a href="#The-Expected-Value-of-a-Reciprocal-Random-Variable" class="headerlink" title="The Expected Value of a Reciprocal Random Variable"></a>The Expected Value of a Reciprocal Random Variable</h3><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.31.37 PM.png" alt="creen Shot 2019-04-03 at 4.31.37 P"></p><h3 id="The-Expected-Value-of-an-Indicator-Random-Variable"><a href="#The-Expected-Value-of-an-Indicator-Random-Variable" class="headerlink" title="The Expected Value of an Indicator Random Variable"></a>The Expected Value of an Indicator Random Variable</h3><p><img src="/2018/05/30/Statistic-110/Screen Shot 2019-04-03 at 4.32.15 PM.png" alt="creen Shot 2019-04-03 at 4.32.15 P"></p><h1 id="Discrete-Random-Variables"><a href="#Discrete-Random-Variables" class="headerlink" title="Discrete Random Variables"></a>Discrete Random Variables</h1><ol><li><p><strong>Probability mass function (PMF)</strong></p><p>it is the probability of a discrete random variable; while in continus setting, the probability of a random variable is called PDF.</p></li><li><p><strong>Expectation</strong></p><script type="math/tex; mode=display">E(X)=\sum_{x}xp_X(x)</script><blockquote><ul><li>Average in large number of repetitions the experiment</li></ul></blockquote></li><li><p><strong>Properties of expectations</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-11 at 5.23.20 PM.png" alt="Screen Shot 2018-06-11 at 5.23.20 PM"></p><blockquote><ul><li>Once x is determined, then y is determined; so the probability of y is the probability of x;</li></ul></blockquote><p>If $\alpha , \beta$ are constants, then:</p><script type="math/tex; mode=display">E(\alpha)=\alpha\\E(\alpha X)=\alpha E(X)\\E(\alpha X+\beta)=\alpha E(X)+\beta</script><p>But if A and B are two independent variables, then</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}E(AB)&=\sum_A\sum_B(ab)P_{A,B}(a,b)\\&=\sum_A\sum_B(ab)P_A(a)P_B(b)\\&=E(A)E(B)\end{aligned}\end{equation}</script></li><li><p><strong>Variance</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-11 at 5.32.18 PM.png" alt="Screen Shot 2018-06-11 at 5.32.18 PM"></p><blockquote><ul><li>since $X$ is random while $E(X)$ is a number, so $X-E(X)$ is a random variable.</li><li>variance measures the average square distance from the mean.</li><li>a big variance means the variable are far away from the center while a small one means the variables are tightly concentrated around the mean value.</li></ul></blockquote><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-13 at 7.09.01 PM.png" alt="Screen Shot 2018-06-13 at 7.09.01 PM"></p><blockquote><p>In the example, since $X=Y$, then $X$ and $Y$ are not independent, so ..</p></blockquote></li><li><p><strong>Standard deviation</strong></p><script type="math/tex; mode=display">\sigma_X=\sqrt{var(X)}</script></li><li><p><strong>Covariance</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-22 at 9.29.21 PM.png" alt="Screen Shot 2018-06-22 at 9.29.21 PM"></p></li><li><p><strong>Correlation coefficient</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-22 at 9.29.27 PM.png" alt="Screen Shot 2018-06-22 at 9.29.27 PM"></p></li></ol><h1 id="Binomial-Random-Variable"><a href="#Binomial-Random-Variable" class="headerlink" title="Binomial Random Variable"></a>Binomial Random Variable</h1><ol><li><p><strong>Definition</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-13 at 9.10.33 PM.png" alt="Screen Shot 2018-06-13 at 9.10.33 PM"></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-13 at 9.10.40 PM.png" alt="Screen Shot 2018-06-13 at 9.10.40 PM"></p></li><li><p><strong>Binomial mean and variance</strong></p><p>$X= # $ of successes in $n$ independent trials</p><p>$X_i=\left\{\begin{align} 1&amp; &amp; \text{if success in trial i}\\0&amp;&amp;\text{otherwise}\end{align}\right.$ </p><ul><li><p>$E(X_i)=p$</p><blockquote><p>$E(X_i)=p<em>1+(1-p)</em>0$</p></blockquote></li><li><p>$E(X)=np$</p><blockquote><p>$E(X)=E(x_1,x_2,…,x_n)=E(x_1)+E(x_2)+…+E(x_n)=np$</p></blockquote></li><li><p>$Var(X_i)=p-p^2$</p><blockquote><p>$Var(X_i)=E(X_i^2)-E(X_i)^2$</p><p>$X_i^2=\left\{\begin{align} 1&amp; &amp; \text{if success in trial i}\\0&amp;&amp;\text{otherwise}\end{align}\right.$</p><p>So $E(X_i^2)=1<em>p+0</em>(1-p)=p$</p><p>$Var(X_i)=E(X_i^2)-E(X_i)^2=p-p^2$</p></blockquote></li><li><p>$Var(X)=n(p-p^2)$</p><blockquote><p>$Var(X)=Var(X_i,X_2,…,X_n)$</p><p>because $X_i$ is independent to each other, so</p><p>$Var(X)=Var(X_i,X_2,…,X_n)=Var(X_1)+Var(X_2)+…+Var(X_n)=nVar(X_i)$</p></blockquote></li></ul></li><li><p><strong>Example-Hat Problem</strong></p><ul><li><p>$n$ people throw their hats in a box and then pick one at random.</p><p>Let $X$ be the number of people who get their own hat. We want to find out $E(X)$. This is Binomial distribution:</p><p>$X_i=\left\{\begin{align} 1&amp; &amp; \text{if i selects his hat}\\0&amp;&amp;\text{otherwise}\end{align}\right.$</p></li><li><p>$P(X_i)=\frac{1}{n}$</p></li><li><p>$E(X_i)=1<em>\frac{1}{n}+0</em>(1-\frac{1}{n})=\frac{1}{n}$</p></li><li><p>the $X_i$ are not independent</p></li><li><p>$E(X)=E(X_1)+E(X_2)+…+E(X_n)$ is always true no matter whether $X_i$ are independent or not.</p><p>$E(X)=E(X_1)+E(X_2)+…+E(X_n)=n*\frac{1}{n}=1$</p></li><li><p>$Var(X)=E(X^2)-E(X)^2=E(X^2)-1$</p><p>because $X$ is the number of people who has his hat, so $X=X_1+X_2+…+X_n$, then $X^2=\sum_i X_i^2+\sum_{i,j}X_iX_j$</p><p>$Var(X)=E(X^2)-1=nE(X_i^2)+n(n-1)E(X_iX_j)-1$</p><p>$E(X_i^2)=E(X_i)=\frac{1}{n}$</p><p>$P(X_iX_j=1)=P(X_i=1)P(X_J=1|X_i=1)=\frac{1}{n}\frac{1}{n-1}=E(X_iX_j)$</p><p>$Var(X)=E(X^2)-1=nE(X_i^2)+n(n-1)E(X_iX_j)-1=n\frac{1}{n}+n(n-1)\frac{1}{n}\frac{1}{n-1}-1=1$</p></li></ul></li></ol><h1 id="Continuous-Random-Variable"><a href="#Continuous-Random-Variable" class="headerlink" title="Continuous Random Variable"></a>Continuous Random Variable</h1><ol><li><p><strong>PDF (Probability Density Function)</strong> : </p><p>$X$ has PDF $f(x)$ if $P(a\le{X}\le{b})=\int_{a}^{b}f(x)$</p><blockquote><ul><li><p>According to this definition, the probability of one point is 0: $P(a\le{X}\le{a})=\int_{a}^{a}f(x)=0$</p></li><li><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 5.57.31 PM.png" alt="Screen Shot 2018-06-14 at 5.57.31 PM"></p><p>so pdf describes the the probability of intervals. </p></li><li><p>n</p></li></ul></blockquote></li><li><p><strong>Means and Variances</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 6.05.16 PM.png" alt="Screen Shot 2018-06-14 at 6.05.16 PM"></p></li><li><p><strong>continuous uniform r.v.</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 6.19.30 PM.png" alt="Screen Shot 2018-06-14 at 6.19.30 PM"></p><ul><li>$f(x)=\frac{1}{b-a},a\le x \le b$</li><li>$E(X)=\int_{a}^{b}x\frac{x}{b-a}=\frac{a+b}{2}$</li><li><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 6.21.59 PM.png" alt="Screen Shot 2018-06-14 at 6.21.59 PM"></li></ul></li><li><p><strong>Cumulative distribution function(CDF)</strong></p><p>if $X$ has PDF $f$, then the CDF is $F(X)=P(X\le{x})=\int_{-\infty}^{x}f(t)dt$</p></li><li><p><strong>Gaussian (normal) PDF</strong></p><ul><li><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 6.31.09 PM.png" alt="Screen Shot 2018-06-14 at 6.31.09 PM"></p><blockquote><p>The factor $\frac{1}{\sqrt{2\pi}}$ is to make the calculus equal 1.</p><p>$E(X)=1, Var(X)=1$</p></blockquote></li><li><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 6.37.15 PM.png" alt="Screen Shot 2018-06-14 at 6.37.15 PM"></p></li><li><p>Let $Y=aX+b$, then</p><p>$E(Y)=a\mu +b$, $Var(Y)=a^2 \sigma^2$</p><p>Fact: $Y\sim N(a\mu+b,a^2\sigma^2)$, which means linear functions of normal random variabls are themselevs normal.</p></li><li><p>If $X\sim N(\mu,\sigma^2)$, then $\frac{X-\mu}{\sigma}\sim N(0,1)$</p><p>so based on this transform, we can simplify our cdf calculation:</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-14 at 6.44.55 PM.png" alt="Screen Shot 2018-06-14 at 6.44.55 PM"></p></li></ul></li></ol><h1 id="Multiple-Continuous-Random-Variables"><a href="#Multiple-Continuous-Random-Variables" class="headerlink" title="Multiple Continuous Random Variables"></a>Multiple Continuous Random Variables</h1><h2 id="Joint-PDF"><a href="#Joint-PDF" class="headerlink" title="Joint PDF"></a>Joint PDF</h2><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-15 at 5.57.12 PM.png" alt="Screen Shot 2018-06-15 at 5.57.12 PM"></p><ul><li><p>From the joint to the marginal:</p><p>$f_X(x)=\int_{-\infin}^{\infin}f_{X,Y}(x,y)dy$</p></li><li><p>$X$ and $Y$ are called <strong>independent</strong> if:</p><p>$f_{X,Y}(x,y)=f_{X}(x)f_{Y}(y)$ </p></li></ul><h2 id="Buffon’s-needle"><a href="#Buffon’s-needle" class="headerlink" title="Buffon’s needle"></a>Buffon’s needle</h2><ol><li><p><strong>problem setting</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-15 at 6.10.37 PM.png" alt="Screen Shot 2018-06-15 at 6.10.37 PM"></p></li><li><p><strong>analysis</strong></p><p>$X$, $\Theta$ are uniform and independent to each other</p><p>$f_{X,\Theta}(x,\theta)=f_X(x)f_{\Theta}(\theta)=\frac{2}{d}\frac{2}{\pi}$, where $0\le x \le \frac{d}{2}$, $0\le \theta \le\frac{\pi}{2}$</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-15 at 6.15.39 PM.png" alt="Screen Shot 2018-06-15 at 6.15.39 PM"></p></li></ol><h2 id="conditioning"><a href="#conditioning" class="headerlink" title="conditioning"></a>conditioning</h2><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-15 at 6.21.18 PM.png" alt="Screen Shot 2018-06-15 at 6.21.18 PM"></p><h2 id="Stick-breaking-example"><a href="#Stick-breaking-example" class="headerlink" title="Stick-breaking example"></a>Stick-breaking example</h2><h3 id="problem-setting"><a href="#problem-setting" class="headerlink" title="problem setting"></a>problem setting</h3><p>Break a stick of length $l$ twice: break at $X$ uniformly in $[0,l]$; then break at $Y$ uniformly in $[0,X]$</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-15 at 6.26.23 PM.png" alt="Screen Shot 2018-06-15 at 6.26.23 PM"></p><h3 id="analysis"><a href="#analysis" class="headerlink" title="analysis"></a>analysis</h3><p>$f_{X,Y}(x,y)=f_{X}(x)f_{Y|X}(y|x)=\frac{1}{l}\frac{1}{x}$</p><p>$E(Y|X=x)=\int yf_{Y|X}(y|X=x)dy=\int_{0}^{x}y\frac{1}{x}dy=\frac{x}{2}$</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-15 at 6.35.34 PM.png" alt="Screen Shot 2018-06-15 at 6.35.34 PM"></p><h1 id="Derived-Distribution"><a href="#Derived-Distribution" class="headerlink" title="Derived Distribution"></a>Derived Distribution</h1><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ol><li><p>The discrete value bayes</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-21 at 6.52.10 PM.png" alt="Screen Shot 2018-06-21 at 6.52.10 PM"></p><p>The continuous counterpart</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-21 at 6.53.27 PM.png" alt="Screen Shot 2018-06-21 at 6.53.27 PM"></p><p>discrete $x$ and continuous $y$</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-21 at 6.58.50 PM.png" alt="Screen Shot 2018-06-21 at 6.58.50 PM"></p><p>continuous $x$ and discrete $y$</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-21 at 6.58.55 PM.png" alt="Screen Shot 2018-06-21 at 6.58.55 PM"></p></li></ol><h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><ol><li><p>Example One</p><p>Say $X : $ uniform on $[0,2]$. And we want to the PDF of $Y=X^3$.</p><blockquote><p>Firstly, we calculate the PMF of $Y$, $F_Y(y)$, where $y\in[0,8]$. According to the definition of PMF, </p><script type="math/tex; mode=display">F_Y(y)=P(Y\le y)</script><p>And $Y=X^3$, so we have:</p><script type="math/tex; mode=display">F_Y(y)=P(Y\le y)=P(X^3 \le y)=P(X\le y^{\frac{1}{3}})</script><p>Now we want to calculate the mass probability of $X\le y^{\frac{1}{3}}$. We know that $X$ is uniform, and according to the following pic,  $P(X\le y^{\frac{1}{3}})$ is the area of length=$ y^{\frac{1}{3}}$ and height=$\frac{1}{2}$</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-21 at 9.15.25 PM.png" alt="Screen Shot 2018-06-21 at 9.15.25 PM"></p><p>so we have</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-21 at 9.15.54 PM.png" alt="Screen Shot 2018-06-21 at 9.15.54 PM"></p></blockquote></li><li><p>Example Two</p><p>Calculate the PDF of $Y=aX+b$.</p><blockquote><script type="math/tex; mode=display">F_Y(y)=P(Y\le y)=P(aX+b \le y)=P(aX \le y-b)\\F_Y(y)=\left\{\begin{align} &P(X\le \frac{y-b}{a})  & \text{if a >=0 }\\&P(X\ge \frac{y-b}{a})=1- P(X\le \frac{y-b}{a})&\text{if a <=0}\end{align}\right.</script><p>Then the PDF is:</p><script type="math/tex; mode=display">F_Y(y)=\left\{\begin{align} &P(X\le \frac{y-b}{a})  & \text{if a >=0 }\\&P(X\ge \frac{y-b}{a})=1- P(X\le \frac{y-b}{a})&\text{if a <=0}\end{align}\right.</script><p>Because the $P(X\le x)=F_X(x)$,</p><script type="math/tex; mode=display">F_Y(y)=\left\{\begin{align} &F_X( \frac{y-b}{a})  & \text{if a >=0 }\\&1- F_X( \frac{y-b}{a}) &\text{if a <=0}\end{align}\right.</script><p>Then the PDF is </p><script type="math/tex; mode=display">f_Y(y)=\left\{\begin{align} &\frac{1}{a}f_X( \frac{y-b}{a})  & \text{if a >=0 }\\&- \frac{1}{a}f_X( \frac{y-b}{a}) &\text{if a <=0}\end{align}\right.</script><p>So overall,</p><script type="math/tex; mode=display">f_Y(y)=\frac{1}{|a|}f_X( \frac{y-b}{a})</script></blockquote></li></ol><h1 id="Bernoulli-process"><a href="#Bernoulli-process" class="headerlink" title="Bernoulli process"></a><a href="http://zhouyichu.com/randomized-algorithm/Randomized-Algorithms-2/" target="_blank" rel="noopener">Bernoulli process</a></h1><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-24 at 10.29.40 PM.png" alt="Screen Shot 2018-06-24 at 10.29.40 PM"></p><ol><li><p><strong>Definition</strong></p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-24 at 9.52.34 PM.png" alt="Screen Shot 2018-06-24 at 9.52.34 PM"></p></li><li><p><strong>Properties</strong></p><p>There is a random process and the sequence of random variables is $X_1, X_2,…$</p><script type="math/tex; mode=display">E(X_t)=1\times p+0\times (1-p)=p\\Var(X_t)=E(X_t^2)-E^2(X_t)=p-p^2</script><p>And for the infinite sequence and no matter what the variable value is, the probability of infinite sequence is :</p><script type="math/tex; mode=display">P=P(X_1)P(X_2)...P(X_\infin)=p^k (1-p)^g\\lim_{k\to\infin,g_\to \infin}p^k (1-p)^g=0\\P=0</script><p>Number of success $S$ in $n$ time slots</p><script type="math/tex; mode=display">P(S=k)=(_{k}^{n})p(1-p)\\E[S]=kp\\Var(S)=kp(1-p)</script><p>Let $T_1$ be the number of trials until first success:</p><blockquote><script type="math/tex; mode=display">P(T_1=t)=(1-p)^{(t-1)}p</script><p>Memoryless peoperty: what happens in the past has no effect on the future random provess</p><script type="math/tex; mode=display">E[T_1]=\frac{1}p{}\\Var(T_1)=\frac{1-p}{p^2}</script></blockquote></li></ol><h1 id="Log"><a href="#Log" class="headerlink" title="Log"></a>Log</h1><p>乘除 —&gt; 加减</p><ol><li>$log_a(MN)=log_a{M}+log_a{N}$</li><li>$log_a{\frac{M}{N}}=log_a{M}-log_a{N}$</li></ol><h1 id="Unbiased-estimator"><a href="#Unbiased-estimator" class="headerlink" title="Unbiased estimator"></a><a href="https://www.zhihu.com/question/22983179/answer/23470969" target="_blank" rel="noopener">Unbiased estimator</a></h1><blockquote><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-06 at 9.20.56 PM.png" alt="Screen Shot 2018-06-06 at 9.20.56 PM"></p></blockquote><h1 id="Additive-Chernoff-Bound"><a href="#Additive-Chernoff-Bound" class="headerlink" title="Additive Chernoff Bound"></a>Additive Chernoff Bound</h1><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-05 at 8.35.00 PM.png" alt="Screen Shot 2018-06-05 at 8.35.00 PM"></p><blockquote><p>上式即</p><script type="math/tex; mode=display">P(|S\ge{\mu}|\ge\epsilon)\le2e^{-2m\epsilon^2}</script><p><img src="/2018/05/30/Statistic-110/929166-20161116214202513-1545949382.png" alt="929166-20161116214202513-1545949382"></p><p>Lemma说明我们用随机变量的均值$\hat{\phi}$取估计参数$\phi$, 估计的参数和实际参数的差超过一个特定数值的概率有一确定的上界，并且随着样本量m的增大，$\hat{\phi}$越接近$\phi$.</p></blockquote><h1 id="Johnson-Lindenstrauss-Lemma"><a href="#Johnson-Lindenstrauss-Lemma" class="headerlink" title="Johnson-Lindenstrauss Lemma"></a><a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma" target="_blank" rel="noopener">Johnson-Lindenstrauss Lemma</a></h1><blockquote><p>The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved.</p><p><img src="/2018/05/30/Statistic-110/Screen Shot 2018-06-06 at 11.01.27 PM.png" alt="Screen Shot 2018-06-06 at 11.01.27 PM"></p><p>Remarks:</p><ol><li>the new dimention $n$ is decided by data sample number $m$ and error bound $\epsilon$; less error (small $\epsilon$) and more sample points require more dimensions, meaning the new dimension $n$ should not be too small; </li><li>Johnson–Lindenstrauss 引理表明任何高维数据集均可以被随机投影到一个较低维度的欧氏空间,同时可以控制pairwise距离的失真.</li></ol></blockquote>]]></content>
      
      <categories>
          
          <category> Probability and Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Probability </tag>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP Application - Random Response</title>
      <link href="/2018/05/28/DP-Application-Random-Response/"/>
      <url>/2018/05/28/DP-Application-Random-Response/</url>
      <content type="html"><![CDATA[<p><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-45871-7_17.pdf" target="_blank" rel="noopener">[2016-Atsushi]</a> gives a comprehensive survey about random response. Randomized response tends to be used in data collection scenario.Randomized response is purely a client-based privacy solution. It does not rely upon a trusted third-party server and puts control over data back to clients. The basic idea is answer truthfully with probability $p$, and answer randomly by picking a answer from the rest choices with probability $1-p$. And the rest choices include the true answer.</p><h3 id="Random-Response-and-its-Variant"><a href="#Random-Response-and-its-Variant" class="headerlink" title="Random Response and its  Variant"></a>Random Response and its  Variant</h3><h4 id="Definition-2016-Wang"><a href="#Definition-2016-Wang" class="headerlink" title="Definition[2016-Wang]"></a>Definition<a href="http://ceur-ws.org/Vol-1558/paper35.pdf" target="_blank" rel="noopener">[2016-Wang]</a></h4><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 10.18.29 PM.png" alt="Screen Shot 2018-05-29 at 10.18.29 PM"></p><h4 id="Warner-Version"><a href="#Warner-Version" class="headerlink" title="Warner Version"></a>Warner Version</h4><p>Random response was first proposed by [1965-Warner]. The aim is to estimate the proportion $\pi(A)$ of people who have some attribute A.</p><p>Each user has  reports her true answer $t\in \{1,-1\}​$ with probability p, and a random answer with probability 1 − p. The latter has the same probability to be −1 and +1;</p><h5 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h5><ol><li>There are red cards and non-red cards in a box, where the ratio of red cards among all the cards is $q$ with $0 &lt; q &lt; 1$ and $q \ne \frac{1}{2}$. And user draws a card.</li><li>If red, answer truthfully to the question “I am a member of A”</li><li>Otherwise, answer truthfully to the question “I am a member of A”</li></ol><h5 id="Utility"><a href="#Utility" class="headerlink" title="Utility"></a>Utility</h5><p>Let $\hat{T}$ be the proportion to which the respondents reply “True.” It is easy to see that the expectation of $\hat{T}$  is:</p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 12.40.16 AM.png" alt="Screen Shot 2018-05-29 at 12.40.16 AM"></p><h5 id="Privacy-Analysis"><a href="#Privacy-Analysis" class="headerlink" title="Privacy Analysis"></a>Privacy Analysis</h5><p>According to the definition of Differential Privacy, the following ratio should be bounded,</p><script type="math/tex; mode=display">P(A|A)\le{e^\epsilon{P(A|not \ A)}}\\P(A|not \ A)\le{e^\epsilon{P(A|A)}}</script><p>$P(A|A)=q​$</p><p>$P(A| not \ A)=(1-q)$</p><p>So combining the above two, we have:</p><p>The randomized response satisfied $\epsilon$-Differential Privacy, where</p><script type="math/tex; mode=display">\epsilon=max\{ln\frac{1-q}{q},ln\frac{q}{1-q}  \}</script><p><img src="/2018/05/28/DP-Application-Random-Response/myplot.jpeg" alt="myplot"></p><h4 id="Kuk-Version"><a href="#Kuk-Version" class="headerlink" title="Kuk Version"></a>Kuk Version</h4><p>Kuk’s proposed another kind of randomized response mechanism to estimate the proportion $\pi_A$, the same as Warner’s mechanism. </p><h5 id="Framework-1"><a href="#Framework-1" class="headerlink" title="Framework"></a>Framework</h5><ol><li>There are two boxes, $BOX_1$ and $BOX_2$. There are red and non-red cards in each box and the ratio of red cards in each box is $q_1,q_2$ respectively, where $0&lt;q_1,q_2&lt;1, q_1\ne{q_2}$. User takes one card from each box.</li><li>If this user is a member of A, then replies “red card” or “non-red card” in accordance<br>with the card taken from $BOX_1$</li><li>Otherwise, he does the same as above except that he takes a card from $BOX_2$.</li></ol><h5 id="Privacy-Analysis-1"><a href="#Privacy-Analysis-1" class="headerlink" title="Privacy Analysis"></a>Privacy Analysis</h5><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 1.08.31 AM.png" alt="Screen Shot 2018-05-29 at 1.08.31 AM"></p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 1.08.51 AM.png" alt="Screen Shot 2018-05-29 at 1.08.51 AM"></p><h4 id="Negative-Survey-Mechanism"><a href="#Negative-Survey-Mechanism" class="headerlink" title="Negative Survey Mechanism"></a>Negative Survey Mechanism</h4><h5 id="Framework-2"><a href="#Framework-2" class="headerlink" title="Framework"></a>Framework</h5><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 1.10.11 AM.png" alt="Screen Shot 2018-05-29 at 1.10.11 AM"></p><h5 id="Privacy-Analysis-2"><a href="#Privacy-Analysis-2" class="headerlink" title="Privacy Analysis"></a>Privacy Analysis</h5><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 1.11.13 AM.png" alt="Screen Shot 2018-05-29 at 1.11.13 AM"></p><h4 id="t-times-Negative-Survey"><a href="#t-times-Negative-Survey" class="headerlink" title="t-times Negative Survey"></a>t-times Negative Survey</h4><h5 id="Privacy-Analysis-3"><a href="#Privacy-Analysis-3" class="headerlink" title="Privacy Analysis"></a>Privacy Analysis</h5><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 1.12.13 AM-7574379.png" alt="Screen Shot 2018-05-29 at 1.12.13 AM-7574379"></p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-05-29 at 1.12.36 AM.png" alt="Screen Shot 2018-05-29 at 1.12.36 AM"></p><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><h4 id="2011-Quercia"><a href="#2011-Quercia" class="headerlink" title="[2011-Quercia] "></a><a href="https://pdfs.semanticscholar.org/7e74/15857ddcc798affff74d7e615b0d29f6bf74.pdf" target="_blank" rel="noopener">[2011-Quercia] </a></h4><blockquote><h5 id="Framework-3"><a href="#Framework-3" class="headerlink" title="Framework"></a>Framework</h5><p>[2011-Quercia] uses random response to obfuscate locations.</p><p>Let $k$ be the number of locations on a map. Then, the main part of SpotMe works as follows:</p><ol><li>the mobile phone chooses the location k uniformly at random with probability $p$</li><li>it chooses the true location with probability $1-p$</li></ol><h5 id="Privacy-Analysis-4"><a href="#Privacy-Analysis-4" class="headerlink" title="Privacy Analysis"></a>Privacy Analysis</h5><p>According to the definition of Differential Privacy, the following ratio should be bounded,</p><script type="math/tex; mode=display">P(L_{noise}|L_{true})\le{e^{\epsilon}P(L_{true}|L^{'}_{true})}</script><p>where $L_{true}$ is user’s actual location and $L^{‘}_{true}$ is the user’s location after making some changes while $L_{noise}$ is user’s obfuscated location.</p><p>$P(L_{true}|L_{true})=(1-p)+p*\frac{1}{k}$</p><p>$P(L_{true}|L^{‘}_{true})=p*\frac{1}{k}$</p><p>so after combining above two formulations, we have privacy bugdet:</p><script type="math/tex; mode=display">\epsilon=ln\frac{k-(k-1)p}{p}</script></blockquote><h4 id="2016-Wang"><a href="#2016-Wang" class="headerlink" title="[2016-Wang]"></a><a href="http://ceur-ws.org/Vol-1558/paper35.pdf" target="_blank" rel="noopener">[2016-Wang]</a></h4><blockquote><p>Randomized Response vs. Laplace Mechanism</p><p><u><strong>Randomized Response</strong></u> </p><p>Without loss of generality, we assume the randomized response still favors the true value, i.e., $p_{00}, p_{11} &gt; 0.5$. Intuitively, under the same privacy standard, the mechanism with larger diagonal elements in the corresponding design matrix tends to achieve better utility.</p><ul><li><img src="/2018/05/28/DP-Application-Random-Response/Screen%20Shot%202018-06-01%20at%205.09.06%20PM.png" alt="Screen Shot 2018-06-01 at 5.09.06 PM"></li><li>What is the proportion of $X=1$? It aims to o learn the population distribution based on the collected randomized dataset. We use $\pi_1$to denote the true proportion of value 1 to be estimated in the original population. The observed proportion of value 1 in the collected dataset is denoted as $\lambda_1$. We denote the unbiased estimator for $\pi_1$ respectively as $\hat{\pi}_1$. </li><li><p><img src="/2018/05/28/DP-Application-Random-Response/Screen%20Shot%202018-06-01%20at%205.13.14%20PM.png" alt="Screen Shot 2018-06-01 at 5.13.14 PM"></p><p><strong><u>Laplace Mechanism</u></strong></p></li><li><p><img src="/2018/05/28/DP-Application-Random-Response/Screen%20Shot%202018-06-01%20at%205.22.11%20PM.png" alt="Screen Shot 2018-06-01 at 5.22.11 PM"></p></li><li><img src="/2018/05/28/DP-Application-Random-Response/Users/daniel/Downloads/Nutstore/Git-PersonalWeb/QingWebsite/website/source/_posts/DP-Application-Random-Response/Screen%20Shot%202018-06-01%20at%205.23.19%20PM-7891843.png" alt="Screen Shot 2018-06-01 at 5.23.19 PM-7891843"></li><li><img src="/2018/05/28/DP-Application-Random-Response/Screen%20Shot%202018-06-01%20at%205.24.26%20PM.png" alt="Screen Shot 2018-06-01 at 5.24.26 PM"></li><li></li></ul></blockquote><h4 id="2006-Huseyin"><a href="#2006-Huseyin" class="headerlink" title="[2006-Huseyin]"></a><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.3576&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">[2006-Huseyin]</a></h4><p><strong>One-Group Scheme</strong>s  vs <strong>Multi-Group Schemes</strong>. </p><blockquote><p>in my understanding, one-group means there arw n users and m items. for a user, all the items rating are in a group. For example, user_i’s true rateing for all items is (0101),with probability theta to upload (0101) and (1-theta) probability to upload (1010).</p><p>And in m-group, for example m=2, then user_i’s rating is (01)and(11). with probability theta1 to upload (01) and (1-theta1) to upload (10); theta2 to upload(11) and (1-theta2) to upload (00)</p><p>Although we achieve decent accuracy in this scheme, the privacy level is very low. We improve privacy level by introducing multi-group schemes, while with increasing M, accuracy decreases because we add more randomness. The accuracy is from how much info is preserved after randomness. The more info, the more accurate. P(true=x|randon resp = x) means preserved info. m-group info preserved: $P^m$</p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-06-02 at 4.16.46 PM.png" alt="Screen Shot 2018-06-02 at 4.16.46 PM"></p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-06-02 at 4.16.55 PM.png" alt="Screen Shot 2018-06-02 at 4.16.55 PM"></p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-06-02 at 4.17.21 PM.png" alt="Screen Shot 2018-06-02 at 4.17.21 PM"></p></blockquote><p><strong>Private Mechanism With Full Privacy</strong></p><blockquote><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-06-02 at 4.20.01 PM.png" alt="Screen Shot 2018-06-02 at 4.20.01 PM"></p></blockquote><h4 id="2014-Sun"><a href="#2014-Sun" class="headerlink" title="[2014-Sun]"></a><a href="https://pdfs.semanticscholar.org/ff20/5884c8e34f8a1d6616809ca532e6ed801393.pdf?_ga=2.49109027.44169730.1528666244-1946321552.1526143877" target="_blank" rel="noopener">[2014-Sun]</a></h4><blockquote><p>The problem setting is how to find frequent itemset with privacy. Say we have the item set $I={I_1,I_2,..,I_n}$. Before each user sends out their transaction which is $n$-dimention vector and ithe entry being 1 means this user has this item or else, he does the the randomized selection for each item. That is, keep true answer with probability $p_i$ for item i and  perturb the answer with $1-p$. And p controls the privacy level.</p><p>For k-itemset, we want to find out its support, which comes from the estimator based on the perturbed data. Say we want to calculate the support of $2$-itemset. And the item set is $\{A,B,C,D\}$. And the candidate domain is $\{AB,AC,AD,BC,BD,CD\}$. The aim is to get the true count of $AB, \ c(AB)$, which is $11$ and may comes from $\{00,01,10,11\}$, with the probability matrix $P=\matrix{p_{00},p_{01}\\p_{10},p_{11}}$. so based on the observed count of $[A=0,B=0],[A=0,B=1],[A=1,B=0],[A=1,B=1]$ we can get estimator count matrix by $C_{observed}=PC_{estimator}\to C_{estimator}=P^{-1}C_{observed}$. And the support for $AB$ is $C_{estimator}[-1]$.</p></blockquote><p>Basic RAPPOR</p><blockquote><p><a href="https://arxiv.org/pdf/1503.01214.pdf" target="_blank" rel="noopener">[Building a RAPPOR with the Unknown: Privacy-Preserving Learning of Associations and Data Dictionaries]</a></p><p><img src="/2018/05/28/DP-Application-Random-Response/Screen Shot 2018-06-17 at 11.08.18 AM.png" alt="Screen Shot 2018-06-17 at 11.08.18 AM"></p></blockquote><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[2011-Quercia] SpotME if you can: randomized responses for location obfuscation on mobile phones</p><p>[1965-Warner] Randomized response: a survey technique for eliminating evasive answer bias</p><p>[2016-Wang] Using Randomized Response for Differential Privacy Preserving Data Collection </p><p>[2006-Huseyin] Achieving Private Recommendations Using Randomized Response Techniques</p><p>[2014-Sun] Personalized Privacy-Preserving Frequent Itemset Mining Using Randomized Response</p><p>[]</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP Application - Location Protection</title>
      <link href="/2018/05/27/DP-Application-Location-Protection/"/>
      <url>/2018/05/27/DP-Application-Location-Protection/</url>
      <content type="html"><![CDATA[<h2 id="Q"><a href="#Q" class="headerlink" title="Q?"></a>Q?</h2><ol><li><p>WHY $r$?</p><p>The author investegate the $l$-privacy within $r$.</p></li><li><p>[2013-Andres] paper don’t understand “polar mechanism is discretized and truncated?”</p></li></ol><h2 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h2><p>The goal of location with DP is that the chance of users being mapped to one specific obfuscated location from any of the actual locations is similar. The more similar the probability for each region is, the harder it is to infer users’ original positions, leading to better privacy protection.</p><a id="more"></a><h2 id="Development-Traces"><a href="#Development-Traces" class="headerlink" title="Development Traces"></a>Development Traces</h2><p><a href="http://www.lix.polytechnique.fr/~catuscia/papers/Geolocation/geo.pdf" target="_blank" rel="noopener">[2013-Andres]</a> is the first one to propose geo-indistinguishability to protect locations.</p><p>However, the protection of geo-indistinguishability is unifrom in space and generate the same amount of noise independently of the real location on the map, i.e., the same protection is applied in a dense city and in a sparse countryside. Also, multiple uses of geo-indistinguishability will result in the disclosure of true locations. Based on these problems, <a href="https://petsymposium.org/2015/papers/13_Chatzikokolakis.pdf" target="_blank" rel="noopener">[2015-Konstantinos Chatzikokolakis]</a> try to solve them.</p><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8125751" target="_blank" rel="noopener">[2017-Jingyu Hua]</a> try to solve the problem that the privacy level decreases quikly when the number of queries increases.</p><p>$d_x$-DP is proposed in <a href="https://arxiv.org/pdf/1806.02389.pdf" target="_blank" rel="noopener">[2018-Parameswaran Kamalaruban]</a>, <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-06880-0_16.pdf" target="_blank" rel="noopener">[2014-Ehab ElSalamouny]</a></p><h2 id="2013-Andres-Geo-indistinguishability"><a href="#2013-Andres-Geo-indistinguishability" class="headerlink" title="[2013-Andres]:Geo-indistinguishability"></a><a href="http://www.lix.polytechnique.fr/~catuscia/papers/Geolocation/geo.pdf" target="_blank" rel="noopener">[2013-Andres]</a>:Geo-indistinguishability</h2><h3 id="A-motivating-scenario"><a href="#A-motivating-scenario" class="headerlink" title="A motivating scenario"></a>A motivating scenario</h3><p>A user visiting Paris, currently located at a point $x$ close to the Eiffel Tower, and wishing to discover nearby restaurants with good reviews. To achieve this goal, the user can query a service provider (for instance, an LBS server); however, to maintain his privacy, the user does not wish to disclose his exact location. Instead, he can provide some approximate information that still allows him to obtain a useful service, for instance, a randomly chosen point $z$ close to his location. </p><p>We fix a circle of radius $r$ centered at the user’s location, and we reason about the user’s level of privacy within this radius.</p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 12.24.24 PM.png" alt="Screen Shot 2018-06-19 at 12.24.24 PM"></p><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><h4 id="Adversary-model"><a href="#Adversary-model" class="headerlink" title="Adversary model"></a>Adversary model</h4><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 12.41.01 PM.png" alt="Screen Shot 2018-06-19 at 12.41.01 PM"></p><h4 id="informal-one"><a href="#informal-one" class="headerlink" title="informal one"></a>informal one</h4><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 12.35.06 PM.png" alt="Screen Shot 2018-06-19 at 12.35.06 PM"></p><h4 id="Geo-indistinguishability-I"><a href="#Geo-indistinguishability-I" class="headerlink" title="Geo-indistinguishability-I"></a>Geo-indistinguishability-I</h4><p>A mechanism satisfies $\epsilon$ -geoindistinguishability iff for all priors $P_X$ and all observations $S \subseteq Z:$</p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 12.49.49 PM.png" alt="Screen Shot 2018-06-19 at 12.49.49 PM"></p><blockquote><ol><li><p>This definition considers that after observing $S$, the attacker assigns similar probabilities to the user being located in $x$ or $x’$.</p></li><li><p>the distance is the Euclidean distance between points.</p></li><li><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-21 at 4.24.04 PM.png" alt="Screen Shot 2018-06-21 at 4.24.04 PM"></p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-21 at 4.24.13 PM.png" alt="Screen Shot 2018-06-21 at 4.24.13 PM"></p><p>Remarks:</p><p>From (17) to (18), we use inequation $e^ {-\epsilon}\le \frac{f(x’|x)}{f(x’|y)} $</p></li></ol></blockquote><h4 id="Geo-indistinguishability-II"><a href="#Geo-indistinguishability-II" class="headerlink" title="Geo-indistinguishability-II"></a>Geo-indistinguishability-II</h4><p>A mechanism satisfies $\epsilon$-geoindistinguishability iff for all priors $P_X$ and all observations $S \subseteq Z:$</p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 12.57.09 PM.png" alt="Screen Shot 2018-06-19 at 12.57.09 PM"></p><blockquote><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 12.57.58 PM.png" alt="Screen Shot 2018-06-19 at 12.57.58 PM"></p></blockquote><h4 id="Geo-indistinguishability-III"><a href="#Geo-indistinguishability-III" class="headerlink" title="Geo-indistinguishability-III"></a>Geo-indistinguishability-III</h4><p>A mechanism satisfies $\epsilon$-geoindistinguishability iff for all observations $S \subseteq Z:$</p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 4.32.21 PM.png" alt="Screen Shot 2018-06-19 at 4.32.21 PM"></p><blockquote><ol><li>This definition requires that points within distance $l$ produce observations with similar probabilities</li></ol></blockquote><p>In above three definition, the privacy level is $l$ while $\epsilon$ can be considered as unit of distance measurement, where $l=\epsilon r$. For example, the user specifies his desired level of privacy, say $l=ln(4)$ within $r=0.2$km, then the $\frac{In(4)}{0.2}$-geo-indistinguishability ensures the user that by revealing his approximate location, the adversary cannot infer his real location with probability 4 times higher than without revealing his location among all locations within 200 meters.</p><h3 id="Mechanism-for-Geo-indistinguishability"><a href="#Mechanism-for-Geo-indistinguishability" class="headerlink" title="Mechanism for Geo-indistinguishability"></a>Mechanism for Geo-indistinguishability</h3><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 5.27.01 PM.png" alt="Screen Shot 2018-06-19 at 5.27.01 PM"></p><blockquote><ol><li>We explore how to define a geoindistinguishable mechanism on the continuous plane.</li><li>we call this function planar laplacian centered in $x_0$</li><li></li></ol></blockquote><h3 id="Random-point-generation"><a href="#Random-point-generation" class="headerlink" title="Random point generation"></a>Random point generation</h3><p>It will be convenient, therefore, to transform the reference system into a system of polar coordinates with origin in $x_0$. </p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 5.37.57 PM.png" alt="Screen Shot 2018-06-19 at 5.37.57 PM"></p><p>Since $r$ and $\theta$ are independent, so</p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 5.55.09 PM.png" alt="Screen Shot 2018-06-19 at 5.55.09 PM"></p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-19 at 5.58.43 PM.png" alt="Screen Shot 2018-06-19 at 5.58.43 PM"></p><h2 id="2015-Konstantinos-Chatzikokolakis-elastic-distinguishability-metrics"><a href="#2015-Konstantinos-Chatzikokolakis-elastic-distinguishability-metrics" class="headerlink" title="[2015-Konstantinos Chatzikokolakis] elastic distinguishability metrics"></a><a href="https://petsymposium.org/2015/papers/13_Chatzikokolakis.pdf" target="_blank" rel="noopener">[2015-Konstantinos Chatzikokolakis]</a> elastic distinguishability metrics</h2><h3 id="The-problem-in-geo-indistinguishability"><a href="#The-problem-in-geo-indistinguishability" class="headerlink" title="The problem in geo-indistinguishability"></a>The problem in geo-indistinguishability</h3><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-25 at 4.17.47 PM.png" alt="Screen Shot 2018-06-25 at 4.17.47 PM"></p><h2 id="2014-Nicolas"><a href="#2014-Nicolas" class="headerlink" title="[2014-Nicolas]"></a><a href="https://arxiv.org/pdf/1402.5029.pdf" target="_blank" rel="noopener">[2014-Nicolas]</a></h2><h2 id="Geo-indistinguishability-with-expected-inference-error"><a href="#Geo-indistinguishability-with-expected-inference-error" class="headerlink" title="Geo-indistinguishability with expected inference error"></a>Geo-indistinguishability with expected inference error</h2><p><a href="https://arxiv.org/pdf/1402.3426.pdf" target="_blank" rel="noopener">[2014-Reza Shokri]</a><a href="https://arxiv.org/pdf/1402.5029.pdf" target="_blank" rel="noopener">[2014-Nicolás E. Bordenabe]</a><a href="http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2017/09/ndss2017_06A-2_Yu_paper.pdf" target="_blank" rel="noopener">[2017-Lei Yu]</a></p><p>All these works combine geo-indistinguishability and expected inference error.</p><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-22 at 8.35.38 PM.png" alt="Screen Shot 2018-06-22 at 8.35.38 PM"></p><h4 id="Utility-Cost"><a href="#Utility-Cost" class="headerlink" title="Utility Cost"></a>Utility Cost</h4><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-22 at 8.36.56 PM.png" alt="Screen Shot 2018-06-22 at 8.36.56 PM"></p><h4 id="Inference-Attack"><a href="#Inference-Attack" class="headerlink" title="Inference Attack"></a>Inference Attack</h4><p><img src="/2018/05/27/DP-Application-Location-Protection/Screen Shot 2018-06-22 at 8.37.47 PM.png" alt="Screen Shot 2018-06-22 at 8.37.47 PM"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[2013-Andres] Geo-indistinguishability: Differential privacy for location-based systems</p><p>[2014-Nicolas] Optimal geo-indistinguishable mechanisms for location privacy</p><p>[2017-Lei Yu] Dynamic Differential Location Privacy with Personalized Error Bounds</p><p>[2014-Reza Shokri] Privacy Games: Optimal User-Centric Data Obfuscation</p><p>[2014-Nicolás E. Bordenabe] Optimal geo-indistinguishable mechanisms for location privacy</p><p>[2015-Konstantinos Chatzikokolakis] Constructing elastic distinguishability metrics for location privacy</p><p>[2017-Jingyu Hua] A Geo-Indistinguishable Location Perturbation Mechanism for Location-Based Services Supporting Frequent Queries</p><p>[2018-Parameswaran Kamalaruban] $d_x$ -Private Mechanisms for Linear Queries</p><p>[2014-Ehab ElSalamouny] Generalized Differential Privacy: Regions of Priors That Admit Robust Optimal Mechanisms</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP Application - Private Spatial Decompositions</title>
      <link href="/2018/05/26/DP-Application-Private-Spatial-Decompositions/"/>
      <url>/2018/05/26/DP-Application-Private-Spatial-Decompositions/</url>
      <content type="html"><![CDATA[<p><a href="http://www.vldb.org/pvldb/vol7/p919-to.pdf" target="_blank" rel="noopener">[2014-To]</a> details the methods of Private Spatial Decompositions.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[2014-To] A Framework for Protecting Worker Location Privacy in Spatial Crowdsourcing</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP Application - CrowdSourcing</title>
      <link href="/2018/05/26/DP%20Application-CrowdSourcing/"/>
      <url>/2018/05/26/DP%20Application-CrowdSourcing/</url>
      <content type="html"><![CDATA[<p><a href="https://sigmodrecord.org/publications/sigmodRecord/1512/pdfs/05_surveys_Pournajaf.pdf" target="_blank" rel="noopener">[2015-Layla]</a> is a survey on the MobileCrowdScoucing privacy.</p><p>Four factors in CrowdSourcing:</p><blockquote><ol><li>sensing data quality, which tries to maximize the data quality measured by a certain metric (mostly used in environmental monitoring tasks)</li><li>incentive cost, which aims at minimizing the total budget (from the task organizer perspective)<br>for an MCS task with different incentive mechanisms, such as pay per participant or pay per task</li><li>energy consumption, whose objective is to identify an optimal collaborative data sensing and uploading scheme with energy-saving techniques such as piggybacking</li><li>travel distance, where the travel distance of a user for accomplishing a task is considered in task allocation, in order to minimize the overall travel distance for all the tasks.</li></ol></blockquote><p><a href="https://arxiv.org/pdf/1805.08418.pdf" target="_blank" rel="noopener">[2018-Wang Jiangtao]</a> provides the comprehensive survey on MCS(MobileCrowdScoucing) task allocation.</p><h2 id="Task-Assignments-in-CrowdSourcing"><a href="#Task-Assignments-in-CrowdSourcing" class="headerlink" title="Task Assignments in CrowdSourcing"></a>Task Assignments in CrowdSourcing</h2><p>In croudsourcing, workers with mobile devices to collect data and send it to task requester for rewards.</p><p>In task assignment, organizers need participants’ precise locations for optimal task allocation. However, the exposure of their locations raises privacy concerns. Especially for those who are not eventually selected for any task, their location privacy is sacrificed in vain.</p><p>So in the differential privacy task assignment croudsourcing, the goal is to design a data release method that accurately represents the distribution of the workers and helps the Server efficiently match workers with tasks without compromising the privacy of their locations.</p><h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><h3 id="2014-To"><a href="#2014-To" class="headerlink" title="[2014-To]"></a><a href="http://www.vldb.org/pvldb/vol7/p919-to.pdf" target="_blank" rel="noopener">[2014-To]</a></h3><p>it is the first one to solve this problem.</p><blockquote><ol><li><p>Framework</p><ul><li>Workers send their locations to a trusted cellular service provider(CSP) </li><li>CSP collects updates and releases a PSD according to privacy budget </li><li>When the SC-server receives a task t, it queries the PSD to determine a geocast region (GR), which is a unique feature of this work. Next, the SC-server initiates a geocast communication process to disseminate t to all workers within GR. Upon receiving request t, a worker w decides whether to perform the task or not.</li><li>Task assignment. <ul><li>Once server get a task request, server needs to query PSD to find a geocast region, balancing between high task assignment success rate and system overload like worker traveling distance and the number of  noticed worker. The author calculate maximum travel distance, models acceptance rate as the function of distance.</li><li>Geocast region construction. </li><li>Optimization. Including Partial Cell Selection and Communication Cost.</li></ul></li></ul></li><li><p>Privacy Model</p><ul><li><p>Privacy leakage: (1) workers disclose information to the task requester once they consent to the task; (2) completion of a task discloses the fact that some worker must have been at that location; (3) but this paper focuses on what happens prior to consent, when worker location and identity must be protected from both task requesters and the SC server.</p></li><li><p>the specific objective is to protect both the location and the identity of workers during task assignment. </p></li><li><p>Private Spatial Decom</p><ul><li><p>AG uses a two-level grid and variable cell granularity.</p></li><li><p>For the first-level, domain is divided into $m_1\times{m1}$ cells. This heuristic method is data-independent, and thus does not consume any privacy budget. For each level-1 cell, it is divided into $m_2\times{m2}$ subcells. </p><script type="math/tex; mode=display">m1 = max(10,\lceil{\frac{1}{4}\sqrt{\frac{N\times{\epsilon}}{k_1=10}}}\rceil)\\m_2=\lceil{\sqrt{\frac{N'\times{\epsilon_2}}{k_2=5}}}\rceil</script></li><li><p>$k_2$ selection. $k_2$ controls the granularity of level-2 domain and small one leads to compactness in level-2 subcells. </p></li><li><p>improving $m_2$</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.27.05 PM.png" alt="Screen Shot 2018-05-30 at 12.27.05 PM"></p></li></ul></li></ul></li></ol></blockquote><h3 id="2018-Yang"><a href="#2018-Yang" class="headerlink" title="[2018-Yang]"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8319407" target="_blank" rel="noopener">[2018-Yang]</a></h3><p>it criticizes that <a href="http://www.vldb.org/pvldb/vol7/p919-to.pdf" target="_blank" rel="noopener">[2014-To]</a> is based on the assumption that workers are uniformly distributed within the domain and and the workers in each cell have the same acceptance rate. So they proposes a workers density-based method.</p><blockquote><ol><li>Framework<ul><li>Workers must submit their location to the CSP, travel to the location designated for the task and collect data using their sensor-equipped device.</li><li>The CSP collects locations from workers and releases data in sanitized form to the Server for task assignment. There is a trust relationship between CSP and workers.</li><li>The Server queries the CSP for a sanitized dataset once it receives a task, where server chooses a geocast region GR to disseminate the task to the workers in GR. It then assigns the task to suitable workers, through the CSP, according to a task assignment algorithm.</li></ul></li><li>Modules<ul><li>Using quadtree to partition the region based on worker density.  But the authors change this method so that the partitioning is based on the worker density instead of choosing the middle point.</li><li>Partitioning point selection. Before partitioning, <code>m</code> initial points are randomly generated within the cell, ==============================(TODO)</li><li>Differential privacy data release. A noisy count of the number of workers in each cell is released to protect the privacy of worker locations, where whether or not a worker<br>within a specific cell cannot be identified.</li><li>Task assignment. Firstly, teh geocast region is selected based on task assignment success rate and system overhead (the distance workers need to travel and the number of workers notified of the task).</li></ul></li><li>Privacy Model<ul><li>The basic idea of private data release is that the domain of worker locations is partitioned into small cells and Laplace noise is added to the count of workers in each cell to achieve a differential privacy guarantee.</li><li>Pervious literature assumes the worker locations are distributed uniformly, and the workers in each cell have the same acceptance rate, which is not the case in real-world scenarios. Partitioning the data domain into a uniform grid would result in sizeable errors. Therefore, we propose a recursive partitioning process based on worker density. </li><li>The aim is to identify dense regions and sparse regions and make the distribution of the workers in each smaller region as near to uniform as possible.</li><li>Adopting data-independent quadtree into workers density-based data-independent technique. <ul><li>Partitioning stop condition. Traditional quadtrees require the data publisher to specify the height of the partitioning. In this paper, the process stops if (1) no workers exist in the cell; (2) the area of cell is less than some threshold, The smaller the cell, the more uniform the distribution of workers within it; (3) the distribution of workers in a cell is relatively uniform， which is measures by the threshold of maximum density difference.</li><li>First, $m$ initial partition points in the location domain need to be selected, where $m=\frac{\sqrt{area \ of \ cell}}{\alpha}$</li><li>check condition (1) and (2), which is the number of points in cell is greater than 0 and the area of cell is greater than the threshold. If one of answers is no, stop partitioning.</li><li>for each partitioning point, calculate the density of the subcells divided by this point. choose the point which has the maximum density difference bbetween subcells.</li><li>If the biggest density difference is greater than the threshold β, the cell is partitioned at point. Otherwise, the cell will not be partitioned as the distribution of worker in the cell is already close to uniform. For example, the density of four subcells is 1, 2, 1, 2, which means in each subcells, there are 1, 2, 1 and 2 points in each subcell, which is kind of uniform distribution in the cell.</li></ul></li></ul></li></ol></blockquote><h3 id="2017-Wang"><a href="#2017-Wang" class="headerlink" title="[2017-Wang]"></a><a href="http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p627.pdf" target="_blank" rel="noopener">[2017-Wang]</a></h3><blockquote><ol><li><p>Framework</p><ul><li>Platform-side : Geo-Obfuscation Function Generation</li><li>User-side : Location Obfuscation</li><li>Platform-side : Obfuscation-aware Task Allocation</li></ul></li><li><p>Privacy Model</p><p>Overall, the aurhor models workers’ travel distance to task locations as the function of geo-obfuscation function and task allocation. And by calculating the optimal function, we can get the Geo-Obfuscation matrix which satisfies DP and task allocation schemes.</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 10.06.41 AM.png" alt="Screen Shot 2018-05-30 at 10.06.41 AM"></p><ul><li><p>the expected travel distance of assigning a task at $l_t$ to a user at (obfuscated) $l*$ given the geo-obfuscation function $P$.</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 10.18.25 AM.png" alt="Screen Shot 2018-05-30 at 10.18.25 AM"></p></li><li><p>optimal function</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 10.55.21 AM.png" alt="Screen Shot 2018-05-30 at 10.55.21 AM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.05.55 AM.png" alt="Screen Shot 2018-05-30 at 11.05.55 AM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 10.56.24 AM.png" alt="Screen Shot 2018-05-30 at 10.56.24 AM"></p></li><li><p>task allocation</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.07.06 AM.png" alt="Screen Shot 2018-05-30 at 11.07.06 AM"></p></li><li><p>Candidate Geo-Distribution Estimation</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.10.54 AM.png" alt="Screen Shot 2018-05-30 at 11.10.54 AM"></p></li><li><p>Laplace</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.27.15 AM.png" alt="Screen Shot 2018-05-30 at 11.27.15 AM"></p></li><li><p>Geo-distribution Estimation</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.50.49 AM.png" alt="Screen Shot 2018-05-30 at 11.50.49 AM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.50.57 AM.png" alt="Screen Shot 2018-05-30 at 11.50.57 AM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 11.51.05 AM.png" alt="Screen Shot 2018-05-30 at 11.51.05 AM"></p></li></ul></li></ol></blockquote><h3 id="2015-Gong"><a href="#2015-Gong" class="headerlink" title="[2015-Gong]"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7296638" target="_blank" rel="noopener">[2015-Gong]</a></h3><h3 id="2016-Jin"><a href="#2016-Jin" class="headerlink" title="[2016-Jin]"></a><a href="http://webhost.engr.illinois.edu/~hjin8/publications/mobihoc16.pdf" target="_blank" rel="noopener">[2016-Jin]</a></h3><p>This paper considers how to effectively incentivize worker participation. And proposes a system framework that integrates an incentive, a weighted  data aggregation, and a data perturbation mechanism which protects the sensing data.</p><blockquote><ol><li><p>Framework</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-06-01 at 11.00.21 AM.png" alt="Screen Shot 2018-06-01 at 11.00.21 AM"></p><ul><li><p>Aggregation Mechanism</p><p>To guarantee that the perturbed results have satisfactory accuracy, the original aggregated results before perturbation need to be accurate enough in the first place. Therefore, we reasonably assume that the platform uses a weighted aggregation method to calculate the aggregated result $x_j$ for each task $\tau_j$ based on workers’ data.</p><p>The motivation for utilizing weighted aggregation is to capture the effect of workers’ diverse skill levels on the calculation of the aggregated results.</p></li><li><p>Incentive Mechanism</p><p>aim to design a pSRC auction that minimizes the platform’s total payment with satisfactory data aggregation</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-06-01 at 11.51.45 AM.png" alt="Screen Shot 2018-06-01 at 11.51.45 AM"></p><p>where $y_i$ means worker is seleted for a task, $\alpha_j$ is the error threshold for task j, $\theta_{i,j}$ is error of  worker i for task j.</p></li></ul></li><li><p>Privacy model</p><ul><li><p>workers locally sense a specific object or phenomenon and server aggregrate the uploaded data and publish a noise aggregated results to protect worker’s privacy.</p></li><li><p>the noise scale is deviated from the $(\alpha,\beta)-accuracy$.</p></li><li><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-06-01 at 12.09.47 PM-7873006.png" alt="Screen Shot 2018-06-01 at 12.09.47 PM-7873006"></p></li><li><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-06-01 at 12.25.25 PM.png" alt="Screen Shot 2018-06-01 at 12.25.25 PM"></p></li><li><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-06-01 at 12.19.28 PM.png" alt="Screen Shot 2018-06-01 at 12.19.28 PM"></p><p><strong>Remarks</strong>:</p><p>$Pr(|N_j|\ge\alpha_j)=Pr(N_j\ge\alpha_j)+Pr(-N_j\le-\alpha_j)$</p><p>according to the symmetry of PDF, $Pr(N_j\ge\alpha_j)=Pr(-N_j\le-\alpha_j)$</p><p>so $Pr(|N_j|\ge\alpha_j)=2Pr(N_j\ge\alpha_j)$</p></li><li></li></ul></li></ol></blockquote><h2 id="Data-Sensing-in-CrowdSourcing"><a href="#Data-Sensing-in-CrowdSourcing" class="headerlink" title="Data Sensing in CrowdSourcing"></a>Data Sensing in CrowdSourcing</h2><h3 id="Solutions-1"><a href="#Solutions-1" class="headerlink" title="Solutions"></a>Solutions</h3><h3 id="2016-Wang"><a href="#2016-Wang" class="headerlink" title="[2016-Wang]"></a><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7837982" target="_blank" rel="noopener">[2016-Wang]</a></h3><p>In this paper, instead of studying task assignment, the author focuses on sparse croudsensing. Due to large target sensing area and limited budget which result in insufficient spatial coverage of mobile users, sparse mobile croudsensing impute information of the uncovered regions by combining historical records with available sensing data from nearby regions.</p><blockquote><ol><li>Framework<ul><li>The server side generates probabilistic obfuscation matrix and data adjustment function in an offline way.</li><li>user sider senses its actual location and then maps the associated region to another region according to probabilistic obfuscation matrix. After that, the data adjustment function alters the original sensing data to fit the properties of the obfuscated region. Finally, mobile client then uploads the modified region and data to the server.</li><li>The server side does data inference : modeled as a matrix completion problem, where each element in the matrix is the value like temperature of a region at time t. In this paper, they use compressive sensing theory for inference.</li></ul></li><li><p>Privacy Model</p><ul><li><p>Privacy leakage : In Sparse MCS, participants report the sensing data with time stamps and geographical coordinates, which may introduce serious privacy risks. </p></li><li><p>Data utility: Due to location obfuscation, the uploaded region data is not actual value of this region, so adjustment is needed to decrease data utility loss. the data quality loss is determined by the difference of sensing data between the actual and the obfuscated locations, instead of the geographic distance. In other words, a participant’s location may be mapped to a place far away, as long as the sensing values of the two locations are close enough.</p></li><li><p>Adversary model - Bayesian attack</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.10.00 PM.png" alt="Screen Shot 2018-05-30 at 12.10.00 PM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.10.48 PM.png" alt="Screen Shot 2018-05-30 at 12.10.48 PM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.11.49 PM.png" alt="Screen Shot 2018-05-30 at 12.11.49 PM"></p></li><li><p>optimal function</p><p>The authors model the sensing data quality loss as the function of location obfuscation matrix.</p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.20.20 PM.png" alt="Screen Shot 2018-05-30 at 12.20.20 PM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.22.12 PM.png" alt="Screen Shot 2018-05-30 at 12.22.12 PM"></p><p><img src="/2018/05/26/DP Application-CrowdSourcing/Screen Shot 2018-05-30 at 12.22.56 PM.png" alt="Screen Shot 2018-05-30 at 12.22.56 PM"></p></li></ul></li></ol></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[2014-To] A Framework for Protecting Worker Location Privacy in Spatial Crowdsourcing</p><p>[2018-Yang] Density-Based Location Preservation for Mobile Crowdsensing With Differential Privacy</p><p>[2016-Wang] Differential Location Privacy for Sparse Mobile Crowdsensing</p><p>[2015-Layla] Participant privacy in mobile crowd sensing task management: A survey of methods and challenges</p><p>[2015-Gong] Protecting Location Privacy for Task Allocation in Ad Hoc Mobile Cloud Computing</p><p>[2016-Jin] INCEPTION: Incentivizing Privacy-Preserving Data Aggregation for Mobile Crowd Sensing Systems</p><p>[2018-Wang Jiangtao] Task Allocation in Mobile Crowd Sensing: State of the Art and Future Opportunities</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Dynamic Programming</title>
      <link href="/2018/05/26/Dynamic-Programming/"/>
      <url>/2018/05/26/Dynamic-Programming/</url>
      <content type="html"><![CDATA[<h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>这篇文章详细记录<a href="https://leetcode.com/" target="_blank" rel="noopener">LeetCode</a>上各种难度的动态规划题目。每道题我都给出题目，思路以及Python代码。</p><a id="more"></a><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><p>如果我们有面值为1元、3元和5元的硬币若干枚，如何用最少的硬币凑够11元？ (表面上这道题可以用贪心算法，但贪心算法无法保证可以求出解，比如1元换成2元的时候)</p><p>首先我们思考一个问题，如何用最少的硬币凑够i元(i&lt;11)？为什么要这么问呢？ 两个原因：1.当我们遇到一个大问题时，总是习惯把问题的规模变小，这样便于分析讨论。 2.这个规模变小后的问题和原来的问题是同质的，除了规模变小，其它的都是一样的， 本质上它还是同一个问题(规模变小后的问题其实是原问题的子问题)。</p><p>好了，让我们从最小的i开始吧。当i=0，即我们需要多少个硬币来凑够0元。 由于1，3，5都大于0，即没有比0小的币值，因此凑够0元我们最少需要0个硬币。 (这个分析很傻是不是？别着急，这个思路有利于我们理清动态规划究竟在做些什么。) 这时候我们发现用一个标记来表示这句“凑够0元我们最少需要0个硬币。”会比较方便， 如果一直用纯文字来表述，不出一会儿你就会觉得很绕了。那么， 我们用d(i)=j来表示凑够i元最少需要j个硬币。于是我们已经得到了d(0)=0， 表示凑够0元最小需要0个硬币。当i=1时，只有面值为1元的硬币可用， 因此我们拿起一个面值为1的硬币，接下来只需要凑够0元即可，而这个是已经知道答案的， 即d(0)=0。所以，d(1)=d(1-1)+1=d(0)+1=0+1=1。当i=2时， 仍然只有面值为1的硬币可用，于是我拿起一个面值为1的硬币， 接下来我只需要再凑够2-1=1元即可(记得要用最小的硬币数量)，而这个答案也已经知道了。 所以d(2)=d(2-1)+1=d(1)+1=1+1=2。一直到这里，你都可能会觉得，好无聊， 感觉像做小学生的题目似的。因为我们一直都只能操作面值为1的硬币！耐心点， 让我们看看i=3时的情况。当i=3时，我们能用的硬币就有两种了：1元的和3元的( 5元的仍然没用，因为你需要凑的数目是3元！5元太多了亲)。 既然能用的硬币有两种，我就有两种方案。如果我拿了一个1元的硬币，我的目标就变为了： 凑够3-1=2元需要的最少硬币数量。即d(3)=d(3-1)+1=d(2)+1=2+1=3。 这个方案说的是，我拿3个1元的硬币；第二种方案是我拿起一个3元的硬币， 我的目标就变成：凑够3-3=0元需要的最少硬币数量。即d(3)=d(3-3)+1=d(0)+1=0+1=1. 这个方案说的是，我拿1个3元的硬币。好了，这两种方案哪种更优呢？ 记得我们可是要用最少的硬币数量来凑够3元的。所以， 选择d(3)=1，怎么来的呢？具体是这样得到的：d(3)=min{d(3-1)+1, d(3-3)+1}。</p><p>OK，码了这么多字讲具体的东西，让我们来点抽象的。从以上的文字中， 我们要抽出动态规划里非常重要的两个概念：状态和状态转移方程。</p><p>上文中d(i)表示凑够i元需要的最少硬币数量，我们将它定义为该问题的”状态”， 这个状态是怎么找出来的呢？我在另一篇文章 动态规划之背包问题(一)中写过： 根据子问题定义状态。你找到子问题，状态也就浮出水面了。 最终我们要求解的问题，可以用这个状态来表示：d(11)，即凑够11元最少需要多少个硬币。 那状态转移方程是什么呢？既然我们用d(i)表示状态，那么状态转移方程自然包含d(i)， 上文中包含状态d(i)的方程是：d(3)=min{d(3-1)+1, d(3-3)+1}。没错， 它就是状态转移方程，描述状态之间是如何转移的。当然，我们要对它抽象一下，</p><p><strong>d(i)=min{ d(i-vj)+1 }，其中i-vj &gt;=0，vj表示第j个硬币的面值;</strong></p><h3 id="746-Min-Cost-Climbing-Stairs"><a href="#746-Min-Cost-Climbing-Stairs" class="headerlink" title="746.Min Cost Climbing Stairs"></a>746.Min Cost Climbing Stairs</h3><blockquote><ul><li><p>题目</p><p>On a staircase, the <code>i</code>-th step has some non-negative cost <code>cost[i]</code> assigned (0 indexed).</p><p>Once you pay the cost, you can either climb one or two steps. You need to find minimum cost to reach the top of the floor, and you can either start from the step with index 0, or the step with index 1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;   Example1:</div><div class="line">&gt;   Input: cost = [<span class="number">1</span>, <span class="number">100</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>, <span class="number">1</span>]</div><div class="line">&gt;   Output: <span class="number">6</span></div><div class="line">&gt;   Explanation: Cheapest <span class="keyword">is</span> start on cost[<span class="number">0</span>], <span class="keyword">and</span> only step on <span class="number">1</span>s, skipping cost[<span class="number">3</span>].</div><div class="line">&gt;   Example2:</div><div class="line">&gt;   Input: cost = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>]</div><div class="line">&gt;   Output: <span class="number">15</span></div><div class="line">&gt;   Explanation: Cheapest <span class="keyword">is</span> start on cost[<span class="number">1</span>], pay that cost <span class="keyword">and</span> go to the top.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>典型动态规划，且只需要一维dp数组维护，考虑到可以从index=0|1开始，所以在dp数组前面插入两个0(因为是加法，乘法则插入1)，同时要表示终点，则在dp尾再插入一个0，统计到达top时的cost。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>&gt;</p><blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="function"><span class="keyword">def</span> <span class="title">minCostClimbingStairs</span><span class="params">(self, cost)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type cost: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           lcost = len(cost)</div><div class="line">&gt;           cost.append(<span class="number">0</span>)</div><div class="line">&gt;           dp = [ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(lcost+<span class="number">3</span>)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,len(dp)):</div><div class="line">&gt;               dp[i] = min(dp[i<span class="number">-2</span>],dp[i<span class="number">-1</span>]) + cost[i<span class="number">-2</span>]</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote></div></div></li></ul></blockquote><h3 id="70-Climbing-Stairs"><a href="#70-Climbing-Stairs" class="headerlink" title="70. Climbing Stairs"></a>70. Climbing Stairs</h3><blockquote><ul><li><p>题意</p><p>You are climbing a stair case. It takes <em>n</em> steps to reach to the top.</p><p>Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="number">2</span></div><div class="line">&gt;   Output: <span class="number">2</span></div><div class="line">&gt;   Explanation: There are two ways to climb to the top.</div><div class="line">&gt;   <span class="number">1.</span> <span class="number">1</span> step + <span class="number">1</span> step</div><div class="line">&gt;   <span class="number">2.</span> <span class="number">2</span> steps</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="number">3</span></div><div class="line">&gt;   Output: <span class="number">3</span></div><div class="line">&gt;   Explanation: There are three ways to climb to the top.</div><div class="line">&gt;   <span class="number">1.</span> <span class="number">1</span> step + <span class="number">1</span> step + <span class="number">1</span> step</div><div class="line">&gt;   <span class="number">2.</span> <span class="number">1</span> step + <span class="number">2</span> steps</div><div class="line">&gt;   <span class="number">3.</span> <span class="number">2</span> steps + <span class="number">1</span> step</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>跟上面一题很相似了，同样是一维，也需要在首个插入</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">climbStairs</span><span class="params">(self, n)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type n: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           l = n+<span class="number">1</span></div><div class="line">&gt;           dp = [ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           dp[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           dp[<span class="number">1</span>] = <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,l):</div><div class="line">&gt;               dp[i] = dp[i<span class="number">-1</span>] + dp[i<span class="number">-2</span>]</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="53-Maximum-Subarray"><a href="#53-Maximum-Subarray" class="headerlink" title="53. Maximum Subarray"></a>53. Maximum Subarray</h3><blockquote><ul><li><p>题意</p><p>Given an integer array <code>nums</code>, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: [<span class="number">-2</span>,<span class="number">1</span>,<span class="number">-3</span>,<span class="number">4</span>,<span class="number">-1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">-5</span>,<span class="number">4</span>],</div><div class="line">&gt;   Output: <span class="number">6</span></div><div class="line">&gt;   Explanation: [<span class="number">4</span>,<span class="number">-1</span>,<span class="number">2</span>,<span class="number">1</span>] has the largest sum = <span class="number">6.</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>一维dp数组，同样在前面插入0，但是返回的时候，要小心不能返回max(dp)，而是max(dp[1:])，防止input是[-1,-2].</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           l = len(nums)</div><div class="line">&gt;           dp = [ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l+<span class="number">1</span>)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l+<span class="number">1</span>):</div><div class="line">&gt;               dp[i] = max(nums[i<span class="number">-1</span>],nums[i<span class="number">-1</span>]+dp[i<span class="number">-1</span>])</div><div class="line">&gt;           <span class="keyword">return</span> max(dp[<span class="number">1</span>:])</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="198-House-Robber"><a href="#198-House-Robber" class="headerlink" title="198. House Robber"></a>198. House Robber</h3><blockquote><ul><li><p>题目</p><p>You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and <strong>it will automatically contact the police if two adjacent houses were broken into on the same night</strong>.</p><p>Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight <strong>without alerting the police</strong>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</div><div class="line">&gt;   Output: <span class="number">4</span></div><div class="line">&gt;   Explanation: Rob house <span class="number">1</span> (money = <span class="number">1</span>) and then rob house <span class="number">3</span> (money = <span class="number">3</span>).</div><div class="line">&gt;                Total amount you can rob = <span class="number">1</span> + <span class="number">3</span> = <span class="number">4</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: [<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">&gt;   Output: <span class="number">4</span></div><div class="line">&gt;   Explanation: Rob house <span class="number">1</span> (money = <span class="number">2</span>), rob house <span class="number">4</span> (money = <span class="number">2</span>) Total amount you can rob = <span class="number">2</span> + <span class="number">2</span> = <span class="number">4</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>dp状态思考错误，dp[i]表示抢到第i个房子时，目前获取的最大利润</p><p>dp状态转移方程，有点错误。错误以为不能抢劫相邻的房子意味着抢的房子之间必须相隔一个房子，其实可以中间可以相隔多个房子不抢。</p><p>则dp转移方程为：即要不要抢当前的房子</p><p>dp[i] = max( dp[i-1] , dp[i-2] + nums[i] )</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">rob</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> nums:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           l = len(nums)</div><div class="line">&gt;           dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">1</span>] = nums[<span class="number">0</span>]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,l+<span class="number">1</span>):</div><div class="line">&gt;               dp[i] = max(nums[i<span class="number">-1</span>] + dp[i<span class="number">-2</span>], dp[i<span class="number">-1</span>])</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h2 id="初级"><a href="#初级" class="headerlink" title="初级"></a>初级</h2><p>一个序列有N个数：A[1],A[2],…,A[N]，求出最长非降子序列的长度。 (讲DP基本都会讲到的一个问题LIS：longest increasing subsequence)</p><p>正如上面我们讲的，面对这样一个问题，我们首先要定义一个“状态”来代表它的子问题， 并且找到它的解。注意，大部分情况下，某个状态只与它前面出现的状态有关， 而独立于后面的状态。</p><p>让我们沿用“入门”一节里那道简单题的思路来一步步找到“状态”和“状态转移方程”。 假如我们考虑求A[1],A[2],…,A[i]的最长非降子序列的长度，其中i&lt;N， 那么上面的问题变成了原问题的一个子问题(问题规模变小了，你可以让i=1,2,3等来分析) 然后我们定义d(i)，表示前i个数中以A[i]结尾的最长非降子序列的长度。OK， 对照“入门”中的简单题，你应该可以估计到这个d(i)就是我们要找的状态。 如果我们把d(1)到d(N)都计算出来，那么最终我们要找的答案就是这里面最大的那个。 状态找到了，下一步找出状态转移方程。</p><p><strong>d(i) = max{1, d(j)+1},其中j&lt;i,A[j]&lt;=A[i]</strong></p><p>用大白话解释就是，想要求d(i)，就把i前面的各个子序列中， 最后一个数不大于A[i]的序列长度加1，然后取出最大的长度即为d(i)。 当然了，有可能i前面的各个子序列中最后一个数都大于A[i]，那么d(i)=1， 即它自身成为一个长度为1的子序列。</p><h3 id="62-Unique-Paths"><a href="#62-Unique-Paths" class="headerlink" title="62. Unique Paths"></a>62. Unique Paths</h3><blockquote><ul><li><p>题目</p><p><script type="math/tex">m \times {n}</script>的矩阵，从（0，0）位置走到（m，n）位置共有多少走法</p></li><li><p>思路</p><p>二维dp，dp[i][j]记录到达位置（i,j）共有的走法，dp[i][j]=dp[i-1][j]+dp[i][j-1].</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">uniquePaths</span><span class="params">(self, m, n)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type m: int</span></div><div class="line"><span class="string">&gt;           :type n: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           dp = [ [ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)] <span class="keyword">for</span> j <span class="keyword">in</span> range(m)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">&gt;               dp[i][<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">&gt;               dp[<span class="number">0</span>][i] = <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):</div><div class="line">&gt;                   dp[i][j] = dp[i<span class="number">-1</span>][j] + dp[i][j<span class="number">-1</span>]</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="63-Unique-Paths-II"><a href="#63-Unique-Paths-II" class="headerlink" title="63. Unique Paths II"></a>63. Unique Paths II</h3><blockquote><ul><li><p>题目</p><p>同样是[m,n]矩阵，但是有1表示障碍，不能过，0为空白可以过去。</p></li><li><p>思路</p><p>dp[i][j]仍然是可能的走法数，但是在初始化时，如果当前是障碍，则dp[][]=0.否则依赖于前面一个。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">uniquePathsWithObstacles</span><span class="params">(self, obstacleGrid)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type obstacleGrid: List[List[int]]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> obstacleGrid : <span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           m = len(obstacleGrid)</div><div class="line">&gt;           n = len(obstacleGrid[<span class="number">0</span>])</div><div class="line">&gt;           dp = [ [ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)] <span class="keyword">for</span> j <span class="keyword">in</span> range(m)]</div><div class="line">&gt;           dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span> <span class="keyword">if</span> <span class="keyword">not</span> obstacleGrid[<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">else</span> <span class="number">0</span> <span class="comment">##update</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m): <span class="comment">##update</span></div><div class="line">&gt;               dp[i][<span class="number">0</span>] = <span class="number">1</span> <span class="keyword">if</span> dp[i<span class="number">-1</span>][<span class="number">0</span>] <span class="keyword">and</span> <span class="keyword">not</span> obstacleGrid[i][<span class="number">0</span>] <span class="keyword">else</span> <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n):<span class="comment">##update</span></div><div class="line">&gt;               dp[<span class="number">0</span>][i] = <span class="number">1</span> <span class="keyword">if</span> dp[<span class="number">0</span>][i<span class="number">-1</span>] <span class="keyword">and</span> <span class="keyword">not</span> obstacleGrid[<span class="number">0</span>][i] <span class="keyword">else</span> <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):</div><div class="line">&gt;                   <span class="keyword">if</span> obstacleGrid[i][j] == <span class="number">1</span>:</div><div class="line">&gt;                       dp[i][j] = <span class="number">0</span></div><div class="line">&gt;                   <span class="keyword">else</span>:</div><div class="line">&gt;                       dp[i][j] = dp[i<span class="number">-1</span>][j] + dp[i][j<span class="number">-1</span>]</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="64-Minimum-Path-Sum"><a href="#64-Minimum-Path-Sum" class="headerlink" title="64. Minimum Path Sum"></a>64. Minimum Path Sum</h3><blockquote><ul><li><p>题目</p><p>从矩阵的左上角走到右下角，矩阵上元素值为代价，求最小代价</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;   Input:</div><div class="line">&gt;   [</div><div class="line">&gt;     [<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>],</div><div class="line">&gt;     [<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>],</div><div class="line">&gt;     [<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>]</div><div class="line">&gt;   ]</div><div class="line">&gt;   Output: <span class="number">7</span></div><div class="line">&gt;   Explanation: Because the path <span class="number">1</span>→<span class="number">3</span>→<span class="number">1</span>→<span class="number">1</span>→<span class="number">1</span> minimizes the sum.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>完全捡苹果题目，需要注意的是创建dp数组时，len_row在外层，len_column在内层。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">minPathSum</span><span class="params">(self, grid)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type grid: List[List[int]]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> grid:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           l1 = len(grid)</div><div class="line">&gt;           l2 = len(grid[<span class="number">0</span>])</div><div class="line">&gt;           dp = [ [ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l2)] <span class="keyword">for</span> j <span class="keyword">in</span> range(l1)] <span class="comment">###l2在里面，l1在外面</span></div><div class="line">&gt;           dp[<span class="number">0</span>][<span class="number">0</span>] = grid[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l1):</div><div class="line">&gt;               dp[i][<span class="number">0</span>] = dp[i<span class="number">-1</span>][<span class="number">0</span>] + grid[i][<span class="number">0</span>]</div><div class="line">&gt;           <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,l2):</div><div class="line">&gt;               dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j<span class="number">-1</span>] + grid[<span class="number">0</span>][j]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l1):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,l2):</div><div class="line">&gt;                   temp = dp[i][j<span class="number">-1</span>] <span class="keyword">if</span> dp[i][j<span class="number">-1</span>]&lt;dp[i<span class="number">-1</span>][j] <span class="keyword">else</span> dp[i<span class="number">-1</span>][j]</div><div class="line">&gt;                   dp[i][j] = temp + grid[i][j]</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="120-Triangle"><a href="#120-Triangle" class="headerlink" title="120. Triangle"></a>120. Triangle</h3><blockquote><ul><li><p>题目</p><p>Given a triangle, find the minimum path sum from top to bottom. Each step you may move to adjacent numbers on the row below.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   [</div><div class="line">&gt;        [<span class="number">2</span>],</div><div class="line">&gt;       [<span class="number">3</span>,<span class="number">4</span>],</div><div class="line">&gt;      [<span class="number">6</span>,<span class="number">5</span>,<span class="number">7</span>],</div><div class="line">&gt;     [<span class="number">4</span>,<span class="number">1</span>,<span class="number">8</span>,<span class="number">3</span>]</div><div class="line">&gt;   ]</div><div class="line">&gt;   The minimum path sum from top to bottom is <span class="number">11</span> (i.e., <span class="number">2</span> + <span class="number">3</span> + <span class="number">5</span> + <span class="number">1</span> = <span class="number">11</span>).</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>与捡苹果很相似，有两种想法，首先dp[i][j]表示到达需要的最少步数。那么状态转移方程有两种想法：</p><p>一种是：dp[i][j]出发有两种走法，dp[i+1][j]和dp[i+1][j+1]，更新到达后的状态dp[i+1][j]和dp[i+1][j+1]；</p><p>另一种是dp[i][j]有两种到达的方法，dp[i-1][j]和dp[i-1][j-1]，更新dp[i][j]</p><p>但是觉得第一种方法比较容易，因为第二种要考虑边界</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">minimumTotal</span><span class="params">(self, triangle)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type triangle: List[List[int]]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> triangle:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           l = len(triangle)</div><div class="line">&gt;           dp = [[ sys.maxsize <span class="keyword">for</span> i <span class="keyword">in</span> range(len(triangle[j])) ] <span class="keyword">for</span> j <span class="keyword">in</span> range(l) ]</div><div class="line">&gt;           dp[<span class="number">0</span>][<span class="number">0</span>] = triangle[<span class="number">0</span>][<span class="number">0</span>]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(l<span class="number">-1</span>):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(len(dp[i])):</div><div class="line">&gt;                   dp[i+<span class="number">1</span>][j] = dp[i+<span class="number">1</span>][j] <span class="keyword">if</span> dp[i+<span class="number">1</span>][j] &lt; (dp[i][j]+triangle[i+<span class="number">1</span>][j]) <span class="keyword">else</span> (dp[i][j]+triangle[i+<span class="number">1</span>][j])</div><div class="line">&gt;                   dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = dp[i+<span class="number">1</span>][j+<span class="number">1</span>] <span class="keyword">if</span> dp[i+<span class="number">1</span>][j+<span class="number">1</span>] &lt; (dp[i][j]+triangle[i+<span class="number">1</span>][j+<span class="number">1</span>]) <span class="keyword">else</span> (dp[i][j]+triangle[i+<span class="number">1</span>][j+<span class="number">1</span>])</div><div class="line">&gt;           <span class="keyword">return</span> min(dp[<span class="number">-1</span>])</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="213-House-Robber-II"><a href="#213-House-Robber-II" class="headerlink" title="213. House Robber II"></a>213. House Robber II</h3><blockquote><ul><li><p>题目</p><p>You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed. All houses at this place are <strong>arranged in a circle.</strong> That means the first house is the neighbor of the last one. Meanwhile, adjacent houses have security system connected and <strong>it will automatically contact the police if two adjacent houses were broken into on the same night</strong>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>]</div><div class="line">&gt;   Output: <span class="number">3</span></div><div class="line">&gt;   Explanation: You cannot rob house <span class="number">1</span> (money = <span class="number">2</span>) and then rob house <span class="number">3</span> (money = <span class="number">2</span>),</div><div class="line">&gt;                because they are adjacent houses.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</div><div class="line">&gt;   Output: <span class="number">4</span></div><div class="line">&gt;   Explanation: Rob house <span class="number">1</span> (money = <span class="number">1</span>) and then rob house <span class="number">3</span> (money = <span class="number">3</span>).</div><div class="line">&gt;                Total amount you can rob = <span class="number">1</span> + <span class="number">3</span> = <span class="number">4</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>房子排成了一个圆圈，则如果抢了第一家，就不能抢最后一家，因为首尾相连了，所以第一家和最后一家只能抢其中的一家，或者都不抢。则把数组分成两份，分别使用动态规划求解。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">rob</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> nums:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">if</span> len(nums)==<span class="number">1</span>:<span class="keyword">return</span> nums[<span class="number">0</span>]</div><div class="line">&gt;           l = len(nums)</div><div class="line">&gt;           dp1 = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           dp2 = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           dp1[<span class="number">1</span>] = nums[<span class="number">0</span>]</div><div class="line">&gt;           dp2[<span class="number">1</span>] = nums[<span class="number">1</span>]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,l):</div><div class="line">&gt;               dp1[i] = max(dp1[i<span class="number">-1</span>],dp1[i<span class="number">-2</span>]+nums[i<span class="number">-1</span>])</div><div class="line">&gt;               dp2[i] = max(dp2[i<span class="number">-1</span>],dp2[i<span class="number">-2</span>]+nums[i])</div><div class="line">&gt;           <span class="keyword">return</span> max(dp1[<span class="number">-1</span>],dp2[<span class="number">-1</span>])</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="279-Perfect-Squares"><a href="#279-Perfect-Squares" class="headerlink" title="279. Perfect Squares"></a>279. Perfect Squares</h3><blockquote><ul><li><p>题目</p><p>Given a positive integer <em>n</em>, find the least number of perfect square numbers (for example, <code>1, 4, 9, 16, ...</code>) which sum to <em>n</em>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;   Example1:</div><div class="line">&gt;   Input: n = <span class="number">12</span></div><div class="line">&gt;   Output: <span class="number">3</span> </div><div class="line">&gt;   Explanation: <span class="number">12</span> = <span class="number">4</span> + <span class="number">4</span> + <span class="number">4</span>.</div><div class="line">&gt;   Example2:</div><div class="line">&gt;   Input: n = <span class="number">13</span></div><div class="line">&gt;   Output: <span class="number">2</span></div><div class="line">&gt;   Explanation: <span class="number">13</span> = <span class="number">4</span> + <span class="number">9</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li><li><p>思路</p><p>这道题就是找零钱。只是零钱的选取是有限的，而这边完美数是无限的，但是要小于n。故遍历零钱数组的操作，需要变成while循环，遍历每一个完美数。</p><p>dp[i]表示最少需要的数字凑成这个数，直到n。两层循环。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="comment">###LeetCode下Python3是超时，Python2可以</span></div><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">numSquares</span><span class="params">(self, n)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type n: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           dp = [sys.maxsize <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">0</span>] = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>):</div><div class="line">&gt;               j = <span class="number">1</span></div><div class="line">&gt;               <span class="keyword">while</span> j*j&lt;=i:</div><div class="line">&gt;                   dp[i] = min(dp[i],dp[i-j*j]+<span class="number">1</span>)</div><div class="line">&gt;                   j = j + <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;           </div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="300-Longest-Increasing-Subsequence"><a href="#300-Longest-Increasing-Subsequence" class="headerlink" title="300. Longest Increasing Subsequence"></a>300. Longest Increasing Subsequence</h3><blockquote><ul><li><p>题目</p><p>Given an unsorted array of integers, find the length of longest increasing subsequence.</p><p>For example,<br>Given <code>[10, 9, 2, 5, 3, 7, 101, 18]</code>,<br>The longest increasing subsequence is <code>[2, 3, 7, 101]</code>, therefore the length is <code>4</code>. Note that there may be more than one LIS combination, it is only necessary for you to return the length.</p></li><li><p>思路</p><p>如果动态规划，则时间复杂度是$O(n^2)$, 第一层循环来更新dp，第二个循环来遍历之前的数字且必须比当前小，不能等于。如样例，<code>[10,9,2,5,3,7,101,18]</code>，我们把原数组分成长度<code>2,3,4,5,...,l</code>的子数组求解最长上升，最终选择最佳的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>]，lis = <span class="number">1</span></div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>],lis = <span class="number">1</span></div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>],lis = <span class="number">2</span></div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>],lis = <span class="number">2</span></div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>],lis = <span class="number">3</span>  [<span class="number">2</span>,<span class="number">5</span>,<span class="number">7</span>]</div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">101</span>],lis = <span class="number">3</span></div><div class="line">&gt;   [<span class="number">10</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">101</span>, <span class="number">18</span>], lis = <span class="number">4</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  使用dp辅助数组记录每个元素与之前元素形成的最长上升。</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>]，dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],lis = max(dp)=<span class="number">1</span>, </div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>],dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],lis = max(dp)=<span class="number">1</span>, </div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>],dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],lis = max(dp)=<span class="number">2</span>, </div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>],dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],lis = max(dp)=<span class="number">2</span>, </div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>],dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],lis = max(dp)=<span class="number">3</span>, </div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">101</span>],dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],lis = max(dp)=<span class="number">2</span>, </div><div class="line">&gt;   [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">101</span>,<span class="number">18</span>],dp = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>],lis = max(dp)=<span class="number">4</span>, </div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> nums:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           l = len(nums)</div><div class="line">&gt;           dp = [<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</div><div class="line">&gt;                   <span class="keyword">if</span> nums[j]&lt;nums[i] <span class="keyword">and</span> dp[i] &lt; dp[j]+<span class="number">1</span>:</div><div class="line">&gt;                       dp[i] = dp[j]+<span class="number">1</span></div><div class="line">&gt;           <span class="keyword">return</span> max(dp)</div><div class="line">&gt;           </div><div class="line">&gt;           </div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="comment">###二分查找的复杂度是O(log(n)),则整体复杂度是O(nlong(n))</span></div><div class="line">&gt;   <span class="keyword">import</span> bisect</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLIS</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           sorted_list = []</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> nums:</div><div class="line">&gt;               pos = bisect.bisect_left(sorted_list,i)</div><div class="line">&gt;               <span class="keyword">if</span> pos==len(sorted_list):</div><div class="line">&gt;                   sorted_list.append(i)</div><div class="line">&gt;               <span class="keyword">else</span>:</div><div class="line">&gt;                   sorted_list[pos] = i</div><div class="line">&gt;           <span class="keyword">return</span> len(sorted_list)</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><h3 id="322-Coin-Change"><a href="#322-Coin-Change" class="headerlink" title="322. Coin Change"></a>322. Coin Change</h3><blockquote><ul><li><p>题目</p><p>换硬币，但是可能失败，即没有1元银币，导致凑不够。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;   coins = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], amount = <span class="number">11</span></div><div class="line">&gt;   <span class="keyword">return</span> <span class="number">3</span> (<span class="number">11</span> = <span class="number">5</span> + <span class="number">5</span> + <span class="number">1</span>)</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;   coins = [<span class="number">2</span>], amount = <span class="number">3</span></div><div class="line">&gt;   <span class="keyword">return</span> -<span class="number">1</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>两层遍历，dp[i]表示凑够i元需要的最少银币数量。疑惑点是如何处理凑不够的情况，返回-1.事实上，只要有提供1元银币，一定可以凑够。那就判断dp[1]是否有更新。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">coinChange</span><span class="params">(self, coins, amount)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type coins: List[int]</span></div><div class="line"><span class="string">&gt;           :type amount: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           dp = [sys.maxsize <span class="keyword">for</span> _ <span class="keyword">in</span> range(amount+<span class="number">1</span>)]</div><div class="line">&gt;           re[<span class="number">0</span>] = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,amount+<span class="number">1</span>):</div><div class="line">&gt;               <span class="keyword">for</span> coin <span class="keyword">in</span> coins:</div><div class="line">&gt;                   <span class="keyword">if</span> coin &lt;= i:</div><div class="line">&gt;                       <span class="keyword">if</span> dp[i] &gt; dp[i-coin]+<span class="number">1</span>:</div><div class="line">&gt;                           dp[i] = dp[i-coin]+<span class="number">1</span></div><div class="line">&gt;           <span class="keyword">if</span> dp[<span class="number">-1</span>] == sys.maxsize:</div><div class="line">&gt;               <span class="keyword">return</span> <span class="number">-1</span></div><div class="line">&gt;           <span class="keyword">else</span>:</div><div class="line">&gt;               <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="338-Counting-Bits"><a href="#338-Counting-Bits" class="headerlink" title="338. Counting Bits"></a>338. Counting Bits</h3><blockquote><ul><li><p>题目</p><p>Given a non negative integer number <strong>num</strong>. For every numbers <strong>i</strong> in the range <strong>0 ≤ i ≤ num</strong> calculate the number of 1’s in their binary representation and return them as an array.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;   For num = <span class="number">5</span> you should <span class="keyword">return</span> [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>].</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>可以使用动态规划，时间复杂度是$O(n)$. 显然dp[i]表示数字里面含有的1的个数。但是状态转移如何表示？计算i的时候，如何利用dp[0]-dp[i-1]呢？利用➗2，即右移操作。一个数右移一位，变成较小的数字，失去的是最右的1或者0.则状态转移变成了</p><p>dp[i] = dp[i&gt;&gt;1] + i%2</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">countBits</span><span class="params">(self, num)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type num: int</span></div><div class="line"><span class="string">&gt;           :rtype: List[int]</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num+<span class="number">1</span>)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,num+<span class="number">1</span>):</div><div class="line">&gt;               dp[i] = dp[i//<span class="number">2</span>] + i%<span class="number">2</span></div><div class="line">&gt;           <span class="keyword">return</span> dp</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="comment">###利用Python内置函数。</span></div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">countBits</span><span class="params">(self, num)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type num: int</span></div><div class="line"><span class="string">&gt;           :rtype: List[int]</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">return</span> [bin(x).count(<span class="string">'1'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(num+<span class="number">1</span>)]</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><h3 id="377-Combination-Sum-IV"><a href="#377-Combination-Sum-IV" class="headerlink" title="377. Combination Sum IV"></a>377. Combination Sum IV</h3><blockquote><ul><li><p>题目</p><p>Given an integer array with all positive numbers and no duplicates, find the number of possible combinations that add up to a positive integer target.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;   nums = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">&gt;   target = <span class="number">4</span></div><div class="line">&gt;   </div><div class="line">&gt;   The possible combination ways are:</div><div class="line">&gt;   (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</div><div class="line">&gt;   (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</div><div class="line">&gt;   (<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</div><div class="line">&gt;   (<span class="number">1</span>, <span class="number">3</span>)</div><div class="line">&gt;   (<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</div><div class="line">&gt;   (<span class="number">2</span>, <span class="number">2</span>)</div><div class="line">&gt;   (<span class="number">3</span>, <span class="number">1</span>)</div><div class="line">&gt;   </div><div class="line">&gt;   Note that different sequences are counted as different combinations.</div><div class="line">&gt;   </div><div class="line">&gt;   Therefore the output is <span class="number">7</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  <strong>Follow up:</strong> What if negative numbers are allowed in the given array? How does it change the problem? What limitation we need to add to the question to allow negative numbers?</p><ul><li><p>思路</p><p>就是找银币了。但是不同于找硬币，其状态转移是dp[i] = dp[i-j] + 1；该问题的状态转移是dp[i] += dp[i-1]</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">combinationSum4</span><span class="params">(self, nums, target)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :type target: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> target == <span class="number">0</span> <span class="keyword">or</span> <span class="keyword">not</span> nums: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">if</span> nums[<span class="number">0</span>]&lt;<span class="number">0</span>:</div><div class="line">&gt;               dif = <span class="number">1</span> - nums[<span class="number">0</span>]</div><div class="line">&gt;               target += dif</div><div class="line">&gt;               <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</div><div class="line">&gt;                   nums[i] += dif</div><div class="line">&gt;           dp = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(target + <span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, target + <span class="number">1</span>):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> nums:</div><div class="line">&gt;                   <span class="keyword">if</span> j &lt;= i:</div><div class="line">&gt;                       dp[i] += dp[i - j]</div><div class="line">&gt;           print(dp)</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="Integer-Break-343"><a href="#Integer-Break-343" class="headerlink" title="Integer Break(343)"></a>Integer Break(343)</h3><blockquote><ul><li><p>题目</p><p>Given a positive integer <em>n</em>, break it into the sum of <strong>at least</strong> two positive integers and maximize the product of those integers. Return the maximum product you can get.</p><p>For example, given <em>n</em> = 2, return 1 (2 = 1 + 1); given <em>n</em> = 10, return 36 (10 = 3 + 3 + 4).</p><p><strong>Note</strong>: You may assume that <em>n</em> is not less than 2 and not larger than 58.</p></li><li><p>思路</p><p>动态规划时间复杂度$O(n)$.难点在于状态转移方程，dp[i]表示i的因数的最大乘积。想到dp[i] = max( dp[i] , dp[j] <em> (i-j) )，其实本质就是将i先分解成（【j】  </em> 【i-j】），但是j可以再分解，同样i-j也可以再分解，选择大的。</p><p>即max（ j , dp[j] ）* max（i-j ，dp[i-j]）</p><p>故状态转移方程是：</p><p>dp[i] = max ( dp[i] , max（ j , dp[j] ）* max（i-j ，dp[i-j]） )</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">integerBreak</span><span class="params">(self, n)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type n: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           dp = [<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n + <span class="number">1</span>):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i):</div><div class="line">&gt;                   dp[i] = max (dp[i], max(dp[j],j)*max(dp[i-j],i-j))</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="Word-Break-139"><a href="#Word-Break-139" class="headerlink" title="Word Break(139)"></a>Word Break(139)</h3><ul><li><p>题目</p><p>Given a <strong>non-empty</strong> string <em>s</em> and a dictionary <em>wordDict</em> containing a list of <strong>non-empty</strong> words, determine if <em>s</em> can be segmented into a space-separated sequence of one or more dictionary words.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Example1:</div><div class="line"></div><div class="line">Input: s = <span class="string">"leetcode"</span>, wordDict = [<span class="string">"leet"</span>, <span class="string">"code"</span>]</div><div class="line">Output: <span class="keyword">true</span></div><div class="line">Explanation: Return <span class="keyword">true</span> because <span class="string">"leetcode"</span> can be segmented as <span class="string">"leet code"</span>.</div><div class="line">    </div><div class="line">Example2:</div><div class="line"></div><div class="line">Input: s = <span class="string">"applepenapple"</span>, wordDict = [<span class="string">"apple"</span>, <span class="string">"pen"</span>]</div><div class="line">Output: <span class="keyword">true</span></div><div class="line">Explanation: Return <span class="keyword">true</span> because <span class="string">"applepenapple"</span> can be segmented as <span class="string">"apple pen apple"</span>. Note that you are allowed to reuse a dictionary word.</div><div class="line"></div><div class="line">Example3:</div><div class="line">Input: s = <span class="string">"catsandog"</span>, wordDict = [<span class="string">"cats"</span>, <span class="string">"dog"</span>, <span class="string">"sand"</span>, <span class="string">"and"</span>, <span class="string">"cat"</span>]</div><div class="line">Output: <span class="keyword">false</span></div></pre></td></tr></table></figure></li><li><p>思路</p><p>$O(n^2)$的复杂度。</p><p>dp[i]表示子串$s[0,1…i-1]$能否由字典表示</p><p>dp[i] = true if dp[j]==true and s[j,j+1,…,i-1] is in dic.</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordBreak</span><span class="params">(self, s, wordDict)</span>:</span></div><div class="line">        dp = [<span class="keyword">False</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)+<span class="number">1</span>)]</div><div class="line">        dp[<span class="number">0</span>] = <span class="keyword">True</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(s)+<span class="number">1</span>):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</div><div class="line">                <span class="keyword">if</span> dp[j] <span class="keyword">and</span> s[j:i] <span class="keyword">in</span> wordDict:</div><div class="line">                    dp[i] = <span class="keyword">True</span></div><div class="line">                    <span class="keyword">break</span></div><div class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>]</div></pre></td></tr></table></figure></li></ul><h2 id="中级"><a href="#中级" class="headerlink" title="中级"></a>中级</h2><p>接下来，让我们来看看如何解决二维的DP问题。</p><p>平面上有N*M个格子，每个格子中放着一定数量的苹果。你从左上角的格子开始， 每一步只能向下走或是向右走，每次走到一个格子上就把格子里的苹果收集起来， 这样下去，你最多能收集到多少个苹果。</p><p>解这个问题与解其它的DP问题几乎没有什么两样。第一步找到问题的“状态”， 第二步找到“状态转移方程”，然后基本上问题就解决了。</p><p>首先，我们要找到这个问题中的“状态”是什么？我们必须注意到的一点是， 到达一个格子的方式最多只有两种：从左边来的(除了第一列)和从上边来的(除了第一行)。 因此为了求出到达当前格子后最多能收集到多少个苹果， 我们就要先去考察那些能到达当前这个格子的格子，到达它们最多能收集到多少个苹果。 (是不是有点绕，但这句话的本质其实是DP的关键：欲求问题的解，先要去求子问题的解)</p><p>经过上面的分析，很容易可以得出问题的状态和状态转移方程。 状态S[i][j]表示我们走到(i, j)这个格子时，最多能收集到多少个苹果。那么， 状态转移方程如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">S[i][j]=A[i][j] + max(S[i-<span class="number">1</span>][j], <span class="keyword">if</span> i&gt;<span class="number">0</span> ; S[i][j-<span class="number">1</span>], <span class="keyword">if</span> j&gt;<span class="number">0</span>)</div></pre></td></tr></table></figure><p>$S[i][j]$有两种计算方式：1.对于每一行，从左向右计算，然后从上到下逐行处理；2. 对于每一列，从上到下计算，然后从左向右逐列处理。 这样做的目的是为了在计算$S[i][j]$时，$S[i-1][j]$和$S[i][j-1]$都已经计算出来了。</p><h3 id="Wiggle-Subsequence"><a href="#Wiggle-Subsequence" class="headerlink" title="Wiggle Subsequence"></a><a href="https://leetcode.com/problems/wiggle-subsequence/description/" target="_blank" rel="noopener">Wiggle Subsequence</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>A sequence of numbers is called a <strong>wiggle sequence</strong> if the differences between successive numbers strictly alternate between positive and negative. The first difference (if one exists) may be either positive or negative. A sequence with fewer than two elements is trivially a wiggle sequence.</p><p>For example, <code>[1,7,4,9,2,5]</code> is a wiggle sequence because the differences (6,-3,5,-7,3) are alternately positive and negative. In contrast, <code>[1,4,7,2,5]</code> and <code>[1,7,4,5,5]</code> are not wiggle sequences, the first because its first two differences are positive and the second because its last difference is zero.</p><p>Given a sequence of integers, return the length of the longest subsequence that is a wiggle sequence. A subsequence is obtained by deleting some number of elements (eventually, also zero) from the original sequence, leaving the remaining elements in their original order.</p><p><strong>Examples:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>]</div><div class="line">Output: <span class="number">6</span></div><div class="line">The entire sequence is a wiggle sequence.</div><div class="line"></div><div class="line">Input: [<span class="number">1</span>,<span class="number">17</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">13</span>,<span class="number">15</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">16</span>,<span class="number">8</span>]</div><div class="line">Output: <span class="number">7</span></div><div class="line">There are several subsequences that achieve <span class="keyword">this</span> length. One is [<span class="number">1</span>,<span class="number">17</span>,<span class="number">10</span>,<span class="number">13</span>,<span class="number">10</span>,<span class="number">16</span>,<span class="number">8</span>].</div><div class="line"></div><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</div><div class="line">Output: <span class="number">2</span></div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p><a href="http://bookshadow.com/weblog/2016/07/21/leetcode-wiggle-subsequence/" target="_blank" rel="noopener">参考</a></p><p>双重循环遍历，时间复杂度$O(n^2)$.</p><p>对于每一个元素，考虑与之前的元素构成最长的wiggle，有两种情况。一是当前元素是上升点，即当前元素值大于前面的，那么这样最长的长度是之前点作为下架点的长度+1；二是当前元素是下降点，即元素值小于前面的，那么长度是之前点作为上升点的长度+1。那么我们设置两个dp数组</p><p>dp_inc[]:如果与之前构成wiggle，当前元素是上升的点时，子序列的最长长度，状态更新</p><p>dp_inc[i] = max ( dp_inc[i] , dp_dec[j] + 1 )   if nums[i]&gt;nums[j]</p><p> dp_dec[]:如果与之前构成wiggle，当前元素是下降的点时，子序列的最长长度，状态更新</p><p>dp_dec[i] = max ( dp_dec[i] , dp_inc[j] + 1 )   if nums[i] &lt; nums[j]</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wiggleMaxLength</span><span class="params">(self, nums)</span>:</span></div><div class="line">        l = len(nums)</div><div class="line">        dp_inc, dp_dec = [<span class="number">1</span>]*l,[<span class="number">1</span>]*l</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l):</div><div class="line">            target = nums[i]</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</div><div class="line">                cur = nums[j]</div><div class="line">                <span class="keyword">if</span> target&gt;cur:</div><div class="line">                    dp_inc[i] = max(dp_inc[i],dp_dec[j]+<span class="number">1</span>)</div><div class="line">                <span class="keyword">elif</span> target&lt;cur:</div><div class="line">                    dp_dec[i] = max ( dp_dec[i] , dp_inc[j] + <span class="number">1</span> )</div><div class="line">        <span class="keyword">return</span> max(dp_inc[<span class="number">-1</span>],dp_dec[<span class="number">-1</span>]) <span class="keyword">if</span> l <span class="keyword">else</span> <span class="number">0</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Ones-and-Zeroes-474"><a href="#Ones-and-Zeroes-474" class="headerlink" title="Ones and Zeroes(474)"></a>Ones and Zeroes(474)</h3><ul><li><p>题目</p><p>In the computer world, use restricted resource you have to generate maximum benefit is what we always want to pursue.</p><p>For now, suppose you are a dominator of <strong>m</strong> <code>0s</code> and <strong>n</strong> <code>1s</code> respectively. On the other hand, there is an array with strings consisting of only <code>0s</code> and <code>1s</code>.</p><p>Now your task is to find the maximum number of strings that you can form with given <strong>m</strong> <code>0s</code> and <strong>n</strong> <code>1s</code>. Each <code>0</code> and <code>1</code> can be used at most <strong>once</strong>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Example1:</div><div class="line"></div><div class="line">Input: Array = &#123;<span class="string">"10"</span>, <span class="string">"0001"</span>, <span class="string">"111001"</span>, <span class="string">"1"</span>, <span class="string">"0"</span>&#125;, m = <span class="number">5</span>, n = <span class="number">3</span></div><div class="line">Output: <span class="number">4</span></div><div class="line"></div><div class="line">Explanation: This are totally <span class="number">4</span> strings can be formed by the using of <span class="number">5</span> <span class="number">0</span>s and <span class="number">3</span> <span class="number">1</span>s, which are “<span class="number">10</span>,”<span class="number">0001</span>”,”<span class="number">1</span>”,”<span class="number">0</span>”</div><div class="line"></div><div class="line">Example2:</div><div class="line"></div><div class="line">Input: Array = &#123;<span class="string">"10"</span>, <span class="string">"0"</span>, <span class="string">"1"</span>&#125;, m = <span class="number">1</span>, n = <span class="number">1</span></div><div class="line">Output: <span class="number">2</span></div><div class="line"></div><div class="line">Explanation: You could form <span class="string">"10"</span>, but then you<span class="string">'d have nothing left. Better form "0" and "1".</span></div></pre></td></tr></table></figure></li><li><p>思路</p></li><li><p>代码</p></li></ul><h3 id="Maximal-Square-221"><a href="#Maximal-Square-221" class="headerlink" title="Maximal Square(221)"></a><em><u>Maximal Square(221)</u></em></h3><blockquote><ul><li><p>题目</p><p>Given a 2D binary matrix filled with 0’s and 1’s, find the largest square containing only 1’s and return its area.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: </div><div class="line">&gt;   </div><div class="line">&gt;   <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span></div><div class="line">&gt;   <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line">&gt;   <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></div><div class="line">&gt;   <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span></div><div class="line">&gt;   </div><div class="line">&gt;   Output: <span class="number">4</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>想不到用dp解决。</p><p>dp[i][j]表示以点[i][j]为右下角顶点的正方形的边长。为了构成全1正方形，该点必须是1，那么如何利用已经计算好的dp状态更新当前dp状态呢？即如何根据该点拓展正方形？与它上方，左方和左上方三个点有关。</p><p>当我们判断以某个点为正方形右下角时最大的正方形时，那它的上方，左方和左上方三个点也一定是某个正方形的右下角，否则该点为右下角的正方形最大就是它自己了。这是定性的判断，那具体的最大正方形边长呢？我们知道，该点为右下角的正方形的最大边长，最多比它的上方，左方和左上方为右下角的正方形的边长多1，最好的情况是是它的上方，左方和左上方为右下角的正方形的大小都一样的，这样加上该点就可以构成一个更大的正方形。但如果它的上方，左方和左上方为右下角的正方形的大小不一样，合起来就会缺了某个角落，这时候只能取那三个正方形中最小的正方形的边长加1了。假设<code>dp[i][j]</code>表示以i,j为右下角的正方形的最大边长，则有</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;   dp[i][j] = min(dp[i-<span class="number">1</span>][j-<span class="number">1</span>], dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>]) + <span class="number">1</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">maximalSquare</span><span class="params">(self, matrix)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type matrix: List[List[str]]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> matrix:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           row = len(matrix)</div><div class="line">&gt;           column = len(matrix[<span class="number">0</span>])</div><div class="line">&gt;           dp = [ [ <span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(column) ] <span class="keyword">for</span> i <span class="keyword">in</span> range(row) ]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</div><div class="line">&gt;               dp[i][<span class="number">0</span>] = int(matrix[i][<span class="number">0</span>])</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(column):</div><div class="line">&gt;               dp[<span class="number">0</span>][i] = int(matrix[<span class="number">0</span>][i])</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,row):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,column):</div><div class="line">&gt;                   <span class="keyword">if</span> matrix[i][j]==<span class="string">'1'</span>:</div><div class="line">&gt;                       dp[i][j] = min(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>], dp[i<span class="number">-1</span>][j<span class="number">-1</span>]) + <span class="number">1</span></div><div class="line">&gt;           re = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</div><div class="line">&gt;               re = max(re,max(dp[i]))</div><div class="line">&gt;           <span class="keyword">return</span> re * re <span class="comment">### re != max(max(dp))</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="96-Unique-Binary-Search-Trees"><a href="#96-Unique-Binary-Search-Trees" class="headerlink" title="96. Unique Binary Search Trees"></a>96. Unique Binary Search Trees</h3><blockquote><ul><li><p>题目</p><p>Given <em>n</em>, how many structurally unique <strong>BST’s</strong> (binary search trees) that store values 1 … <em>n</em>?</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="number">3</span></div><div class="line">&gt;   Output: <span class="number">5</span></div><div class="line">&gt;   Explanation:</div><div class="line">&gt;   Given n = <span class="number">3</span>, there are a total of <span class="number">5</span> unique BST<span class="string">'s:</span></div><div class="line"><span class="string">&gt;   </span></div><div class="line"><span class="string">&gt;      1         3     3      2      1</span></div><div class="line"><span class="string">&gt;       \       /     /      / \      \</span></div><div class="line"><span class="string">&gt;        3     2     1      1   3      2</span></div><div class="line"><span class="string">&gt;       /     /       \                 \</span></div><div class="line"><span class="string">&gt;      2     1         2                 3</span></div><div class="line"><span class="string">&gt;</span></div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>不会想到用动态规划。但是知道可以dp，有大致的想法，我最初认为dp[i]表示数字1，2，…，i能形成的二叉树的数量。这样在状态转移的时候就有麻烦了。其实，与数字大小无关，即dp[i]也可以是9，10，11，…，i+9形成的二叉树，实际上，dp[i]状态表示的是i个有序上升点能形成的二叉树个数。这样问题就简单了。状态转移：</p><p>dp[i] += dp[j-1] * dp[i-j] 遍历j从1-i，表示以j为顶点形成的二叉树。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">numTrees</span><span class="params">(self, n)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type n: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> n==<span class="number">0</span>:<span class="keyword">return</span> n</div><div class="line">&gt;           dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           dp[<span class="number">1</span>] = <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,n+<span class="number">1</span>):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,i+<span class="number">1</span>):</div><div class="line">&gt;                   dp[i] += dp[j<span class="number">-1</span>] * dp[i-j] <span class="comment">## += 且 j-1</span></div><div class="line">&gt;           print(dp)</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="91-Decode-Ways"><a href="#91-Decode-Ways" class="headerlink" title="91. Decode Ways"></a>91. Decode Ways</h3><blockquote><ul><li><p>题目</p><p>A message containing letters from <code>A-Z</code> is being encoded to numbers using the following mapping:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="string">'A'</span> -&gt; <span class="number">1</span></div><div class="line">&gt;   <span class="string">'B'</span> -&gt; <span class="number">2</span></div><div class="line">&gt;   ...</div><div class="line">&gt;   <span class="string">'Z'</span> -&gt; <span class="number">26</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  Given a <strong>non-empty</strong> string containing only digits, determine the total number of ways to decode it.</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="string">"12"</span></div><div class="line">&gt;   Output: <span class="number">2</span></div><div class="line">&gt;   Explanation: It could be decoded as <span class="string">"AB"</span> (<span class="number">1</span> <span class="number">2</span>) or <span class="string">"L"</span> (<span class="number">12</span>).</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="string">"226"</span></div><div class="line">&gt;   Output: <span class="number">3</span></div><div class="line">&gt;   Explanation: It could be decoded as <span class="string">"BZ"</span> (<span class="number">2</span> <span class="number">26</span>), <span class="string">"VF"</span> (<span class="number">22</span> <span class="number">6</span>), or <span class="string">"BBF"</span> (<span class="number">2</span> <span class="number">2</span> <span class="number">6</span>).</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>可以想到用dp，但是一直纠结于dp的维数。</p><p>dp[i]表示1-i字符串的decode种类，那么状态转移方程呢？与具体的字符串有关。因为数字只能是一位或者两位，且一位的数字不能是0.那么如果当前的数字是0，那么不能以一位解码；否则可以以该种方式解码，则dp[i] += dp[i-1];如果i-1和i的两位数是合法的，即位于10和26之间，那么是可以以两位数解码，dp[i] += dp[i-2] </p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;   dp[i] = </div><div class="line">&gt;       <span class="number">0</span>                   <span class="keyword">if</span> s[i] and s[i-<span class="number">1</span>][i] invalid</div><div class="line">&gt;       dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>]   <span class="keyword">if</span> s[i] and s[i-<span class="number">1</span>][i] valid</div><div class="line">&gt;       dp[i-<span class="number">1</span>]             <span class="keyword">if</span> s[i] valid</div><div class="line">&gt;       dp[i-<span class="number">2</span>]             <span class="keyword">if</span> s[i-<span class="number">1</span>][i] valid</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">numDecodings</span><span class="params">(self, s)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type s: str</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> s: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           l = len(s)</div><div class="line">&gt;           dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(l + <span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           dp[<span class="number">1</span>] = <span class="number">1</span> <span class="keyword">if</span> s[<span class="number">0</span>] !=<span class="string">'0'</span> <span class="keyword">else</span> <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, l):</div><div class="line">&gt;               one_digit = s[i]</div><div class="line">&gt;               two_digit = int(s[i - <span class="number">1</span>:i + <span class="number">1</span>])</div><div class="line">&gt;               <span class="keyword">if</span> one_digit != <span class="string">'0'</span>:</div><div class="line">&gt;                   dp[i + <span class="number">1</span>] += dp[i]</div><div class="line">&gt;               <span class="keyword">if</span> two_digit &lt;= <span class="number">26</span> <span class="keyword">and</span> two_digit &gt;= <span class="number">10</span>:</div><div class="line">&gt;                   dp[i + <span class="number">1</span>] += dp[i - <span class="number">1</span>]</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="368-Largest-Divisible-Subset"><a href="#368-Largest-Divisible-Subset" class="headerlink" title="368. Largest Divisible Subset"></a>368. Largest Divisible Subset</h3><blockquote><ul><li><p>题目</p><p>Given a set of <strong>distinct</strong> positive integers, find the largest subset such that every pair (Si, Sj) of elements in this subset satisfies: Si % Sj = 0 or Sj % Si = 0.</p><p>If there are multiple solutions, return any subset is fine.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   nums: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">&gt;   </div><div class="line">&gt;   Result: [<span class="number">1</span>,<span class="number">2</span>] (of course, [<span class="number">1</span>,<span class="number">3</span>] will also be ok)</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   nums: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>]</div><div class="line">&gt;   </div><div class="line">&gt;   Result: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>题目是很复杂了，而且不会想到用dp，因为这边求解的是list，而非极值。看了网上代码，还是em。</p><p>首先不论返回的list，单纯看最大的可以是多大，因为取余有个性质，i&gt;j&gt;k, i % j = 0，j % k = 0, 那么i % k = 0.所以给了dp状态转移的思路，</p><p>dp[i]表示数字i以及i之前的数组能形成的最大子集，状态的更新只需要考虑i之前的数字能否取余为零</p><p>dp[i] = max( dp[i] , dp[j] +1 ) if i % j == 0</p><p>接下来难点就是如何记录结果list，当然我们可以设置一个二维数组来保存，一旦更新dp的时候，也要更新对应的list，即把i值加上上一个list中。但是我们可以把$O(n^2)$的空间复杂度变成$D(n)$, pre[i]记录i之前能取余且得到最大的子集的元素的下标。 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   https:<span class="comment">//www.cnblogs.com/godlei/p/5621990.html</span></div><div class="line">&gt;   </div><div class="line">&gt;   如果a%b==<span class="number">0</span>，则a=mb，所以如果把数组排序后如果a%b==<span class="number">0</span>，且b%c==<span class="number">0</span>则a%c==<span class="number">0</span>。这就为用动态规划实现提供了可能性。设置一个数组result，result[i]表示i出包含的满足条件的子集个数。则如果nums[i]%nums[j]==<span class="number">0</span>，则result[i]=result[j]+<span class="number">1</span>;同时由于函数要返回的是一个List，所以我们要保存最长集合的路径。这个功能可以通过设置一个pre数组保存能被nums[i]整除的上一个数的索引。并在保存max值的同时保存max所在的位置maxIndex即可。</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">largestDivisibleSubset</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: List[int]</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           l = len(nums)</div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> nums: <span class="keyword">return</span> []</div><div class="line">&gt;           nums = sorted(nums)</div><div class="line">&gt;           dp = [<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)]  <span class="comment">##表示集合元素个数,初始是1，表示至少自己单独成为合法的集合</span></div><div class="line">&gt;           size_max = <span class="number">1</span></div><div class="line">&gt;           index_max = <span class="number">0</span></div><div class="line">&gt;           pre = [<span class="number">-1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, l):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i):</div><div class="line">&gt;                   <span class="keyword">if</span> nums[i] % nums[j] == <span class="number">0</span> <span class="keyword">and</span> dp[i] &lt; dp[j] + <span class="number">1</span>:</div><div class="line">&gt;                       dp[i] = dp[j] + <span class="number">1</span></div><div class="line">&gt;                       pre[i] = j <span class="comment">##</span></div><div class="line">&gt;                       <span class="keyword">if</span> dp[i]&gt;size_max: <span class="comment">## 等于也ok</span></div><div class="line">&gt;                           size_max = dp[i]</div><div class="line">&gt;                           index_max = i</div><div class="line">&gt;           re = []</div><div class="line">&gt;           <span class="keyword">while</span> index_max != <span class="number">-1</span>:</div><div class="line">&gt;               re.append(nums[index_max])</div><div class="line">&gt;               index_max = pre[index_max]</div><div class="line">&gt;           <span class="keyword">return</span> re</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="740-Delete-and-Earn"><a href="#740-Delete-and-Earn" class="headerlink" title="740. Delete and Earn"></a>740. Delete and Earn</h3><blockquote><p>题目</p><p>Given an array <code>nums</code> of integers, you can perform operations on the array.</p><p>In each operation, you pick any <code>nums[i]</code> and delete it to earn <code>nums[i]</code> points. After, you must delete <strong>every</strong> element equal to <code>nums[i] - 1</code> or <code>nums[i] + 1</code>.</p><p>You start with 0 points. Return the maximum number of points you can earn by applying such operations.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt; Example1:</div><div class="line">&gt; Input: nums = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>]</div><div class="line">&gt; Output: <span class="number">6</span></div><div class="line">&gt; Explanation: </div><div class="line">&gt; Delete <span class="number">4</span> to earn <span class="number">4</span> points, consequently <span class="number">3</span> is also deleted.</div><div class="line">&gt; Then, delete <span class="number">2</span> to earn <span class="number">2</span> points. <span class="number">6</span> total points are earned.</div><div class="line">&gt; </div><div class="line">&gt; Example2:</div><div class="line">&gt; Input: nums = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>]</div><div class="line">&gt; Output: <span class="number">9</span></div><div class="line">&gt; Explanation: </div><div class="line">&gt; Delete <span class="number">3</span> to earn <span class="number">3</span> points, deleting both <span class="number">2</span><span class="string">'s and the 4.</span></div><div class="line"><span class="string">&gt; Then, delete 3 again to earn 3 points, and 3 again to earn 3 points.</span></div><div class="line"><span class="string">&gt; 9 total points are earned.</span></div><div class="line"><span class="string">&gt;</span></div></pre></td></tr></table></figure><ul><li><p>思路</p><p>这道题本质上就是198题House Rober。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> collections</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">deleteAndEarn</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type nums: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> nums:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           dic = collections.Counter(nums)</div><div class="line">&gt;           N = max(nums)</div><div class="line">&gt;           new_nums = []</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(N+<span class="number">1</span>): <span class="comment">#防止情况[3,1]</span></div><div class="line">&gt;               v = dic[i] <span class="keyword">if</span> i <span class="keyword">in</span> dic.keys() <span class="keyword">else</span> <span class="number">0</span></div><div class="line">&gt;               new_nums.append(i*v)</div><div class="line">&gt;           dp = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(new_nums)+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">1</span>] = new_nums[<span class="number">1</span>]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,len(new_nums)):</div><div class="line">&gt;               dp[i] = max(dp[i<span class="number">-1</span>],dp[i<span class="number">-2</span>]+new_nums[i])</div><div class="line">&gt;           <span class="keyword">return</span> max(dp)</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="718-Maximum-Length-of-Repeated-Subarray"><a href="#718-Maximum-Length-of-Repeated-Subarray" class="headerlink" title="718. Maximum Length of Repeated Subarray"></a>718. Maximum Length of Repeated Subarray</h3><blockquote><ul><li><p>题目</p><p>Given two integer arrays <code>A</code> and <code>B</code>, return the maximum length of an subarray that appears in both arrays.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;   Input:</div><div class="line">&gt;   A: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]</div><div class="line">&gt;   B: [<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">7</span>]</div><div class="line">&gt;   Output: <span class="number">3</span></div><div class="line">&gt;   Explanation: </div><div class="line">&gt;   The repeated subarray with maximum length is [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>].</div><div class="line">&gt;</div></pre></td></tr></table></figure></li><li><p>思路</p><p>经典动态规划。A[i]：表示A的子串0-i，B[j]：表示B的子串0-j</p><p>dp[i][j]：A的子串与B的子串最长重合，状态更新dp[i][j] = dp[i-1][j-1] + 1 if 两个子串的最后字符相同</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">findLength</span><span class="params">(self, A, B)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type A: List[int]</span></div><div class="line"><span class="string">&gt;           :type B: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           la = len(A)</div><div class="line">&gt;           lb = len(B)</div><div class="line">&gt;           dp = [ [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(la+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(lb+<span class="number">1</span>) ]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(la):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(lb):</div><div class="line">&gt;                   <span class="keyword">if</span> A[i]==B[j]:</div><div class="line">&gt;                       dp[i+<span class="number">1</span>][j+<span class="number">1</span>] = dp[i][j] + <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">return</span> max(max(row) <span class="keyword">for</span> row <span class="keyword">in</span> dp)</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="Minimum-ASCII-Delete-Sum-for-Two-Strings-712"><a href="#Minimum-ASCII-Delete-Sum-for-Two-Strings-712" class="headerlink" title="Minimum ASCII Delete Sum for Two Strings(712)"></a>Minimum ASCII Delete Sum for Two Strings(712)</h3><blockquote><ul><li><p>题目</p><p>Given two strings <code>s1, s2</code>, find the lowest ASCII sum of deleted characters to make two strings equal. All elements of each string will have an ASCII value in <code>[97, 122]</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;   Example1:</div><div class="line">&gt;   Input: s1 = <span class="string">"sea"</span>, s2 = <span class="string">"eat"</span></div><div class="line">&gt;   Output: <span class="number">231</span></div><div class="line">&gt;   Explanation: Deleting <span class="string">"s"</span> from <span class="string">"sea"</span> adds the ASCII value of <span class="string">"s"</span> (<span class="number">115</span>) to the sum.</div><div class="line">&gt;   Deleting <span class="string">"t"</span> from <span class="string">"eat"</span> adds <span class="number">116</span> to the sum.</div><div class="line">&gt;   At the end, both strings are equal, and <span class="number">115</span> + <span class="number">116</span> = <span class="number">231</span> is the minimum sum possible to achieve <span class="keyword">this</span>.</div><div class="line">&gt;   </div><div class="line">&gt;   Example2:</div><div class="line">&gt;   Input: s1 = <span class="string">"delete"</span>, s2 = <span class="string">"leet"</span></div><div class="line">&gt;   Output: <span class="number">403</span></div><div class="line">&gt;   Explanation: Deleting <span class="string">"dee"</span> from <span class="string">"delete"</span> to turn the string into <span class="string">"let"</span>,</div><div class="line">&gt;   adds <span class="number">100</span>[d]+<span class="number">101</span>[e]+<span class="number">101</span>[e] to the sum.  Deleting <span class="string">"e"</span> from <span class="string">"leet"</span> adds <span class="number">101</span>[e] to the sum.</div><div class="line">&gt;   At the end, both strings are equal to <span class="string">"let"</span>, and the answer is <span class="number">100</span>+<span class="number">101</span>+<span class="number">101</span>+<span class="number">101</span> = <span class="number">403</span>.</div><div class="line">&gt;   If instead we turned both strings into <span class="string">"lee"</span> or <span class="string">"eet"</span>, we would get answers of <span class="number">433</span> or <span class="number">417</span>, which are higher.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li><li><p>思路</p><p>这道题就是LCS (Longest Common Subsequence).</p><p>在LCS中，<code>d[i][j]</code>表示<code>s1.substring(0,i)</code>和<code>s2.substring(0,j)</code>的LCS的长度, 那么</p><p><code>d[i][j]</code>=<code>d[i-1][j-1]</code>+1                                    if <code>s1[i-1]</code> == <code>s2[j-1]</code></p><p><code>d[i][j]</code>=max(<code>d[i-1][j]</code>, <code>d[i]]j-1</code>)              if <code>s1[i-1]</code> != <code>s2[j-1]</code></p><p>那么在本题背景下，<code>d[i][j]</code>表示<code>s1.substring(0,i)</code>和<code>s2.substring(0,j)</code>删除若干个字符后相等的最小<code>cost</code>，那么</p><p><code>d[i][j]</code>=<code>d[i-1][j-1]</code>+0                                                                        if <code>s1[i-1]</code> == <code>s2[j-1]</code></p><p><code>d[i][j]</code>=min(<code>d[i-1][j]</code>+<code>s1[i-1]</code>, <code>d[i]]j-1</code>+<code>s2[j-1]</code>)              if <code>s1[i-1]</code> != <code>s2[j-1]</code></p><p>但是，这道题需要和718题的Maximum Length of Repeated Subarray作个比较,因为718题的状态转移方程是：</p><p><code>d[i][j]</code>=<code>d[i-1][j-1]</code>+1          if <code>s1[i-1]</code> == <code>s2[j-1]</code> </p><p><code>d[i][j]</code>=0                                     if <code>s1[i-1]</code> != <code>s2[j-1]</code></p><p>差异在于 if <code>s1[i-1]</code> != <code>s2[j-1]</code>，在718中，一旦不等，意味着连续的相等的子串已经消失，则当前的计算得重新开始，重点是“连续”</p><p>而在本题以及LCS中，一旦不等，不必重新开始，因为没有要求“连续”，大不了跳过这个不等，则当前不等的计算就依赖于前面</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">minimumDeleteSum</span><span class="params">(self, s1, s2)</span>:</span></div><div class="line">&gt;           mv = sys.maxsize</div><div class="line">&gt;           l1 = len(s1)</div><div class="line">&gt;           l2 = len(s2)</div><div class="line">&gt;           dp = [[ mv <span class="keyword">for</span> _ <span class="keyword">in</span> range(l2+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(l1+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(l1):</div><div class="line">&gt;               dp[i + <span class="number">1</span>][<span class="number">0</span>] = dp[i][<span class="number">0</span>]+ord(s1[i])</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(l2):</div><div class="line">&gt;               dp[<span class="number">0</span>][i + <span class="number">1</span>] = dp[<span class="number">0</span>][i]+ord(s2[i])</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l1+<span class="number">1</span>):</div><div class="line">&gt;                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,l2+<span class="number">1</span>):</div><div class="line">&gt;                   <span class="keyword">if</span> s1[i<span class="number">-1</span>]==s2[j<span class="number">-1</span>]:</div><div class="line">&gt;                       dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>]</div><div class="line">&gt;                   <span class="keyword">else</span>:</div><div class="line">&gt;                       dp[i][j] = min(dp[i<span class="number">-1</span>][j]+ord(s1[i<span class="number">-1</span>]) , dp[i][j<span class="number">-1</span>]+ord(s2[j<span class="number">-1</span>]))</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="Knight-Probability-in-Chessboard-688"><a href="#Knight-Probability-in-Chessboard-688" class="headerlink" title="Knight Probability in Chessboard(688)"></a>Knight Probability in Chessboard(688)</h3><blockquote><ul><li><p>题目</p><p>On an <code>N</code>x<code>N</code> chessboard, a knight starts at the <code>r</code>-th row and <code>c</code>-th column and attempts to make exactly <code>K</code> moves. The rows and columns are 0 indexed, so the top-left square is <code>(0, 0)</code>, and the bottom-right square is <code>(N-1, N-1)</code>.</p><p>A chess knight has 8 possible moves it can make, as illustrated below. Each move is two squares in a cardinal direction, then one square in an orthogonal direction.</p><p><img src="/2018/05/26/Dynamic-Programming/knight.png" alt=""></p><p>Each time the knight is to move, it chooses one of eight possible moves uniformly at random (even if the piece would go off the chessboard) and moves there.</p><p>The knight continues moving until it has made exactly <code>K</code> moves or has moved off the chessboard. Return the probability that the knight remains on the board after it has stopped moving.</p></li><li><p>思路</p><p><code>dp[i][j][k]</code>:表示经过<code>k</code>次移动之后，knight到达位置<code>[i,j]</code>的所有可能方法数。那么状态转移有两种，一种是如果knight当前位置是<code>[i][j]</code>,且当前是第<code>k</code>次移动，那么更新下一次从当前<code>[i][j]</code>位置可能到达的其他位置<code>dp[i+x][j+y][k+1]</code>;另一种是当前位置是<code>[i][j]</code>,且当前是第<code>k</code>次移动，那么上一次<code>k-1</code>移动是如何到达当前位置。</p><p>本代码实现第一种方法，因为当前状态只与上一次状态有关，所以通过降维，使用<code>dp0[]</code>和<code>dp1[]</code>表示<code>k-1</code>和<code>k</code>的可能性，’状态转移是:</p><p><code>dp1[i][j] += dp0[i+x][j+y]</code></p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">knightProbability</span><span class="params">(self, N, K, r, c)</span>:</span></div><div class="line">&gt;           dp0 = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]</div><div class="line">&gt;           dp0[r][c] = <span class="number">1</span></div><div class="line">&gt;           dirs = [[<span class="number">-2</span>,<span class="number">-1</span>],[<span class="number">-2</span>,<span class="number">1</span>],[<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">-1</span>],[<span class="number">1</span>,<span class="number">-2</span>],[<span class="number">-1</span>,<span class="number">-2</span>]]</div><div class="line">&gt;           <span class="keyword">for</span> step <span class="keyword">in</span> range(K):</div><div class="line">&gt;               dp1 = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]</div><div class="line">&gt;               <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div><div class="line">&gt;                   <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</div><div class="line">&gt;                       <span class="keyword">for</span> d <span class="keyword">in</span> dirs:</div><div class="line">&gt;                           x = i+d[<span class="number">0</span>]</div><div class="line">&gt;                           y = j+d[<span class="number">1</span>]</div><div class="line">&gt;                           <span class="keyword">if</span> x&lt;<span class="number">0</span> <span class="keyword">or</span> x&gt;N<span class="number">-1</span> <span class="keyword">or</span> y&lt;<span class="number">0</span> <span class="keyword">or</span> y&gt;N<span class="number">-1</span>:</div><div class="line">&gt;                               <span class="keyword">pass</span></div><div class="line">&gt;                           <span class="keyword">else</span>:</div><div class="line">&gt;                               dp1[x][y] += dp0[i][j]</div><div class="line">&gt;               dp0 = dp1</div><div class="line">&gt;           re = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div><div class="line">&gt;               re += sum(dp0[i])</div><div class="line">&gt;           <span class="keyword">return</span> re/(<span class="number">8</span>**K)</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="Number-of-Longest-Increasing-Subsequence-673"><a href="#Number-of-Longest-Increasing-Subsequence-673" class="headerlink" title="Number of Longest Increasing Subsequence(673)"></a>Number of Longest Increasing Subsequence(673)</h3><blockquote><ul><li><p>题目</p><p>Given an unsorted array of integers, find the number of longest increasing subsequence.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;   Example1:</div><div class="line">&gt;   Input: [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">7</span>]</div><div class="line">&gt;   Output: <span class="number">2</span></div><div class="line">&gt;   Explanation: The two longest increasing subsequence are [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>] and [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>].</div><div class="line">&gt;       </div><div class="line">&gt;   Example2:</div><div class="line">&gt;   Input: [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">&gt;   Output: <span class="number">5</span></div><div class="line">&gt;   Explanation: The length of longest continuous increasing subsequence is <span class="number">1</span>, and there are <span class="number">5</span> subsequences<span class="string">' length is 1, so output 5.</span></div><div class="line"><span class="string">&gt;       </span></div><div class="line"><span class="string">&gt;   Example3:</span></div><div class="line"><span class="string">&gt;   Input: [1,2,4,2,3]</span></div><div class="line"><span class="string">&gt;   Output: 3</span></div><div class="line"><span class="string">&gt;   Explanation: The two longest increasing subsequence are [1, 2, 4] and [1, 2, 3] and [1, 2, 3].</span></div><div class="line"><span class="string">&gt;</span></div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>dp[i]表示子串S[0] - S[i-1]的最长上升子串长度。动态转移方程是dp[i] = max (dp[i], dp[j] + 1), j 的范围是[0, i-1]。所以代码实现时，需要双重循环。但是，我们还需要记录一个构成最长子串的方式数。比如子串【1，2，4，2，3】，最长上升子串长度是3，但是有两种方式实现，【1，2，3】(2来自于index=1)和【1，2，3】[2来自于index=3]。</p><p>dp[i][2]:dp[i][0]表示子串S[0] - S[i-1]的最长上升子串长度，dp[i][1]第二维记录构成该最长子串的方式数</p><p>if dp[i][0]&gt;dp[j][1]  then dp[i][0] = dp[j][0] + 1, dp[i][1]  = dp[i][1] </p><p>else if dp[i][0]==dp[j][1]  then dp[i][1]  += dp[i][1] </p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">findNumberOfLIS</span><span class="params">(self, nums)</span>:</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> nums :<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           l = len(nums)</div><div class="line">&gt;           max_len = <span class="number">1</span></div><div class="line">&gt;           dp = [[<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, l):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</div><div class="line">&gt;                   <span class="keyword">if</span> nums[i] &gt; nums[j]:</div><div class="line">&gt;                       <span class="keyword">if</span> dp[i][<span class="number">0</span>]&lt;dp[j][<span class="number">0</span>] + <span class="number">1</span>:</div><div class="line">&gt;                           dp[i][<span class="number">0</span>] = dp[j][<span class="number">0</span>] + <span class="number">1</span></div><div class="line">&gt;                           dp[i][<span class="number">1</span>] = dp[j][<span class="number">1</span>]</div><div class="line">&gt;                           max_len = max(max_len,dp[i][<span class="number">0</span>])</div><div class="line">&gt;                       <span class="keyword">elif</span> dp[i][<span class="number">0</span>] == dp[j][<span class="number">0</span>] + <span class="number">1</span>:</div><div class="line">&gt;                           dp[i][<span class="number">1</span>] += dp[j][<span class="number">1</span>]</div><div class="line">&gt;           re = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">&gt;               <span class="keyword">if</span> dp[i][<span class="number">0</span>]==max_len:</div><div class="line">&gt;                   re+=dp[i][<span class="number">1</span>]</div><div class="line">&gt;           <span class="keyword">return</span> re</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><h3 id="2-Keys-Keyboard"><a href="#2-Keys-Keyboard" class="headerlink" title="2 Keys Keyboard"></a>2 Keys Keyboard</h3><blockquote><ul><li><p>题目</p><p>Initially on a notepad only one character ‘A’ is present. You can perform two operations on this notepad for each step:</p><ol><li><code>Copy All</code>: You can copy all the characters present on the notepad (partial copy is not allowed).</li><li><code>Paste</code>: You can paste the characters which are copied <strong>last time</strong>.</li></ol><p>Given a number <code>n</code>. You have to get <strong>exactly</strong> <code>n</code> ‘A’ on the notepad by performing the minimum number of steps permitted. Output the minimum number of steps to get <code>n</code> ‘A’.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;   Example1:</div><div class="line">&gt;   </div><div class="line">&gt;   Input: <span class="number">3</span></div><div class="line">&gt;   Output: <span class="number">3</span></div><div class="line">&gt;   Explanation:</div><div class="line">&gt;   Intitally, we have one character <span class="string">'A'</span>.</div><div class="line">&gt;   In step <span class="number">1</span>, we use Copy All operation.</div><div class="line">&gt;   In step <span class="number">2</span>, we use Paste operation to get <span class="string">'AA'</span>.</div><div class="line">&gt;   In step <span class="number">3</span>, we use Paste operation to get <span class="string">'AAA'</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li>思路</li></ul><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">minSteps</span><span class="params">(self, n)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type n: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           mv = sys.maxsize</div><div class="line">&gt;           <span class="keyword">if</span> n==<span class="number">0</span>:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">&gt;           dp = [mv <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">1</span>] = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,n+<span class="number">1</span>):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,i):</div><div class="line">&gt;                   <span class="keyword">if</span> i%j == <span class="number">0</span>:</div><div class="line">&gt;                       dp[i] = min(dp[i],dp[j] + i//j)</div><div class="line">&gt;           <span class="keyword">return</span> dp[<span class="number">-1</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><h3 id="Out-of-Boundary-Paths-576"><a href="#Out-of-Boundary-Paths-576" class="headerlink" title="Out of Boundary Paths(576)"></a>Out of Boundary Paths(576)</h3><ul><li><p>题目</p><p>There is an <strong>m</strong> by <strong>n</strong> grid with a ball. Given the start coordinate <strong>(i,j)</strong> of the ball, you can move the ball to <strong>adjacent</strong> cell or cross the grid boundary in four directions (up, down, left, right). However, you can <strong>at most</strong> move <strong>N</strong> times. Find out the number of paths to move the ball out of grid boundary. The answer may be very large, return it after mod 109 + 7.</p><p><img src="/2018/05/26/Dynamic-Programming/Screen Shot 2018-05-29 at 8.25.29 PM.png" alt="Screen Shot 2018-05-29 at 8.25.29 PM"></p></li></ul><ul><li><p>思路</p><p><a href="http://www.cnblogs.com/grandyang/p/6927921.html" target="_blank" rel="noopener">参考</a></p><p>这道题给了我们一个二维的数组，某个位置放个足球，每次可以在上下左右四个方向中任意移动一步，总共可以移动N步，问我们总共能有多少种移动方法能把足球移除边界，由于结果可能是个巨大的数，所以让我们对一个大数取余。那么我们知道对于这种结果很大的数如果用递归解法很容易爆栈，所以最好考虑使用DP来解。那么我们使用一个三维的DP数组，其中dp[k][i][j]表示总共走k步，从(i,j)位置走出边界的总路径数。那么我们来找递推式，对于dp[k][i][j]，走k步出边界的总路径数等于其周围四个位置的走k-1步出边界的总路径数之和，如果周围某个位置已经出边界了，那么就直接加上1，否则就在dp数组中找出该值，这样整个更新下来，我们就能得出每一个位置走任意步数的出界路径数了，最后只要返回dp[N][i][j]就是所求结果了，参见代码如下.</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findPaths</span><span class="params">(self, m, n, N, i, j)</span>:</span></div><div class="line">        dp = [[[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(N+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(m)]<span class="comment">##N+1是为了防止出现N=0情况。</span></div><div class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>,N+<span class="number">1</span>):</div><div class="line">            <span class="keyword">for</span> ii <span class="keyword">in</span> range(m):</div><div class="line">                <span class="keyword">for</span> jj <span class="keyword">in</span> range(n):</div><div class="line">                    v1 = <span class="number">1</span> <span class="keyword">if</span> ii==<span class="number">0</span> <span class="keyword">else</span> dp[ii<span class="number">-1</span>][jj][k<span class="number">-1</span>]</div><div class="line">                    v2 = <span class="number">1</span> <span class="keyword">if</span> ii==m<span class="number">-1</span> <span class="keyword">else</span> dp[ii+<span class="number">1</span>][jj][k<span class="number">-1</span>]</div><div class="line">                    v3 = <span class="number">1</span> <span class="keyword">if</span> jj==<span class="number">0</span> <span class="keyword">else</span> dp[ii][jj<span class="number">-1</span>][k<span class="number">-1</span>]</div><div class="line">                    v4 = <span class="number">1</span> <span class="keyword">if</span> jj==n<span class="number">-1</span> <span class="keyword">else</span> dp[ii][jj+<span class="number">1</span>][k<span class="number">-1</span>]</div><div class="line">                    dp[ii][jj][k] = (v1+v2+v3+v4)%<span class="number">1000000007</span></div><div class="line">        <span class="keyword">return</span> dp[i][j][N]</div></pre></td></tr></table></figure></li></ul><h3 id="Longest-Palindromic-Subsequence-516-—for-Length"><a href="#Longest-Palindromic-Subsequence-516-—for-Length" class="headerlink" title="Longest Palindromic Subsequence(516)—for(Length)"></a><em><u>Longest Palindromic Subsequence(516)—for(Length)</u></em></h3><ul><li><p>题目</p><p>Given a string s, find the longest palindromic subsequence’s length in s. You may assume that the maximum length of s is 1000.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Example1:</div><div class="line"></div><div class="line">Input:<span class="string">"bbbab"</span> Output:<span class="number">4</span> </div><div class="line">One possible longest palindromic subsequence is <span class="string">"bbbb"</span>.</div><div class="line"></div><div class="line">Example2:</div><div class="line">Input:<span class="string">"cbbd"</span> Output:<span class="number">2</span> </div><div class="line">One possible longest palindromic subsequence is <span class="string">"bb"</span>.</div></pre></td></tr></table></figure></li><li><p>思路</p><p>dp[i][j]表示字符串s[i…j]的最长回文串长度。</p><p>dp[i][j] = 1 表示单个单个字符s[i]就是一个回文串</p><p>dp[i][j] = dp[i+1][j-1] + 2 if s[i]==s[j], else</p><p>dp[i][j] = max (dp[i+1][j], dp[i][j-1])</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestPalindromeSubseq</span><span class="params">(self, s)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type s: str</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        l = len(s)</div><div class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)]</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">            dp[i][i] = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> ll <span class="keyword">in</span> range(<span class="number">1</span>,l+<span class="number">1</span>):</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,l-ll):</div><div class="line">                j = i+ll</div><div class="line">                <span class="keyword">if</span> s[i]==s[j]:</div><div class="line">                    dp[i][j] = dp[i+<span class="number">1</span>][j<span class="number">-1</span>] + <span class="number">2</span></div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    dp[i][j] = dp[i+<span class="number">1</span>][j] <span class="keyword">if</span> dp[i+<span class="number">1</span>][j] &gt; dp[i][j<span class="number">-1</span>] <span class="keyword">else</span> dp[i][j<span class="number">-1</span>]</div><div class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][l<span class="number">-1</span>]</div></pre></td></tr></table></figure></li></ul><h3 id="Predict-the-Winner-486-—for-Length"><a href="#Predict-the-Winner-486-—for-Length" class="headerlink" title="Predict the Winner(486)—for(Length)"></a>Predict the Winner(486)—for(Length)</h3><ul><li><p>题目</p><p>Given an array of scores that are non-negative integers. Player 1 picks one of the numbers from either end of the array followed by the player 2 and then player 1 and so on. Each time a player picks a number, that number will not be available for the next player. This continues until all the scores have been chosen. The player with the maximum score wins.</p><p>Given an array of scores, predict whether player 1 is the winner. You can assume each player plays to maximize his score.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Example1:</div><div class="line"></div><div class="line">Input: [<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>]</div><div class="line">Output: False</div><div class="line">Explanation: Initially, player <span class="number">1</span> can choose between <span class="number">1</span> and <span class="number">2</span>. </div><div class="line">If he chooses <span class="number">2</span> (or <span class="number">1</span>), then player <span class="number">2</span> can choose from <span class="number">1</span> (or <span class="number">2</span>) and <span class="number">5</span>. If player <span class="number">2</span> chooses <span class="number">5</span>, then player <span class="number">1</span> will be left with <span class="number">1</span> (or <span class="number">2</span>). </div><div class="line">So, <span class="keyword">final</span> score of player <span class="number">1</span> is <span class="number">1</span> + <span class="number">2</span> = <span class="number">3</span>, and player <span class="number">2</span> is <span class="number">5</span>. </div><div class="line">Hence, player <span class="number">1</span> will never be the winner and you need to <span class="keyword">return</span> False.</div><div class="line"></div><div class="line">Example2:</div><div class="line"></div><div class="line">Input: [<span class="number">1</span>, <span class="number">5</span>, <span class="number">233</span>, <span class="number">7</span>]</div><div class="line">Output: True</div><div class="line">Explanation: Player <span class="number">1</span> first chooses <span class="number">1</span>. Then player <span class="number">2</span> have to choose between <span class="number">5</span> and <span class="number">7</span>. No matter which number player <span class="number">2</span> choose, player <span class="number">1</span> can choose <span class="number">233</span>.</div><div class="line">Finally, player <span class="number">1</span> <span class="function">has more <span class="title">score</span> <span class="params">(<span class="number">234</span>)</span> than player 2 <span class="params">(<span class="number">12</span>)</span>, so you need to return True representing player1 can win.</span></div></pre></td></tr></table></figure></li><li><p>思路</p><p>dp[i][j]表示子串s[i…j]供选择时两个玩家间的总分数差，无论是玩家1还是玩家2，都希望最大化差异，那么状态转移就是：</p><p>dp[i][j] = max(nums[i] - dp[i+1][j] , nums[j] - dp[i][j-1])</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PredictTheWinner</span><span class="params">(self, nums)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type nums: List[int]</span></div><div class="line"><span class="string">        :rtype: bool</span></div><div class="line"><span class="string">        """</span></div><div class="line">        l = len(nums)</div><div class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)]</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">            dp[i][i] = nums[i]</div><div class="line">        <span class="keyword">for</span> dis <span class="keyword">in</span> range(<span class="number">1</span>,l):  <span class="comment">#the distance between index of i and j</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(l-dis):</div><div class="line">                j = i+dis</div><div class="line">                dp[i][j] = nums[i]-dp[i+<span class="number">1</span>][j] <span class="keyword">if</span> nums[i]-dp[i+<span class="number">1</span>][j]&gt;nums[j]-dp[i][j<span class="number">-1</span>] <span class="keyword">else</span> nums[j]-dp[i][j<span class="number">-1</span>]</div><div class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][l<span class="number">-1</span>]&gt;=<span class="number">0</span></div></pre></td></tr></table></figure></li></ul><h3 id="Target-Sum-494"><a href="#Target-Sum-494" class="headerlink" title="Target Sum(494)"></a>Target Sum(494)</h3><ul><li><p>题目</p><p>You are given a list of non-negative integers, a1, a2, …, an, and a target, S. Now you have 2 symbols <code>+</code> and <code>-</code>. For each integer, you should choose one from <code>+</code> and <code>-</code> as its new symbol.</p><p>Find out how many ways to assign symbols to make sum of integers equal to target S.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Input: nums is [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], S is <span class="number">3</span>. </div><div class="line">Output: <span class="number">5</span></div><div class="line">Explanation: </div><div class="line"></div><div class="line">-<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span> = <span class="number">3</span></div><div class="line">+<span class="number">1</span>-<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span> = <span class="number">3</span></div><div class="line">+<span class="number">1</span>+<span class="number">1</span>-<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span> = <span class="number">3</span></div><div class="line">+<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span>-<span class="number">1</span>+<span class="number">1</span> = <span class="number">3</span></div><div class="line">+<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span>+<span class="number">1</span>-<span class="number">1</span> = <span class="number">3</span></div><div class="line"></div><div class="line">There are <span class="number">5</span> ways to assign symbols to make the sum of nums be target <span class="number">3</span>.</div></pre></td></tr></table></figure></li><li><p>思路</p><p><strong>解法1：</strong></p><p>经典动态规划。首先，将问题简单化，只考虑通过组合number，是否能组成得到target。</p><p>$V_i$表示使用num[0],num[1],…,nums[i]个数字组成可以得到的所有可能数字</p><p>那么$V_0={0}$，$V_i=\{V_{i-1}+num[i]\}\cup{\{V_{i-1}-num[i]\}}$</p><p>则判断$target\in{V_n}$</p><p>现在考虑数字组合得到target的方法数目，使用dp[i][j]来记录num[0],num[1],…,num[i-1]个数字组成得到sum=j的不同方式数。那么状态转移就是：</p><p>dp[i][j] = dp[i-1][j+num[i]] + dp[i-1][j-num[i]]</p><p>令num[0], num[1], num[2],…的和为TSUM，而所有可能的sum值，即dp第二维的长度是2*TSUM+1,即{-TSUM, -TSUM+1,…-1,0,1,…,TSUM-1,TSUM}。</p><p>初始化：dp[-1][0] = 1 组合使和0有一种方式，即什么都不操作。而sum=0的index是TSUM即dp[-1][TSUM]=1</p><p><strong>解法2：</strong></p><p>$P$ 表示集合，其中数字的符号只有“+”；$N$ 表示负数集合</p><p>那么$P\cup{N}=\{a_1,a_2,…,a_n\}$, $P\cap{N}=\empty$</p><p>那么问题就变成寻找集合$P$和$Q$，使$sum(P)-sum(N)=target$.</p><p>两边同时加上$sum(P)+sum(N)$</p><p>$sum(P)-sum(N)+sum(P)+sum(N)=target+sum(P)+sum(N)$</p><p>化简得到，$2*sum(P)=sum(a_1,a_2,…,a_n)+target$</p><p>即$sum(P)=\frac{sum(a_1,a_2,…,a_n)+target}{2}$</p><p>这样问题就变成了0-1背包问题，从$a_1,a_2,..,a_n$中凑成价值为$sum(P)$的方案。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">####解法1</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findTargetSumWays</span><span class="params">(self, nums, S)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type nums: List[int]</span></div><div class="line"><span class="string">        :type S: int</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        sums = sum(nums)</div><div class="line">        <span class="keyword">if</span> sums&lt;S:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        l = len(nums)</div><div class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>*sums+<span class="number">1</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(l+<span class="number">1</span>)]</div><div class="line">        dp[<span class="number">0</span>][sums] = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(nums[i],<span class="number">2</span>*sums+<span class="number">1</span>-nums[i]):</div><div class="line">                <span class="keyword">if</span> (dp[i][j]!=<span class="number">0</span>): <span class="comment">#表示前i-1个数字可以组成(j-tsum),那么通过吸收+num[i]，dp[i][j]可以更新 dp[i+1][j+nums[i]] 和 dp[i+1][j-nums[i]]</span></div><div class="line">                    dp[i+<span class="number">1</span>][j+nums[i]] += dp[i][j]</div><div class="line">                    dp[i+<span class="number">1</span>][j-nums[i]] += dp[i][j]</div><div class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>][S+sums]</div><div class="line"></div><div class="line"><span class="comment">####解法2：</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findTargetSumWays</span><span class="params">(self, nums, S)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type nums: List[int]</span></div><div class="line"><span class="string">        :type S: int</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        sumA = sum(nums)</div><div class="line">        <span class="keyword">if</span> (sumA + S) % <span class="number">2</span> == <span class="number">1</span> <span class="keyword">or</span> sumA &lt; abs(S): <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        target = (sumA+S)//<span class="number">2</span></div><div class="line">        dp = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(target+<span class="number">1</span>)]</div><div class="line">        dp[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(target,<span class="number">-1</span>,<span class="number">-1</span>):</div><div class="line">                <span class="keyword">if</span> j&gt;=num:</div><div class="line">                    dp[j] += dp[j-num]</div><div class="line">        <span class="keyword">return</span> dp[target]</div></pre></td></tr></table></figure></li></ul><h2 id="高级"><a href="#高级" class="headerlink" title="高级"></a>高级</h2><h3 id="801-Minimum-Swaps-To-Make-Sequences-Increasing"><a href="#801-Minimum-Swaps-To-Make-Sequences-Increasing" class="headerlink" title="801. Minimum Swaps To Make Sequences Increasing"></a><em><u>801. Minimum Swaps To Make Sequences Increasing</u></em></h3><blockquote><ul><li><p>题目</p><p>We have two integer sequences <code>A</code> and <code>B</code> of the same non-zero length.</p><p>We are allowed to swap elements <code>A[i]</code> and <code>B[i]</code>.  Note that both elements are in the same index position in their respective sequences.</p><p>At the end of some number of swaps, <code>A</code> and <code>B</code> are both strictly increasing.  (A sequence is <em>strictly increasing</em> if and only if <code>A[0] &lt; A[1] &lt; A[2] &lt; ... &lt; A[A.length - 1]</code>.)</p><p>Given A and B, return the minimum number of swaps to make both sequences strictly increasing.  It is guaranteed that the given input always makes it possible.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   Example:</div><div class="line">&gt;   Input: A = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>], B = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>]</div><div class="line">&gt;   Output: <span class="number">1</span></div><div class="line">&gt;   Explanation: </div><div class="line">&gt;   Swap A[<span class="number">3</span>] and B[<span class="number">3</span>].  Then the sequences are:</div><div class="line">&gt;   A = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>] and B = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div><div class="line">&gt;   which are both strictly increasing.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>文字  <a href="https://blog.csdn.net/magicbean2/article/details/79826617" target="_blank" rel="noopener">https://blog.csdn.net/magicbean2/article/details/79826617</a> </p><p>图片  <a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-801-minimum-swaps-to-make-sequences-increasing/" target="_blank" rel="noopener">http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-801-minimum-swaps-to-make-sequences-increasing/</a></p><p> 这道题实在是太难了，不过看图片还是可以稍微有点顺畅。</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">minSwap</span><span class="params">(self, A, B)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type A: List[int]</span></div><div class="line"><span class="string">&gt;           :type B: List[int]</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           mv = sys.maxsize</div><div class="line">&gt;           l = len(A)</div><div class="line">&gt;           keep = [mv <span class="keyword">for</span> i <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           swap = [mv <span class="keyword">for</span> j <span class="keyword">in</span> range(l)]</div><div class="line">&gt;           keep[<span class="number">0</span>] = <span class="number">0</span></div><div class="line">&gt;           swap[<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># 初始是1，不是0，因为意味着A[0]和B[0]交换一次</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l):</div><div class="line">&gt;               a1 = A[i<span class="number">-1</span>]</div><div class="line">&gt;               b1 = B[i<span class="number">-1</span>]</div><div class="line">&gt;               a2 = A[i]</div><div class="line">&gt;               b2 = B[i]</div><div class="line">&gt;               <span class="keyword">if</span> a1 &lt; a2 <span class="keyword">and</span> b1 &lt; b2:</div><div class="line">&gt;                   keep[i] = keep[i<span class="number">-1</span>] <span class="comment"># no swap for both i-1, i</span></div><div class="line">&gt;                   swap[i] = swap[i<span class="number">-1</span>] + <span class="number">1</span> <span class="comment"># swap for both i-1, i; swap[i-1] means swap i-1, 1 means swap i</span></div><div class="line">&gt;               <span class="keyword">if</span> a1 &lt; b2 <span class="keyword">and</span> b1 &lt; a2:</div><div class="line">&gt;                   keep[i] = min(keep[i], swap[i<span class="number">-1</span>])</div><div class="line">&gt;                   swap[i] = min(swap[i], keep[i<span class="number">-1</span>]+<span class="number">1</span>)</div><div class="line">&gt;           <span class="keyword">return</span> min(keep[<span class="number">-1</span>],swap[<span class="number">-1</span>])</div><div class="line">&gt;           </div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="787-Cheapest-Flights-Within-K-Stops"><a href="#787-Cheapest-Flights-Within-K-Stops" class="headerlink" title="787. Cheapest Flights Within K Stops"></a>787. Cheapest Flights Within K Stops</h3><blockquote><ul><li><p>题目</p><p>There are <code>n</code> cities connected by <code>m</code> flights. Each fight starts from city <code>u</code>and arrives at <code>v</code> with a price <code>w</code>.</p><p>Now given all the cities and fights, together with starting city <code>src</code> and the destination <code>dst</code>, your task is to find the cheapest price from <code>src</code> to <code>dst</code> with up to <code>k</code> stops. If there is no such route, output <code>-1</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   Example <span class="number">1</span>:</div><div class="line">&gt;   Input: </div><div class="line">&gt;   n = <span class="number">3</span>, edges = [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">100</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">100</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">500</span>]]</div><div class="line">&gt;   src = <span class="number">0</span>, dst = <span class="number">2</span>, k = <span class="number">1</span></div><div class="line">&gt;   Output: <span class="number">200</span></div><div class="line">&gt;   Explanation: </div><div class="line">&gt;   The cheapest price from city <span class="number">0</span> to city <span class="number">2</span> with at most <span class="number">1</span> stop costs <span class="number">200</span>, as marked red in the picture.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   Example <span class="number">2</span>:</div><div class="line">&gt;   Input: </div><div class="line">&gt;   n = <span class="number">3</span>, edges = [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">100</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">100</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">500</span>]]</div><div class="line">&gt;   src = <span class="number">0</span>, dst = <span class="number">2</span>, k = <span class="number">0</span></div><div class="line">&gt;   Output: <span class="number">500</span></div><div class="line">&gt;   Explanation: </div><div class="line">&gt;   The cheapest price from city <span class="number">0</span> to city <span class="number">2</span> with at most <span class="number">0</span> stop costs <span class="number">500</span>, as marked blue in the picture.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>dp[i][j]表示从src城市出发，经过i站，到达城市j</p><p>状态转移:</p><p>dp[i][j] = min ( dp[i][j] , dp[i-1][mid]+cost[mid][j] )</p></li><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="keyword">import</span> sys</div><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">findCheapestPrice</span><span class="params">(self, n, flights, src, dst, K)</span>:</span></div><div class="line">&gt;           mv = <span class="number">1000000</span></div><div class="line">&gt;           dp = [ [ mv <span class="keyword">for</span> _ <span class="keyword">in</span> range(n) ] <span class="keyword">for</span> _ <span class="keyword">in</span> range(K+<span class="number">2</span>) ] </div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(K+<span class="number">2</span>):</div><div class="line">&gt;               dp[i][src] = <span class="number">0</span> <span class="comment"># 从scr出发到scr，不需要cost</span></div><div class="line">&gt;           <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>,K+<span class="number">2</span>):</div><div class="line">&gt;               <span class="keyword">for</span> flight <span class="keyword">in</span> flights:</div><div class="line">&gt;                  s = flight[<span class="number">0</span>]</div><div class="line">&gt;                  d = flight[<span class="number">1</span>]</div><div class="line">&gt;                  cost = flight[<span class="number">2</span>]</div><div class="line">&gt;                  dp[k][d] = min(dp[k][d],dp[k<span class="number">-1</span>][s]+cost)</div><div class="line">&gt;           <span class="keyword">return</span> <span class="number">-1</span> <span class="keyword">if</span> dp[K+<span class="number">1</span>][dst]==mv <span class="keyword">else</span> dp[K+<span class="number">1</span>][dst]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="790-Domino-and-Tromino-Tiling"><a href="#790-Domino-and-Tromino-Tiling" class="headerlink" title="790. Domino and Tromino Tiling"></a>790. Domino and Tromino Tiling</h3><blockquote><ul><li><p>题目</p><p>We have two types of tiles: a 2x1 domino shape, and an “L” tromino shape. These shapes may be rotated.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   XX  &lt;- domino</div><div class="line">&gt;   </div><div class="line">&gt;   XX  &lt;- <span class="string">"L"</span> tromino</div><div class="line">&gt;   X</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  Given N, how many ways are there to tile a 2 x N board? <strong>Return your answer modulo 10^9 + 7</strong>.</p><p>  (In a tiling, every square must be covered by a tile. Two tilings are different if and only if there are two 4-directionally adjacent cells on the board such that exactly one of the tilings has both squares occupied by a tile.)</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;   Example:</div><div class="line">&gt;   Input: <span class="number">3</span></div><div class="line">&gt;   Output: <span class="number">5</span></div><div class="line">&gt;   Explanation: </div><div class="line">&gt;   The five different ways are listed below, different letters indicates different tiles:</div><div class="line">&gt;   XYZ XXZ XYY XXY XYY</div><div class="line">&gt;   XYZ YYZ XZZ XYY XXY</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>图片：<a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-790-domino-and-tromino-tiling/" target="_blank" rel="noopener">http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-790-domino-and-tromino-tiling/</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;   dp[i]表示高度为<span class="number">2</span>，长度为i的形状可能组法。</div><div class="line">&gt;   只有xx多米诺时，有两种拼法，用一个xx，但是把它立起来，这样的拼法是dp[i-<span class="number">1</span>];另一种是用两个</div><div class="line">&gt;               xx</div><div class="line">&gt;   xx，组成正方形xx，这样拼法是dp[i-<span class="number">2</span>]</div><div class="line">&gt;   dp[i] = dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>],斐波那契数列。</div><div class="line">&gt;   考虑L型多米诺，具体看图，不好描述。</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">numTilings</span><span class="params">(self, N)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type N: int</span></div><div class="line"><span class="string">&gt;           :rtype: int</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           m = <span class="number">1000000007</span></div><div class="line">&gt;           dp = [ [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(N+<span class="number">1</span>)]</div><div class="line">&gt;           dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           dp[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,N+<span class="number">1</span>):</div><div class="line">&gt;               dp[i][<span class="number">0</span>] = (dp[i<span class="number">-1</span>][<span class="number">0</span>] + dp[i<span class="number">-2</span>][<span class="number">0</span>] + <span class="number">2</span>*dp[i<span class="number">-1</span>][<span class="number">1</span>])%m</div><div class="line">&gt;               dp[i][<span class="number">1</span>] = (dp[i<span class="number">-2</span>][<span class="number">0</span>] + dp[i<span class="number">-1</span>][<span class="number">1</span>])%m</div><div class="line">&gt;           <span class="keyword">return</span> dp[N][<span class="number">0</span>]</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="5-Longest-Palindromic-Substring"><a href="#5-Longest-Palindromic-Substring" class="headerlink" title="5. Longest Palindromic Substring"></a>5. Longest Palindromic Substring</h3><blockquote><ul><li><p>题目</p><p>Given a string <strong>s</strong>, find the longest palindromic substring in <strong>s</strong>. You may assume that the maximum length of <strong>s</strong> is 1000.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="string">"babad"</span></div><div class="line">&gt;   Output: <span class="string">"bab"</span></div><div class="line">&gt;   Note: <span class="string">"aba"</span> is also a valid answer.</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: <span class="string">"cbbd"</span></div><div class="line">&gt;   Output: <span class="string">"bb"</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><p>这道题的难点在于，使用下标i和j来表示substring，i和j具体应该如何变化。实际上，i和j分别表示子串的结束字符和开头字符，如果s[i] != s[j]，那么dp[i][j]=false; 否则查看j+1 至 i-1之间是否回文。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;   dp(i, j) <span class="function">represents whether <span class="title">s</span><span class="params">(i ... j)</span> can form a palindromic substring, <span class="title">dp</span><span class="params">(i, j)</span> is <span class="keyword">true</span> when <span class="title">s</span><span class="params">(i)</span> equals to <span class="title">s</span><span class="params">(j)</span> and <span class="title">s</span><span class="params">(i+<span class="number">1</span> ... j<span class="number">-1</span>)</span> is a palindromic substring. When we found a palindrome, check <span class="keyword">if</span> it's the longest one. Time complexity <span class="title">O</span><span class="params">(n^<span class="number">2</span>)</span>.</span></div><div class="line"><span class="function">&gt;</span></div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self, s)</span>:</span></div><div class="line">&gt;           <span class="string">"""</span></div><div class="line"><span class="string">&gt;           :type s: str</span></div><div class="line"><span class="string">&gt;           :rtype: str</span></div><div class="line"><span class="string">&gt;           """</span></div><div class="line">&gt;           <span class="keyword">if</span> <span class="keyword">not</span> s: <span class="keyword">return</span> <span class="string">""</span></div><div class="line">&gt;           re = <span class="string">""</span></div><div class="line">&gt;           l = len(s)</div><div class="line">&gt;           dp = [ [<span class="keyword">False</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(l)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(l) ]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">&gt;               dp[i][i] = <span class="keyword">True</span></div><div class="line">&gt;           <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,l):</div><div class="line">&gt;               <span class="keyword">for</span> i <span class="keyword">in</span> range(j,<span class="number">-1</span>,<span class="number">-1</span>):<span class="comment">#start from (j-1), end in 0, not including -1</span></div><div class="line">&gt;                   dp[i][j] = s[i]==s[j] <span class="keyword">and</span> (j-i+<span class="number">1</span>&lt;<span class="number">3</span> <span class="keyword">or</span> dp[i+<span class="number">1</span>][j<span class="number">-1</span>])</div><div class="line">&gt;                   <span class="keyword">if</span> dp[i][j]:</div><div class="line">&gt;                       re = s[i:j+<span class="number">1</span>] <span class="keyword">if</span> len(re)&lt;(j-i+<span class="number">1</span>) <span class="keyword">else</span> re</div><div class="line">&gt;           <span class="keyword">return</span> re</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><h3 id="764-Largest-Plus-Sign"><a href="#764-Largest-Plus-Sign" class="headerlink" title="764. Largest Plus Sign"></a><em><u>764. Largest Plus Sign</u></em></h3><blockquote><ul><li><p>题目</p><p>In a 2D <code>grid</code> from (0, 0) to (N-1, N-1), every cell contains a <code>1</code>, except those cells in the given list <code>mines</code> which are <code>0</code>. What is the largest axis-aligned plus sign of <code>1</code>s contained in the grid? Return the order of the plus sign. If there is none, return 0.</p><p>An “<em>axis-aligned plus sign of 1s</em> of order <strong>k</strong>“ has some center <code>grid[x][y] = 1</code> along with 4 arms of length <code>k-1</code> going up, down, left, and right, and made of <code>1</code>s. This is demonstrated in the diagrams below. Note that there could be <code>0</code>s or <code>1</code>s beyond the arms of the plus sign, only the relevant area of the plus sign is checked for 1s.</p><p><strong>Examples of Axis-Aligned Plus Signs of Order k:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&gt;   Order <span class="number">1</span>:</div><div class="line">&gt;   <span class="number">000</span></div><div class="line">&gt;   <span class="number">010</span></div><div class="line">&gt;   <span class="number">000</span></div><div class="line">&gt;   </div><div class="line">&gt;   Order <span class="number">2</span>:</div><div class="line">&gt;   <span class="number">00000</span></div><div class="line">&gt;   <span class="number">00100</span></div><div class="line">&gt;   <span class="number">01110</span></div><div class="line">&gt;   <span class="number">00100</span></div><div class="line">&gt;   <span class="number">00000</span></div><div class="line">&gt;   </div><div class="line">&gt;   Order <span class="number">3</span>:</div><div class="line">&gt;   <span class="number">0000000</span></div><div class="line">&gt;   <span class="number">0001000</span></div><div class="line">&gt;   <span class="number">0001000</span></div><div class="line">&gt;   <span class="number">0111110</span></div><div class="line">&gt;   <span class="number">0001000</span></div><div class="line">&gt;   <span class="number">0001000</span></div><div class="line">&gt;   <span class="number">0000000</span></div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><p>  <strong>Example1</strong></p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: N = <span class="number">5</span>, mines = [[<span class="number">4</span>, <span class="number">2</span>]]</div><div class="line">&gt;   Output: <span class="number">2</span></div><div class="line">&gt;   Explanation:</div><div class="line">&gt;   <span class="number">11111</span></div><div class="line">&gt;   <span class="number">11111</span></div><div class="line">&gt;   <span class="number">11111</span></div><div class="line">&gt;   <span class="number">11111</span></div><div class="line">&gt;   <span class="number">11011</span></div><div class="line">&gt;   In the above grid, the largest plus sign can only be order <span class="number">2</span>.  One of them is marked in bold.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>  <strong>Example2</strong></p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: N = <span class="number">2</span>, mines = []</div><div class="line">&gt;   Output: <span class="number">1</span></div><div class="line">&gt;   Explanation:</div><div class="line">&gt;   There is no plus sign of order <span class="number">2</span>, but there is of order <span class="number">1</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><p>  <strong>Example3</strong></p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;   Input: N = <span class="number">1</span>, mines = [[<span class="number">0</span>, <span class="number">0</span>]]</div><div class="line">&gt;   Output: <span class="number">0</span></div><div class="line">&gt;   Explanation:</div><div class="line">&gt;   There is no plus sign, so <span class="keyword">return</span> <span class="number">0</span>.</div><div class="line">&gt;</div></pre></td></tr></table></figure></blockquote><p>&gt;</p><blockquote><ul><li><p>思路</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;   动态规划，分别记录<span class="number">4</span>个方向上的最大连续<span class="number">1</span>的个数。比如”<span class="number">1001111</span>”， 每个位置出现的最大连续<span class="number">1</span>的个数分别为：”<span class="number">1001234</span>”，有了<span class="number">4</span>个方向的最长连续<span class="number">1</span>，order就是这四个方向的最小值，遍历每个位置的order，求出最大order即可。</div><div class="line">&gt;   设置<span class="number">4</span>个状态转移矩阵，lf[][],rt[][],dn[][],up[][]</div><div class="line">&gt;   如果当前的grid[i][j]=<span class="number">1</span>,那么</div><div class="line">&gt;   lf[i][j] = lf[i][j-<span class="number">1</span>]+<span class="number">1</span></div><div class="line">&gt;   rt、dn、up同理。</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote><ul><li><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">&gt;   <span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">&gt;       <span class="function"><span class="keyword">def</span> <span class="title">orderOfLargestPlusSign</span><span class="params">(self, N, mines)</span>:</span></div><div class="line">&gt;           grid = [[<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(N)] <span class="keyword">for</span> j <span class="keyword">in</span> range(N)]</div><div class="line">&gt;           <span class="keyword">for</span> mine <span class="keyword">in</span> mines:</div><div class="line">&gt;               a = mine[<span class="number">0</span>]</div><div class="line">&gt;               b = mine[<span class="number">1</span>]</div><div class="line">&gt;               grid[a][b] = <span class="number">0</span></div><div class="line">&gt;           lf = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(N)] <span class="keyword">for</span> j <span class="keyword">in</span> range(N)]</div><div class="line">&gt;           rt = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(N)] <span class="keyword">for</span> j <span class="keyword">in</span> range(N)]</div><div class="line">&gt;           dn = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(N)] <span class="keyword">for</span> j <span class="keyword">in</span> range(N)]</div><div class="line">&gt;           up = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(N)] <span class="keyword">for</span> j <span class="keyword">in</span> range(N)]</div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</div><div class="line">&gt;                   <span class="keyword">if</span> grid[i][j]==<span class="number">1</span>:</div><div class="line">&gt;                       lf[i][j] = <span class="number">1</span> <span class="keyword">if</span> j==<span class="number">0</span> <span class="keyword">else</span> lf[i][j<span class="number">-1</span>]+<span class="number">1</span></div><div class="line">&gt;                   <span class="keyword">if</span> grid[j][i]==<span class="number">1</span>:<span class="comment">##trick</span></div><div class="line">&gt;                       dn[j][i] = <span class="number">1</span> <span class="keyword">if</span> j==<span class="number">0</span> <span class="keyword">else</span> dn[j<span class="number">-1</span>][i]+<span class="number">1</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(N<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</div><div class="line">&gt;                   <span class="keyword">if</span> grid[i][j]==<span class="number">1</span>:</div><div class="line">&gt;                       rt[i][j] = <span class="number">1</span> <span class="keyword">if</span> j==N<span class="number">-1</span> <span class="keyword">else</span> rt[i][j+<span class="number">1</span>]+<span class="number">1</span></div><div class="line">&gt;                   <span class="keyword">if</span> grid[j][i]==<span class="number">1</span>: <span class="comment">##trick</span></div><div class="line">&gt;                       up[j][i] = <span class="number">1</span> <span class="keyword">if</span> j==N<span class="number">-1</span> <span class="keyword">else</span> up[j+<span class="number">1</span>][i]+<span class="number">1</span></div><div class="line">&gt;           re = <span class="number">0</span></div><div class="line">&gt;           <span class="keyword">for</span> i <span class="keyword">in</span> range(N):<span class="comment">##使用min、max函数会超时</span></div><div class="line">&gt;               <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</div><div class="line">&gt;                   tre = lf[i][j]</div><div class="line">&gt;                   tre = rt[i][j] <span class="keyword">if</span> rt[i][j]&lt;tre <span class="keyword">else</span> tre</div><div class="line">&gt;                   tre = dn[i][j] <span class="keyword">if</span> dn[i][j]&lt;tre <span class="keyword">else</span> tre</div><div class="line">&gt;                   tre = up[i][j] <span class="keyword">if</span> up[i][j]&lt;tre <span class="keyword">else</span> tre</div><div class="line">&gt;                   re = tre <span class="keyword">if</span> tre&gt;re <span class="keyword">else</span> re</div><div class="line">&gt;           <span class="keyword">return</span> re</div><div class="line">&gt;</div></pre></td></tr></table></figure></li></ul></blockquote><p>&gt;</p><blockquote></blockquote>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DP-Definition</title>
      <link href="/2018/05/25/DP-Definition/"/>
      <url>/2018/05/25/DP-Definition/</url>
      <content type="html"><![CDATA[<p>This post is about the definition of Differential Privacy. DP ensures privacy by randomness. So before the definition of DP, we need to know what is randomized algorithms.</p><p>In this post, Domain is represented as $\mathbb{N}^{|X|}$, where $|X|$ means the element number of the domain; Range is represented as $\mathbb{R}^{|k|}$, where $k$ means the element is $k$ dimentions.</p><p>Range(A) means the output of the mechanism A.</p><a id="more"></a><h2 id="Randomized-algorithms"><a href="#Randomized-algorithms" class="headerlink" title="Randomized algorithms"></a>Randomized algorithms</h2><p>In general, a randomized algorithm with domain $A$ and (discrete) range $B$ will be associated with a mapping from $A$ to the probability simplex over $B$, denoted $\Delta(B)$:</p><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 10.25.01 AM.png" alt="Screen Shot 2018-06-02 at 10.25.01 AM"></p><blockquote><ol><li></li><li></li></ol></blockquote><h2 id="Differential-Privacy"><a href="#Differential-Privacy" class="headerlink" title="Differential Privacy"></a>Differential Privacy</h2><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 10.31.11 AM.png" alt="Screen Shot 2018-06-02 at 10.31.11 AM"></p><blockquote><p>Remarks:</p><ol><li><p>Assuming the existence of a trusted and trustworthy curator who holds the data of individuals in a database </p></li><li><p>For particitants, differential privacy promises to protect individuals from any additional harm that they might face due to their data being in the private database x that they would not have faced had their data not been part of x. </p><p>For adversaries, differential privacy asserts that for all pairs of adjacent databases x, y and all outputs $o$, an adversary cannot distinguish which is the true database on the basis of observing $o$, which implies the adversary’s prior and posterior views about an individual should not be too different.</p></li><li><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 3.02.26 PM.png" alt="Screen Shot 2018-06-02 at 3.02.26 PM"></p></li><li><p>DP merely ensures that one’s participation in a survey will not in itself be disclosed, nor will participation lead to disclosure of any specifics that one has contributed to the survey. </p></li></ol></blockquote><h3 id="Understanding-of-DP-definition"><a href="#Understanding-of-DP-definition" class="headerlink" title="Understanding of DP definition"></a>Understanding of DP definition</h3><ol><li><p>How DP mean by saying it can limit the knowledge of adversaries by quering the database?</p><blockquote><p>According to the definition, $Pr(o|D)\le{e^{\epsilon}Pr(o|D’)}$, by observing the output $o$, the adversary cannot reliably infer whether the database is $D$ or $D’$, which means it cannot know whether someone is added or removed to the database. Indeed, the smaller the $\epsilon$ is, the closer the likehood ratio of $D$ to $D’$ is to 1. Therefore, when $\epsilon$ is small, the adversary cannot distinguish the true database.</p></blockquote></li></ol><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><blockquote><ol><li>Post-Processing : Differential privacy is immune to post-processing</li></ol></blockquote><h2 id="Vector-DP"><a href="#Vector-DP" class="headerlink" title="Vector DP"></a>Vector DP</h2><blockquote><p>This following details are from<a href="http://www.vldb.org/pvldb/vol7/p919-to.pdf" target="_blank" rel="noopener">[2014-To]</a>.</p></blockquote><h2 id="Group-DP"><a href="#Group-DP" class="headerlink" title="Group DP"></a>Group DP</h2><blockquote><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 9.41.37 AM.png" alt="Screen Shot 2018-06-02 at 9.41.37 AM"></p><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-05-17 at 10.07.34 AM.png" alt="Screen Shot 2018-05-17 at 10.07.34 AM"></p><ol><li>the strength of the privacy guarantee drops linearly with the size of the group</li></ol></blockquote><h2 id="Event-VS-User"><a href="#Event-VS-User" class="headerlink" title="Event VS User"></a>Event VS User</h2><blockquote><p>Differential privacy promises that the behavior of an algorithm will be roughly unchanged even if a single entry in the database is modified. But what constitutes a single entry in the database? Consider for example a database that takes the form of a graph. Such a database might encode a social network: each individual i ∈ [n] is represented by a vertex in the graph, and friendships between<br>individuals are represented by edges.</p><p>We could consider differential privacy at a level of granularity corresponding to individuals: that is, we could require that differentially private algorithms be insensitive to the addition or removal of any vertex from the graph.</p><p>We could on the other hand consider differential privacy at a level of granularity corresponding to edges, and ask our algorithms to be insensitive only to the addition or removal of single, or small numbers of, edges from the graph.</p><p>As another example, a differentially private movie recommendation system can be designed to protect the data in the training set at the “<strong>event</strong>” level of single movies, hiding the viewing/rating of any single<br>movie but not, say, hiding an individual’s enthusiasm for cowboy westerns or gore, or at the “<strong>user</strong>” level of an individual’s entire viewing and rating history.</p></blockquote><h2 id="Non-interactive-VS-Interactive"><a href="#Non-interactive-VS-Interactive" class="headerlink" title="Non-interactive VS Interactive"></a>Non-interactive VS Interactive</h2><h3 id="Non-interactive"><a href="#Non-interactive" class="headerlink" title="Non-interactive"></a>Non-interactive</h3><blockquote><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-01 at 8.50.43 PM.png" alt="Screen Shot 2018-06-01 at 8.50.43 PM"></p></blockquote><h3 id="Interactive"><a href="#Interactive" class="headerlink" title="Interactive"></a>Interactive</h3><blockquote><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-01 at 8.51.55 PM.png" alt="Screen Shot 2018-06-01 at 8.51.55 PM"></p></blockquote><h2 id="Geo-indistinguishability"><a href="#Geo-indistinguishability" class="headerlink" title="Geo-indistinguishability"></a>Geo-indistinguishability</h2><p>Enjoying $l$-privacy within $r$ means that for any two locations $x_1$ and $x_2$ $s.t. d_2(x_1,x_2)\le r$, the probability of generating the same perturbed location $z$ should be similar, where $\frac{P(z|x_1)}{P(z|x_2)}\le e^{\epsilon d_2(x_1,x_2)}$ and $l=\epsilon r$. In short, $\epsilon $-geo-indistinguishability provides $\epsilon d_2$-privacy.</p><blockquote><ol><li>It guarantees that obfuscated locations are statistically indistinguishable from other locations within a radius around the users’ real location.</li><li>And the parameter $\epsilon$ is no longer considered as the privacy budget; actually it is determined as $\epsilon=\frac{l}{r}$, where $l$ is the privacy level and $r$ is the privacy radius. This ensures that, when Alice is in location $x$ and releases a perturbed location $z$, her location is statistically indistinguishable from all the other locations $x’$ within a radius of $r$ around her, i.e., $\frac{P(z|x)}{P(z|x’)}\le {e^{\epsilon d(x,x’)}}\le {e^l}$.</li><li>Note also that standard differential privacy simply corresponds to $\epsilon d_h(x, x’ )$-privacy, where $d_h$ is the Hamming distance between databases $x,x’$ , i.e. the number of individuals in which they differ. And standard differential privacy is too strong because it aims at completely protecting the value of a single individual; while in geo-indistinguishability the user’s secret is a single location.</li></ol></blockquote><h2 id="Privacy-loss"><a href="#Privacy-loss" class="headerlink" title="Privacy loss"></a>Privacy loss</h2><blockquote><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 3.04.16 PM.png" alt="Screen Shot 2018-06-02 at 3.04.16 PM"></p></blockquote><h2 id="DP-Utility-Analysis"><a href="#DP-Utility-Analysis" class="headerlink" title="DP Utility Analysis"></a>DP Utility Analysis</h2><blockquote><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 3.11.05 PM.png" alt="Screen Shot 2018-06-02 at 3.11.05 PM"></p><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 3.11.19 PM.png" alt="Screen Shot 2018-06-02 at 3.11.19 PM"></p><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-02 at 3.12.22 PM.png" alt="Screen Shot 2018-06-02 at 3.12.22 PM"></p><p>This following details are from <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8319407" target="_blank" rel="noopener">[2018-Yang]</a>.</p><p><img src="/2018/05/25/DP-Definition/2018-YANG.png" alt="2018-YANG"></p><p><img src="/2018/05/25/DP-Definition/2018-YANG1.png" alt="2018-YANG1-7391264"></p></blockquote><h2 id="Remarks"><a href="#Remarks" class="headerlink" title="Remarks"></a>Remarks</h2><h3 id="Prupose"><a href="#Prupose" class="headerlink" title="Prupose"></a>Prupose</h3><ul><li>differential privacy aims to ensure that the output of the algorithm does not significantly depend on any particular individual’s data and ensures that an adversary should not be able to confidently infer whether a particular individual is present in a database even with access to every other entry in the database and an unbounded computational power. <a href="http://ceur-ws.org/Vol-1558/paper35.pdf" target="_blank" rel="noopener">[2016-Wang]</a></li></ul><h2 id="Adversary-Models"><a href="#Adversary-Models" class="headerlink" title="Adversary Models"></a>Adversary Models</h2><h3 id="2017-Mousumi"><a href="#2017-Mousumi" class="headerlink" title="[2017-Mousumi]"></a><a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-59870-3_14.pdf" target="_blank" rel="noopener">[2017-Mousumi]</a></h3><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-04 at 10.31.07 AM.png" alt="Screen Shot 2018-06-04 at 10.31.07 AM"></p><h3 id="2016-Chen"><a href="#2016-Chen" class="headerlink" title="[2016-Chen]"></a><a href="http://www.shivakasiviswanathan.com/ICDE16b.pdf" target="_blank" rel="noopener">[2016-Chen]</a></h3><p><img src="/2018/05/25/DP-Definition/Screen Shot 2018-06-06 at 12.11.08 PM.png" alt="Screen Shot 2018-06-06 at 12.11.08 PM"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[2014-To] A Framework for Protecting Worker Location Privacy in Spatial Crowdsourcing</p><p>[2018-Yang] Density-Based Location Preservation for Mobile Crowdsensing With Differential Privacy</p><p>[2016-Wang] Using Randomized Response for Differential Privacy Preserving Data Collection </p><p>[2017-Mousumi] Computing Aggregates Over Numeric Data with Personalized Local Differential Privacy</p><p>[2016-Chen] Private Spatial Data Aggregation in the Local Setting</p>]]></content>
      
      <categories>
          
          <category> Differential Privacy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Differential Privacy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Greedy Algorithm</title>
      <link href="/2018/05/23/Greedy-Algorithm/"/>
      <url>/2018/05/23/Greedy-Algorithm/</url>
      <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_32400847/article/details/51336300" target="_blank" rel="noopener">从零开始学贪心算法</a></p><p>贪心算法是指在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，只做出在某种意义上的局部最优解。贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择，选择的贪心策略必须具备无后效性，即某个状态以前的过程不会影响以后的状态，只与当前状态有关。</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"></div></div><a id="more"></a>## 经典问题### 活动选择1. 题目   <div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>有n个需要在同一天使用同一个教室的活动a1,a2,…,an，教室同一时刻只能由一个活动使用。每个活动ai都有一个开始时间si和结束时间fi 。一旦被选择后，活动ai就占据半开时间区间[si,fi)。如果[si,fi]和[sj,fj]互不重叠，ai和aj两个活动就可以被安排在这一天。该问题就是要安排这些活动使得尽量多的活动能不冲突的举行。例如下图所示的活动集合S，其中各项活动按照结束时间单调递增排序。</p><p><img src="/2018/05/23/Greedy-Algorithm/20160509195427891.jpeg" alt="20160509195427891"></p><p>为了方便，我们用不同颜色的线条代表每个活动，线条的长度就是活动所占据的时间段，蓝色的线条表示我们已经选择的活动；红色的线条表示我们没有选择的活动。</p><p>如果我们每次都选择开始时间最早的活动，不能得到最优解：</p><p><img src="/2018/05/23/Greedy-Algorithm/20160507100634641 (1" alt="20160507100634641 (1)">.jpeg)</p><p>如果我们每次都选择持续时间最短的活动，不能得到最优解:</p><p><img src="/2018/05/23/Greedy-Algorithm/20160507101904360.jpeg" alt="20160507101904360"></p><p>我们的贪心策略应该是每次选取结束时间最早的活动, 按这种方法选择相容活动为未安排活动留下尽可能多的时间。</p></div></div><h3 id="钱币找零"><a href="#钱币找零" class="headerlink" title="钱币找零"></a>钱币找零</h3><p>假设1元、2元、5元、10元、20元、50元、100元的纸币分别有c0, c1, c2, c3, c4, c5, c6张。现在要用这些钱来支付K元，至少要用多少张纸币？用贪心算法的思想，很显然，每一步尽可能用面值大的纸币即可。</p><h4 id="Lemonade-Change"><a href="#Lemonade-Change" class="headerlink" title="Lemonade Change"></a><a href="https://leetcode.com/problems/lemonade-change/description/" target="_blank" rel="noopener">Lemonade Change</a></h4><ol><li><p><a href="https://leetcode.com/problems/lemonade-change/description/" target="_blank" rel="noopener">题目</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>At a lemonade stand, each lemonade costs <code>$5</code>. </p><p>Customers are standing in a queue to buy from you, and order one at a time (in the order specified by <code>bills</code>).</p><p>Each customer will only buy one lemonade and pay with either a <code>$5</code>, <code>$10</code>, or <code>$20</code> bill.  You must provide the correct change to each customer, so that the net transaction is that the customer pays $5.</p><p>Note that you don’t have any change in hand at first.</p><p>Return <code>true</code> if and only if you can provide every customer with correct change.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">20</span>]</div><div class="line">Output: <span class="keyword">true</span></div><div class="line">Explanation: </div><div class="line">From the first <span class="number">3</span> customers, we collect three $<span class="number">5</span> bills in order.</div><div class="line">From the fourth customer, we collect a $<span class="number">10</span> bill and give back a $<span class="number">5</span>.</div><div class="line">From the fifth customer, we give a $<span class="number">10</span> bill and a $<span class="number">5</span> bill.</div><div class="line">Since all customers got correct change, we output <span class="keyword">true</span>.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">5</span>,<span class="number">5</span>,<span class="number">10</span>]</div><div class="line">Output: <span class="keyword">true</span></div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">10</span>,<span class="number">10</span>]</div><div class="line">Output: <span class="keyword">false</span></div></pre></td></tr></table></figure><p><strong>Example 4:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">5</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">10</span>,<span class="number">20</span>]</div><div class="line">Output: <span class="keyword">false</span></div><div class="line">Explanation: </div><div class="line">From the first two customers in order, we collect two $<span class="number">5</span> bills.</div><div class="line">For the next two customers in order, we collect a $<span class="number">10</span> bill and give back a $<span class="number">5</span> bill.</div><div class="line">For the last customer, we can<span class="string">'t give change of $15 back because we only have two $10 bills.</span></div><div class="line"><span class="string">Since not every customer received correct change, the answer is false.</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://leetcode.com/problems/lemonade-change/discuss/160917/Python-code-with-explanations" target="_blank" rel="noopener">思路 &amp; 代码</a></p></li></ol><h3 id="背包问题"><a href="#背包问题" class="headerlink" title="背包问题"></a>背包问题</h3><p>我们考虑这样一种背包问题：在选择物品i装入背包时，可以选择物品的一部分，而不一定要全部装入背包。这时便可以使用贪心算法求解了。计算每种物品的单位重量价值作为贪心选择的依据指标，选择单位重量价值最高的物品，将尽可能多的该物品装入背包，依此策略一直地进行下去，直到背包装满为止。在零一背包问题中贪心选择之所以不能得到最优解原因是贪心选择无法保证最终能将背包装满，部分闲置的背包空间使每公斤背包空间的价值降低了。</p><h3 id="多机调度"><a href="#多机调度" class="headerlink" title="多机调度"></a>多机调度</h3><p>个作业组成的作业集，可由m台相同机器加工处理。要求给出一种作业调度方案，使所给的n个作业在尽可能短的时间内由m台机器加工处理完成。作业不能拆分成更小的子作业；每个作业均可在任何一台机器上加工处理。这个问题是NP完全问题，还没有有效的解法(求最优解)，但是可以用贪心选择策略设计出较好的近似算法(求次优解)。当n&lt;=m时，只要将作业时间区间分配给作业即可；当n&gt;m时，首先将n个作业从大到小排序，然后依此顺序将作业分配给空闲的处理机。也就是说从剩下的作业中，选择需要处理时间最长的，然后依次选择处理时间次长的，直到所有的作业全部处理完毕，或者机器不能再处理其他作业为止。如果我们每次是将需要处理时间最短的作业分配给空闲的机器，那么可能就会出现其它所有作业都处理完了只剩所需时间最长的作业在处理的情况，这样势必效率较低。</p><h2 id="简单"><a href="#简单" class="headerlink" title="简单"></a>简单</h2><h3 id="Score-After-Flipping-Matrix"><a href="#Score-After-Flipping-Matrix" class="headerlink" title="Score After Flipping Matrix"></a><a href="https://leetcode.com/problems/score-after-flipping-matrix/description/" target="_blank" rel="noopener">Score After Flipping Matrix</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>We have a two dimensional matrix <code>A</code> where each value is <code>0</code> or <code>1</code>.</p><p>A move consists of choosing any row or column, and toggling each value in that row or column: changing all <code>0</code>s to <code>1</code>s, and all <code>1</code>s to <code>0</code>s.</p><p>After making any number of moves, every row of this matrix is interpreted as a binary number, and the score of the matrix is the sum of these numbers.</p><p>Return the highest possible score.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]]</div><div class="line">Output: <span class="number">39</span></div><div class="line">Explanation:</div><div class="line">Toggled to [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]].</div><div class="line"><span class="number">0b1111</span> + <span class="number">0b1001</span> + <span class="number">0b1111</span> = <span class="number">15</span> + <span class="number">9</span> + <span class="number">15</span> = <span class="number">39</span></div></pre></td></tr></table></figure><p><strong>Note:</strong></p><ol><li><code>1 &lt;= A.length &lt;= 20</code></li><li><code>1 &lt;= A[0].length &lt;= 20</code></li><li><code>A[i][j]</code> is <code>0</code> or <code>1</code>.</li></ol></div></div></li><li><p><a href="https://blog.ksgin.com/2018/07/07/Score-After-Flipping-Matrix/" target="_blank" rel="noopener">思路</a> </p><p>贪心算法，我们知道二进制串中越高位的占的比重越大，所以我们可以直接从高到底来判断是否进行 toggling 操作。</p><p>首先为行，行中每个数字的权重不一样，从高往低，所以在行中我们只需要对第一位进行判断（即数组第一个数字），当他为0的时候我们要将他变为1 就需要 toggling ，这样可以使这一行数字的二进制串最大。</p><p>列中每一列的权重一样，所以我们需要判断这一列中1的个数和0的个数，当0的个数大于1的个数时我们进行 toggling 操作。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">toggle_row</span><span class="params">(self,row)</span>:</span></div><div class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(row)):</div><div class="line">            <span class="keyword">if</span> row[idx] == <span class="number">0</span>:</div><div class="line">                row[idx] = <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                row[idx] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> row</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">toggle_col</span><span class="params">(self,col)</span>:</span></div><div class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(col)):</div><div class="line">            <span class="keyword">if</span> col[idx] == <span class="number">0</span>:</div><div class="line">                col[idx] = <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                col[idx] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> col</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">matrixScore</span><span class="params">(self, A)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type A: List[List[int]]</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        result = <span class="number">0</span></div><div class="line">        A = np.array(A)</div><div class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(A)):</div><div class="line">            <span class="keyword">if</span> A[idx][<span class="number">0</span>] == <span class="number">0</span>:</div><div class="line">                A[idx] = self.toggle_row(A[idx])</div><div class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">1</span>,len(A[<span class="number">0</span>])):</div><div class="line">            col = A[:,idx]</div><div class="line">            <span class="keyword">if</span> sum(col) &lt; len(col)/<span class="number">2</span>:</div><div class="line">                A[:, idx] = self.toggle_col(col)</div><div class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> A:</div><div class="line">            t = <span class="string">''</span></div><div class="line">            <span class="keyword">for</span> ele <span class="keyword">in</span> row:</div><div class="line">                t += str(ele)</div><div class="line">            result += int(t,<span class="number">2</span>)</div><div class="line">        <span class="keyword">return</span> result</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Advantage-Shuffle"><a href="#Advantage-Shuffle" class="headerlink" title="Advantage Shuffle"></a><a href="https://leetcode.com/problems/advantage-shuffle/description/" target="_blank" rel="noopener">Advantage Shuffle</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given two arrays <code>A</code> and <code>B</code> of equal size, the <em>advantage of A with respect to B</em> is the number of indices <code>i</code> for which <code>A[i] &gt; B[i]</code>.</p><p>Return <strong>any</strong> permutation of <code>A</code> that maximizes its advantage with respect to <code>B</code>.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: A = [<span class="number">2</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">15</span>], B = [<span class="number">1</span>,<span class="number">10</span>,<span class="number">4</span>,<span class="number">11</span>]</div><div class="line">Output: [<span class="number">2</span>,<span class="number">11</span>,<span class="number">7</span>,<span class="number">15</span>]</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: A = [<span class="number">12</span>,<span class="number">24</span>,<span class="number">8</span>,<span class="number">32</span>], B = [<span class="number">13</span>,<span class="number">25</span>,<span class="number">32</span>,<span class="number">11</span>]</div><div class="line">Output: [<span class="number">24</span>,<span class="number">32</span>,<span class="number">8</span>,<span class="number">12</span>]</div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://leetcode.com/problems/advantage-shuffle/discuss/153153/Python-simple-binary-search-greedy-solution" target="_blank" rel="noopener">思路</a> </p><p>题目大意是对于给定两个长度相同的数组$A$和$B$，要求重排$A$，尽量使$A$中每一个元素值都大于$B$中对应元素。</p><p>这就是田忌赛马，尽量用小的数去对应对方的最大的数，然后用差距不大的数对应对方数。实现的时候，先对$A$排序，然后对每一个$B$中元素使用<code>bisect.bisect_right</code>    找出离他最近的$A$中的元素，与之对应；如果没有，则使用$A$中最小的与之对应。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> bisect</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">advantageCount</span><span class="params">(self, A, B)</span>:</span></div><div class="line">        sorted_a = sorted(A)</div><div class="line">        result = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> B:</div><div class="line">            idx = bisect.bisect_right(sorted_a,i)</div><div class="line">            <span class="keyword">if</span> idx == len(sorted_a):result.append(sorted_a.pop(<span class="number">0</span>))</div><div class="line">            <span class="keyword">else</span>: result.append(sorted_a.pop(idx))</div><div class="line">        <span class="keyword">return</span> result</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Boats-to-Save-People"><a href="#Boats-to-Save-People" class="headerlink" title="Boats to Save People"></a><a href="https://leetcode.com/problems/boats-to-save-people/description/" target="_blank" rel="noopener">Boats to Save People</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>The <code>i</code>-th person has weight <code>people[i]</code>, and each boat can carry a maximum weight of <code>limit</code>.</p><p>Each boat carries at most 2 people at the same time, provided the sum of the weight of those people is at most <code>limit</code>.</p><p>Return the minimum number of boats to carry every given person.  (It is guaranteed each person can be carried by a boat.)</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: people = [<span class="number">1</span>,<span class="number">2</span>], limit = <span class="number">3</span></div><div class="line">Output: <span class="number">1</span></div><div class="line">Explanation: <span class="number">1</span> boat (<span class="number">1</span>, <span class="number">2</span>)</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: people = [<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], limit = <span class="number">3</span></div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation: <span class="number">3</span> boats (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">2</span>) and (<span class="number">3</span>)</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: people = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>], limit = <span class="number">5</span></div><div class="line">Output: <span class="number">4</span></div><div class="line">Explanation: <span class="number">4</span> boats (<span class="number">3</span>), (<span class="number">3</span>), (<span class="number">4</span>), (<span class="number">5</span>)</div></pre></td></tr></table></figure></div></div> </li><li><p><a href="https://leetcode.com/problems/boats-to-save-people/discuss/156740/C++JavaPython-Two-Pointers" target="_blank" rel="noopener">思路</a></p><p>题目大概是要让一群人过河，每一艘船只能载两个人且承重最多是$limit$。问把所有人送过河所需的最少船只。</p><p>贪心策略：把体重排序之后，每次选择最重的人和最轻的人，如果体重之和在船只承重范围内，则一次送走两个人；否则，只能送走最重的那个人。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numRescueBoats</span><span class="params">(self, people, limit)</span>:</span></div><div class="line">        people = sorted(people)</div><div class="line">        front,back = <span class="number">0</span>,len(people)<span class="number">-1</span></div><div class="line">        count = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> front&lt;=back:</div><div class="line">            <span class="keyword">if</span> people[front] + people[back] &lt;= limit:</div><div class="line">                front += <span class="number">1</span></div><div class="line">                back -= <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                back -= <span class="number">1</span></div><div class="line">            count += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> count</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Is-Subsequence"><a href="#Is-Subsequence" class="headerlink" title="Is Subsequence"></a><a href="https://leetcode.com/problems/is-subsequence/description/" target="_blank" rel="noopener">Is Subsequence</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a string <strong>s</strong> and a string <strong>t</strong>, check if <strong>s</strong> is subsequence of <strong>t</strong>.</p><p>You may assume that there is only lower case English letters in both <strong>s</strong> and <strong>t</strong>. <strong>t</strong> is potentially a very long (length ~= 500,000) string, and <strong>s</strong> is a short string (&lt;=100).</p><p>A subsequence of a string is a new string which is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (ie, <code>&quot;ace&quot;</code> is a subsequence of <code>&quot;abcde&quot;</code> while <code>&quot;aec&quot;</code> is not).</p><p><strong>Example 1:</strong><br><strong>s</strong> = <code>&quot;abc&quot;</code>, <strong>t</strong> = <code>&quot;ahbgdc&quot;</code></p><p>Return <code>true</code>.</p><p><strong>Example 2:</strong><br><strong>s</strong> = <code>&quot;axc&quot;</code>, <strong>t</strong> = <code>&quot;ahbgdc&quot;</code></p><p>Return <code>false</code>.</p></div></div></li><li><p>思路</p><p>题意大概是判断一个字符串<code>s</code>是否为另一个<code>t</code>的子串，一个串的子串：通过删去一些字符得到的字符串。</p><p>很显然，就从头比较两个字符串，设置两个指针<code>p1</code>和<code>p2</code>，初始分别指向字符串<code>s</code>和<code>t</code>的首字符。如果相同，则同时移动两个指针指向下一个字符；否则只移动<code>p2</code>指向<code>t</code>的下一个字符，直到找到与<code>p1</code>指向的字符。遍历一遍即可，如果<code>p1</code>不能超过<code>s</code>的最后一个字符，则不能是子串。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSubsequence</span><span class="params">(self, s, t)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s: <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        cur_s = <span class="number">0</span></div><div class="line">        cur_t = <span class="number">0</span></div><div class="line">        ls = len(s)</div><div class="line">        lt = len(t)</div><div class="line">        <span class="keyword">while</span> cur_s&lt;ls <span class="keyword">and</span> cur_t&lt;lt:</div><div class="line">            <span class="keyword">if</span> s[cur_s]==t[cur_t]:</div><div class="line">                cur_s += <span class="number">1</span></div><div class="line">            cur_t += <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> cur_s==ls:<span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Couples-Holding-Hands"><a href="#Couples-Holding-Hands" class="headerlink" title="Couples Holding Hands"></a><a href="https://leetcode.com/problems/couples-holding-hands/description/" target="_blank" rel="noopener">Couples Holding Hands</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>N couples sit in 2N seats arranged in a row and want to hold hands. We want to know the minimum number of swaps so that every couple is sitting side by side. A <em>swap</em> consists of choosing <strong>any</strong> two people, then they stand up and switch seats.</p><p>The people and seats are represented by an integer from <code>0</code> to <code>2N-1</code>, the couples are numbered in order, the first couple being <code>(0, 1)</code>, the second couple being <code>(2, 3)</code>, and so on with the last couple being <code>(2N-2, 2N-1)</code>.</p><p>The couples’ initial seating is given by <code>row[i]</code> being the value of the person who is initially sitting in the i-th seat.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: row = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</div><div class="line">Output: <span class="number">1</span></div><div class="line">Explanation: We only need to swap the second (row[<span class="number">1</span>]) <span class="keyword">and</span> third (row[<span class="number">2</span>]) person.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: row = [<span class="number">3</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>]</div><div class="line">Output: <span class="number">0</span></div><div class="line">Explanation: All couples are already seated side by side.</div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/u014688145/article/details/79057475" target="_blank" rel="noopener">思路</a></p><p>贪心，不过不好证明。从位置0开始，看位置1是否是它的partner，如果是，直接忽略这组couple，如果不是，从后面的数组中，找到partner直接交换，并且记录交换次数一次。遍历完整个数组即为最优解。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minSwapsCouples</span><span class="params">(self, row)</span>:</span></div><div class="line">        pos = <span class="number">0</span></div><div class="line">        step = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> pos&lt;len(row):</div><div class="line">            wife = row[pos]</div><div class="line">            husband = row[pos+<span class="number">1</span>]</div><div class="line">            <span class="keyword">if</span> wife%<span class="number">2</span>==<span class="number">0</span>:</div><div class="line">                <span class="keyword">if</span> wife+<span class="number">1</span>==husband:</div><div class="line">                    <span class="keyword">pass</span></div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    husband_pos = row.index(wife + <span class="number">1</span>)</div><div class="line">                    row[pos + <span class="number">1</span>], row[husband_pos] = row[husband_pos], row[pos + <span class="number">1</span>]  <span class="comment"># 找到husband的位置，交换</span></div><div class="line">                    step += <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">if</span> wife<span class="number">-1</span>==husband:</div><div class="line">                    <span class="keyword">pass</span></div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    husband_pos = row.index(wife - <span class="number">1</span>)</div><div class="line">                    row[pos + <span class="number">1</span>], row[husband_pos] = row[husband_pos], row[pos + <span class="number">1</span>]  <span class="comment"># 找到husband的位置，交换</span></div><div class="line">                    step += <span class="number">1</span></div><div class="line">            pos += <span class="number">2</span></div><div class="line">        <span class="keyword">return</span> step</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Jump-Game"><a href="#Jump-Game" class="headerlink" title="Jump Game"></a><a href="https://leetcode.com/problems/jump-game/description/" target="_blank" rel="noopener">Jump Game</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of non-negative integers, you are initially positioned at the first index of the array.</p><p>Each element in the array represents your maximum jump length at that position.</p><p>Determine if you are able to reach the last index.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>]</div><div class="line">Output: <span class="keyword">true</span></div><div class="line">Explanation: Jump <span class="number">1</span> step from index <span class="number">0</span> to <span class="number">1</span>, then <span class="number">3</span> steps to the last index.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">4</span>]</div><div class="line">Output: <span class="keyword">false</span></div><div class="line">Explanation: You will always arrive at index <span class="number">3</span> no matter what. Its maximum</div><div class="line">             jump length is <span class="number">0</span>, which makes it impossible to reach the last index.</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>从index=i的位置出发，其能到达的最远位置是 i+nums[i]。那么我们设置一个青蛙能达到的最远位置为reach，如果最后reach大于等于数组的长度，那么可以到达。可是如何更新reach呢？因为reach表示最远可到达，那么我们一遍扫描数组nums，pos从0开始，表示初始位置，那么如果青蛙从当前位置可以达到的下一个位置是pos+nums[pos]，那么reach=max(reach, pos+nums[pos])。但是在比较之前，注意reach要大于等于pos，否则青蛙无法跳跃到pos，则肯定无法成功跳跃到最后一步。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canJump</span><span class="params">(self, nums)</span>:</span></div><div class="line">        reach = <span class="number">0</span></div><div class="line">        pos = <span class="number">0</span></div><div class="line">        l = len(nums)</div><div class="line">        <span class="keyword">while</span> pos&lt;=reach <span class="keyword">and</span> pos&lt;l:</div><div class="line">            reach = max(reach, pos+nums[pos])</div><div class="line">            pos += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> reach&gt;=l<span class="number">-1</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Jump-Game-II"><a href="#Jump-Game-II" class="headerlink" title="Jump Game II"></a><a href="https://leetcode.com/problems/jump-game-ii/description/" target="_blank" rel="noopener">Jump Game II</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given an array of non-negative integers, you are initially positioned at the first index of the array.</p><p>Each element in the array represents your maximum jump length at that position.</p><p>Your goal is to reach the last index in the minimum number of jumps.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>]</div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: The minimum number of jumps to reach the last index <span class="keyword">is</span> <span class="number">2.</span></div><div class="line">    Jump <span class="number">1</span> step <span class="keyword">from</span> index <span class="number">0</span> to <span class="number">1</span>, then <span class="number">3</span> steps to the last index.</div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/makuiyu/article/details/43698969" target="_blank" rel="noopener">思路</a></p><p>上一道题目的拓展，思路不变，扫描一次，查看当前的pos是否在reach里面。那么这道题的重点是扫描时候需要跳跃？青蛙从pos=0出发，跳跃一次，其一次能到达的最大位置是reach = nums[pos]+pos，那么当前的位置pos大于reach，说明一次跳跃不够，需要再一次跳跃。所以跳跃的条件是 pos&gt;reach。那么第二次跳跃的最大reach是多少呢？青蛙一次跳跃可以到达的地点是pos从1至reach，那么二次跳跃的最大reach是</p><p>max_reach = max( max_reach ,  nums[pos]+pos )  when 1&lt;=pos&lt;=reach</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jump</span><span class="params">(self, nums)</span>:</span></div><div class="line">        reach = <span class="number">0</span></div><div class="line">        pos = <span class="number">0</span></div><div class="line">        l = len(nums)</div><div class="line">        step = <span class="number">0</span></div><div class="line">        last = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> cur <span class="keyword">in</span> range(l):</div><div class="line">            <span class="keyword">if</span> cur&gt;last:</div><div class="line">                step += <span class="number">1</span></div><div class="line">                last = reach</div><div class="line">            reach = max(reach, nums[cur]+cur)</div><div class="line">        <span class="keyword">return</span> step</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Gas-Station"><a href="#Gas-Station" class="headerlink" title="Gas Station"></a><a href="https://leetcode.com/problems/gas-station/description/" target="_blank" rel="noopener">Gas Station</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">There are N gas stations along a circular route, where the amount of gas at station i <span class="keyword">is</span> gas[i].</div><div class="line"></div><div class="line">You have a car <span class="keyword">with</span> an unlimited gas tank <span class="keyword">and</span> it costs cost[i] of gas to travel <span class="keyword">from</span> station i to its next station (i+<span class="number">1</span>). You begin the journey <span class="keyword">with</span> an empty tank at one of the gas stations.</div><div class="line"></div><div class="line">Return the starting gas station<span class="string">'s index if you can travel around the circuit once in the clockwise direction, otherwise return -1.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Note:</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">If there exists a solution, it is guaranteed to be unique.</span></div><div class="line"><span class="string">Both input arrays are non-empty and have the same length.</span></div><div class="line"><span class="string">Each element in the input arrays is a non-negative integer.</span></div><div class="line"><span class="string">Example 1:</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Input: </span></div><div class="line"><span class="string">gas  = [1,2,3,4,5]</span></div><div class="line"><span class="string">cost = [3,4,5,1,2]</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Output: 3</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Explanation:</span></div><div class="line"><span class="string">Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4</span></div><div class="line"><span class="string">Travel to station 4. Your tank = 4 - 1 + 5 = 8</span></div><div class="line"><span class="string">Travel to station 0. Your tank = 8 - 2 + 1 = 7</span></div><div class="line"><span class="string">Travel to station 1. Your tank = 7 - 3 + 2 = 6</span></div><div class="line"><span class="string">Travel to station 2. Your tank = 6 - 4 + 3 = 5</span></div><div class="line"><span class="string">Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.</span></div><div class="line"><span class="string">Therefore, return 3 as the starting index.</span></div><div class="line"><span class="string">Example 2:</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Input: </span></div><div class="line"><span class="string">gas  = [2,3,4]</span></div><div class="line"><span class="string">cost = [3,4,3]</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Output: -1</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Explanation:</span></div><div class="line"><span class="string">You can'</span>t start at station <span class="number">0</span> <span class="keyword">or</span> <span class="number">1</span>, <span class="keyword">as</span> there <span class="keyword">is</span> <span class="keyword">not</span> enough gas to travel to the next station.</div><div class="line">Let<span class="string">'s start at station 2 and fill up with 4 unit of gas. Your tank = 0 + 4 = 4</span></div><div class="line"><span class="string">Travel to station 0. Your tank = 4 - 3 + 2 = 3</span></div><div class="line"><span class="string">Travel to station 1. Your tank = 3 - 3 + 3 = 3</span></div><div class="line"><span class="string">You cannot travel back to station 2, as it requires 4 unit of gas but you only have 3.</span></div><div class="line"><span class="string">Therefore, you can'</span>t travel around the circuit once no matter where you start.</div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/qq508618087/article/details/50990076" target="_blank" rel="noopener">思路</a></p><p>从index=0的位置出发，如果left = gas[index]-cost[index]大于等于0，说明可以到达下一个地点。那么从下一个地点计算left = left + gas[index]-cost[index]，表示用上一次剩下的油和当前地点提供的油，是否可以到达下一个地点，如果大于等于0，则可以。</p><p>如果left一直是大于0，那么就可以一直走下去。否则，如果left小于0，那么说明不能继续往下走了，那么记录当前lack=left，将欠下来的油记录下来，同时left=0，从下一个位置重新走，如果最终遍历完数组剩下的油能够弥补之前欠下的，那么可以到达，并返回最后一次开始的位置。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canCompleteCircuit</span><span class="params">(self, gas, cost)</span>:</span></div><div class="line">        left = <span class="number">0</span></div><div class="line">        lack = <span class="number">0</span></div><div class="line">        begin = <span class="number">0</span></div><div class="line">        l = len(gas)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">            left += gas[i]-cost[i]</div><div class="line">            <span class="keyword">if</span> left&lt;<span class="number">0</span>:</div><div class="line">                lack += left</div><div class="line">                left = <span class="number">0</span></div><div class="line">                begin = i+<span class="number">1</span></div><div class="line">        <span class="keyword">return</span> begin <span class="keyword">if</span> lack+left&gt;=<span class="number">0</span> <span class="keyword">else</span> <span class="number">-1</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Best-Time-to-Buy-and-Sell-Stock-II"><a href="#Best-Time-to-Buy-and-Sell-Stock-II" class="headerlink" title="Best Time to Buy and Sell Stock II"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/description/" target="_blank" rel="noopener">Best Time to Buy and Sell Stock II</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">Say you have an array <span class="keyword">for</span> which the ith element <span class="keyword">is</span> the price of a given stock on day i.</div><div class="line"></div><div class="line">Design an algorithm to find the maximum profit. You may complete <span class="keyword">as</span> many transactions <span class="keyword">as</span> you like (i.e., buy one <span class="keyword">and</span> sell one share of the stock multiple times).</div><div class="line"></div><div class="line">Note: You may <span class="keyword">not</span> engage <span class="keyword">in</span> multiple transactions at the same time (i.e., you must sell the stock before you buy again).</div><div class="line"></div><div class="line">Example <span class="number">1</span>:</div><div class="line"></div><div class="line">Input: [<span class="number">7</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]</div><div class="line">Output: <span class="number">7</span></div><div class="line">Explanation: Buy on day <span class="number">2</span> (price = <span class="number">1</span>) <span class="keyword">and</span> sell on day <span class="number">3</span> (price = <span class="number">5</span>), profit = <span class="number">5</span><span class="number">-1</span> = <span class="number">4.</span></div><div class="line">             Then buy on day <span class="number">4</span> (price = <span class="number">3</span>) <span class="keyword">and</span> sell on day <span class="number">5</span> (price = <span class="number">6</span>), profit = <span class="number">6</span><span class="number">-3</span> = <span class="number">3.</span></div><div class="line">Example <span class="number">2</span>:</div><div class="line"></div><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</div><div class="line">Output: <span class="number">4</span></div><div class="line">Explanation: Buy on day <span class="number">1</span> (price = <span class="number">1</span>) <span class="keyword">and</span> sell on day <span class="number">5</span> (price = <span class="number">5</span>), profit = <span class="number">5</span><span class="number">-1</span> = <span class="number">4.</span></div><div class="line">             Note that you cannot buy on day <span class="number">1</span>, buy on day <span class="number">2</span> <span class="keyword">and</span> sell them later, <span class="keyword">as</span> you are</div><div class="line">             engaging multiple transactions at the same time. You must sell before buying again.</div><div class="line">Example <span class="number">3</span>:</div><div class="line"></div><div class="line">Input: [<span class="number">7</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>]</div><div class="line">Output: <span class="number">0</span></div><div class="line">Explanation: In this case, no transaction <span class="keyword">is</span> done, i.e. max profit = <span class="number">0.</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://www.jianshu.com/p/34bbb0594bd9" target="_blank" rel="noopener">思路</a></p><p>股票买卖，当然是低价买入，高价卖出。那么如果今天的价格高于前一天，那么我们就可以赚钱，因为可以前一天买入，今天卖出。如果明日的价格更高，那么可以今日再买入，明天再卖出。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices)</span>:</span></div><div class="line">        profit = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(prices)<span class="number">-1</span>):</div><div class="line">            <span class="keyword">if</span> prices[i]&lt;prices[i+<span class="number">1</span>]:</div><div class="line">                profit += prices[i+<span class="number">1</span>] - prices[i]</div><div class="line">        <span class="keyword">return</span> profit</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Best-Time-to-Buy-and-Sell-Stock-with-Transaction-Fee"><a href="#Best-Time-to-Buy-and-Sell-Stock-with-Transaction-Fee" class="headerlink" title="Best Time to Buy and Sell Stock with Transaction Fee"></a><a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/description/" target="_blank" rel="noopener">Best Time to Buy and Sell Stock with Transaction Fee</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Your are given an array of integers <code>prices</code>, for which the <code>i</code>-th element is the price of a given stock on day <code>i</code>; and a non-negative integer <code>fee</code> representing a transaction fee.</p><p>You may complete as many transactions as you like, but you need to pay the transaction fee for each transaction. You may not buy more than 1 share of a stock at a time (ie. you must sell the stock share before you buy again.)</p><p>Return the maximum profit you can make.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: prices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>], fee = <span class="number">2</span></div><div class="line">Output: <span class="number">8</span></div><div class="line">Explanation: The maximum profit can be achieved by:</div><div class="line">Buying at prices[<span class="number">0</span>] = <span class="number">1</span>Selling at prices[<span class="number">3</span>] = <span class="number">8</span>Buying at prices[<span class="number">4</span>] = <span class="number">4</span>Selling at prices[<span class="number">5</span>] = <span class="number">9</span>The total profit <span class="keyword">is</span> ((<span class="number">8</span> - <span class="number">1</span>) - <span class="number">2</span>) + ((<span class="number">9</span> - <span class="number">4</span>) - <span class="number">2</span>) = <span class="number">8.</span></div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>解法1：题意大概是，可以进行无限次的交易(买入-卖出算一次交易)，但是每一次交易需要代价fee。完全就是<a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/description/" target="_blank" rel="noopener">Best Time to Buy and Sell Stock II</a>，只是多了交易费用。会想用动态规划求解，因为贪心算法比较不明显，dp[i]表示第i天的最大收益，对于每一天，都有两种选择，买进，卖出或者没有操作。说到这里，有点像题目<a href="https://leetcode.com/problems/wiggle-subsequence/description/" target="_blank" rel="noopener">Wiggle Subsequence</a> 设置两个dp数组, dp_buy[i]和dp_sell[i]：</p><p>dp_buy[i]表示以当前价格买进stock，可以获取的最大利润</p><p>dp_sell[i]表示以当前价格卖出stock，可以获取的最大利润</p><p>dp_buy[i] = max( dp_buy[i] , dp_sell[j] - price[i] )</p><p>dp_sell[i] = max( dp_sell[i] , dp_buy[j] + price[i] + fee )</p><p>解法2：</p><p>上述方法的时间复杂度是$O(n^2)$,最终超时。所以，必须只能扫描一遍，$O(n)$复杂度。设置两个全局变量，dp_buy和dp_sell，分别表示在操作prices[i]之前，当前买进和卖出最大的获利，状态更新不变。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">####超时</span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices, fee)</span>:</span></div><div class="line">        l = len(prices)</div><div class="line">        dp_buy = [-sys.maxsize]*l</div><div class="line">        dp_sell = [<span class="number">0</span>]*l</div><div class="line">        dp_buy[<span class="number">0</span>] = -prices[<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</div><div class="line">                dp_buy[i] = max( dp_buy[i] , dp_sell[j] - prices[i] )</div><div class="line">                dp_sell[i] = max( dp_sell[i] , dp_buy[j] + prices[i] - fee )</div><div class="line">        print(dp_buy,dp_sell)</div><div class="line">        <span class="keyword">return</span> max(dp_sell)</div><div class="line">    </div><div class="line">    </div><div class="line">    </div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices, fee)</span>:</span></div><div class="line">        dp_buy = -sys.maxsize</div><div class="line">        dp_sell = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(prices)):</div><div class="line">            dp_buy = max(dp_buy,dp_sell-prices[i])</div><div class="line">            dp_sell = max(dp_sell,dp_buy + prices[i]-fee)</div><div class="line">        <span class="keyword">return</span> dp_sell</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Non-overlapping-Intervals"><a href="#Non-overlapping-Intervals" class="headerlink" title="Non-overlapping Intervals"></a><a href="https://leetcode.com/problems/non-overlapping-intervals/description/" target="_blank" rel="noopener">Non-overlapping Intervals</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><ol><li><p>Given a collection of intervals, find the minimum number of intervals you need to remove to make the rest of the intervals non-overlapping.</p><p><strong>Note:</strong></p><ol><li>You may assume the interval’s end point is always bigger than its start point.</li><li>Intervals like [1,2] and [2,3] have borders “touching” but they don’t overlap each other.</li></ol><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [ [<span class="number">1</span>,<span class="number">2</span>], [<span class="number">2</span>,<span class="number">3</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">1</span>,<span class="number">3</span>] ]</div><div class="line">Output: <span class="number">1</span></div><div class="line">Explanation: [<span class="number">1</span>,<span class="number">3</span>] can be removed <span class="keyword">and</span> the rest of intervals are non-overlapping.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [ [<span class="number">1</span>,<span class="number">2</span>], [<span class="number">1</span>,<span class="number">2</span>], [<span class="number">1</span>,<span class="number">2</span>] ]</div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: You need to remove two [<span class="number">1</span>,<span class="number">2</span>] to make the rest of intervals non-overlapping.</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [ [<span class="number">1</span>,<span class="number">2</span>], [<span class="number">2</span>,<span class="number">3</span>] ]</div><div class="line">Output: <span class="number">0</span></div><div class="line">Explanation: You don<span class="string">'t need to remove any of the intervals since they'</span>re already non-overla</div></pre></td></tr></table></figure></li></ol></div></div></li><li><p><a href="https://blog.csdn.net/a921122/article/details/56829047" target="_blank" rel="noopener">思路</a></p><p>这道题实际上是活动选择问题，这个题可以转化为：给定一些任务，以及它们的开始时间和结束时间。并且给定一段时间，尽可能安排多的任务。很容易想到，我们希望尽可能的安排比较早结束的任务（也就是end比较小的），然后是考虑start紧跟上一个end的任务，最后，总任务数量，减去排好的任务，就是要删除的任务。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Definition for an interval.</span></div><div class="line"><span class="comment"># class Interval:</span></div><div class="line"><span class="comment">#     def __init__(self, s=0, e=0):</span></div><div class="line"><span class="comment">#         self.start = s</span></div><div class="line"><span class="comment">#         self.end = e</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eraseOverlapIntervals</span><span class="params">(self, intervals)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type intervals: List[Interval]</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> intervals: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        sort_intervals = sorted(intervals,key=<span class="keyword">lambda</span> x:x.end)</div><div class="line">        count = <span class="number">1</span></div><div class="line">        end = sort_intervals[<span class="number">0</span>].end</div><div class="line">        <span class="keyword">for</span> interval <span class="keyword">in</span> sort_intervals:</div><div class="line">            <span class="keyword">if</span> interval.start &gt;= end:</div><div class="line">                count += <span class="number">1</span></div><div class="line">                end = interval.end</div><div class="line">        <span class="keyword">return</span> len(intervals)-count</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Partition-Labels"><a href="#Partition-Labels" class="headerlink" title="Partition Labels"></a><a href="https://leetcode.com/problems/partition-labels/description/" target="_blank" rel="noopener">Partition Labels</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>A string <code>S</code> of lowercase letters is given. We want to partition this string into as many parts as possible so that each letter appears in at most one part, and return a list of integers representing the size of these parts.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: S = <span class="string">"ababcbacadefegdehijhklij"</span></div><div class="line">Output: [<span class="number">9</span>,<span class="number">7</span>,<span class="number">8</span>]</div><div class="line">Explanation:</div><div class="line">The partition is <span class="string">"ababcbaca"</span>, <span class="string">"defegde"</span>, <span class="string">"hijhklij"</span>.</div><div class="line">This is a partition so that each letter appears in at most one part.</div><div class="line">A partition like <span class="string">"ababcbacadefegde"</span>, <span class="string">"hijhklij"</span> is incorrect, because it splits S into less part</div></pre></td></tr></table></figure></div></div></li><li><p><a href="http://zxi.mytechroad.com/blog/string/leetcode-763-partition-labels/" target="_blank" rel="noopener">思路</a></p><p>å题目大意：把字符串分割成尽量多的不重叠子串，输出子串的长度数组。要求相同字符只能出现在一个子串中。</p><p><img src="/2018/05/23/Greedy-Algorithm/Screen Shot 2018-06-24 at 9.04.01 PM.png" alt="Screen Shot 2018-06-24 at 9.04.01 PM"></p><p><strong>解法一</strong>是暴力检索。从第一个字符开始<code>a</code>, 因为<code>a</code>只能出现在当前子串中，所以我们就要找到<code>a</code>最后出现的位置，这样我们就有了暂时的第一个子串start=0，end=9.继续扫描，发现<code>b</code>的结束位置是5，所以<code>b</code>被包含在start=0，end=9子串中，继续下去，我们就找到了第一个子串start=0，end=9。</p><p>同样start=start+1，重新扫描，找下一个子串。start=10，end=15. 继续扫描，发现<code>e</code>的结束位置是end=17，则更新子串start=10，end=17。以此类推。</p><p>可以发现，我们需要双重循环，外循环扫描一次input字符串，内循环扫描candidate子串，所以是$O(n^2)$的时间复杂度，但是我们没有使用额外空间。</p><p><strong>解法二</strong>借用哈希表，使用空间代价获取时间效用。可以发现，内循环用于找到每个字符串的结束位置，所以我们使用哈希表将结束位置存储起来，这样时间复杂度就降为$O(n)$. 至于空间复杂度，如果哈希表使用26个字母表示，则空间复杂度是$O(26)$。如果使用ASCII表示，则是$O(128)$.</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partitionLabels</span><span class="params">(self, S)</span>:</span></div><div class="line">        end_pos = &#123;&#125;</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(S)):</div><div class="line">            end_pos[S[i]] = i</div><div class="line">        start = <span class="number">0</span></div><div class="line">        end = <span class="number">0</span></div><div class="line">        re = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(S)):</div><div class="line">            end=max(end,end_pos[S[i]])</div><div class="line">            <span class="keyword">if</span> end==i:</div><div class="line">                re.append(end-start+<span class="number">1</span>)</div><div class="line">                start = end+<span class="number">1</span></div><div class="line">        <span class="keyword">return</span> re</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Split-Array-into-Consecutive-Subsequences"><a href="#Split-Array-into-Consecutive-Subsequences" class="headerlink" title="Split Array into Consecutive Subsequences"></a><a href="https://leetcode.com/problems/split-array-into-consecutive-subsequences/description/" target="_blank" rel="noopener">Split Array into Consecutive Subsequences</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>You are given an integer array sorted in ascending order (may contain duplicates), you need to split them into several subsequences, where each subsequences consist of at least 3 consecutive integers. Return whether you can make such a split.</p><p><strong>Example 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: [1,2,3,3,4,5]</div><div class="line">Output: True</div><div class="line">Explanation:</div><div class="line">You can split them into two consecutive subsequences : </div><div class="line">1, 2, 3</div><div class="line">3, 4, 5</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: [1,2,3,3,4,4,5,5]</div><div class="line">Output: True</div><div class="line">Explanation:</div><div class="line">You can split them into two consecutive subsequences : </div><div class="line">1, 2, 3, 4, 5</div><div class="line">3, 4, 5</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: [1,2,3,4,4,5]</div><div class="line">Output: False</div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/Jack_CJZ/article/details/78306831" target="_blank" rel="noopener">思路</a></p><p>题意大概就是：不减数组，能否将数组分割，使得到的子数组长度至少3，且是数组中的元素是连续的。</p><p>比如<code>[1,2,3,3,4,5]</code>，对于每一位数字，都有两种情况，一个是作为新数组的开头，二是作为已知数组的后续元素。对于第一种情况，如果数字是i且作为开头，那么为了有解，必须存在i+1和i+2，这样新数组才有可能。那么对于后续元素i+3，那么它可以加入[i, i+1, i+2]之中，或者作为开头[i+3]且判断i+4,i+5是否存在。那么基于贪心，是加入已知数组作为后续，因为如果两个数组[i,i+1,i+2]和[i+3,i+4,i+5]可以合并[i,i+1,i+2, i+3,i+4,i+5]。而且如果不存在i+4或者i+5，就不能两个数组，但是仍然可以添加到已知数组中。</p><p>所以我们需要一个continue[i]数组，初始是0，如果i可以添加到已经数组中，则+1。比如已知数组[i,i+1,i+2]，那么我们就设置continue[i+3] += 1.这样我们扫描的时候，先检查continue数组，如果该元素对应的continue数组是否为0，如果是0，那么说明没有已知数组可以让该元素插入，则得开盘新数组让该元素作为开头，同时检查i+1和i+2是否存在；如果不为0，那么就说明可以添加到已知数组，continue -= 1。</p><p><code>[1,2,3,3,4,5]</code>。</p><p>我们借助一个哈希表frequency{ }存储每个数字的出现次数，continue[0,0,0,0,0,0 ]</p><p>frequency[1] = 1, frequency[2]=1, frequency[3]=2, frequency[4]=1,frequency[5]=1</p><p>第一个数字是<code>1</code>, frequency[1]=1,则说明它要么插入已知数组，要么开辟新数组。因为continue是0，则开辟新数组，同时检查是否有i+1,i+2，即2和3。都有，说明我们有了一下新数组[1,2,3]，同时如果有4，则可以插入[1,2,3]中。continue[0,0,0,0,1,0],</p><p>frequency[1] = 0, frequency[2]=0, frequency[3]=1, frequency[4]=1,frequency[5]=1</p><p>第二个数字是2，但是frequency[2]=0,跳过。</p><p>第三个数字是3，frequency[3]=1，单数continue[3]=0,不能插入已知数组，重新开辟数组，同时frequency[4]和frequency[5]等于1，则得到第二个数字[3,4,5].</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isPossible</span><span class="params">(self, nums)</span>:</span></div><div class="line">        frequency = collections.Counter(nums)</div><div class="line">        continue_ = collections.defaultdict(int)</div><div class="line">        <span class="keyword">for</span> ele <span class="keyword">in</span> nums:</div><div class="line">            <span class="keyword">if</span> frequency[ele]==<span class="number">0</span>:<span class="keyword">continue</span></div><div class="line">            <span class="keyword">if</span> continue_[ele]&gt;<span class="number">0</span>:</div><div class="line">                frequency[ele] -= <span class="number">1</span></div><div class="line">                continue_[ele] -= <span class="number">1</span></div><div class="line">                continue_[ele+<span class="number">1</span>] += <span class="number">1</span> <span class="comment">####impotant</span></div><div class="line">            <span class="keyword">elif</span> frequency[ele+<span class="number">1</span>]&gt;<span class="number">0</span> <span class="keyword">and</span> frequency[ele+<span class="number">2</span>]&gt;<span class="number">0</span>:</div><div class="line">                frequency[ele] -= <span class="number">1</span></div><div class="line">                frequency[ele+<span class="number">1</span>] -= <span class="number">1</span></div><div class="line">                frequency[ele+<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">                continue_[ele+<span class="number">3</span>] += <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Dota2-Senate"><a href="#Dota2-Senate" class="headerlink" title="Dota2 Senate"></a><a href="https://leetcode.com/problems/dota2-senate/description/" target="_blank" rel="noopener">Dota2 Senate</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>In the world of Dota2, there are two parties: the <code>Radiant</code> and the <code>Dire</code>.</p><p>The Dota2 senate consists of senators coming from two parties. Now the senate wants to make a decision about a change in the Dota2 game. The voting for this change is a round-based procedure. In each round, each senator can exercise <code>one</code> of the two rights:</p><ol><li><code>Ban one senator&#39;s right</code>:<br>A senator can make another senator lose <strong>all his rights</strong> in this and all the following rounds.</li><li><code>Announce the victory</code>:<br>If this senator found the senators who still have rights to vote are all from <strong>the same party</strong>, he can announce the victory and make the decision about the change in the game.</li></ol><p>Given a string representing each senator’s party belonging. The character ‘R’ and ‘D’ represent the <code>Radiant</code>party and the <code>Dire</code> party respectively. Then if there are <code>n</code> senators, the size of the given string will be <code>n</code>.</p><p>The round-based procedure starts from the first senator to the last senator in the given order. This procedure will last until the end of voting. All the senators who have lost their rights will be skipped during the procedure.</p><p>Suppose every senator is smart enough and will play the best strategy for his own party, you need to predict which party will finally announce the victory and make the change in the Dota2 game. The output should be <code>Radiant</code> or <code>Dire</code>.</p><p><strong>Example 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Input: &quot;RD&quot;</div><div class="line">Output: &quot;Radiant&quot;</div><div class="line">Explanation: The first senator comes from Radiant and he can just ban the next senator&apos;s right in the round 1. </div><div class="line">And the second senator can&apos;t exercise any rights any more since his right has been banned. </div><div class="line">And in the round 2, the first senator can just announce the victory since he is the only guy in the senate who can vote.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input: &quot;RDD&quot;</div><div class="line">Output: &quot;Dire&quot;</div><div class="line">Explanation: </div><div class="line">The first senator comes from Radiant and he can just ban the next senator&apos;s right in the round 1. </div><div class="line">And the second senator can&apos;t exercise any rights anymore since his right has been banned. </div><div class="line">And the third senator comes from Dire and he can ban the first senator&apos;s right in the round 1. </div><div class="line">And in the round 2, the third senator can just announce the victory since he is the only</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>简单而言就是比较哪一方的人多，如果一样多，就看哪一方的人比较前面，极端情况就是<code>R</code>的人全部在前面，<code>D</code>的人全部在后面，这样<code>D</code>的人完全没有机会选择或者发言。所以，就比较顺序了。</p><p>从序列第一个先查看，假设是<code>R</code>,那么找到第一个<code>D</code>，移除，但是<code>R</code>仍然保留在序列中。继续扫描下一个，类似操作。扫描完之后，会发现序列中剩下一些，继续从头扫描，直到序列剩下一个。为了简化操作，在移除对手之后，我们将该Senate移到序列的最后，即其位置设置为（index+l）。同时移除时有查找对手index的操作，我们设置两个数组保存双方的index，开始时两个数组都将第一个元素pop，比较，较小的index获胜，将它移动到序列的最后，操作就是append(index+l)。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predictPartyVictory</span><span class="params">(self, senate)</span>:</span></div><div class="line">        d_queue = []</div><div class="line">        r_queue = []</div><div class="line">        l = len(senate)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</div><div class="line">            <span class="keyword">if</span> senate[i]==<span class="string">'R'</span>:</div><div class="line">                r_queue.append(i)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                d_queue.append(i)</div><div class="line">        <span class="keyword">while</span> d_queue <span class="keyword">and</span> r_queue:</div><div class="line">            d_index = d_queue.pop(<span class="number">0</span>)</div><div class="line">            r_index = r_queue.pop(<span class="number">0</span>)</div><div class="line">            <span class="keyword">if</span> d_index&gt;r_index:</div><div class="line">                r_queue.append(r_index+l)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                d_queue.append(d_index+l)</div><div class="line">        <span class="keyword">return</span> <span class="string">'Radiant'</span> <span class="keyword">if</span> r_queue <span class="keyword">else</span> <span class="string">'Dire'</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Minimum-Number-of-Arrows-to-Burst-Balloons"><a href="#Minimum-Number-of-Arrows-to-Burst-Balloons" class="headerlink" title="Minimum Number of Arrows to Burst Balloons"></a><a href="https://leetcode.com/problems/minimum-number-of-arrows-to-burst-balloons/description/" target="_blank" rel="noopener">Minimum Number of Arrows to Burst Balloons</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>There are a number of spherical balloons spread in two-dimensional space. For each balloon, provided input is the start and end coordinates of the horizontal diameter. Since it’s horizontal, y-coordinates don’t matter and hence the x-coordinates of start and end of the diameter suffice. Start is always smaller than end. There will be at most 104 balloons.</p><p>An arrow can be shot up exactly vertically from different points along the x-axis. A balloon with xstart and xendbursts by an arrow shot at x if xstart ≤ x ≤ xend. There is no limit to the number of arrows that can be shot. An arrow once shot keeps travelling up infinitely. The problem is to find the minimum number of arrows that must be shot to burst all balloons.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">[[<span class="number">10</span>,<span class="number">16</span>], [<span class="number">2</span>,<span class="number">8</span>], [<span class="number">1</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">12</span>]]</div><div class="line"></div><div class="line">Output:</div><div class="line"><span class="number">2</span></div><div class="line"></div><div class="line">Explanation:</div><div class="line">One way <span class="keyword">is</span> to shoot one arrow <span class="keyword">for</span> example at x = <span class="number">6</span> (bursting the balloons [<span class="number">2</span>,<span class="number">8</span>] <span class="keyword">and</span> [<span class="number">1</span>,<span class="number">6</span>]) <span class="keyword">and</span> another arrow at x = <span class="number">11</span> (bursting the other two balloons).</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>题目大概：给你一堆气球，这些气球沿X轴方向摆放，每个气球大小可能不同，一个气球占据的区间可以表示为[Xstart,Xend]，气球可以重叠摆放。一支坐标为x的箭，可以扎破所有满足 Xstart &lt;= x &lt;= Xend 的气球，求出最少射几支箭可以将所有气球扎破。</p><p><img src="/2018/05/23/Greedy-Algorithm/20170311103822129.png" alt="20170311103822129"></p><p>题目的图像表示如上，将气球按照end排序，那么只要前一个气球的end不小于下一个气球的start，则一箭可以同时刺穿。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMinArrowShots</span><span class="params">(self, points)</span>:</span></div><div class="line">        sort_points = sorted(points,key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> points:<span class="keyword">return</span> <span class="number">0</span></div><div class="line">        count = <span class="number">1</span></div><div class="line">        end = sort_points[<span class="number">0</span>][<span class="number">1</span>]</div><div class="line">        <span class="keyword">for</span> balloon <span class="keyword">in</span> sort_points[<span class="number">1</span>:]:</div><div class="line">            <span class="keyword">if</span> end&lt;balloon[<span class="number">0</span>]:</div><div class="line">                count += <span class="number">1</span></div><div class="line">                end = balloon[<span class="number">1</span>]</div><div class="line">        <span class="keyword">return</span> count</div></pre></td></tr></table></figure></div></div></li></ol><h2 id="中等"><a href="#中等" class="headerlink" title="中等"></a>中等</h2><h3 id="Walking-Robot-Simulation"><a href="#Walking-Robot-Simulation" class="headerlink" title="Walking Robot Simulation"></a><a href="https://leetcode.com/problems/walking-robot-simulation/description/" target="_blank" rel="noopener">Walking Robot Simulation</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>A robot on an infinite grid starts at point (0, 0) and faces north.  The robot can receive one of three possible types of commands:</p><ul><li><code>-2</code>: turn left 90 degrees</li><li><code>-1</code>: turn right 90 degrees</li><li><code>1 &lt;= x &lt;= 9</code>: move forward <code>x</code> units</li></ul><p>Some of the grid squares are obstacles. </p><p>The <code>i</code>-th obstacle is at grid point <code>(obstacles[i][0], obstacles[i][1])</code></p><p>If the robot would try to move onto them, the robot stays on the previous grid square instead (but still continues following the rest of the route.)</p><p>Return the <strong>square</strong> of the maximum Euclidean distance that the robot will be from the origin. </p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: commands = [<span class="number">4</span>,-<span class="number">1</span>,<span class="number">3</span>], obstacles = []</div><div class="line">Output: <span class="number">25</span></div><div class="line">Explanation: <span class="function">robot will go <span class="title">to</span> <span class="params">(<span class="number">3</span>, <span class="number">4</span>)</span></span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: commands = [<span class="number">4</span>,-<span class="number">1</span>,<span class="number">4</span>,-<span class="number">2</span>,<span class="number">4</span>], obstacles = [[<span class="number">2</span>,<span class="number">4</span>]]</div><div class="line">Output: <span class="number">65</span></div><div class="line">Explanation: <span class="function">robot will be stuck <span class="title">at</span> <span class="params">(<span class="number">1</span>, <span class="number">4</span>)</span> before turning left and going <span class="title">to</span> <span class="params">(<span class="number">1</span>, <span class="number">8</span>)</span></span></div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>问题大意：有一个机器人在无限大的平面上，初始时面对正北方向位于$(0,0)$。机器人接受一系列指令：-2则要左转；-1则要右转；1-9之间的数字则要前进相应的步数。但是该平面上有一些障碍$(i,j)$，如果机器人在行进过程中遇到了障碍，则停留在当前位置不动。求机器人能到达的最大欧氏距离。</p><p>该问题有两个难点：一个是方向如何表示，二是如何表示方向的改变。</p><p>假如现在在$(0,0)$，如果前进$a$个长度：</p><p>假设向$N$前进，则前进后的位置是$N(0,a)=N(0,0)+(0,a)$</p><p>假设现在在E，前进后的位置是E(a,0)=E(0,0)+(a,0)$</p><p>假设现在在$S$，则前进后的位置是$S(a,0)=S(0,0)+(0,-a)$</p><p>假设现在在W，则前进后的位置是$W(a,0)=E(0,0)+(-a,0)$</p><p>基于此，我们可以N,E,S,W四个方向转化为$[(0,1),(1,0),(0,-1),(-1,0)]$ </p><p>那么该如何改变方向呢？即如何选择上述四个方向呢？</p><p><img src="/2018/05/23/Greedy-Algorithm/79f0f736afc379317be931cfedc4b74542a91156.jpg" alt="79f0f736afc379317be931cfedc4b74542a91156"></p><p>可以发现无论当前处于东南西北的哪一个方向，接受到指令$-2$，你选择左边的方向，$-1$则选择右边的方向。如果用$direction$表示当前方向，则-2时，你更新你的方向为</p><p>$ direction = (direction-1)%4$；-1时，更新$direction = (direction + 1) % 4$ 。</p></li><li><p><a href="https://leetcode.com/problems/walking-robot-simulation/discuss/157505/Simple-Python-solution-Accepted" target="_blank" rel="noopener">代码</a> </p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">robotSim</span><span class="params">(self, commands, obstacles)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type commands: List[int]</span></div><div class="line"><span class="string">        :type obstacles: List[List[int]]</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        directions = [(<span class="number">0</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">-1</span>),(<span class="number">-1</span>,<span class="number">0</span>)] <span class="comment"># N,E,S,W</span></div><div class="line">        obstacles = set(map(tuple,obstacles))</div><div class="line">        max_dis,x,y,direction = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></div><div class="line">        <span class="keyword">for</span> command <span class="keyword">in</span> commands:</div><div class="line">            <span class="keyword">if</span> command == <span class="number">-2</span>:</div><div class="line">                direction = (direction<span class="number">-1</span>)%<span class="number">4</span></div><div class="line">            <span class="keyword">elif</span> command == <span class="number">-1</span> :</div><div class="line">                direction = (direction + <span class="number">1</span>) % <span class="number">4</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                x_offset,y_offset = directions[direction][<span class="number">0</span>],directions[direction][<span class="number">1</span>]</div><div class="line">                <span class="keyword">while</span> command&gt;<span class="number">0</span>:</div><div class="line">                    <span class="keyword">if</span> (x+x_offset,y+y_offset) <span class="keyword">not</span> <span class="keyword">in</span> obstacles:</div><div class="line">                        x += x_offset</div><div class="line">                        y += y_offset</div><div class="line">                    command -= <span class="number">1</span></div><div class="line">                max_dis = max(max_dis,x**<span class="number">2</span>+y**<span class="number">2</span>)</div><div class="line">        <span class="keyword">return</span> max_dis</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Queue-Reconstruction-by-Height"><a href="#Queue-Reconstruction-by-Height" class="headerlink" title="Queue Reconstruction by Height"></a><a href="https://leetcode.com/problems/queue-reconstruction-by-height/description/" target="_blank" rel="noopener">Queue Reconstruction by Height</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Suppose you have a random list of people standing in a queue. <span class="function">Each person is described by a pair of <span class="title">integers</span> <span class="params">(h, k)</span>, where h is the height of the person and k is the number of people in front of <span class="keyword">this</span> person who have a height greater than or equal to h. Write an algorithm to reconstruct the queue.</span></div><div class="line"><span class="function"></span></div><div class="line"><span class="function">Note:</span></div><div class="line"><span class="function">The number of people is less than 1,100.</span></div><div class="line"><span class="function"></span></div><div class="line"><span class="function"></span></div><div class="line"><span class="function">Example</span></div><div class="line"><span class="function"></span></div><div class="line"><span class="function">Input:</span></div><div class="line"><span class="function">[[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]]</span></div><div class="line"><span class="function"></span></div><div class="line"><span class="function">Output:</span></div><div class="line"><span class="function">[[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]]</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://leetcode.com/problems/queue-reconstruction-by-height/discuss/89345/Easy-concept-with-PythonC++Java-Solution" target="_blank" rel="noopener">思路</a></p><p>题目大致的意思是：每一个由一组整数(h, k)表示，h是其身高，k是队列之中身高不低于h的人数。现在要重构这个队列。</p><p>首先，这个题目类似于作业调度，需要根据（h，k）排序，问题是先根据哪一个排序呢？答案是以h排序，使身高从大到小排序，即先为身高有优势的在队列中找到位置，这样，身高比较矮的人在队列中的位置就是任意了，因为无论他们不会对身高高的人造成影响，即已经找到位置的身高优势的人，不会因为矮的人的插入而重新排列。而对于身高相同的人，显然是k值小的先插入。</p><p>所以，先根据h排序，h相同时根据k排序。</p><p>针对上述例子：</p><p>根据h排序之后，[7,0], [7,1], [6,1], [5,0], [5,2], [4,4]</p><p>根据k排序之后，已经有序</p><p>对于第一个人[7,0]，根据k值插入数组[ [7,0] ]</p><p>[7,1]，k=1插入[ [7,0] , [7,1] ]</p><p>[6,1], k=1插入[[7,0],[6,1],[7,1]]需要注意的是，无论[6,1]插入位置，都不会对已经插入的元素造成影响</p><p>…</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstructQueue</span><span class="params">(self, people)</span>:</span></div><div class="line">        new_people = sorted(people,key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</div><div class="line">        new_people = sorted(new_people,key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>],reverse=<span class="keyword">True</span>)</div><div class="line">        queue = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> new_people:</div><div class="line">            queue.insert(i[<span class="number">1</span>],i)</div><div class="line">        <span class="keyword">return</span> queue</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Remove-K-Digits"><a href="#Remove-K-Digits" class="headerlink" title="Remove K Digits"></a><a href="https://leetcode.com/problems/remove-k-digits/description/" target="_blank" rel="noopener">Remove K Digits</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a non-negative integer <em>num</em> represented as a string, remove <em>k</em> digits from the number so that the new number is the smallest possible.</p><p><strong>Note:</strong></p><ul><li>The length of <em>num</em> is less than 10002 and will be ≥ <em>k</em>.</li><li>The given <em>num</em> does not contain any leading zero.</li></ul><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = <span class="string">"1432219"</span>, k = <span class="number">3</span></div><div class="line">Output: <span class="string">"1219"</span></div><div class="line">Explanation: Remove the three digits <span class="number">4</span>, <span class="number">3</span>, and <span class="number">2</span> to form the <span class="keyword">new</span> number <span class="number">1219</span> which is the smallest.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = <span class="string">"10200"</span>, k = <span class="number">1</span></div><div class="line">Output: <span class="string">"200"</span></div><div class="line">Explanation: Remove the leading <span class="number">1</span> and the number is <span class="number">200</span>. Note that the output must not contain leading zeroes.</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = <span class="string">"10"</span>, k = <span class="number">2</span></div><div class="line">Output: <span class="string">"0"</span></div><div class="line">Explanation: Remove all the digits from the number and it is left with nothing which is <span class="number">0</span>.</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>首先，理解一下什么样的数才是尽可能最小的。</p><p>尽量维持一个从左至右递增的顺序。否则如果有num[i]&gt;nums[i+1]，即出现了递减，则删除num[i+1]之前的数字，使维持递增。扫描结束之后，得到递增数列，但是删除的数字未满k个，则从后往前删除使实现删除k个数字。值得一提的是，数字的首位不能是0.</p><p>为实现递增，我们借助栈，如果出现当前元素小于栈顶，则持续出栈，直到栈顶小于当前元素，然后入栈。</p></li><li><p><a href="https://www.liuchuo.net/archives/3223" target="_blank" rel="noopener">代码</a></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeKdigits</span><span class="params">(self, num, k)</span>:</span></div><div class="line">        l = len(num)</div><div class="line">        stack = []</div><div class="line">        cur = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> cur &lt; l :</div><div class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> stack[<span class="number">-1</span>] &gt; num[cur] <span class="keyword">and</span> k &gt; <span class="number">0</span>:</div><div class="line">                stack.pop()</div><div class="line">                k -= <span class="number">1</span></div><div class="line">            stack.append(num[cur])</div><div class="line">            cur += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">while</span> k &gt; <span class="number">0</span>:</div><div class="line">            stack.pop()</div><div class="line">            k -= <span class="number">1</span></div><div class="line">        re = <span class="string">""</span></div><div class="line">        <span class="keyword">while</span> stack:</div><div class="line">            re = stack.pop() + re</div><div class="line">        <span class="keyword">return</span> str(int(re)) <span class="keyword">if</span> re <span class="keyword">else</span> str(<span class="number">0</span>)</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Monotone-Increasing-Digits"><a href="#Monotone-Increasing-Digits" class="headerlink" title="Monotone Increasing Digits"></a><a href="https://leetcode.com/problems/monotone-increasing-digits/description/" target="_blank" rel="noopener">Monotone Increasing Digits</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a non-negative integer <code>N</code>, find the largest number that is less than or equal to <code>N</code> with monotone increasing digits.</p><p>(Recall that an integer has <em>monotone increasing digits</em> if and only if each pair of adjacent digits <code>x</code> and <code>y</code>satisfy <code>x &lt;= y</code>.)</p><p><strong>Example 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: N = 10</div><div class="line">Output: 9</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: N = 1234</div><div class="line">Output: 1234</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: N = 332</div><div class="line">Output: 299</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>首先，题目有一个模棱两可的地方， <strong>monotone increasing</strong>意思是单调递增，即不减，所以相等也可以。即<code>22</code>也是满足 <strong>monotone increasing</strong>的。</p><p>所以我们扫描一次input，找到第一个下降的位置，比如<code>668841</code>，开始下降的index=3。正常是num[index] = num[index]-1，然后后面全部变成9,即<code>668799</code>。显然有问题，因<code>88</code>的存在and减法操作使出现下降。所以，在找到开始下降的index之后，往前扫描，直到找到第一个num[index]的数字，然后将该数字减法1，后面的全部翻转为<code>9</code>。</p><p>编程的时候，遇到一个问题就是Python中string的操作。如果num_str = str(num)，那么num_str就是常量，常量不可以修改，即num_str[i] = ‘a’,是会报错的。可以使用string连接操作<code>+</code>，或者replace等。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">monotoneIncreasingDigits</span><span class="params">(self, N)</span>:</span></div><div class="line">        num = list(str(N))</div><div class="line">        end = len(num)</div><div class="line">        flag = <span class="keyword">True</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)<span class="number">-1</span>):</div><div class="line">            <span class="keyword">if</span> num[i]&gt;num[i+<span class="number">1</span>]:</div><div class="line">                flag=<span class="keyword">False</span></div><div class="line">                end = i</div><div class="line">                <span class="keyword">break</span></div><div class="line">        <span class="keyword">if</span> flag:<span class="keyword">return</span> N</div><div class="line">        <span class="keyword">while</span> end&gt;<span class="number">0</span> <span class="keyword">and</span> num[end]==num[end<span class="number">-1</span>]:</div><div class="line">            end-=<span class="number">1</span></div><div class="line">        num[end] = str(int(num[end]) - <span class="number">1</span>)</div><div class="line">        num = <span class="string">''</span>.join(num[:end+<span class="number">1</span>]) + <span class="string">'9'</span>*(len(num)-end<span class="number">-1</span>)</div><div class="line">        <span class="keyword">return</span> int(num)</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Task-Scheduler"><a href="#Task-Scheduler" class="headerlink" title="Task Scheduler"></a><a href="https://leetcode.com/problems/task-scheduler/description/" target="_blank" rel="noopener">Task Scheduler</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a char array representing tasks CPU need to do. It contains capital letters A to Z where different letters represent different tasks.Tasks could be done without original order. Each task could be done in one interval. For each interval, CPU could finish one task or just be idle.</p><p>However, there is a non-negative cooling interval <strong>n</strong> that means between two <strong>same tasks</strong>, there must be at least n intervals that CPU are doing different tasks or just be idle.</p><p>You need to return the <strong>least</strong> number of intervals the CPU will take to finish all the given tasks.</p><p><strong>Example 1:</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: tasks = [<span class="string">"A"</span>,<span class="string">"A"</span>,<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"B"</span>,<span class="string">"B"</span>], n = <span class="number">2</span></div><div class="line">Output: <span class="number">8</span></div><div class="line">Explanation: A -&gt; B -&gt; idle -&gt; A -&gt; B -&gt; idle -&gt; A -&gt; B.</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p><img src="/2018/05/23/Greedy-Algorithm/Screen Shot 2018-06-27 at 10.29.49 AM.png" alt="Screen Shot 2018-06-27 at 10.29.49 AM"></p><p>题目大概就是模拟CPU任务分配，A 到 Z表示不同的任务，任务可以以不同顺序进行。每个任务可以在一个时间间隔中完成。对于一个时间间隔，CPU可以执行一个任务或者是闲置。但是，两个同样的任务之间需要有 n 个冷却时间，也就是说假如A执行后，那么未来n个时间间隔内，A是不允许再执行的。</p><p>自然的，我们找出frequency最大的字符，因为中间相隔n个字符，以<code>AAABB</code>为例，那么我们可以有模板<code>AXX|AXX|A</code>，频率少的字符填充，即代替<code>X</code>。如上图所示，因为<code>A</code>出现次数最多，那么我们构造k-1个模板，每个模板大小是n+1，至于最后一个模板，其长度是：频率为k的字符个数。所以，最终的长度是(k-1)*(n+1)+p.</p><p>但是有一种特殊情况，考虑上述模板，中间填充的永远只有n个字符，如果字符串是<code>AAAABBBCCCDDD</code>，n=2，我们的模板是<code>AXX|AXX|AXX|A</code>，得到的长度是(k-1)(n+1)+p=(4-1)*(2+1)+1=10。但是，实际上，字符长度是13，正确排列是$ABCD|ABCD|ABCD|A$，可以看出中间的填充是n=3.</p><p>所以，返回的结果应该是max( len(str) , (k-1)(n+1)+p )</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">leastInterval</span><span class="params">(self, tasks, n)</span>:</span></div><div class="line">        frequency = collections.Counter(tasks)</div><div class="line">        sort_fre = sorted(frequency.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>]) <span class="comment">#对(key-value)字典排序，以value的大小排序</span></div><div class="line">        <span class="comment"># 找k值</span></div><div class="line">        k = sort_fre[<span class="number">-1</span>][<span class="number">1</span>]</div><div class="line">        <span class="comment"># 找p值</span></div><div class="line">        p = <span class="number">1</span></div><div class="line">        pos = len(sort_fre)<span class="number">-2</span></div><div class="line">        <span class="keyword">while</span> pos&gt;=<span class="number">0</span> <span class="keyword">and</span> sort_fre[pos][<span class="number">1</span>]==k:</div><div class="line">            pos -= <span class="number">1</span></div><div class="line">            p += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> max(len(tasks),(k<span class="number">-1</span>)*(n+<span class="number">1</span>)+p)</div></pre></td></tr></table></figure></div></div></li></ol><h2 id="高级"><a href="#高级" class="headerlink" title="高级"></a>高级</h2><h3 id="Candy"><a href="#Candy" class="headerlink" title="Candy"></a><a href="https://leetcode.com/problems/candy/description/" target="_blank" rel="noopener">Candy</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>There are <em>N</em> children standing in a line. Each child is assigned a rating value.</p><p>You are giving candies to these children subjected to the following requirements:</p><ul><li>Each child must have at least one candy.</li><li>Children with a higher rating get more candies than their neighbors.</li></ul><p>What is the minimum candies you must give?</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]</div><div class="line">Output: <span class="number">5</span></div><div class="line">Explanation: You can allocate to the first, second and third child with <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span> candies respectively.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]</div><div class="line">Output: <span class="number">4</span></div><div class="line">Explanation: You can allocate to the first, second and third child with <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span> candies respectively.</div><div class="line">             The third child gets <span class="number">1</span> candy because it satisfies the above two conditions.</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>题意大概是：为每个小孩分配至少一个糖果，且等级高的小孩分配到的糖果比其左右小孩得到的多。</p><p>初始化candy数组全1，使每个孩子都有糖果。重点是如果根据等级再分配糖果。两遍扫描，一次从左到右，只要当前的小孩的等级比他左边的小孩高，就根据其左边孩子的糖果数量多分配一个糖果candy_left；二次扫描，只要当前的小孩的等级比他右边的小孩高，就根据其右边孩子的糖果数量多分配一个糖果candy_right，最终的糖果数量是max(candy_left,candy_right).</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">candy</span><span class="params">(self, ratings)</span>:</span></div><div class="line">        l = len(ratings)</div><div class="line">        candy = [<span class="number">1</span>]*l</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,l):</div><div class="line">            left_child = ratings[i<span class="number">-1</span>]</div><div class="line">            right_child = ratings[i]</div><div class="line">            <span class="keyword">if</span> right_child&gt;left_child:</div><div class="line">                candy[i] = candy[i<span class="number">-1</span>]+<span class="number">1</span></div><div class="line">        pos = l<span class="number">-2</span></div><div class="line">        <span class="keyword">while</span> pos&gt;<span class="number">-1</span>:</div><div class="line">            left_child = ratings[pos]</div><div class="line">            right_child = ratings[pos+<span class="number">1</span>]</div><div class="line">            <span class="keyword">if</span> left_child&gt;right_child:</div><div class="line">                candy[pos] = max(candy[pos],candy[pos+<span class="number">1</span>]+<span class="number">1</span>)</div><div class="line">            pos -= <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> sum(candy)</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Remove-Duplicate-Letters"><a href="#Remove-Duplicate-Letters" class="headerlink" title="Remove Duplicate Letters"></a><a href="https://leetcode.com/problems/remove-duplicate-letters/description/" target="_blank" rel="noopener">Remove Duplicate Letters</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a string which contains only lowercase letters, remove duplicate letters so that every letter appear once and only once. You must make sure your result is the smallest in lexicographical order among all possible results.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: <span class="string">"bcabc"</span></div><div class="line">Output: <span class="string">"abc"</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: <span class="string">"cbacdcbc"</span></div><div class="line">Output: <span class="string">"acdb"</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="http://bookshadow.com/weblog/2015/12/09/leetcode-remove-duplicate-letters/" target="_blank" rel="noopener">思路</a></p><p>枚举字符串前缀，直到遇到首个唯一字符为止，从前缀中挑选出最小的字符作为首字符。</p><p>然后从剩余字符串中移除所有与首字母相同的字母。</p><p>重复此过程，直到选出所有唯一字符为止。时间复杂度是$O(k*n)$，$k$是字符串中不同的字符个数， $n$是字符串长度。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicateLetters</span><span class="params">(self, s)</span>:</span></div><div class="line">        ans = <span class="string">''</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(set(s))):</div><div class="line">            front,index = s[<span class="number">0</span>],<span class="number">0</span></div><div class="line">            s_counter = collections.Counter(s)</div><div class="line">            <span class="keyword">for</span> pos,val <span class="keyword">in</span> enumerate(s):</div><div class="line">                <span class="keyword">if</span> front&gt;val: <span class="comment">#找前缀中字典序小的字母</span></div><div class="line">                    front,index = val,pos</div><div class="line">                <span class="keyword">if</span> s_counter[val]==<span class="number">1</span>: <span class="comment">#字符串出现首个唯一字符</span></div><div class="line">                    <span class="keyword">break</span></div><div class="line">                s_counter[val] -= <span class="number">1</span></div><div class="line">            ans += front</div><div class="line">            s = s[index+<span class="number">1</span>:].replace(front,<span class="string">''</span>)</div><div class="line">        <span class="keyword">return</span> ans</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Create-Maximum-Number"><a href="#Create-Maximum-Number" class="headerlink" title="Create Maximum Number"></a><a href="https://leetcode.com/problems/create-maximum-number/description/" target="_blank" rel="noopener">Create Maximum Number</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given two arrays of length <code>m</code> and <code>n</code> with digits <code>0-9</code> representing two numbers. Create the maximum number of length <code>k &lt;= m + n</code>from digits of the two. The relative order of the digits from the same array must be preserved. Return an array of the <code>k</code> digits.</p><p><strong>Note:</strong> You should try to optimize your time and space complexity.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">nums1 = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>]</div><div class="line">nums2 = [<span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">3</span>]</div><div class="line">k = <span class="number">5</span></div><div class="line">Output:</div><div class="line">[<span class="number">9</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">nums1 = [<span class="number">6</span>, <span class="number">7</span>]</div><div class="line">nums2 = [<span class="number">6</span>, <span class="number">0</span>, <span class="number">4</span>]</div><div class="line">k = <span class="number">5</span></div><div class="line">Output:</div><div class="line">[<span class="number">6</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">4</span>]</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input:</div><div class="line">nums1 = [<span class="number">3</span>, <span class="number">9</span>]</div><div class="line">nums2 = [<span class="number">8</span>, <span class="number">9</span>]</div><div class="line">k = <span class="number">3</span></div><div class="line">Output:</div><div class="line">[<span class="number">9</span>, <span class="number">8</span>, <span class="number">9</span>]</div></pre></td></tr></table></figure></div></div></li><li><p><a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-321-create-maximum-number/" target="_blank" rel="noopener">思路</a></p><p>题意大概就是：给定两个数组，从数组中共选出k个元素，使构成的值是最大的，要求选出的元素相对顺序不能改变。</p><p>我们先考虑一个数组的情况，即如何从一个数组中选出k个元素，在不改变元素顺序的情况下，组合得到的值最大。贪心方法，使用一个栈，扫描一次数组，如果栈空或者当前的元素值比栈顶大，则持续出栈直到栈顶元素比较大。有个要求就是为了保证最后有k个数字，我们最多只能出栈len(num)-k个元素，具体过程在下图的左上角, 时间复杂度是$O(n)$.</p><p>接下来，我们考虑另一个问题，给定两个数组，在不考虑k的限制，即如何用两个数组排列出最大的值。那么显然是贪心了，同时扫描两个数组，分别比较数组的第一个值，选择大值作为新值的一部分，然后弹出该值，继续扫描。</p><p>那么将上述两个步骤合并就能解决我们的问题了。因为要选择k个，那么在第一个数组中选择i个，第一个数组中选择(k-i)个，得到的两个子数组构成的值都是相对最优的；那么再将这两个子数组合并，就得到了k长度的解。</p><p><img src="/2018/05/23/Greedy-Algorithm/123.png" alt="123"></p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="comment"># 从数组中选择m个数，使m个数的组合最大</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">subNumber</span><span class="params">(self, nums, m)</span>:</span></div><div class="line">        max_pop = len(nums) - m  <span class="comment"># 最多抛弃的个数</span></div><div class="line">        pop_num = <span class="number">0</span>  <span class="comment"># 记录当前已经抛弃的数目</span></div><div class="line">        cur_pos = <span class="number">0</span></div><div class="line">        result = []</div><div class="line">        <span class="comment">#必须比较数组，而不是数组的数字，为了防止【6，7】和【6，0，1】如果第一个相等，必须比较第二个数字</span></div><div class="line">        <span class="keyword">while</span> cur_pos &lt; len(nums):</div><div class="line">            <span class="keyword">if</span> pop_num &lt; max_pop <span class="keyword">and</span> result <span class="keyword">and</span> result[<span class="number">-1</span>] &lt; nums[cur_pos]:</div><div class="line">                pop_num += <span class="number">1</span></div><div class="line">                result.pop()</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                result.append(nums[cur_pos])</div><div class="line">                cur_pos += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> result[:m]  <span class="comment"># 返回前m个数</span></div><div class="line"></div><div class="line">    <span class="comment"># 合并两个数组，使组合得到最大数</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge_two_nums</span><span class="params">(self, nums1, nums2)</span>:</span></div><div class="line">        result = []</div><div class="line">        <span class="keyword">while</span> len(nums1) <span class="keyword">and</span> len(nums2):</div><div class="line">            <span class="keyword">if</span> nums1&gt;nums2:</div><div class="line">                result.append(nums1.pop(<span class="number">0</span>))</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                result.append(nums2.pop(<span class="number">0</span>))</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> len(nums1):</div><div class="line">            result.extend(nums2)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            result.extend(nums1)</div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxNumber</span><span class="params">(self, nums1, nums2, k)</span>:</span></div><div class="line">        result = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k + <span class="number">1</span>):</div><div class="line">            <span class="keyword">if</span> i &gt; len(nums1) <span class="keyword">or</span> (k - i) &gt; len(nums2):</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            subnum1 = self.subNumber(nums1, i)</div><div class="line">            subnum2 = self.subNumber(nums2, k - i)</div><div class="line">            result = max(result, self.merge_two_nums(subnum1, subnum2))</div><div class="line">        <span class="keyword">return</span> result</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Set-Intersection-Size-At-Least-Two"><a href="#Set-Intersection-Size-At-Least-Two" class="headerlink" title="Set Intersection Size At Least Two"></a><a href="https://leetcode.com/problems/set-intersection-size-at-least-two/description/" target="_blank" rel="noopener">Set Intersection Size At Least Two</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>An integer interval <code>[a, b]</code> (for integers <code>a &lt; b</code>) is a set of all consecutive integers from <code>a</code> to <code>b</code>, including <code>a</code> and <code>b</code>.</p><p>Find the minimum size of a set S such that for every integer interval A in <code>intervals</code>, the intersection of S with A has size at least 2.</p><p><strong>Example 1:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Input: intervals = [[1, 3], [1, 4], [2, 5], [3, 5]]</div><div class="line">Output: 3</div><div class="line">Explanation:</div><div class="line">Consider the set S = &#123;2, 3, 4&#125;.  For each interval, there are at least 2 elements from S in the interval.</div><div class="line">Also, there isn&apos;t a smaller size set that fulfills the above condition.</div><div class="line">Thus, we output the size of this set, which is 3.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Input: intervals = [[1, 2], [2, 3], [2, 4], [4, 5]]</div><div class="line">Output: 5</div><div class="line">Explanation:</div><div class="line">An example of a minimum sized set is &#123;1, 2, 3, 4, 5&#125;.</div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/magicbean2/article/details/79568849" target="_blank" rel="noopener">思路1</a>  <a href="https://www.cnblogs.com/grandyang/p/8503476.html" target="_blank" rel="noopener">思路2</a></p><p> 题意大概是：每个连续整数间隔数组都由两个数表示：<code>a</code>表示连续整数的开始，<code>b</code>表示连续整数的结束。现在给出一堆这样的连续整数间隔，需要找到一个最小的数组，使该数组与每个连续数组都至少有两个数是相同的，不要求该数组是由连续整数构成。</p><p> 也就是说我们需要维护一个动态数组，那么该数组与已知间隔数组会有三种情况：</p><ul><li>二者没有交集，那么我们就需要从当前间隔数组中取出两个数字加入动态数组；为了尽可能少使用数字，我们取当前间隔中的最大两个数字，因为这样更有可能与后面的间隔有交集。</li><li>二者有一个交集，那么这个交集一定是间隔的起始位置，则我们只需要选取一个数字加入动态数组，根据上面分析，我们选择最大的数字，即间隔的结束位置。</li><li>二者有多于一个的交集，则不做任何处理</li></ul><p>所以在代码实现时，我们设置一个初始化一个数组为<code>[-1,-1]</code>，给区间按照结束位置排序，然后遍历每个区间，如果区间的开始位置小于等于动态数组的倒数第二个元素，则表示覆盖元素大于1，跳过；如果区间的开始位置小于等于动态数组的最后一个元素，即区间的开始位置介在动态数组的倒数第二个元素和倒数第一个元素之间，则覆盖一个元素，故选择区间的结束位置加入动态数组；如果区间的开始位置大于动态数组的最后一个元素，则没有覆盖，选择区间的结束位置-1，结束位置加入动态数组。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersectionSizeTwo</span><span class="params">(self, intervals)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line"><span class="string">        :type intervals: List[List[int]]</span></div><div class="line"><span class="string">        :rtype: int</span></div><div class="line"><span class="string">        """</span></div><div class="line">        intervals = sorted(intervals,key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</div><div class="line">        print(intervals)</div><div class="line">        result = [<span class="number">-1</span>,<span class="number">-1</span>]</div><div class="line">        <span class="keyword">for</span> interval <span class="keyword">in</span> intervals:</div><div class="line">            <span class="keyword">if</span> interval[<span class="number">0</span>] &lt;= result[<span class="number">-2</span>]:<span class="keyword">continue</span></div><div class="line">            <span class="keyword">elif</span> interval[<span class="number">0</span>] &lt;= result[<span class="number">-1</span>]: result.append(interval[<span class="number">1</span>])</div><div class="line">            <span class="keyword">elif</span> interval[<span class="number">0</span>] &gt; result[<span class="number">-1</span>]:</div><div class="line">                result.append(interval[<span class="number">1</span>]<span class="number">-1</span>)</div><div class="line">                result.append(interval[<span class="number">1</span>])</div><div class="line">        <span class="keyword">return</span> len(result)<span class="number">-2</span></div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Course-Schedule-III"><a href="#Course-Schedule-III" class="headerlink" title="Course Schedule III"></a><a href="https://leetcode.com/problems/course-schedule-iii/description/" target="_blank" rel="noopener">Course Schedule III</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>There are <code>n</code> different online courses numbered from <code>1</code> to <code>n</code>. Each course has some duration(course length) <code>t</code> and closed on <code>dth</code>day. A course should be taken <strong>continuously</strong> for <code>t</code> days and must be finished before or on the <code>dth</code> day. You will start at the <code>1st</code> day.</p><p>Given <code>n</code> online courses represented by pairs <code>(t,d)</code>, your task is to find the maximal number of courses that can be taken.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Input: [[<span class="number">100</span>, <span class="number">200</span>], [<span class="number">200</span>, <span class="number">1300</span>], [<span class="number">1000</span>, <span class="number">1250</span>], [<span class="number">2000</span>, <span class="number">3200</span>]]</div><div class="line">Output: <span class="number">3</span></div><div class="line">Explanation: </div><div class="line">There<span class="string">'re totally 4 courses, but you can take 3 courses at most:</span></div><div class="line"><span class="string">First, take the 1st course, it costs 100 days so you will finish it on the 100th day, and ready to take the next course on the 101st day.</span></div><div class="line"><span class="string">Second, take the 3rd course, it costs 1000 days so you will finish it on the 1100th day, and ready to take the next course on the 1101st day. </span></div><div class="line"><span class="string">Third, take the 2nd course, it costs 200 days so you will finish it on the 1300th day. </span></div><div class="line"><span class="string">The 4th course cannot be taken now, since you will finish it on the 3300th day, which exceeds the closed date.</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/jitianyu123/article/details/74611466" target="_blank" rel="noopener">思路</a></p><p>题目大意：$n$个课程，每一个课程使用$(t,d)$表示，其中$t$是课程所需时间，$d$表示课程必须在$d$之前完成。如何选课才能选到最多的课程？</p><p>我们的贪心选择是 - 选择截止日期越早的课程。比如（300，1000）和（500，600），我们可以先选择（500，600），这样课程结束时间是500&lt;600。然后选择（300，1000）,这样课程结束时间是500+300 &lt; 1000。选择2门课。</p><p>而非优先选择所需时间最短的课程。优选选择课时短的（300，1000），结束时间是300. 如果再选择（500，600），上完课的时间就是 300+500 &gt; 600，就不能选了。</p><p>但是，为了防止这种情况：<code>[[5,5],[4,6],[2,6]]</code>，如果单纯用上述方法，则返回1。实际上，我们需要一个优先队列，将选择的课程按照持续时间排列，扫描课程，先吸收入优先队列，更新start时间；然后判断该课程是否合理，如果不合理，则从队列中选择持续时间最长的课程，抛弃之。</p></li><li><p><a href="http://fisherlei.blogspot.com/2017/07/leetcode-course-schedule-iii-solution.html" target="_blank" rel="noopener">代码</a></p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> heapq</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scheduleCourse</span><span class="params">(self, courses)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> courses: <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        sel_c = []</div><div class="line">        sorted_c = sorted(courses,key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</div><div class="line">        start = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> sorted_c:</div><div class="line">            duration = c[<span class="number">0</span>]</div><div class="line">            end = c[<span class="number">1</span>]</div><div class="line">            heapq.heappush(sel_c,-duration)</div><div class="line">            start = duration + start</div><div class="line">            <span class="keyword">if</span> start &gt; end:</div><div class="line">                d = heapq.heappop(sel_c)</div><div class="line">                start += d</div><div class="line">        <span class="keyword">return</span> len(sel_c)</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Patching-Array"><a href="#Patching-Array" class="headerlink" title="Patching Array"></a><a href="https://leetcode.com/problems/patching-array/description/" target="_blank" rel="noopener">Patching Array</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a sorted positive integer array <em>nums</em> and an integer <em>n</em>, add/patch elements to the array such that any number in range <code>[1, n]</code> inclusive can be formed by the sum of some elements in the array. Return the minimum number of patches required.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Input: nums = [<span class="number">1</span>,<span class="number">3</span>], n = <span class="number">6</span></div><div class="line">Output: <span class="number">1</span> </div><div class="line">Explanation:</div><div class="line">Combinations of nums are [<span class="number">1</span>], [<span class="number">3</span>], [<span class="number">1</span>,<span class="number">3</span>], which form possible sums of: <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>.</div><div class="line">Now <span class="keyword">if</span> we add/patch <span class="number">2</span> to nums, the combinations are: [<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">1</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>].</div><div class="line">Possible sums are <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, which now covers the range [<span class="number">1</span>, <span class="number">6</span>].</div><div class="line">So we only need <span class="number">1</span> patch.</div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: nums = [<span class="number">1</span>,<span class="number">5</span>,<span class="number">10</span>], n = <span class="number">20</span></div><div class="line">Output: <span class="number">2</span></div><div class="line">Explanation: The two patches can be [<span class="number">2</span>, <span class="number">4</span>].</div></pre></td></tr></table></figure><p><strong>Example 3:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>], n = <span class="number">5</span></div><div class="line">Output: <span class="number">0</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://www.hrwhisper.me/leetcode-patching-array/" target="_blank" rel="noopener">思路</a> </p><p>题意大概是：给定一个数组和一个目标值<code>n</code>，要求往数组中增加最少的元素，使数组中几个元素的和可以表示<code>[1,n]</code>范围内的数字.</p><p>我们设置一个变量<code>known_num</code>表示当前数字可以表示的最大<code>[1,known_num)</code>。扫描一遍数组，如果当前的元素不比<code>known_num</code>大，则我们可以更新我们的表示范围<code>[1,known_num + ele_</code>；如果当前的元素比<code>known_num</code>大，那么我们需要添加元素<code>known_num</code>，更新表示范围<code>[1,known_num*2)</code>。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minPatches</span><span class="params">(self, nums, n)</span>:</span></div><div class="line">        known_num = <span class="number">1</span></div><div class="line">        add_num = <span class="number">0</span></div><div class="line">        pos = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> known_num<span class="number">-1</span> &lt; n:</div><div class="line">            <span class="keyword">if</span> pos &lt; len(nums) <span class="keyword">and</span> nums[pos] &lt;= known_num:</div><div class="line">                known_num += nums[pos]</div><div class="line">                pos += <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                known_num *= <span class="number">2</span></div><div class="line">                add_num += <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> add_num</div></pre></td></tr></table></figure></div></div></li></ol><h3 id="Reorganize-String"><a href="#Reorganize-String" class="headerlink" title="Reorganize String"></a><a href="https://leetcode.com/problems/reorganize-string/description/" target="_blank" rel="noopener">Reorganize String</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Given a string <code>S</code>, check if the letters can be rearranged so that two characters that are adjacent to each other are not the same.</p><p>If possible, output any possible result.  If not possible, return the empty string.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: S = <span class="string">"aab"</span></div><div class="line">Output: <span class="string">"aba"</span></div></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Input: S = <span class="string">"aaab"</span></div><div class="line">Output: <span class="string">""</span></div></pre></td></tr></table></figure></div></div></li><li><p><a href="https://blog.csdn.net/magicbean2/article/details/79629519" target="_blank" rel="noopener">思路</a> </p><p>题目大致说给出一个字符串，要求重排字符顺序，使得相邻的字符总是不同。</p><p>首先，我们先找出不能重构字符串的条件：出现最多次的字符的出现次数大于字符串长度的一半，直接返回空字符串。</p><p>其次，如果可以重构，我们借助一个优先队列，每次都从中选择出现次数最多的字符构成字符串，如此循环直至优先队列空。</p><p>但是，如果<code>aaaabbcc</code>，优先队列先弹出<code>a</code>，加入重组字符串；然后会继续弹出<code>a</code>，但是如果直接加入重组字符串，则出现相邻的<code>a</code>。所以，这时，我们继续弹出出现次数第二多的字符<code>b</code>，加入重组字符串。</p></li><li><p><a href="https://leetcode.com/problems/reorganize-string/discuss/130825/Python-solution-with-detailed-explanation" target="_blank" rel="noopener">代码</a> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="keyword">import</span> heapq</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reorganizeString</span><span class="params">(self, S)</span>:</span></div><div class="line">        s_counter = collections.Counter(S)</div><div class="line">        most_freq = s_counter.most_common(<span class="number">1</span>)</div><div class="line">        <span class="keyword">if</span> len(S) &lt; most_freq[<span class="number">0</span>][<span class="number">1</span>]*<span class="number">2</span><span class="number">-1</span>:</div><div class="line">            <span class="keyword">return</span> <span class="string">""</span></div><div class="line">        re_string = <span class="string">""</span></div><div class="line">        h = []</div><div class="line">        <span class="keyword">for</span> k,v <span class="keyword">in</span> s_counter.items():</div><div class="line">            heapq.heappush(h,(-v,k))</div><div class="line">        <span class="keyword">while</span> h:</div><div class="line">            v,k = heapq.heappop(h)</div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> re_string <span class="keyword">or</span> k != re_string[<span class="number">-1</span>]: <span class="comment"># 初始时 或者 重构字符串的最后一个字符串不同于弹出的字符串</span></div><div class="line">                re_string += k</div><div class="line">                <span class="keyword">if</span> v != <span class="number">-1</span> :</div><div class="line">                    heapq.heappush(h,(v+<span class="number">1</span>,k))</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                v1,k1 = heapq.heappop(h) <span class="comment">#弹出出现次数第二多的字符</span></div><div class="line">                re_string += k1</div><div class="line">                heapq.heappush(h,(v,k))</div><div class="line">                <span class="keyword">if</span> v1 != <span class="number">-1</span>:</div><div class="line">                    heapq.heappush(h,(v1+<span class="number">1</span>,k))</div><div class="line">        <span class="keyword">return</span> re_string</div></pre></td></tr></table></figure></li></ol><h3 id="IPO"><a href="#IPO" class="headerlink" title="IPO"></a><a href="https://leetcode.com/problems/ipo/description/" target="_blank" rel="noopener">IPO</a></h3><ol><li><p>题目</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><p>Suppose LeetCode will start its IPO soon. In order to sell a good price of its shares to Venture Capital, LeetCode would like to work on some projects to increase its capital before the IPO. Since it has limited resources, it can only finish at most <strong>k</strong> distinct projects before the IPO. Help LeetCode design the best way to maximize its total capital after finishing at most <strong>k</strong> distinct projects.</p><p>You are given several projects. For each project <strong>i</strong>, it has a pure profit <strong>Pi</strong> and a minimum capital of <strong>Ci</strong> is needed to start the corresponding project. Initially, you have <strong>W</strong> capital. When you finish a project, you will obtain its pure profit and the profit will be added to your total capital.</p><p>To sum up, pick a list of at most <strong>k</strong> distinct projects from given projects to maximize your final capital, and output your final maximized capital.</p><p><strong>Example 1:</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Input: k=<span class="number">2</span>, W=<span class="number">0</span>, Profits=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], Capital=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>].</div><div class="line"></div><div class="line">Output: <span class="number">4</span></div><div class="line"></div><div class="line">Explanation: Since your initial capital is <span class="number">0</span>, you can only start the project indexed <span class="number">0</span>.</div><div class="line">             After finishing it you will obtain profit <span class="number">1</span> and your capital becomes <span class="number">1</span>.</div><div class="line">             With capital <span class="number">1</span>, you can either start the project indexed <span class="number">1</span> or the project indexed <span class="number">2</span>.</div><div class="line">             Since you can choose at most <span class="number">2</span> projects, you need to finish the project indexed <span class="number">2</span> to get the maximum capital.</div><div class="line">             Therefore, output the <span class="keyword">final</span> maximized capital, which is <span class="number">0</span> + <span class="number">1</span> + <span class="number">3</span> = <span class="number">4</span>.</div></pre></td></tr></table></figure></div></div></li><li><p>思路</p><p>大概题意是：你有启动资金$W$，并且你只能完成$k$个任务。现有一堆任务，每个任务需要一定的资金启动，一旦任务完成，你就会获得一定的利润，而且该利润可以添加到你的启动资金里。问最终能达到的最大启动资金？</p><p>比较直观，先选择任务的启动资金小于等于你当前的启动资金$W$，然后在可以做的任务里选择获利最大的任务去完成，使最大化你的启动资金$W$。</p><p>实现的时候，利用一个优先队列，里面存储的是可以做的任务的获利，扫描任务list，一旦发现任务的启动资金小于等于你的$W$，将该任务的获利送入优先队列。</p></li><li><p>代码</p><div><div class="fold_hider"><div class="close hider_title">点击显/隐内容</div></div><div class="fold"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> heapq</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMaximizedCapital</span><span class="params">(self, k, W, Profits, Capital)</span>:</span></div><div class="line">        sorted_capital = sorted(zip(Profits,Capital),key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</div><div class="line">        available_task = []</div><div class="line">        pos = <span class="number">0</span></div><div class="line">        <span class="keyword">while</span> k&gt;<span class="number">0</span>:</div><div class="line">            k -= <span class="number">1</span></div><div class="line">            <span class="keyword">while</span> pos&lt;len(sorted_capital) <span class="keyword">and</span> sorted_capital[pos][<span class="number">1</span>] &lt;= W:</div><div class="line">                heapq.heappush(available_task,-sorted_capital[pos][<span class="number">0</span>])</div><div class="line">                pos += <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> available_task:</div><div class="line">                W -= heapq.heappop(available_task)</div><div class="line">        <span class="keyword">return</span> W</div></pre></td></tr></table></figure></div></div></li></ol>]]></content>
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> LeetCode </tag>
            
            <tag> Greedy Algorithm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hexo Commands</title>
      <link href="/2018/05/23/Hexo-Commands/"/>
      <url>/2018/05/23/Hexo-Commands/</url>
      <content type="html"><![CDATA[<h1 id="Hexo-Basic-Commands"><a href="#Hexo-Basic-Commands" class="headerlink" title="Hexo Basic Commands"></a>Hexo Basic Commands</h1><h2 id="create-a-blog"><a href="#create-a-blog" class="headerlink" title="create a blog"></a>create a blog</h2><p>hexo n “name of blog” </p><h2 id="post-a-blog"><a href="#post-a-blog" class="headerlink" title="post a blog"></a>post a blog</h2><a id="more"></a><p>hexo  clean</p><p>hexo g</p><p>hexo d</p><p>hexo generate —debug</p><h2 id="online-markdown"><a href="#online-markdown" class="headerlink" title="online markdown"></a>online markdown</h2><p>hexo server -d</p><h2 id="article-folding"><a href="#article-folding" class="headerlink" title="article folding"></a>article folding</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;!-- more --&gt;</div></pre></td></tr></table></figure><h2 id="tags"><a href="#tags" class="headerlink" title="tags"></a>tags</h2><p>create a new pages firstly: <strong>hexo new page tags</strong></p><p>opne new page and set its type as “tages”</p><h2 id="category"><a href="#category" class="headerlink" title="category"></a>category</h2><p>create a new pages firstly: <strong>hexo new page categories</strong></p><p>opne new page and set its type as “categories”</p><h2 id="MathJax"><a href="#MathJax" class="headerlink" title="MathJax"></a>MathJax</h2><p><a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">[Insert mathematical formula in hexo articles]</a></p><h2 id="Pictures"><a href="#Pictures" class="headerlink" title="Pictures"></a>Pictures</h2><p><a href="https://blog.csdn.net/sugar_rainbow/article/details/57415705" target="_blank" rel="noopener">[Insert pics in hexo articles]</a></p><h2 id="内容折叠"><a href="#内容折叠" class="headerlink" title="内容折叠"></a>内容折叠</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;% fold 点击显/隐内容 %&#125;&#123;% endfold %&#125;</div><div class="line"></div><div class="line">xbdh</div></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> Website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Website </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
